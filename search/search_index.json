{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Introduction to Data Science Curriculum (version 7.0) Robert Gould Suyen Machado Terri Anna Johnson James Molyneux Sponsors & Supporters This curriculum was created under the auspices of the National Science Foundation, Mathematics and Science Partnership grant, \"MOBILIZE: Mobilizing for Innovative Computer Science Teaching and Learning\". Lead Principal Investigator: Robert Gould (UCLA, Statistics). Contributing Authors LAUSD: Monica Casillas and Heidi Estevez UCLA: Amelia McNamara and Linda Zanontian Acknowledgments and Special Thanks Co-Principal Investigators: Deborah Estrin (UCLA, CENS), Joanna Goode (University of Oregon), Mark Hansen (UCLA, Statistics), Jane Margolis (UCLA, Center X), Thomas Philip (UCLA, Center X), Jody Priselac (UCLA, GSEIS), Derrick Chau (LAUSD), Gerardo Loera (LAUSD) and Todd Ullah (LAUSD); Mobilize Project Director: LeeAnn Trusela LAUSD IDS Pilot Teachers Robert Montgomery, Carole Sailer, Joy Lee, Monica Casillas, Roberta Ross, Velia Valle, Jose Guzman, Pamela Amaya, Arlene Pascua, Christopher Marangopoulos This material is based upon work supported by the National Science Foundation under Grant Number 0962919. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the National Science Foundation. This work is licensed under the Creative Commons Attribution-ShareAlike 4.0 International License. To view a copy of this license, visit http://creativecommons.org/licenses/by-sa/4.0 For additional information related to IDS visit: https://www.idsucla.org Mobilize Mobilize, an innovative partnership between UCLA and LAUSD, was funded in 2010 by the National Science Foundation to develop barrier-breaking curriculum in science, mathematics, and computer science to teach students to think creatively, constructively, and critically about the role of data in science and in everyday life. The Mobilize curricula center around Participatory Sensing campaigns, through which students use their mobile devices to collect and share data about their community and their lives, and analyze these data to gain a greater understanding about their world. Mobilize broke barriers by teaching students to apply concepts and practices from computer science and statistics to learning science and mathematics, and it was uniquely dynamic in that each Mobilize class collects its own data, and each class has the opportunity to make unique discoveries. Across all Mobilize curricula, mobile devices are used not as gimmicks to capture students' attention, but as legitimate tools that bring scientific enquiry into their everyday lives. Since 2011, LAUSD high school mathematics, science, and computer science teachers have attended the summer institutes designed by the Mobilize grant to learn to use the participatory sensing (PS) methods, tools, and materials to deepen their knowledge of computer science (CS) concepts and to support student CS, math, and science learning. First implemented in 2014 under the auspices of the Mobilize grant, Introduction to Data Science (IDS) began as a pilot program with 10 LAUSD mathematics teachers, and by the 5 th printing of the curriculum in 2018 has expanded to 30+ schools in seven Southern California public school districts, serving over 4,000 students and counting. In addition to addressing the Common Core State Standards (CCSS) for High School Statistics and Probability IDS leads students to: understand how data are used by professionals to address real-world problems; understand that data are used in all facets of modern life; understand how data support science to identify and tackle real-world problems in our communities; analyze statistical graphics to identify patterns in data and to connect these patterns back to the real world; understand that by treating photos, words, numbers, and sounds as data, we can gain insight into the real world; learn to analyze data, including: posing questions that can be answered by considering relations among variables in a data set, using collected data to generate hypotheses for future data collection, critically evaluating shortcomings and strengths in the data and the data collection process, and informally evaluating hypotheses using data at hand.","title":"Home"},{"location":"#introduction-to-data-science-curriculum","text":"(version 7.0) Robert Gould Suyen Machado Terri Anna Johnson James Molyneux","title":"Introduction to Data Science Curriculum"},{"location":"#sponsors-supporters","text":"This curriculum was created under the auspices of the National Science Foundation, Mathematics and Science Partnership grant, \"MOBILIZE: Mobilizing for Innovative Computer Science Teaching and Learning\". Lead Principal Investigator: Robert Gould (UCLA, Statistics).","title":"Sponsors &amp; Supporters"},{"location":"#contributing-authors","text":"LAUSD: Monica Casillas and Heidi Estevez UCLA: Amelia McNamara and Linda Zanontian","title":"Contributing Authors"},{"location":"#acknowledgments-and-special-thanks","text":"Co-Principal Investigators: Deborah Estrin (UCLA, CENS), Joanna Goode (University of Oregon), Mark Hansen (UCLA, Statistics), Jane Margolis (UCLA, Center X), Thomas Philip (UCLA, Center X), Jody Priselac (UCLA, GSEIS), Derrick Chau (LAUSD), Gerardo Loera (LAUSD) and Todd Ullah (LAUSD); Mobilize Project Director: LeeAnn Trusela","title":"Acknowledgments and Special Thanks"},{"location":"#lausd-ids-pilot-teachers","text":"Robert Montgomery, Carole Sailer, Joy Lee, Monica Casillas, Roberta Ross, Velia Valle, Jose Guzman, Pamela Amaya, Arlene Pascua, Christopher Marangopoulos This material is based upon work supported by the National Science Foundation under Grant Number 0962919. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the National Science Foundation. This work is licensed under the Creative Commons Attribution-ShareAlike 4.0 International License. To view a copy of this license, visit http://creativecommons.org/licenses/by-sa/4.0 For additional information related to IDS visit: https://www.idsucla.org","title":"LAUSD IDS Pilot Teachers"},{"location":"#mobilize","text":"Mobilize, an innovative partnership between UCLA and LAUSD, was funded in 2010 by the National Science Foundation to develop barrier-breaking curriculum in science, mathematics, and computer science to teach students to think creatively, constructively, and critically about the role of data in science and in everyday life. The Mobilize curricula center around Participatory Sensing campaigns, through which students use their mobile devices to collect and share data about their community and their lives, and analyze these data to gain a greater understanding about their world. Mobilize broke barriers by teaching students to apply concepts and practices from computer science and statistics to learning science and mathematics, and it was uniquely dynamic in that each Mobilize class collects its own data, and each class has the opportunity to make unique discoveries. Across all Mobilize curricula, mobile devices are used not as gimmicks to capture students' attention, but as legitimate tools that bring scientific enquiry into their everyday lives. Since 2011, LAUSD high school mathematics, science, and computer science teachers have attended the summer institutes designed by the Mobilize grant to learn to use the participatory sensing (PS) methods, tools, and materials to deepen their knowledge of computer science (CS) concepts and to support student CS, math, and science learning. First implemented in 2014 under the auspices of the Mobilize grant, Introduction to Data Science (IDS) began as a pilot program with 10 LAUSD mathematics teachers, and by the 5 th printing of the curriculum in 2018 has expanded to 30+ schools in seven Southern California public school districts, serving over 4,000 students and counting. In addition to addressing the Common Core State Standards (CCSS) for High School Statistics and Probability IDS leads students to: understand how data are used by professionals to address real-world problems; understand that data are used in all facets of modern life; understand how data support science to identify and tackle real-world problems in our communities; analyze statistical graphics to identify patterns in data and to connect these patterns back to the real world; understand that by treating photos, words, numbers, and sounds as data, we can gain insight into the real world; learn to analyze data, including: posing questions that can be answered by considering relations among variables in a data set, using collected data to generate hypotheses for future data collection, critically evaluating shortcomings and strengths in the data and the data collection process, and informally evaluating hypotheses using data at hand.","title":"Mobilize"},{"location":"applications/","text":"Applications Real-time Data Collection App Collect data on one or two numerical variables from your class. No login required. Author: IDS Team Births Data Animation Author: James Molyneux Mean, Median, Mode Author: IDS Team Titanic Shuffle Author: IDS Team K-means Clustering Author: IDS Team Violin Plots Author: Albert Cao Palmer Penguins Authors: James Molyneux and Heidi Bullock .md-nav--primary .md-nav__link[for=__toc] ~ .md-nav { display: none !important; } .md-sidebar { display: none !important; } .md-header{ pointer-events: none; }","title":"Applications"},{"location":"applications/#applications","text":"Real-time Data Collection App Collect data on one or two numerical variables from your class. No login required. Author: IDS Team Births Data Animation Author: James Molyneux Mean, Median, Mode Author: IDS Team Titanic Shuffle Author: IDS Team K-means Clustering Author: IDS Team Violin Plots Author: Albert Cao Palmer Penguins Authors: James Molyneux and Heidi Bullock .md-nav--primary .md-nav__link[for=__toc] ~ .md-nav { display: none !important; } .md-sidebar { display: none !important; } .md-header{ pointer-events: none; }","title":"Applications"},{"location":"data/","text":"external data sets. cdc_2019 cdc_2019 extra_data('cdc_2019') cdc_2019.rda file atus atus_2020 extra_data('atus_2020') atus_2020.rda file retail retail extra_data('retail') retail.rda file census census extra_data('census') census.rda file food_ids food_ids extra_data('food_ids') food_ids.rda file timeuse_ids timeuse_ids extra_data('timeuse_ids') timeuse_ids.rda file colors_ids colors_ids extra_data('colors_ids') colors_ids.rda file time_exp time_exp extra_data('time_exp') time_exp.rda file c6_colors c6_colors extra_data('c6_colors') c6_colors.rda file c6_stress c6_stress extra_data('c6_stress') c6_stress.rda file c6_colors_stress c6_colors_stress extra_data('c6_colors_stress') c6_colors_stress.rda file babies babies extra_data('babies') babies.rda file hiring hiring extra_data('hiring') hiring.rda file","title":"Data"},{"location":"data/#cdc_2019","text":"cdc_2019 extra_data('cdc_2019') cdc_2019.rda file","title":"cdc_2019"},{"location":"data/#atus","text":"atus_2020 extra_data('atus_2020') atus_2020.rda file","title":"atus"},{"location":"data/#retail","text":"retail extra_data('retail') retail.rda file","title":"retail"},{"location":"data/#census","text":"census extra_data('census') census.rda file","title":"census"},{"location":"data/#food_ids","text":"food_ids extra_data('food_ids') food_ids.rda file","title":"food_ids"},{"location":"data/#timeuse_ids","text":"timeuse_ids extra_data('timeuse_ids') timeuse_ids.rda file","title":"timeuse_ids"},{"location":"data/#colors_ids","text":"colors_ids extra_data('colors_ids') colors_ids.rda file","title":"colors_ids"},{"location":"data/#time_exp","text":"time_exp extra_data('time_exp') time_exp.rda file","title":"time_exp"},{"location":"data/#c6_colors","text":"c6_colors extra_data('c6_colors') c6_colors.rda file","title":"c6_colors"},{"location":"data/#c6_stress","text":"c6_stress extra_data('c6_stress') c6_stress.rda file","title":"c6_stress"},{"location":"data/#c6_colors_stress","text":"c6_colors_stress extra_data('c6_colors_stress') c6_colors_stress.rda file","title":"c6_colors_stress"},{"location":"data/#babies","text":"babies extra_data('babies') babies.rda file","title":"babies"},{"location":"data/#hiring","text":"hiring extra_data('hiring') hiring.rda file","title":"hiring"},{"location":"overview/","text":"Introduction to Data Science: Overview & Philosophy Course Overview Goals Introduction to Data Science (IDS) is designed to introduce students to the exciting opportunities available at the intersection of data analysis, computing, and mathematics through hands-on activities. Data are everywhere, and this curriculum will help prepare students to live in a world of data. The curriculum focuses on practical applications of data analysis to give students concrete and applicable skills. Instead of using small, tailored, curated data sets as in a traditional statistics curriculum, this curriculum engages students with a wider world of data that fall into the \"Big Data\" paradigm and are relevant to students' lives. In contrast to the traditional formula-based approach, in IDS, statistical inference is taught algorithmically, using modern randomization and simulation techniques. Students will learn to find and communicate meaning in data, and to think critically about arguments based on data. This curriculum was developed in partnership with the Los Angeles Unified School District for a culturally, linguistically, and socially diverse group of students. Upon first publication of the IDS curriculum in 2015, the district-wide student ethnicities included .3% American Indian, 3.7% Asian, .4% Pacific Islander, 2.3% Filipino, 73.0% Latino, 10.9% African American, 8.8% White, and .6% other/multiple responses. Over 38% of students were English-language learners \u2013 most of whom spoke Spanish as their primary language \u2013 and 74% of students qualified for free or reduced lunches. Standards The standards used for the IDS curriculum are based on the High School Probability and Statistics Mathematics Common Core State Standards (CCSS-M) , and include the Standards for Mathematical Practice (SMP) . Specific standards are delineated in the scope and sequence section. The Computer Science Teacher\u2019s Association (CSTA) K-12 Computer Science Standards were also consulted and incorporated. Applied Computational Thinking Standards (ACT) delineate the application of Data Science concepts using technology. Hardware An ideal laboratory environment has a 1:1 computer to student ratio. The computers can be either Apple, PC, or Chromebook, depending upon availability. Internet access is required for the use of RStudio on an external server. The IDS instructor must have access to a computer and a projector for daily use. Software Each computer in the classroom should have a modern, updated web browser installed (such as Firefox or Google Chrome). This will allow students to access RStudio from an external server, and to perform searches and make use of a variety of websites and Internet tools. RStudio is available at https://portal.idsucla.org . The IDS team will provide the remainder of the software used in the IDS curriculum, available at https://portal.idsucla.org This software includes the IDS UCLA app , which is deployed for Android and iOS smartphones and tablets, as well as through the web browser on a desktop or laptop computer. The app allows students to collect the Participatory Sensing data that is a motivational foundation for the course. In addition to the app, students will use the IDS software to access and manipulate their Participatory Sensing data, and to author their own campaigns. All computer-based assignments will be completed in class to avoid the assumption that students have access to computers at home. However, if a student misses a lab assignment, they will need to make it up on their own time. All the software required for the curriculum is available via the Internet, so students can complete the assignment on any Internet-enabled computer (e.g., at the school or public library). Prerequisites It is required that students successfully complete a first-year Algebra course prior to taking IDS. With this background, the curriculum provides a rigorous but accessible introduction to data science and statistics. No previous statistics or computer science courses are required to take this course. The Instructional Philosophy of Introduction to Data Science IDS uses a project-based learning approach to instruction. Finkle and Torp (1955) define Project-Based Learning (PBL) as a curriculum development and instructional system that simultaneously develops both problem-solving strategies and disciplinary knowledge bases and skills by placing students in the active role of problem solvers confronted with an ill-structured problem that mirrors real-world problems. PBL, therefore, is a model for teaching and learning that focuses on the main concepts and principles of a discipline, involves students in problem-solving investigations and other meaningful tasks, allows students to construct their own knowledge through inquiry, and culminates in a project. Because IDS is a mathematical science, the BSCS 5-E Instructional Model provides a planned sequence of instruction that places students at the center of their learning experiences. This model encourages students to explore, create their own meaning of concepts, and relate their understanding to other concepts. The units in IDS contain lessons that, together, fit the 5-E Instructional Model: Stage of Inquiry in an Inquiry-Based Science Program Possible Student Behavior Possible Teacher Strategy Engage Asks questions such as, Why did this happen? What do I already know about this? What can I find out about this? How can I solve this problem? Shows interest in the topic. Creates interest. Generates curiosity. Raises questions and problems. Elicits responses that uncover student knowledge about the concept/topic. Explore Thinks creatively within the limits of the activity. Tests predictions and hypotheses. Forms new predictions and hypotheses. Tries alternatives to solve a problem and discusses them with others. Records observations and ideas. Suspends judgment. Tests ideas. Encourages students to work together without direct instruction from the teacher. Observes and listens to students as they interact. Asks probing questions to redirect students' investigations when necessary. Provides time for students to puzzle through problems. Acts as a consultant for students. Explain Explains their thinking, ideas, and possible solutions or answers to other students. Listens critically to other students' explanations. Questions other students' explanations. Listens to and tries to comprehend explanations offered by the teacher. Refers to previous activities. Uses recorded data in explanations. Encourages students to explain concepts and definitions in their own words. Asks for justification (evidence) and clarification from students. Formally provides definitions, explanations, and new vocabulary. Uses students' previous experiences as the basis for explaining concepts. Elaborate Applies scientific concepts, labels, definitions, explanations, and skills in new, but similar situations. Uses previous information to ask questions, propose solutions, make decisions, and design experiments. Draws reasonable conclusions from evidence. Records observations and explanations. Expects students to use vocabulary, definitions, and explanations provided previously in new context. Encourages students to apply the concepts and skills in new situations. Reminds students of alternative explanations. Refers students to alternative explanations. Evaluate Checks for understanding among peers. Answers open-ended questions by using observations, evidence, and previously accepted explanations. Demonstrates an understanding or knowledge of the concept or skill. Evaluates his or her own progress and knowledge. Asks related questions that would encourage future investigations. Refers students to existing data and evidence and asks, What do you know? Why do you think...? Observes students as they apply new concepts and skills. Assesses students' knowledge and/or skills. Looks for evidence that students have changed their thinking. Allows students to assess their learning and group process skills. Asks open-ended questions such as, Why do you think...? What evidence do you have? What do you know about the problem? How would you answer the question? IDS is designed to develop students' computational and statistical thinking skills. Computationally, students will learn to write code to enhance analyses of data, to break large problems into smaller pieces, and to understand and employ algorithms to solve problems. Statistical thinking skills include developing a data \"habit of mind\" in which one learns to seek data to answer questions or support (or undermine) claims; thinking critically about the ability of particular data to support claims; learning to interpret analyses of data; and learning to communicate findings. IDS employs Participatory Sensing to give students control of the data collection process, and to enable them to collect data about things that are important to them. The curriculum is organized around a series of Participatory Sensing \"campaigns\" in which students engage in all stages of the statistical process, which we call the Data Cycle: asking questions, examining and collecting data, analyzing data, interpreting data and, if necessary, beginning again. As students progress, they engage in the Data Cycle in a deeper way. Initially, analysis and interpretation is purely descriptive. Later, randomization-based algorithms and simulations are used to develop notions of inference and to make students more critical of the data collection process. By engaging in the Data Cycle repeatedly in different contexts - some of which include the students' own designs - students will learn to think like data scientists. Student Team Collaboration Many of the activities in the IDS curriculum are based on students collaborating with each other. Activities may call on pairs or teams of students. It is imperative that teams and team roles be established as close to the beginning of the course as possible. Expectations about teamwork should be introduced as soon as teams are formed. The ideal team comprises four students. The Teacher Resources section provides a list of instructional strategies and a description of team roles to use for effective student team collaboration. If student teams are unfamiliar with these instructional strategies, it is important for the instructor to take the time to model each strategy. Classroom Discussions Because this is an inquiry-based curriculum, classroom discussion will be especially important. It is important to set classroom discussion norms from the beginning of the course. All students should be encouraged to contribute to the classroom discussion, and the learning environment should be as non-judgmental and as open as possible. Instead of one right answer, most questions in this class have many right answers. In fact, even yes/no questions could have two right answers, both with valid supporting evidence. Teachers should create an environment to help students hold each other accountable so that all voices are heard, meaning that if there are a few students who tend to share a lot, invite them to encourage their peers so other voices can be heard. If there are students who tend to avoid contributing to the class discussion, encourage them to share so that their voices are heard. Assignments & Homework As much as possible, IDS work will take place in the classroom. Lessons are designed for a 50-60 minute class period. Classes on block schedule will need to complete two lessons; however, it is up to the teacher to decide where to stop in each lesson. There will be open-ended assignments that are sent home. Assignments that require the computer will be completed in class, to avoid the assumption that students have access to computers at home. The exception to this is if a student misses lab time, in which case they will need to find a time to complete the assignment outside of class. As discussed in the software section above, they can use an Internet-enabled computer to do their make-up work. IDS assignments will not be drill-based. Instead, they will follow the inquiry-based instructional model. Again, most questions will not have one right answer. Instead, students will learn to support their claims with evidence and to participate in data-based discussions. Newspaper or other periodical or digital articles are available via links in the lessons. If desired, articles may be downloaded and printed. On average, students will complete a lab assignment in RStudio approximately once per week. It will be at the discretion of the teacher whether or not to collect lab assignments. Calculators should be available every day for students to use. Every day, students will be expected to bring their Data Science (DS) journal, a notebook where they record their notes, work on small assignments, and sketch plots. Teachers may choose to check DS journals and other assignments in the curriculum for credit. End of Unit Projects, oral presentations, and Practicums are designed as application exercises. Scoring guides are provided as an aid for student performance expectations. It will be up to the teacher to score or attach a grade to these assignments. Overview of Instructional Topics The purpose of IDS is to introduce students to dynamic data analysis. The four major components of this curriculum are based on the conceptual categories called upon by the Common Core State Standards High School - Statistics and Probability: I. Interpreting Categorical and Quantitative Data II. Making Inferences and Justifying Conclusions III. Conditional Probability and the Rules of Probability IV. Using Probability to Make Decisions IDS will emphasize the use of statistics and computation as tools for creative work, and as a means of telling stories with data. Seen in this way, its content will also prepare students to \"read\" and think critically about existing data stories. Ultimately, this course will be about how we discern good stories from bad through a practice that involves compiling evidence from one or more sources, and which often requires hands-on examination of one or more data sets. IDS will develop the tools, techniques, and principles for reasoning about the world with data. It will present a process that is iterative and authentically inquiry-based, comparing multiple \"views\" of one or more data sets. Inevitably, these views are the result of some kind of computation, producing numerical summaries or graphical displays. Their interpretation relies on a special kind of computation known as simulation to describe the uncertainty in each view. This kind of reasoning is exploratory and investigatory, sometimes framed as hypothesis evaluation, and sometimes as hypothesis generation. Interpreting Categorical and Quantitative Data A handful of data interpretations are standard. Some, including summaries of shape, center, and spread of one or more variables in a data set - as well as graphical displays like histograms and scatterplots - are standard in the sense that they provide interpretable information in a number of research contexts. They are portable from one set of data to the next, and the rules for their use are simple. And yet, our interpretation of data is rarely \u201cstandard.\u201d Data have no natural look - even a spreadsheet or a table of numbers embeds within it a certain representational strategy. We construct multiple views of data in an attempt to uncover stories about the world. In addition to numerical data, this course will consider time, location, text, and image as data types, and will examine views that uncover patterns or stories. Throughout the course, simulation will be used to calibrate our interpretation of a view, or of a numerical or graphical summary, so that we understand what \u201cstory-less\u201d data (i.e., pure noise, no association) look like. In addition to summaries and simple graphics, students will engage in a modeling practice aligned with the CCSS mathematical practices in order to learn how statistical analyses can explain and describe real-world phenomena. Students will practice fitting and evaluating standard mathematical and statistical models, such as the least-squares regression line. Modeling comes into play when students are asked to design and implement probabilistic simulations in order to test and compare hypothetical chance processes to real-world data. Making Inferences and Justifying Conclusions Data are becoming increasingly plentiful, supported by a host of new \"publication\" techniques or services. Post-Web 2.0, data are interoperable, flowing out of one service and into another, helping us easily build a detailed data version of many phenomena in the world. Reasoning with data, then, starts with the sources and the mechanics of this flow. Which sources do we trust? How do data from different organizations compare? What stories have been told previously with these data, and by whom? This course answers these questions, in part, by using the tools and techniques already mentioned. The ability to read and critique published stories and visualizations are additions to these tools and techniques. Finally, as an act of comparison, students should also be able to formulate questions, identify existing data sets, and evaluate how the new stories stack up against the old. To support this cycle of inquiry, students will examine the basic publication mechanisms for data and develop a set of questions to ask of any data source - computation meets critical thinking. In some cases, data will exhibit special structures that can be used to aid in inference. The simulation techniques for calibrating different views of a data set take on new life when some form of random process was followed to generate the data. Polls, for example, rely on random samples of the population, and clinical trials randomly assign patients to treatment and control groups. A simulation strategy that repeats these random mechanisms can be used to assess uncertainty in the data, assigning a margin of error to poll results, or identifying new drugs that have a \"significant\" effect on some health outcome. In many cases, data will not possess this kind of special origin story. A census, for example, is meant to be a complete enumeration of a population, and we can reason in a very direct way from the data. In other cases, no formal principle was applied, perhaps being a sample \"of convenience.\" The techniques for telling stories from these kinds of data will also rely on a mix of simulation and subsetting. Finally, this course will introduce Participatory Sensing as a technique for collecting data. The idea of a data collection campaign will be introduced as a means of formalizing a question to be addressed with data. Campaigns will be informed by research and data analysis, and will build on, augment, or challenge existing sources. The \"culture\" behind the existing sources and the summaries or views they promote will be part of the classroom discussions. It is worth noting that everything described so far depends on computation, using a piece of statistical software on a computer. Students will be taught simple programming tools for accessing data, creating views or fitting models, and then assessing their importance via simulation. Computation becomes a medium through which students learn about data. The more expressive the language, the more elaborate the stories we can tell. Probability Since simulation is our main tool for reasoning with data, interpreting the output of simulations requires understanding some basic rules of probability. First and foremost, this course will discuss the ways in which a computer can generate random phenomena (e.g., How does a computer toss a coin?). Simple probability calculations will be used to describe what we expect to see from random phenomena, then students will compare their results to simulations. The point is to both rehearse these basic calculations and to make a formal tie between simulation and theory in simple cases. In that vein, this course will motivate the relationship between frequency and probability. Students will essentially be simulating independent trials and creating summaries of those simulations. In turn, they should understand that the frequency with which an event occurs in a series of independent simulations tends to the probability for that event as the number of simulations gets large (the Law of Large Numbers, a topic that is often taught in introductory statistics courses). From here, students will simulate a variety of random processes to aid in formal statistical inference when some random mechanism was applied as part of the data design. In short, probability becomes a ruler of sorts for assessing the importance of any story we might tell. In this approach to probability, a combination of direct mathematical calculation and computer simulations will be used in order to give students a deep sense of the underlying statistical concepts. Topic Outline This outline describes only the scope of the course; the sequence is described in each unit. I. Interpreting Data A. Types of data B. Numerical and graphical summaries Measures of center and spread, boxplots Bar plots Histograms Scatterplots Graphical summaries of multivariate data C. Simulation and visual inference Side-by-side bar plots and association Scatterplots D. Models Linear models k-means Smoothing Learning and tree-based models II. Making Inferences and Justifying Conclusions A. Aggregating data Identification of sources Mechanics of Web 2.0 Comparison of sources B. Data with special structures Random sampling Random assignment and A/B testing Simulation-based inference C. Participatory Sensing Designing a campaign Participation as a data collection strategy III. Probability A. Computers and randomness Web services Pseudo-random numbers (optional) B. Frequency and probability C. Probability calculations IV. Algebra in RStudio Vectors Algorithms Functions Evaluating and fitting models to data Graphical representations of multivariate data Numerical summaries of distributions and interpreting in context","title":"Overview & Philosophy"},{"location":"overview/#introduction-to-data-science-overview-philosophy","text":"","title":"Introduction to Data Science: Overview &amp; Philosophy"},{"location":"overview/#course-overview","text":"","title":"Course Overview"},{"location":"overview/#goals","text":"Introduction to Data Science (IDS) is designed to introduce students to the exciting opportunities available at the intersection of data analysis, computing, and mathematics through hands-on activities. Data are everywhere, and this curriculum will help prepare students to live in a world of data. The curriculum focuses on practical applications of data analysis to give students concrete and applicable skills. Instead of using small, tailored, curated data sets as in a traditional statistics curriculum, this curriculum engages students with a wider world of data that fall into the \"Big Data\" paradigm and are relevant to students' lives. In contrast to the traditional formula-based approach, in IDS, statistical inference is taught algorithmically, using modern randomization and simulation techniques. Students will learn to find and communicate meaning in data, and to think critically about arguments based on data. This curriculum was developed in partnership with the Los Angeles Unified School District for a culturally, linguistically, and socially diverse group of students. Upon first publication of the IDS curriculum in 2015, the district-wide student ethnicities included .3% American Indian, 3.7% Asian, .4% Pacific Islander, 2.3% Filipino, 73.0% Latino, 10.9% African American, 8.8% White, and .6% other/multiple responses. Over 38% of students were English-language learners \u2013 most of whom spoke Spanish as their primary language \u2013 and 74% of students qualified for free or reduced lunches.","title":"Goals"},{"location":"overview/#standards","text":"The standards used for the IDS curriculum are based on the High School Probability and Statistics Mathematics Common Core State Standards (CCSS-M) , and include the Standards for Mathematical Practice (SMP) . Specific standards are delineated in the scope and sequence section. The Computer Science Teacher\u2019s Association (CSTA) K-12 Computer Science Standards were also consulted and incorporated. Applied Computational Thinking Standards (ACT) delineate the application of Data Science concepts using technology.","title":"Standards"},{"location":"overview/#hardware","text":"An ideal laboratory environment has a 1:1 computer to student ratio. The computers can be either Apple, PC, or Chromebook, depending upon availability. Internet access is required for the use of RStudio on an external server. The IDS instructor must have access to a computer and a projector for daily use.","title":"Hardware"},{"location":"overview/#software","text":"Each computer in the classroom should have a modern, updated web browser installed (such as Firefox or Google Chrome). This will allow students to access RStudio from an external server, and to perform searches and make use of a variety of websites and Internet tools. RStudio is available at https://portal.idsucla.org . The IDS team will provide the remainder of the software used in the IDS curriculum, available at https://portal.idsucla.org This software includes the IDS UCLA app , which is deployed for Android and iOS smartphones and tablets, as well as through the web browser on a desktop or laptop computer. The app allows students to collect the Participatory Sensing data that is a motivational foundation for the course. In addition to the app, students will use the IDS software to access and manipulate their Participatory Sensing data, and to author their own campaigns. All computer-based assignments will be completed in class to avoid the assumption that students have access to computers at home. However, if a student misses a lab assignment, they will need to make it up on their own time. All the software required for the curriculum is available via the Internet, so students can complete the assignment on any Internet-enabled computer (e.g., at the school or public library).","title":"Software"},{"location":"overview/#prerequisites","text":"It is required that students successfully complete a first-year Algebra course prior to taking IDS. With this background, the curriculum provides a rigorous but accessible introduction to data science and statistics. No previous statistics or computer science courses are required to take this course.","title":"Prerequisites"},{"location":"overview/#the-instructional-philosophy-of-introduction-to-data-science","text":"IDS uses a project-based learning approach to instruction. Finkle and Torp (1955) define Project-Based Learning (PBL) as a curriculum development and instructional system that simultaneously develops both problem-solving strategies and disciplinary knowledge bases and skills by placing students in the active role of problem solvers confronted with an ill-structured problem that mirrors real-world problems. PBL, therefore, is a model for teaching and learning that focuses on the main concepts and principles of a discipline, involves students in problem-solving investigations and other meaningful tasks, allows students to construct their own knowledge through inquiry, and culminates in a project. Because IDS is a mathematical science, the BSCS 5-E Instructional Model provides a planned sequence of instruction that places students at the center of their learning experiences. This model encourages students to explore, create their own meaning of concepts, and relate their understanding to other concepts. The units in IDS contain lessons that, together, fit the 5-E Instructional Model: Stage of Inquiry in an Inquiry-Based Science Program Possible Student Behavior Possible Teacher Strategy Engage Asks questions such as, Why did this happen? What do I already know about this? What can I find out about this? How can I solve this problem? Shows interest in the topic. Creates interest. Generates curiosity. Raises questions and problems. Elicits responses that uncover student knowledge about the concept/topic. Explore Thinks creatively within the limits of the activity. Tests predictions and hypotheses. Forms new predictions and hypotheses. Tries alternatives to solve a problem and discusses them with others. Records observations and ideas. Suspends judgment. Tests ideas. Encourages students to work together without direct instruction from the teacher. Observes and listens to students as they interact. Asks probing questions to redirect students' investigations when necessary. Provides time for students to puzzle through problems. Acts as a consultant for students. Explain Explains their thinking, ideas, and possible solutions or answers to other students. Listens critically to other students' explanations. Questions other students' explanations. Listens to and tries to comprehend explanations offered by the teacher. Refers to previous activities. Uses recorded data in explanations. Encourages students to explain concepts and definitions in their own words. Asks for justification (evidence) and clarification from students. Formally provides definitions, explanations, and new vocabulary. Uses students' previous experiences as the basis for explaining concepts. Elaborate Applies scientific concepts, labels, definitions, explanations, and skills in new, but similar situations. Uses previous information to ask questions, propose solutions, make decisions, and design experiments. Draws reasonable conclusions from evidence. Records observations and explanations. Expects students to use vocabulary, definitions, and explanations provided previously in new context. Encourages students to apply the concepts and skills in new situations. Reminds students of alternative explanations. Refers students to alternative explanations. Evaluate Checks for understanding among peers. Answers open-ended questions by using observations, evidence, and previously accepted explanations. Demonstrates an understanding or knowledge of the concept or skill. Evaluates his or her own progress and knowledge. Asks related questions that would encourage future investigations. Refers students to existing data and evidence and asks, What do you know? Why do you think...? Observes students as they apply new concepts and skills. Assesses students' knowledge and/or skills. Looks for evidence that students have changed their thinking. Allows students to assess their learning and group process skills. Asks open-ended questions such as, Why do you think...? What evidence do you have? What do you know about the problem? How would you answer the question? IDS is designed to develop students' computational and statistical thinking skills. Computationally, students will learn to write code to enhance analyses of data, to break large problems into smaller pieces, and to understand and employ algorithms to solve problems. Statistical thinking skills include developing a data \"habit of mind\" in which one learns to seek data to answer questions or support (or undermine) claims; thinking critically about the ability of particular data to support claims; learning to interpret analyses of data; and learning to communicate findings. IDS employs Participatory Sensing to give students control of the data collection process, and to enable them to collect data about things that are important to them. The curriculum is organized around a series of Participatory Sensing \"campaigns\" in which students engage in all stages of the statistical process, which we call the Data Cycle: asking questions, examining and collecting data, analyzing data, interpreting data and, if necessary, beginning again. As students progress, they engage in the Data Cycle in a deeper way. Initially, analysis and interpretation is purely descriptive. Later, randomization-based algorithms and simulations are used to develop notions of inference and to make students more critical of the data collection process. By engaging in the Data Cycle repeatedly in different contexts - some of which include the students' own designs - students will learn to think like data scientists.","title":"The Instructional Philosophy of Introduction to Data Science"},{"location":"overview/#student-team-collaboration","text":"Many of the activities in the IDS curriculum are based on students collaborating with each other. Activities may call on pairs or teams of students. It is imperative that teams and team roles be established as close to the beginning of the course as possible. Expectations about teamwork should be introduced as soon as teams are formed. The ideal team comprises four students. The Teacher Resources section provides a list of instructional strategies and a description of team roles to use for effective student team collaboration. If student teams are unfamiliar with these instructional strategies, it is important for the instructor to take the time to model each strategy.","title":"Student Team Collaboration"},{"location":"overview/#classroom-discussions","text":"Because this is an inquiry-based curriculum, classroom discussion will be especially important. It is important to set classroom discussion norms from the beginning of the course. All students should be encouraged to contribute to the classroom discussion, and the learning environment should be as non-judgmental and as open as possible. Instead of one right answer, most questions in this class have many right answers. In fact, even yes/no questions could have two right answers, both with valid supporting evidence. Teachers should create an environment to help students hold each other accountable so that all voices are heard, meaning that if there are a few students who tend to share a lot, invite them to encourage their peers so other voices can be heard. If there are students who tend to avoid contributing to the class discussion, encourage them to share so that their voices are heard.","title":"Classroom Discussions"},{"location":"overview/#assignments-homework","text":"As much as possible, IDS work will take place in the classroom. Lessons are designed for a 50-60 minute class period. Classes on block schedule will need to complete two lessons; however, it is up to the teacher to decide where to stop in each lesson. There will be open-ended assignments that are sent home. Assignments that require the computer will be completed in class, to avoid the assumption that students have access to computers at home. The exception to this is if a student misses lab time, in which case they will need to find a time to complete the assignment outside of class. As discussed in the software section above, they can use an Internet-enabled computer to do their make-up work. IDS assignments will not be drill-based. Instead, they will follow the inquiry-based instructional model. Again, most questions will not have one right answer. Instead, students will learn to support their claims with evidence and to participate in data-based discussions. Newspaper or other periodical or digital articles are available via links in the lessons. If desired, articles may be downloaded and printed. On average, students will complete a lab assignment in RStudio approximately once per week. It will be at the discretion of the teacher whether or not to collect lab assignments. Calculators should be available every day for students to use. Every day, students will be expected to bring their Data Science (DS) journal, a notebook where they record their notes, work on small assignments, and sketch plots. Teachers may choose to check DS journals and other assignments in the curriculum for credit. End of Unit Projects, oral presentations, and Practicums are designed as application exercises. Scoring guides are provided as an aid for student performance expectations. It will be up to the teacher to score or attach a grade to these assignments.","title":"Assignments &amp; Homework"},{"location":"overview/#overview-of-instructional-topics","text":"The purpose of IDS is to introduce students to dynamic data analysis. The four major components of this curriculum are based on the conceptual categories called upon by the Common Core State Standards High School - Statistics and Probability: I. Interpreting Categorical and Quantitative Data II. Making Inferences and Justifying Conclusions III. Conditional Probability and the Rules of Probability IV. Using Probability to Make Decisions IDS will emphasize the use of statistics and computation as tools for creative work, and as a means of telling stories with data. Seen in this way, its content will also prepare students to \"read\" and think critically about existing data stories. Ultimately, this course will be about how we discern good stories from bad through a practice that involves compiling evidence from one or more sources, and which often requires hands-on examination of one or more data sets. IDS will develop the tools, techniques, and principles for reasoning about the world with data. It will present a process that is iterative and authentically inquiry-based, comparing multiple \"views\" of one or more data sets. Inevitably, these views are the result of some kind of computation, producing numerical summaries or graphical displays. Their interpretation relies on a special kind of computation known as simulation to describe the uncertainty in each view. This kind of reasoning is exploratory and investigatory, sometimes framed as hypothesis evaluation, and sometimes as hypothesis generation.","title":"Overview of Instructional Topics"},{"location":"overview/#interpreting-categorical-and-quantitative-data","text":"A handful of data interpretations are standard. Some, including summaries of shape, center, and spread of one or more variables in a data set - as well as graphical displays like histograms and scatterplots - are standard in the sense that they provide interpretable information in a number of research contexts. They are portable from one set of data to the next, and the rules for their use are simple. And yet, our interpretation of data is rarely \u201cstandard.\u201d Data have no natural look - even a spreadsheet or a table of numbers embeds within it a certain representational strategy. We construct multiple views of data in an attempt to uncover stories about the world. In addition to numerical data, this course will consider time, location, text, and image as data types, and will examine views that uncover patterns or stories. Throughout the course, simulation will be used to calibrate our interpretation of a view, or of a numerical or graphical summary, so that we understand what \u201cstory-less\u201d data (i.e., pure noise, no association) look like. In addition to summaries and simple graphics, students will engage in a modeling practice aligned with the CCSS mathematical practices in order to learn how statistical analyses can explain and describe real-world phenomena. Students will practice fitting and evaluating standard mathematical and statistical models, such as the least-squares regression line. Modeling comes into play when students are asked to design and implement probabilistic simulations in order to test and compare hypothetical chance processes to real-world data.","title":"Interpreting Categorical and Quantitative Data"},{"location":"overview/#making-inferences-and-justifying-conclusions","text":"Data are becoming increasingly plentiful, supported by a host of new \"publication\" techniques or services. Post-Web 2.0, data are interoperable, flowing out of one service and into another, helping us easily build a detailed data version of many phenomena in the world. Reasoning with data, then, starts with the sources and the mechanics of this flow. Which sources do we trust? How do data from different organizations compare? What stories have been told previously with these data, and by whom? This course answers these questions, in part, by using the tools and techniques already mentioned. The ability to read and critique published stories and visualizations are additions to these tools and techniques. Finally, as an act of comparison, students should also be able to formulate questions, identify existing data sets, and evaluate how the new stories stack up against the old. To support this cycle of inquiry, students will examine the basic publication mechanisms for data and develop a set of questions to ask of any data source - computation meets critical thinking. In some cases, data will exhibit special structures that can be used to aid in inference. The simulation techniques for calibrating different views of a data set take on new life when some form of random process was followed to generate the data. Polls, for example, rely on random samples of the population, and clinical trials randomly assign patients to treatment and control groups. A simulation strategy that repeats these random mechanisms can be used to assess uncertainty in the data, assigning a margin of error to poll results, or identifying new drugs that have a \"significant\" effect on some health outcome. In many cases, data will not possess this kind of special origin story. A census, for example, is meant to be a complete enumeration of a population, and we can reason in a very direct way from the data. In other cases, no formal principle was applied, perhaps being a sample \"of convenience.\" The techniques for telling stories from these kinds of data will also rely on a mix of simulation and subsetting. Finally, this course will introduce Participatory Sensing as a technique for collecting data. The idea of a data collection campaign will be introduced as a means of formalizing a question to be addressed with data. Campaigns will be informed by research and data analysis, and will build on, augment, or challenge existing sources. The \"culture\" behind the existing sources and the summaries or views they promote will be part of the classroom discussions. It is worth noting that everything described so far depends on computation, using a piece of statistical software on a computer. Students will be taught simple programming tools for accessing data, creating views or fitting models, and then assessing their importance via simulation. Computation becomes a medium through which students learn about data. The more expressive the language, the more elaborate the stories we can tell.","title":"Making Inferences and Justifying Conclusions"},{"location":"overview/#probability","text":"Since simulation is our main tool for reasoning with data, interpreting the output of simulations requires understanding some basic rules of probability. First and foremost, this course will discuss the ways in which a computer can generate random phenomena (e.g., How does a computer toss a coin?). Simple probability calculations will be used to describe what we expect to see from random phenomena, then students will compare their results to simulations. The point is to both rehearse these basic calculations and to make a formal tie between simulation and theory in simple cases. In that vein, this course will motivate the relationship between frequency and probability. Students will essentially be simulating independent trials and creating summaries of those simulations. In turn, they should understand that the frequency with which an event occurs in a series of independent simulations tends to the probability for that event as the number of simulations gets large (the Law of Large Numbers, a topic that is often taught in introductory statistics courses). From here, students will simulate a variety of random processes to aid in formal statistical inference when some random mechanism was applied as part of the data design. In short, probability becomes a ruler of sorts for assessing the importance of any story we might tell. In this approach to probability, a combination of direct mathematical calculation and computer simulations will be used in order to give students a deep sense of the underlying statistical concepts.","title":"Probability"},{"location":"overview/#topic-outline","text":"This outline describes only the scope of the course; the sequence is described in each unit.","title":"Topic Outline"},{"location":"overview/#i-interpreting-data","text":"A. Types of data B. Numerical and graphical summaries Measures of center and spread, boxplots Bar plots Histograms Scatterplots Graphical summaries of multivariate data C. Simulation and visual inference Side-by-side bar plots and association Scatterplots D. Models Linear models k-means Smoothing Learning and tree-based models","title":"I. Interpreting Data"},{"location":"overview/#ii-making-inferences-and-justifying-conclusions","text":"A. Aggregating data Identification of sources Mechanics of Web 2.0 Comparison of sources B. Data with special structures Random sampling Random assignment and A/B testing Simulation-based inference C. Participatory Sensing Designing a campaign Participation as a data collection strategy","title":"II. Making Inferences and Justifying Conclusions"},{"location":"overview/#iii-probability","text":"A. Computers and randomness Web services Pseudo-random numbers (optional) B. Frequency and probability C. Probability calculations","title":"III. Probability"},{"location":"overview/#iv-algebra-in-rstudio","text":"Vectors Algorithms Functions Evaluating and fitting models to data Graphical representations of multivariate data Numerical summaries of distributions and interpreting in context","title":"IV. Algebra in RStudio"},{"location":"scope/","text":"Scope and Sequence Unit 1 This unit will introduce the idea of \u201cdata,\u201d fundamental to the rest of the course. While most people think of data simply as a spreadsheet or a table of numbers, almost anything can be considered data, including images, text, GPS coordinates, and much more. Our world has become increasingly data-centric, and we are constantly generating data, whether we know it or not. From posts on Facebook, to shopping records created when you swipe your credit card, to driving over sensors embedded in highway on-ramps, we leave behind a stream of data wherever we go. These data are used to generate stories about our world, whether it is for political forecasting, marketing, scientific research, or even Netflix recommendations. Traditional statistics courses consist of understanding data from only a small subset of data generation processes, namely those collected through random sampling or random assignment in scientific experiments. This unit exposes students to a wider world of data, and will help students see how to make sense of these ubiquitous data types. This unit will motivate the idea that data and data products (charts, graphs, statistics) can be analyzed and evaluated just like other arguments, such as those used by journalists. We want to know how the evidence was collected, what the perspective or bias of the creator might be, and look behind the scenes to the process used to create the product. Even the way data are represented embeds within it decisions on the part of the data creator. Using the techniques of descriptive statistics, students will begin learning how to construct multiple views of data in an attempt to uncover new insights about the world. This will require the introduction of the computational tool R through the interface of RStudio. Standard graphical displays like histograms and scatterplots will be introduced in RStudio, as well as measures of center and spread. Focus Statistics CCSS-M S-ID 1. Represent data with plots on the real number line (dotplots, histograms, and boxplots). S-ID 2: Use statistics appropriate to the shape of the data distribution to compare center (median, mean) of two or more different data sets (measures of spread will be studied in Unit 2). S-ID 3: Interpret differences in shape, center, and spread in the context of the data sets, accounting for possible effects of extreme data points (outliers). S-ID 5. Summarize categorical data for two categories in two-way frequency tables. Interpret relative frequencies in the context of the data (joint, marginal, and conditional relative frequencies). Recognize possible associations and trends in the data. S-ID 6. Represent data on two quantitative variables on a scatterplot, and describe how the variables are related. S-IC 6. Evaluate reports based on data.* *This standard is woven throughout the course. It is a recurring standard for every unit. Focus Standards for Mathematical Practices SMP-3. Construct viable arguments and critique the reasoning of others. SMP-5. Use appropriate tools strategically. Upon completion of Unit 1, students will be able to: Give examples of where they leave data traces. Understand that rows and columns are a form of data structure. Explain why the relationship between the variables might exist, or, if there is no relationship, why that might be so. Construct and interpret a frequency table. Critically read reports from media sources to evaluate their claims. Read plots (identify the name of the plot, interpret the axes, look for trends, identify confounding factors). Calculate conditional and marginal probabilities using frequency tables. Provide a real-world explanation for why the conditional or independent probabilities make sense, using critical thinking skills and background knowledge. Communicate their evaluations in written or verbal form using different types of media. Load data into RStudio. Create basic plots in RStudio. Create frequency tables in RStudio. Unit 2 This unit deepens the informal reasoning skills developed in Unit 1 by enriching students' technical vocabulary and developing more precise analytical tools. Most importantly, this unit introduces the formal concept of probability as a tool for understanding that sometimes patterns observed in data are not \"real.\" Traditional courses attempt to develop this understanding through the development of abstract mathematical probability concepts, but IDS creates enduring understanding by teaching students to design and implement simulations using pseudo-random number generators. This activity also develops computational thinking by teaching students about some basic programming structures. Then, the use of models will come to the foreground. Students will be introduced to linear models - the most common form of modeling in introductory statistics classes - which will serve as the foundation to learn more complex modeling techniques that use the computer technology available to them later in the course, including smoothing techniques and tree-based models. Focus Statistics CCSS-M S-ID 2: Use statistics appropriate to the shape of the data distribution to compare center (median, mean) and spread (interquartile range, standard deviation) of two or more different data sets. S-ID 3: Interpret differences in shape, center, and spread in the context of the data sets, accounting for possible effects of extreme data points (outliers). S-ID 4. Use the mean and standard deviation of a data set to fit it to a normal distribution and to estimate population percentages. Understand that there are data sets for which such a procedure is not appropriate. Use calculators, spreadsheets, and tables to estimate areas under the normal curve. S-IC 2. Decide if a specified model is consistent with results from a given data-generating process, e.g., using simulation. S-IC 6. Evaluate reports based on data.* *This standard is woven throughout the course. It is a recurring standard for every unit. S-CP 2. Understand that two events A and B are independent if the probability of A and B occurring together is the product of their probabilities, and use this characterization to determine if they are independent. S-CP 9. (+) Use permutations to perform [informal] inference. *This standard will be addressed in the context of data science. Focus SMPs SMP-4. Model with mathematics. SMP-5. Use appropriate tools strategically. Upon completion of Unit 2, students will be able to: Create a boxplot by calculating the five-number summary, upper and lower fences, and determining outliers. Explain what \u201cstandard deviation\u201d means in context. Explain why the measures of central tendency and spread may or may not be accurate descriptions of the data from which they came. Use permutations of data to solve problems. Read/interpret a normal curve/distribution. Explain where the normal distribution came from. Describe situations where the normal distribution may model the phenomena, and others where it may not. Simulate normal distribution. Simulate from a model. Compare real data to simulation. Determine if model and data appear consistent. Merge data by columns/rows, and verify that merging is successful. Learn for() loops and apply() functions in RStudio. Create functions. Unit 3 Unit 3 focuses on data collection methods, including traditional methods of designed experiments and observational studies and surveys. It introduces students to sampling error and bias, which cause problems in analysis made from survey data. Participatory Sensing is presented as another method of data collection, and students learn to design Participatory Sensing campaigns that will allow them to address particular statistical questions. Participatory Sensing is a unique data collection method because it uses sensors. Furthermore, this method emphasizes the involvement of citizens and community groups in the process of sensing and documenting where they live, work, and play. Triggers play an important role in the Participatory Sensing data collection process. The response to the triggers may or may not be the same each time. Data takes on a variety of forms online and requires a different style of representation. Students enhance computing skills by learning about modern data structures, and by learning to \"scrape\" data stored in XML format. Focus Statistics CCSS-M S-IC 1. Understand statistics as a process for making inferences about population parameters based on a random sample from that population. S-IC 3. Recognize the purposes of and differences among sample surveys, experiments, and observational studies; explain how randomization relates to each. S-IC 6. Evaluate reports based on data.* *This standard is woven throughout the course. It is a recurring standard for every unit. Focus SMPs SMP-1. Make sense of problems and persevere in solving them. SMP-4. Model with mathematics. SMP-8. Look for and express regularity in repeated reasoning. Upon completion of Unit 3, students will be able to: Provide a loose definition of \u201cstatistics\u201d in their own words. Compare and contrast population vs. sample. Compare and contrast parameter vs. statistic. Explain the difference between special data structures, particularly as they relate to inference. Exploit special data structures for re-randomization analysis. Explain situations where one measure of central tendency or spread may be more appropriate than others. Read/interpret boxplots (In-depth look into samples size and their relationship to the population parameters). Identify reports that use special data structures (census, survey, observational study, and randomized experiment). Do data scraping. Use HTML and XML formats. Use RStudio to re-randomize data. Compute measures of central tendency and spread in RStudio. Unit 4 This unit will develop modeling skills, beginning with learning to fit and interpret least squares regression lines and learning to use regression to make predictions. Students will learn to evaluate the success of these predictions and so compare models for their predictive accuracy. Modern algorithmic approaches to regression are presented, and students will strengthen algorithmic thinking skills by understanding how and why these algorithms help data scientists make accurate predictions from data. Students engage in a complete modeling experience in which they apply the skills and concepts learned in the previous units. The modeling experience is designed to make students\u2019 thinking visible and audible by encouraging them to be metacognitive about the process of inventing and testing a model, ask questions as they go through the process, and recognize the iterative nature of modeling. Focus Statistics Standards S-IC 2. Decide if a specified model is consistent with results from a given data-generating process, e.g., using simulation. S-ID 6. Represent data on two quantitative variables on a scatter plot, and describe how the variables are related. a. Fit a function to the data; use functions fitted to data to solve problems in the context of the data. Use given functions or choose a function suggested by the context. Emphasize linear models. b. Informally assess the fit of a function by plotting and analyzing residuals. c. Fit a linear function for a scatter plot that suggests a linear association. S-ID 7. Interpret the slope (rate of change) and the intercept (constant term) of a linear model in the context of the data. S-ID 8. Compute (using technology) and interpret the correlation coefficient of a linear fit. S-IC 6. Evaluate reports based on data.* *This standard is woven throughout the course. It is a recurring standard for every unit. Focus SMPs SMP-2. Reason abstractly and quantitatively. SMP-4. Model with mathematics. SMP-7. Look for and make use of structure. Upon completion of Unit 4, students will be able to: Describe how well the linear model fits the data (or does not). Provide a real-world explanation of why the model may or may not fit, using critical thinking skills and background knowledge. Interpret the slope and intercept on a plot. Compute the correlation coefficient using RStudio. Interpret linear models in reports, including the correlation coefficient. Determine if a trend is \u201creal\u201d or if it could have arisen from randomness. Use critical thinking skills to explain why a trend may or may not make sense. Fit a regression line. Extract the slope, intercept, correlation coefficient, coefficient of determination, and residuals using RStudio. Use RStudio to predict y given an x value. Explore what happens to the line and the response variable if we multiply (divide) or add (subtract) a constant from the predictor. Design and execute their own Participatory Sensing Campaigns. Use RStudio to compute permutations and combinations. Create Classification and Regression Tree (CART) models. Understand non-linear models.","title":"Scope and Sequence"},{"location":"scope/#scope-and-sequence","text":"","title":"Scope and Sequence"},{"location":"scope/#unit-1","text":"This unit will introduce the idea of \u201cdata,\u201d fundamental to the rest of the course. While most people think of data simply as a spreadsheet or a table of numbers, almost anything can be considered data, including images, text, GPS coordinates, and much more. Our world has become increasingly data-centric, and we are constantly generating data, whether we know it or not. From posts on Facebook, to shopping records created when you swipe your credit card, to driving over sensors embedded in highway on-ramps, we leave behind a stream of data wherever we go. These data are used to generate stories about our world, whether it is for political forecasting, marketing, scientific research, or even Netflix recommendations. Traditional statistics courses consist of understanding data from only a small subset of data generation processes, namely those collected through random sampling or random assignment in scientific experiments. This unit exposes students to a wider world of data, and will help students see how to make sense of these ubiquitous data types. This unit will motivate the idea that data and data products (charts, graphs, statistics) can be analyzed and evaluated just like other arguments, such as those used by journalists. We want to know how the evidence was collected, what the perspective or bias of the creator might be, and look behind the scenes to the process used to create the product. Even the way data are represented embeds within it decisions on the part of the data creator. Using the techniques of descriptive statistics, students will begin learning how to construct multiple views of data in an attempt to uncover new insights about the world. This will require the introduction of the computational tool R through the interface of RStudio. Standard graphical displays like histograms and scatterplots will be introduced in RStudio, as well as measures of center and spread.","title":"Unit 1"},{"location":"scope/#focus-statistics-ccss-m","text":"S-ID 1. Represent data with plots on the real number line (dotplots, histograms, and boxplots). S-ID 2: Use statistics appropriate to the shape of the data distribution to compare center (median, mean) of two or more different data sets (measures of spread will be studied in Unit 2). S-ID 3: Interpret differences in shape, center, and spread in the context of the data sets, accounting for possible effects of extreme data points (outliers). S-ID 5. Summarize categorical data for two categories in two-way frequency tables. Interpret relative frequencies in the context of the data (joint, marginal, and conditional relative frequencies). Recognize possible associations and trends in the data. S-ID 6. Represent data on two quantitative variables on a scatterplot, and describe how the variables are related. S-IC 6. Evaluate reports based on data.* *This standard is woven throughout the course. It is a recurring standard for every unit.","title":"Focus Statistics CCSS-M"},{"location":"scope/#focus-standards-for-mathematical-practices","text":"SMP-3. Construct viable arguments and critique the reasoning of others. SMP-5. Use appropriate tools strategically.","title":"Focus Standards for Mathematical Practices"},{"location":"scope/#upon-completion-of-unit-1-students-will-be-able-to","text":"Give examples of where they leave data traces. Understand that rows and columns are a form of data structure. Explain why the relationship between the variables might exist, or, if there is no relationship, why that might be so. Construct and interpret a frequency table. Critically read reports from media sources to evaluate their claims. Read plots (identify the name of the plot, interpret the axes, look for trends, identify confounding factors). Calculate conditional and marginal probabilities using frequency tables. Provide a real-world explanation for why the conditional or independent probabilities make sense, using critical thinking skills and background knowledge. Communicate their evaluations in written or verbal form using different types of media. Load data into RStudio. Create basic plots in RStudio. Create frequency tables in RStudio.","title":"Upon completion of Unit 1, students will be able to:"},{"location":"scope/#unit-2","text":"This unit deepens the informal reasoning skills developed in Unit 1 by enriching students' technical vocabulary and developing more precise analytical tools. Most importantly, this unit introduces the formal concept of probability as a tool for understanding that sometimes patterns observed in data are not \"real.\" Traditional courses attempt to develop this understanding through the development of abstract mathematical probability concepts, but IDS creates enduring understanding by teaching students to design and implement simulations using pseudo-random number generators. This activity also develops computational thinking by teaching students about some basic programming structures. Then, the use of models will come to the foreground. Students will be introduced to linear models - the most common form of modeling in introductory statistics classes - which will serve as the foundation to learn more complex modeling techniques that use the computer technology available to them later in the course, including smoothing techniques and tree-based models.","title":"Unit 2"},{"location":"scope/#focus-statistics-ccss-m_1","text":"S-ID 2: Use statistics appropriate to the shape of the data distribution to compare center (median, mean) and spread (interquartile range, standard deviation) of two or more different data sets. S-ID 3: Interpret differences in shape, center, and spread in the context of the data sets, accounting for possible effects of extreme data points (outliers). S-ID 4. Use the mean and standard deviation of a data set to fit it to a normal distribution and to estimate population percentages. Understand that there are data sets for which such a procedure is not appropriate. Use calculators, spreadsheets, and tables to estimate areas under the normal curve. S-IC 2. Decide if a specified model is consistent with results from a given data-generating process, e.g., using simulation. S-IC 6. Evaluate reports based on data.* *This standard is woven throughout the course. It is a recurring standard for every unit. S-CP 2. Understand that two events A and B are independent if the probability of A and B occurring together is the product of their probabilities, and use this characterization to determine if they are independent. S-CP 9. (+) Use permutations to perform [informal] inference. *This standard will be addressed in the context of data science.","title":"Focus Statistics CCSS-M"},{"location":"scope/#focus-smps","text":"SMP-4. Model with mathematics. SMP-5. Use appropriate tools strategically.","title":"Focus SMPs"},{"location":"scope/#upon-completion-of-unit-2-students-will-be-able-to","text":"Create a boxplot by calculating the five-number summary, upper and lower fences, and determining outliers. Explain what \u201cstandard deviation\u201d means in context. Explain why the measures of central tendency and spread may or may not be accurate descriptions of the data from which they came. Use permutations of data to solve problems. Read/interpret a normal curve/distribution. Explain where the normal distribution came from. Describe situations where the normal distribution may model the phenomena, and others where it may not. Simulate normal distribution. Simulate from a model. Compare real data to simulation. Determine if model and data appear consistent. Merge data by columns/rows, and verify that merging is successful. Learn for() loops and apply() functions in RStudio. Create functions.","title":"Upon completion of Unit 2, students will be able to:"},{"location":"scope/#unit-3","text":"Unit 3 focuses on data collection methods, including traditional methods of designed experiments and observational studies and surveys. It introduces students to sampling error and bias, which cause problems in analysis made from survey data. Participatory Sensing is presented as another method of data collection, and students learn to design Participatory Sensing campaigns that will allow them to address particular statistical questions. Participatory Sensing is a unique data collection method because it uses sensors. Furthermore, this method emphasizes the involvement of citizens and community groups in the process of sensing and documenting where they live, work, and play. Triggers play an important role in the Participatory Sensing data collection process. The response to the triggers may or may not be the same each time. Data takes on a variety of forms online and requires a different style of representation. Students enhance computing skills by learning about modern data structures, and by learning to \"scrape\" data stored in XML format.","title":"Unit 3"},{"location":"scope/#focus-statistics-ccss-m_2","text":"S-IC 1. Understand statistics as a process for making inferences about population parameters based on a random sample from that population. S-IC 3. Recognize the purposes of and differences among sample surveys, experiments, and observational studies; explain how randomization relates to each. S-IC 6. Evaluate reports based on data.* *This standard is woven throughout the course. It is a recurring standard for every unit.","title":"Focus Statistics CCSS-M"},{"location":"scope/#focus-smps_1","text":"SMP-1. Make sense of problems and persevere in solving them. SMP-4. Model with mathematics. SMP-8. Look for and express regularity in repeated reasoning.","title":"Focus SMPs"},{"location":"scope/#upon-completion-of-unit-3-students-will-be-able-to","text":"Provide a loose definition of \u201cstatistics\u201d in their own words. Compare and contrast population vs. sample. Compare and contrast parameter vs. statistic. Explain the difference between special data structures, particularly as they relate to inference. Exploit special data structures for re-randomization analysis. Explain situations where one measure of central tendency or spread may be more appropriate than others. Read/interpret boxplots (In-depth look into samples size and their relationship to the population parameters). Identify reports that use special data structures (census, survey, observational study, and randomized experiment). Do data scraping. Use HTML and XML formats. Use RStudio to re-randomize data. Compute measures of central tendency and spread in RStudio.","title":"Upon completion of Unit 3, students will be able to:"},{"location":"scope/#unit-4","text":"This unit will develop modeling skills, beginning with learning to fit and interpret least squares regression lines and learning to use regression to make predictions. Students will learn to evaluate the success of these predictions and so compare models for their predictive accuracy. Modern algorithmic approaches to regression are presented, and students will strengthen algorithmic thinking skills by understanding how and why these algorithms help data scientists make accurate predictions from data. Students engage in a complete modeling experience in which they apply the skills and concepts learned in the previous units. The modeling experience is designed to make students\u2019 thinking visible and audible by encouraging them to be metacognitive about the process of inventing and testing a model, ask questions as they go through the process, and recognize the iterative nature of modeling.","title":"Unit 4"},{"location":"scope/#focus-statistics-standards","text":"S-IC 2. Decide if a specified model is consistent with results from a given data-generating process, e.g., using simulation. S-ID 6. Represent data on two quantitative variables on a scatter plot, and describe how the variables are related. a. Fit a function to the data; use functions fitted to data to solve problems in the context of the data. Use given functions or choose a function suggested by the context. Emphasize linear models. b. Informally assess the fit of a function by plotting and analyzing residuals. c. Fit a linear function for a scatter plot that suggests a linear association. S-ID 7. Interpret the slope (rate of change) and the intercept (constant term) of a linear model in the context of the data. S-ID 8. Compute (using technology) and interpret the correlation coefficient of a linear fit. S-IC 6. Evaluate reports based on data.* *This standard is woven throughout the course. It is a recurring standard for every unit.","title":"Focus Statistics Standards"},{"location":"scope/#focus-smps_2","text":"SMP-2. Reason abstractly and quantitatively. SMP-4. Model with mathematics. SMP-7. Look for and make use of structure.","title":"Focus SMPs"},{"location":"scope/#upon-completion-of-unit-4-students-will-be-able-to","text":"Describe how well the linear model fits the data (or does not). Provide a real-world explanation of why the model may or may not fit, using critical thinking skills and background knowledge. Interpret the slope and intercept on a plot. Compute the correlation coefficient using RStudio. Interpret linear models in reports, including the correlation coefficient. Determine if a trend is \u201creal\u201d or if it could have arisen from randomness. Use critical thinking skills to explain why a trend may or may not make sense. Fit a regression line. Extract the slope, intercept, correlation coefficient, coefficient of determination, and residuals using RStudio. Use RStudio to predict y given an x value. Explore what happens to the line and the response variable if we multiply (divide) or add (subtract) a constant from the predictor. Design and execute their own Participatory Sensing Campaigns. Use RStudio to compute permutations and combinations. Create Classification and Regression Tree (CART) models. Understand non-linear models.","title":"Upon completion of Unit 4, students will be able to:"},{"location":"table/","text":".tg {border-collapse:collapse;border-spacing:0;} .tg td{font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;} .tg th{font-family:Arial, sans-serif;font-size:14px;font-weight:normal;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;} .tg .tg-7klg{font-weight:bold;font-size:24px;border-color:#ffffff;text-align:center;vertical-align:top} .tg .tg-km2t{font-weight:bold;border-color:#ffffff;text-align:left;vertical-align:top} .tg .tg-w1ir{font-weight:bold;background-color:#c0c0c0;border-color:#ffffff;text-align:center;vertical-align:top} .tg .tg-zv4m{border-color:#ffffff;text-align:left;vertical-align:top} .tg .tg-u6gx{font-weight:bold;background-color:#c0c0c0;border-color:#ffffff;text-align:left;vertical-align:top} .tg .tg-8jgo{border-color:#ffffff;text-align:center;vertical-align:top} .tg .tg-aw21{font-weight:bold;border-color:#ffffff;text-align:center;vertical-align:top} .tg .tg-gwzd{font-weight:bold;background-color:#9b9b9b;border-color:#ffffff;text-align:center;vertical-align:top} Table of Contents Content Page Overview and Philosophy 9 Scope and Sequence 16 UNIT 1 Campaign Topics Daily Overview 22 Essential Concepts 23 Section 1: Data are all Around 25 Lesson 1: Data Trails Defining data, consumer privacy 27 Lesson 2: Stick Figures Organizing & collecting data 29 Lesson 3: Data Structures Organizing data, rows & columns, variables 31 Lesson 4: The Data Cycle Data cycle, statistical questions 34 Lesson 5: So Many Questions Statistical questions, variability 38 Lesson 6: What Do I Eat? Food Habits Collecting data, statistical questions 40 Lesson 7: Setting the Stage Food Habits \u2013 data Participatory sensing 43 Campaign Guidelines: Food Habits Campaign Food Habits - Guidelines Section 2: Visualizing Data 47 Lesson 8: Tangible Plots Food Habits \u2013 data Dotplots, minimum/maximum, frequency 49 Lesson 9: What Is Typical? Food Habits \u2013 data Typical value, center 53 Lesson 10: Making Histograms Food Habits \u2013 data Histograms, bin widths 55 Lesson 11: What Shape Are You In? Food Habits \u2013 data Shape, center, spread 58 Lesson 12: Exploring Food Habits Food Habits \u2013 data Single & multi-variable plots 60 Lesson 13: RStudio Basics Food Habits \u2013 data Intro to RStudio 62 Lab 1A: Data, Code & RStudio Food Habits \u2013 data RStudio basics 65 Lab 1B: Get the Picture? Food Habits \u2013 data Variable types, bar graphs, histograms 68 Lab 1C: Export, Upload, Import Importing data 71 Lesson 14: Variables, Variables, Variables Multi-variable plots 75 Lab 1D: Zooming Through Data Subsetting 80 Lab 1E: What\u2019s the Relationship? Multi-variable plots 83 Practicum: The Data Cycle & My Food Habits Food Habits Data cycle, variability 86 Section 3: Would You Look at the Time 88 Lesson 15: Americans\u2019 Time on Task Time Use \u2013 data Evaluating claims 90 Campaign Guidelines Time Use Campaign Time Use \u2013 Guideline 90 Lab 1F: A Diamond In the Rough Time Use \u2013 data Cleaning names, categories, and strings 94 Lesson 16: Categorical Associations Time Use \u2013 data Joint relative frequencies in 2- way tables 98 Lesson 17: Interpreting Two-Way Tables Time Use \u2013 data Marginal & conditional relative frequencies 100 Lab 1G: What\u2019s the FREQ? Time Use \u2013 data 2-way tables, tally 105 Practicum: Teen Depression Time Use Statistical questions, interpreting plots 107 Lab 1H: Our Time Data cycle, synthesis 109 End of Unit Project and Oral Presentation: Analyzing Data to Evaluate Claims Data cycle 110 UNIT 2 Campaign Topics Daily Overview 112 Essential Concepts 113 Section 1: What is Your True Color? 115 Lesson 1: What Is Your True Color? Personality Color - data Subsets, relative frequency 117 Lesson 2: What Does Mean Mean? Personality Color Measures of center \u2013 mean 120 Lesson 3: Median In the Middle Personality Color Measures of center \u2013 median 124 Lesson 4: How Far Is It from Typical? Personality Color Measures of spread \u2013 MAD 128 Lab 2A: All About Distributions Personality Color Measures of center & spread \u2013 mean, median, MAD 132 Lesson 5: Human Boxplots Boxplots, IQR 134 Lesson 6: Face Off Comparing distributions 137 Lesson 7: Plot Match Comparing distributions 140 Lab 2B: Oh, the Summaries\u2026 Personality Color Boxplots, IQR, numerical summaries, custom functions 143 Practicum: The Summaries Food Habits or Personality Color Statistical questions, comparing distributions 146 Section 2: How Likely is it? 148 Lesson 8: How Likely is It? Probability, simulations 150 Lesson 9: Bias Detective Simulations to detect bias 153 Lesson 10: Marbles, Marbles Probability, with replacement 157 Lab 2C: Which Song Plays Next? Probability of simple events, do loops, set.seed() 159 Lesson 11: This AND/OR That Compound probabilities 162 Lab 2D: Queue It Up! Probability with & without replacement, sample() 166 Practicum: Win, Win, Win Probability estimation through repeated simulations 169 Section 3: Are You Stressing or Chilling? 170 Lesson 12: Don\u2019t Take My Stress Away Stress/Chill \u2013 data Introduction to campaign 172 Campaign Guidelines Stress/Chill Campaign Stress/Chill \u2013 Guideline 90 Lesson 13: The Horror Movie Shuffle Stress/Chill \u2013 data Chance differences \u2013 cat var 176 Lab 2E: The Horror Movie Shuffle Stress/Chill \u2013 data Inference for categorical variable, do loops, shuffle() 180 Lesson 14: The Titanic Shuffle Stress/Chill \u2013 data Chance differences \u2013 num var 183 Lab 2F: The Titanic Shuffle Stress/Chill \u2013 data Inference for numerical variable, do loops, shuffle() 187 Lesson 15: Tangible Data Merging Stress/Chill \u2013 data Merging data sets 189 Lab 2G: Getting It Together Stress/Chill & Personality Color Merging data sets, stacking vs. joining 192 Practicum: What Stresses Us? Stress/Chill & Personality Color Answering statistical questions of merged data 194 Section 4: What\u2019s Normal? 195 Lesson 16: What Is Normal? Introduction to normal curve 197 Lesson 17: Normal Measure of Spread Measures of spread - SD 201 Lesson 18: What\u2019s Your Z-Score? z-scores, shuffling 204 Lab 2H: Eyeballing Normal Normal curves overlaid on distributions & simulated data 220 Lab 2I: R\u2019s Normal Distribution Alphabet 212 End of Unit Project: Asking and Answering Statistical Questions of Our Own Data Stress/Chill, Personality Color, Habits, or Time Use Synthesis of above 214 UNIT 3 Campaign Topics Daily Overview 216 Essential Concepts 217 Section 1: Testing, Testing\u20261, 2, 3\u2026 219 Lesson 1: Anecdotes vs. Data Reading articles critically, data 221 Lesson 2: What is an Experiment? Experiments, causation 224 Lesson 3: Let\u2019s Try an Experiment! Random assignments, confounding factors 227 Lesson 4: Predictions, Predictions Visualizations, predictions 229 Lesson 5: Time Perception Experiment Elements of an experiment 231 Lab 3A: The results are in! Analyzing experiment data 233 Practicum: Music to my Ears Design an experiment 234 Section 2: Would You Look at That? 235 Lesson 6: Observational Studies Observational study 237 Lesson 7: Observational Studies vs. Experiments Observational study, experiment 239 Lesson 8: Monsters that Hide in Observational Studies Observational study, confounding factors 241 Lab 3B: Confound it all! Confounding factors 245 Section 3: Are You Asking Me? 247 Lesson 9: Survey Says\u2026 Survey 249 Lesson 10: We\u2019re So Random Data collection, random samples 252 Lesson 11: The Gettysburg Address Sampling bias 256 Lab 3C: Random Sampling Random sampling 261 Lesson 12: Bias in Survey Sampling Bias, sampling methods 263 Lesson 13: The Confidence Game Confidence intervals 266 Lesson 14: How Confident Are You? Confidence intervals, margin of error 269 Lab 3D: Are You Sure about That? Bootstrapping 271 Practicum: Let\u2019s Build a Survey! Non-biased survey design 274 Section 4: What\u2019s the Trigger? 275 Lesson 15 Ready, Sense, Go! Sensors, data collection 277 Lesson 16: Does it have a Trigger? Survey questions, sensor questions 280 Lesson 17: Creating Our Own Participatory Sensing Campaign Participatory sensing campaign creation 283 Lesson 18: Evaluating Our Own Participatory Sensing Campaign Statistical questions, evaluate campaign 286 Lesson 19: Implementing Our Own Participatory Sensing Campaign Class Campaign\u2014data Mock-implement campaign, campaign creation, data collection 288 Section 5: Webpages 290 Lesson 20: Online Data-ing Class Campaign\u2014data Data on the internet 292 Lab 3E: Scraping web data Class Campaign\u2014data Scraping data from the internet 296 Lab 3F: Maps Class Campaign\u2014data Making maps with data from the internet 299 Lesson 21: Learning to Love XML Class Campaign\u2014data Data storage, XML 301 Lesson 22: Changing Orientation Class Campaign\u2014data Converting XML files 303 Practicum: What Does Our Campaign Data Say? Class Campaign Statistical questions, visualizations, numerical summaries 305 End of Unit Project: TB or Not TB Class Campaign Simulation using experiment data 306 UNIT 4 Campaign Topics Daily Overview 309 Essential Concepts 311 Section 1: Campaigns and Community 313 Lesson 1: Trash Modeling to answer real world problems, official data sets 315 Lesson 2: Drought Exploratory data analysis, campaign creation 319 Lesson 3: Community Connection Team Campaign\u2014data Community topic research, campaign creation 321 Lesson 4: Evaluate and Implement the Campaign Team Campaign\u2014data Statistical questions, evaluate & mock implement campaign 323 Lesson 5: Refine and Create the Campaign Team Campaign\u2014data Revise and edit campaign, data collection 325 Section 2: Predictions and Models 326 Lesson 6: Statistical Predictions Using One Variable Team Campaign\u2014data One-variable predictions using a rule 328 Lesson 7: Statistical Predictions by Applying the Rule Team Campaign\u2014data Predictions applying mean square deviation, mean absolute error 333 Lesson 8: Statistical Predictions Using Two Variables Team Campaign\u2014data Two-variable statistical predictions, scatterplots 335 Lesson 9: The Spaghetti Line Team Campaign\u2014data Estimate line of best fit, single linear regression 337 LAB 4A: If the Line Fits\u2026 Team Campaign\u2014data Estimate line of best fit 339 Lesson 10: What\u2019s the Best Line? Team Campaign\u2014data Predictions based on linear models 343 LAB 4B: What\u2019s the Score? Team Campaign\u2014data Comparing predictions to real data 345 LAB 4C: Cross-Validation Team Campaign\u2014data Use training and test data for predictions 346 Lesson 11: What\u2019s the Trend? Team Campaign\u2014data Trend, associations, linear model 348 Lesson 12: How Strong Is It? Team Campaign\u2014data Correlation coefficient, strength of trend 351 LAB 4D: Interpreting Correlations Team Campaign\u2014data Use correlation coefficient to determine best model 353 Lesson 13: Improving Your Model Team Campaign\u2014data Non-linear regression 355 LAB 4E: Some Models Have Curves Team Campaign\u2014data Non-linear regression 357 Section 3: Piecing it Together 358 Lesson 14: More Variables to Make Better Predictions Team Campaign\u2014data Multiple linear regression 359 Lesson 15: Combination of Variables Team Campaign\u2014data Multiple linear regression 361 LAB 4F: This Model Is Big Enough for All of Us Team Campaign\u2014data Multiple linear regression 364 Practicum: Predictions Team Campaign\u2014data Linear regression 365 Section 4: Decisions, Decisions! 368 Lesson 16: Footbal or Futbol? Team Campaign\u2014data Multiple predictors, classifying into groups, decision trees 369 Lesson 17: Grow Your Own Decision Tree Team Campaign\u2014data Decision trees based on training and test data 371 LAB 4G: Growing Trees Team Campaign\u2014data Decision trees to classify observations 374 Section 5: Ties That Bind 378 Lesson 18: Where Do I Belong? Team Campaign\u2014data Clustering, k-means 379 LAB 4H: Finding Clusters Team Campaign\u2014data Clustering, k-means 382 Lesson 19: Our Class Network Team Campaign\u2014data Clustering, networks 385 End of Unit 4 Modeling Activity Project and Presentations: Community Issue Team Campaign Synthesis of above 388","title":"Table of Contents"},{"location":"updates/","text":"If you have any question, please contact us at support@idsucla.org LAUSD teachers please email ids_support@lausd.net Introduction to Data Science, https://www.idsucla.org This website was last updated on May 9, 2024. Changes that have been made to version 6.0 All Units Labs now also have text which direct students to complete tasks within the RStudio/Posit interface in blue font GPS no longer reports precise location There are anonymized datasets available for the Food Habits, Personality Colors, and Time Use (unformatted) Campaigns. They can be accessed by running the following code: extra_data('food_ids') extra_data('color_ids') extra_data('timeuse_ids') Unit 1 Food Habits and Time Use campaign questions, in the campaign guidelines, have been modified for clarity Lesson 1: LMR_1.1 changed \"Facebook\" to \"TikTok\" Lesson 14: updated LMR_1.16 with new data/color palette Lesson 17: updated plots and cleaned up formatting on LMR_118 Lab 1B has more clarification on options/arguments Labs 1C and 1F now have embedded videos to help guide students Lab 1D has more explanation about filtering and the assignment operator Unit 2 Stress/Chill campaign questions have been modified for clarity (and picture question has been removed) Unit 3 Unit 4 Section 1 is now Campaigns and Community where we set the stage for the End of Unit Project (also updated) Section 2 has been rehauled to improve the flow for the lessons on linear regression Section 2 also caps off with non-linear models with Lesson 13: Improving your Model (previously Lesson 14) Section 3 is now Piecing it Together with only the two lessons on multivariable regression The lessons in Section 4 have been slightly rearranged - part of Lesson 17 was moved to Lesson 16 for better building of concepts. Lesson 18: the LMR was modified for clarity. We also added instructions for the k-means clustering app. The End of Unit Project is student choice around a community issue Unit 4 Assessments have been updated (access through the Documents tool on Portal) - link to lungs data and solutions. Lab 4D lab solutions, MSE for 2nd variable changed to use y-variable, critics_rating , in residual calculation/code","title":"Updates"},{"location":"video/","text":"Instructional Videos On Using The Tools Campaign Manager Managing Campaign Settings Managing Campaign Responses Export, Upload, Import Creating a Campaign Survey Taking Survey Taking via Browser Survey Taking via App Dashboard Navigating the Dashboard PlotApp PlotApp Campaign Monitoring Teacher Campaign Monitoring Tool Student Campaign Monitoring Tool Documents No video IDS MobileApps Instructions How to Download & Use App Class Management Create and Manage a Class Splitting Columns in Excel RStudio Accessing RStudio through Posit Cloud Posit Cloud Admin View Rstudio Basics (Unit 1 Lesson 13) Lab 1A Export, Upload, Import Decluttering Environment Moving Files Between Projects Dropdown Menu Navigating the Dropdown Menu Real-time Data Collection App Real-time Data Collection App .cards { display: flex; flex-direction: row; flex-wrap: wrap; } .card { width: 320px; padding: 16px; border: 1px solid lightgrey; } .md-nav--primary .md-nav__link[for=__toc] ~ .md-nav { display: none !important; } .md-sidebar { display: none !important; } .md-header{ pointer-events: none; } \u00d7 #videoModal { display: none; position: fixed; z-index: 10000; left: 0; top: 0; width: 100%; height: 100%; overflow: auto; background-color: rgba(0, 0, 0, 0.8); } #videoContainer { position: relative; margin: 10% auto; padding: 20px; width: 80%; max-width: 800px; } #closeVideo { color: #fff; float: right; font-size: 28px; font-weight: bold; cursor: pointer; } function openVideoModal(src){ document.getElementById(\"videoModal\").style.display = \"block\"; var screenWidth = window.innerWidth || document.documentElement.clientWidth || document.body.clientWidth; var iframeWidth = Math.min(0.8 * screenWidth, 800); document.getElementById(\"videoFrame\").style.width = iframeWidth + \"px\"; document.getElementById(\"videoFrame\").style.height = iframeWidth*0.75 + \"px\"; document.getElementById(\"videoFrame\").src = src; } document.getElementById(\"closeVideo\").addEventListener(\"click\", function() { document.getElementById(\"videoModal\").style.display = \"none\"; document.getElementById(\"videoFrame\").src = \"\"; });","title":"How to..Video"},{"location":"video/#instructional-videos-on-using-the-tools","text":"Campaign Manager Managing Campaign Settings Managing Campaign Responses Export, Upload, Import Creating a Campaign Survey Taking Survey Taking via Browser Survey Taking via App Dashboard Navigating the Dashboard PlotApp PlotApp Campaign Monitoring Teacher Campaign Monitoring Tool Student Campaign Monitoring Tool Documents No video IDS MobileApps Instructions How to Download & Use App Class Management Create and Manage a Class Splitting Columns in Excel RStudio Accessing RStudio through Posit Cloud Posit Cloud Admin View Rstudio Basics (Unit 1 Lesson 13) Lab 1A Export, Upload, Import Decluttering Environment Moving Files Between Projects Dropdown Menu Navigating the Dropdown Menu Real-time Data Collection App Real-time Data Collection App .cards { display: flex; flex-direction: row; flex-wrap: wrap; } .card { width: 320px; padding: 16px; border: 1px solid lightgrey; } .md-nav--primary .md-nav__link[for=__toc] ~ .md-nav { display: none !important; } .md-sidebar { display: none !important; } .md-header{ pointer-events: none; } \u00d7 #videoModal { display: none; position: fixed; z-index: 10000; left: 0; top: 0; width: 100%; height: 100%; overflow: auto; background-color: rgba(0, 0, 0, 0.8); } #videoContainer { position: relative; margin: 10% auto; padding: 20px; width: 80%; max-width: 800px; } #closeVideo { color: #fff; float: right; font-size: 28px; font-weight: bold; cursor: pointer; } function openVideoModal(src){ document.getElementById(\"videoModal\").style.display = \"block\"; var screenWidth = window.innerWidth || document.documentElement.clientWidth || document.body.clientWidth; var iframeWidth = Math.min(0.8 * screenWidth, 800); document.getElementById(\"videoFrame\").style.width = iframeWidth + \"px\"; document.getElementById(\"videoFrame\").style.height = iframeWidth*0.75 + \"px\"; document.getElementById(\"videoFrame\").src = src; } document.getElementById(\"closeVideo\").addEventListener(\"click\", function() { document.getElementById(\"videoModal\").style.display = \"none\"; document.getElementById(\"videoFrame\").src = \"\"; });","title":"Instructional Videos On Using The Tools"},{"location":"download/Algebra/","text":"Preview? Enable here Loading... document.getElementById('request_iframe').onload = function(){ document.body.scrollTop = 0; document.documentElement.scrollTop = 0; }","title":"Algebra"},{"location":"download/Biology/","text":"Preview? Enable here Loading... document.getElementById('request_iframe').onload = function(){ document.body.scrollTop = 0; document.documentElement.scrollTop = 0; }","title":"Biology"},{"location":"download/IDS_Curriculum_v_4.0/","text":"LAUSD only. Download or preview here .","title":"IDS Curriculum v 4.0"},{"location":"download/IDS_Curriculum_v_5.0/","text":"IDS_Curriculum_v_5.0.zip Preview? Enable here Loading... document.getElementById('request_iframe').onload = function(){ document.body.scrollTop = 0; document.documentElement.scrollTop = 0; }","title":"IDS Curriculum v 5.0"},{"location":"download/app/","text":"Depending on the device, there are 3 different options available: Android. A native Android application called \u201c IDS UCLA (authorized) \u201d is available from the Google Play Store. iOS (Apple devices) The mobile application called \u201c IDS UCLA (authorized) \u201d is available from the iOS App Store. No mobile device - browser-based version. For students that do not have a mobile device (or an unsupported device, such as a Windows phone or Blackberry), a browser-based version to perform data collection is available at https://portal.idsucla.org","title":"AppDownloads"},{"location":"download/curriculum/","text":"IDS_Curriculum Introduction to Data Science_Curriculum_v_6.0.pdf IDS_Intro+Unit1_v 6.0_FINAL.pdf IDS_Unit2_v 6.0.pdf IDS_Unit3_v 6.0.pdf IDS_Unit4_v 6.0.pdf Introduction to Data Science_Curriculum_v_5.0.pdf","title":"IDS_Curriculum"},{"location":"download/curriculum/#ids_curriculum","text":"Introduction to Data Science_Curriculum_v_6.0.pdf IDS_Intro+Unit1_v 6.0_FINAL.pdf IDS_Unit2_v 6.0.pdf IDS_Unit3_v 6.0.pdf IDS_Unit4_v 6.0.pdf Introduction to Data Science_Curriculum_v_5.0.pdf","title":"IDS_Curriculum"},{"location":"download/labs/","text":"IDS_Lab Response Sheets Lab Sheets_Unit 1_v_6.0 LRS_1A_v_6.0.pdf LRS_1B_v_6.0.pdf LRS_1C_v_6.0.pdf LRS_1D_v_6.0.pdf LRS_1E_v_6.0.pdf LRS_1F_v_6.0.pdf LRS_1G_v_6.0.pdf LRS_1H_v_6.0.pdf Lab Sheets_Unit 2_v_6.0 LRS_2A_v_6.0.pdf LRS_2B_v_6.0.pdf LRS_2C_v_6.0.pdf LRS_2D_v_6.0.pdf LRS_2E_v_6.0.pdf LRS_2F_v_6.0.pdf LRS_2G_v_6.0.pdf LRS_2H_v_6.0.pdf LRS_2I_v_6.0.pdf Lab Sheets_Unit 3_v_6.0 LRS_3A_v_6.0.pdf LRS_3B_v_6.0.pdf LRS_3C_v_6.0.pdf LRS_3D_v_6.0.pdf LRS_3E_v_6.0.pdf LRS_3F_v_6.0.pdf Lab Sheets_Unit 4_v_6.0 LRS_4A_v_6.0.pdf LRS_4B_v_6.0.pdf LRS_4C_v_6.0.pdf LRS_4D_v_6.0.pdf LRS_4E_v_6.0.pdf LRS_4F_v_6.0.pdf LRS_4G_v_6.0.pdf LRS_4H_v_6.0.pdf","title":"IDS_Lab Response Sheets"},{"location":"download/labs/#ids_lab-response-sheets","text":"","title":"IDS_Lab Response Sheets"},{"location":"download/labs/#lab-sheets_unit-1_v_60","text":"LRS_1A_v_6.0.pdf LRS_1B_v_6.0.pdf LRS_1C_v_6.0.pdf LRS_1D_v_6.0.pdf LRS_1E_v_6.0.pdf LRS_1F_v_6.0.pdf LRS_1G_v_6.0.pdf LRS_1H_v_6.0.pdf","title":"Lab Sheets_Unit 1_v_6.0"},{"location":"download/labs/#lab-sheets_unit-2_v_60","text":"LRS_2A_v_6.0.pdf LRS_2B_v_6.0.pdf LRS_2C_v_6.0.pdf LRS_2D_v_6.0.pdf LRS_2E_v_6.0.pdf LRS_2F_v_6.0.pdf LRS_2G_v_6.0.pdf LRS_2H_v_6.0.pdf LRS_2I_v_6.0.pdf","title":"Lab Sheets_Unit 2_v_6.0"},{"location":"download/labs/#lab-sheets_unit-3_v_60","text":"LRS_3A_v_6.0.pdf LRS_3B_v_6.0.pdf LRS_3C_v_6.0.pdf LRS_3D_v_6.0.pdf LRS_3E_v_6.0.pdf LRS_3F_v_6.0.pdf","title":"Lab Sheets_Unit 3_v_6.0"},{"location":"download/labs/#lab-sheets_unit-4_v_60","text":"LRS_4A_v_6.0.pdf LRS_4B_v_6.0.pdf LRS_4C_v_6.0.pdf LRS_4D_v_6.0.pdf LRS_4E_v_6.0.pdf LRS_4F_v_6.0.pdf LRS_4G_v_6.0.pdf LRS_4H_v_6.0.pdf","title":"Lab Sheets_Unit 4_v_6.0"},{"location":"download/lmrs/","text":"IDS_LMRs IDS_LMR_Unit 1_v_6.0 LMR_1.1_Data Diary.pdf LMR_1.2_Stick Figures.pdf LMR_1.3_Data Cycle.pdf LMR_1.4_Data Cycle Spinners.pdf LMR_1.5_Bros & Dudes Graphics.pdf LMR_1.6_Statistical Questions Scenarios.pdf LMR_1.7_Nutrition Facts Cutouts.pdf LMR_1.8_Food Habits Data Collection.pdf LMR_1.9_Tangible Plot.pdf LMR_1.10_Sorting Histograms.pdf LMR_1.11_Food Habits Check-In.pdf LMR_1.12_Exploring Our Food Habits.pdf LMR_1.13_Scatterplot of Heights & Weights.pdf LMR_1.14_Scatterplot of Heights & Weights & Gender.pdf LMR_1.15_Side-by-Side Bar Chart.pdf LMR_1.16_ Faceted_Histograms_v4.1.pdf LMR_1.17_Summarizing Multi-Variable Plots.pdf LMR_1.18_Analyzing Categorical Variables.pdf LMR_1.19_Interpreting Categorical Variables.pdf LMR_U1_Campaign_Food Habits.pdf LMR_U1_Campaign_Time Use.pdf LMR_U1_End of Unit Project.pdf LMR_U1_Practicum_Data Cycle.pdf LMR_U1_Practicum_Depression.pdf LMR_U1_Practicum_Depression_Fact Sheet.pdf IDS_LMR_Unit 2_v_6.0 LMR_2.1_True Colors Personality Test.pdf LMR_2.2_Pennies on a Ruler.pdf LMR_2.2b_Balancing_Point.pdf LMR_2.3_Mr. Jones Run Times.pdf LMR_2.4_Medians - Dotplots or Histograms.pdf LMR_2.5_Where is the Middle.pdf LMR_2.6_How Far Apart.pdf LMR_2.7_Oscar Ages.pdf LMR_2.8_Commute Times - Dotplots.pdf LMR_2.9_Exam Scores - Histograms.pdf LMR_2.10_Fuel Efficiency - Boxplots.pdf LMR_2.11_Dotplots to Boxplots.pdf LMR_2.12_Plot Match.pdf LMR_2.13_Compound Probabilities.pdf LMR_2.14_Tangible Data Merging.pdf LMR_2.15_Normal Plots.pdf LMR_2.16_How Far Apart SD.pdf LMR_Titanic Strips.pdf LMR_U2_Campaign_StressChill.pdf LMR_U2_Design Project.pdf LMR_U2_Practicum_The Summaries.pdf LMR_U2_Practicum_What Stresses Us.pdf LMR_U2_Practicum_Win Win Win.pdf IDS_LMR_Unit 3_v_6.0 LMR_Miracle_Cafe.pdf LMR_Trophy_Hunting_Conservation.pdf LMR_3.1_Experiment Predictions.pdf LMR_3.2_Observations to Data.pdf LMR_3.3_ObsStudies vs Experiments.pdf LMR_3.4_Designing a Survey.pdf LMR_3.5_Populations and Samples.pdf LMR_3.6_Parameters and Statistics.pdf LMR_3.6Q_Parameters and Statistics.pdf LMR_3.7_Gettysburg Address.pdf LMR_3.8_Sampling the Gettysburg Address.pdf LMR_3.9_Gettysburg Histogram.pdf LMR_3.10_Gettysburg Words.pdf LMR_3.11_Identifying Biased Samples.pdf LMR_3.11Q_Identifying Biased Samples.pdf LMR_3.12_Survey Sampling.pdf LMR_3.13_Confidence Game.pdf LMR_3.14_Sensor or Survey.pdf LMR_3.15_Food Habits Qs.pdf LMR_3.16_Campaign Creation.pdf LMR_3.17_Wikipedia - Video Games.pdf LMR_3.18_Video Games - CSV.pdf LMR_3.19_Online Data-ing.pdf LMR_3.20_Mountains - HTML vs. XML.pdf LMR_3.21_From XML to Data Tables.pdf LMR_3.22_From Data Tables to XML.pdf LMR_U3_End of Unit Project.pdf LMR_U3_End of Unit Project_Sample Solution.pdf LMR_U3_Practicum_Build a Survey.pdf LMR_U3_Practicum_Music to My Ears.pdf LMR_U3_Practicum_Our Own Campaign .pdf IDS_LMR_Unit 4_v_6.0 LMR_4.5_Campaign Authoring.pdf LMR_4.2_Trash Campaign Exploration.pdf LMR_4.3_Trash Campaign Creation.pdf LMR_4.6_HS Student Heights.pdf LMR_4.8_Prediction Games.pdf LMR_4.9_Arm Span vs Height.pdf LMR_4.12_What\u2019s the Trend.pdf LMR_4.14_Strength of Association.pdf LMR_4.10_The Spaghetti Line.pdf LMR_4.13_Predicting Values.pdf LMR_4.15_Correlation Coefficient.pdf LMR_4.4_Team Creation Campaign.pdf LMR_4.7_A Tale of Two Rules.pdf LMR_4.16_Describe the Association.pdf LMR_4.11_Testing Line of Best Fit.pdf LMR_4.1_Landfill Readiness Questions.pdf LMR_4.17_Landfill Activity.pdf LMR_4.18_Trash Data Exploration.pdf LMR_4.20_CART Player Stats.pdf LMR_4.21_CART Round 1.pdf LMR_4.22_CART Round 2.pdf LMR_4.19_CART Heart Attacks.pdf LMR_4.23_Your Own Decision Tree.pdf LMR_4.24_Find the Clusters.pdf LMR_4.25_Friend Network Graphic.pdf LMR_4.26_Network Code R Script LMR_U4_Modeling Activity Project.pdf LMR_U4_Practicum_Predictions.pdf","title":"IDS_LMRs"},{"location":"download/lmrs/#ids_lmrs","text":"","title":"IDS_LMRs"},{"location":"download/lmrs/#ids_lmr_unit-1_v_60","text":"LMR_1.1_Data Diary.pdf LMR_1.2_Stick Figures.pdf LMR_1.3_Data Cycle.pdf LMR_1.4_Data Cycle Spinners.pdf LMR_1.5_Bros & Dudes Graphics.pdf LMR_1.6_Statistical Questions Scenarios.pdf LMR_1.7_Nutrition Facts Cutouts.pdf LMR_1.8_Food Habits Data Collection.pdf LMR_1.9_Tangible Plot.pdf LMR_1.10_Sorting Histograms.pdf LMR_1.11_Food Habits Check-In.pdf LMR_1.12_Exploring Our Food Habits.pdf LMR_1.13_Scatterplot of Heights & Weights.pdf LMR_1.14_Scatterplot of Heights & Weights & Gender.pdf LMR_1.15_Side-by-Side Bar Chart.pdf LMR_1.16_ Faceted_Histograms_v4.1.pdf LMR_1.17_Summarizing Multi-Variable Plots.pdf LMR_1.18_Analyzing Categorical Variables.pdf LMR_1.19_Interpreting Categorical Variables.pdf LMR_U1_Campaign_Food Habits.pdf LMR_U1_Campaign_Time Use.pdf LMR_U1_End of Unit Project.pdf LMR_U1_Practicum_Data Cycle.pdf LMR_U1_Practicum_Depression.pdf LMR_U1_Practicum_Depression_Fact Sheet.pdf","title":"IDS_LMR_Unit 1_v_6.0"},{"location":"download/lmrs/#ids_lmr_unit-2_v_60","text":"LMR_2.1_True Colors Personality Test.pdf LMR_2.2_Pennies on a Ruler.pdf LMR_2.2b_Balancing_Point.pdf LMR_2.3_Mr. Jones Run Times.pdf LMR_2.4_Medians - Dotplots or Histograms.pdf LMR_2.5_Where is the Middle.pdf LMR_2.6_How Far Apart.pdf LMR_2.7_Oscar Ages.pdf LMR_2.8_Commute Times - Dotplots.pdf LMR_2.9_Exam Scores - Histograms.pdf LMR_2.10_Fuel Efficiency - Boxplots.pdf LMR_2.11_Dotplots to Boxplots.pdf LMR_2.12_Plot Match.pdf LMR_2.13_Compound Probabilities.pdf LMR_2.14_Tangible Data Merging.pdf LMR_2.15_Normal Plots.pdf LMR_2.16_How Far Apart SD.pdf LMR_Titanic Strips.pdf LMR_U2_Campaign_StressChill.pdf LMR_U2_Design Project.pdf LMR_U2_Practicum_The Summaries.pdf LMR_U2_Practicum_What Stresses Us.pdf LMR_U2_Practicum_Win Win Win.pdf","title":"IDS_LMR_Unit 2_v_6.0"},{"location":"download/lmrs/#ids_lmr_unit-3_v_60","text":"LMR_Miracle_Cafe.pdf LMR_Trophy_Hunting_Conservation.pdf LMR_3.1_Experiment Predictions.pdf LMR_3.2_Observations to Data.pdf LMR_3.3_ObsStudies vs Experiments.pdf LMR_3.4_Designing a Survey.pdf LMR_3.5_Populations and Samples.pdf LMR_3.6_Parameters and Statistics.pdf LMR_3.6Q_Parameters and Statistics.pdf LMR_3.7_Gettysburg Address.pdf LMR_3.8_Sampling the Gettysburg Address.pdf LMR_3.9_Gettysburg Histogram.pdf LMR_3.10_Gettysburg Words.pdf LMR_3.11_Identifying Biased Samples.pdf LMR_3.11Q_Identifying Biased Samples.pdf LMR_3.12_Survey Sampling.pdf LMR_3.13_Confidence Game.pdf LMR_3.14_Sensor or Survey.pdf LMR_3.15_Food Habits Qs.pdf LMR_3.16_Campaign Creation.pdf LMR_3.17_Wikipedia - Video Games.pdf LMR_3.18_Video Games - CSV.pdf LMR_3.19_Online Data-ing.pdf LMR_3.20_Mountains - HTML vs. XML.pdf LMR_3.21_From XML to Data Tables.pdf LMR_3.22_From Data Tables to XML.pdf LMR_U3_End of Unit Project.pdf LMR_U3_End of Unit Project_Sample Solution.pdf LMR_U3_Practicum_Build a Survey.pdf LMR_U3_Practicum_Music to My Ears.pdf LMR_U3_Practicum_Our Own Campaign .pdf","title":"IDS_LMR_Unit 3_v_6.0"},{"location":"download/lmrs/#ids_lmr_unit-4_v_60","text":"LMR_4.5_Campaign Authoring.pdf LMR_4.2_Trash Campaign Exploration.pdf LMR_4.3_Trash Campaign Creation.pdf LMR_4.6_HS Student Heights.pdf LMR_4.8_Prediction Games.pdf LMR_4.9_Arm Span vs Height.pdf LMR_4.12_What\u2019s the Trend.pdf LMR_4.14_Strength of Association.pdf LMR_4.10_The Spaghetti Line.pdf LMR_4.13_Predicting Values.pdf LMR_4.15_Correlation Coefficient.pdf LMR_4.4_Team Creation Campaign.pdf LMR_4.7_A Tale of Two Rules.pdf LMR_4.16_Describe the Association.pdf LMR_4.11_Testing Line of Best Fit.pdf LMR_4.1_Landfill Readiness Questions.pdf LMR_4.17_Landfill Activity.pdf LMR_4.18_Trash Data Exploration.pdf LMR_4.20_CART Player Stats.pdf LMR_4.21_CART Round 1.pdf LMR_4.22_CART Round 2.pdf LMR_4.19_CART Heart Attacks.pdf LMR_4.23_Your Own Decision Tree.pdf LMR_4.24_Find the Clusters.pdf LMR_4.25_Friend Network Graphic.pdf LMR_4.26_Network Code R Script LMR_U4_Modeling Activity Project.pdf LMR_U4_Practicum_Predictions.pdf","title":"IDS_LMR_Unit 4_v_6.0"},{"location":"download/resources/","text":"IDS_Teacher Resources TR_Building an Argument.pdf TR_CPM_Team Roles.pdf TR_Instructional Strategies.pdf TR_K-L-W Chart.pdf TR_List of General Data Sites.pdf TR_Plot Analysis Guide.pdf TR_Statistical Questions.pdf TR_Time Use Data Collection Template .pdf","title":"IDS_Teacher Resources"},{"location":"download/resources/#ids_teacher-resources","text":"TR_Building an Argument.pdf TR_CPM_Team Roles.pdf TR_Instructional Strategies.pdf TR_K-L-W Chart.pdf TR_List of General Data Sites.pdf TR_Plot Analysis Guide.pdf TR_Statistical Questions.pdf TR_Time Use Data Collection Template .pdf","title":"IDS_Teacher Resources"},{"location":"lab_code/unit1/","text":"Unit 1 Lab Ojectives and Functions: The documentation below lists the new functions that will appear in each lab. The lab itself will explain how to use each function. For more examples, refer to the RStudio Lab Codes and Functions file. Lab1A Objective: Students will learn how to import a pre-loaded dataset onto their Environment Pane and view the data in a spreadsheet. They will understand the importance of syntax and learn the syntax for making plots. data(_) View(_) ?_ dim(_) nrow(_) ncol(_) names(_) bargraph(~x, data) histogram(~x, data) xyplot(y~x, data) Lab1B Objective: Students will learn what type of plot is appropriate for a categorical variable versus a numerical variable and will create plots that allow for the comparison of groups. str(_) histogram(~x, data, nint) bargraph(~x, data, groups) histogram(~x | y, data) Lab1C Objective: Students will learn that loading data onto the Environment Pane from participatory sensing campaigns uses a different process (export, upload, import). Lab1D Objective: Students will learn the command to create a dotplot to analyze a numerical variable and how to stack graphs in order to compare their center, shape and spread. They will also learn how to create a new data frame that contains a subset of the food habits data. dotPlot( ~x | y, data, nint, cex, layout, main) <-, ==, <, > filter(data, \u2026) head(x) tail(x) Lab1E Objective: Students will learn How to create a scatterplot that uses both faceting and the groups argument as well as how to facet a histogram and dotplot on more than one variable. xyplot( y ~ x | u, groups, data) histogram(~x | u + v, data) dotPlot(~x | u + v, data) Lab1F Objective: Students will learn how to fix variable names, change the types of variables (ex: string to numeric) and recode categorical variables (ex: number to category). rename(data, \u2026) as.numeric(x) mutate(data, \u2026) tally(~x, data) recode(x, \u2026) save(data, file) as.factor(x) Lab1G Objective: Students will learn how to create two-way frequency tables to detect relationships between categorical variables. tally(~y | x, data, format, margin) Lab1H Objective: Students will learn how to format the timeuse data such that each row represents a typical day for each person in the group and combine all of the ideas and topics they have learned to find out how their class spends their time. timeuse_format(_)","title":"Unit 1 Lab Code"},{"location":"lab_code/unit1/#lab1a","text":"Objective: Students will learn how to import a pre-loaded dataset onto their Environment Pane and view the data in a spreadsheet. They will understand the importance of syntax and learn the syntax for making plots. data(_) View(_) ?_ dim(_) nrow(_) ncol(_) names(_) bargraph(~x, data) histogram(~x, data) xyplot(y~x, data)","title":"Lab1A"},{"location":"lab_code/unit1/#lab1b","text":"Objective: Students will learn what type of plot is appropriate for a categorical variable versus a numerical variable and will create plots that allow for the comparison of groups. str(_) histogram(~x, data, nint) bargraph(~x, data, groups) histogram(~x | y, data)","title":"Lab1B"},{"location":"lab_code/unit1/#lab1c","text":"Objective: Students will learn that loading data onto the Environment Pane from participatory sensing campaigns uses a different process (export, upload, import).","title":"Lab1C"},{"location":"lab_code/unit1/#lab1d","text":"Objective: Students will learn the command to create a dotplot to analyze a numerical variable and how to stack graphs in order to compare their center, shape and spread. They will also learn how to create a new data frame that contains a subset of the food habits data. dotPlot( ~x | y, data, nint, cex, layout, main) <-, ==, <, > filter(data, \u2026) head(x) tail(x)","title":"Lab1D"},{"location":"lab_code/unit1/#lab1e","text":"Objective: Students will learn How to create a scatterplot that uses both faceting and the groups argument as well as how to facet a histogram and dotplot on more than one variable. xyplot( y ~ x | u, groups, data) histogram(~x | u + v, data) dotPlot(~x | u + v, data)","title":"Lab1E"},{"location":"lab_code/unit1/#lab1f","text":"Objective: Students will learn how to fix variable names, change the types of variables (ex: string to numeric) and recode categorical variables (ex: number to category). rename(data, \u2026) as.numeric(x) mutate(data, \u2026) tally(~x, data) recode(x, \u2026) save(data, file) as.factor(x)","title":"Lab1F"},{"location":"lab_code/unit1/#lab1g","text":"Objective: Students will learn how to create two-way frequency tables to detect relationships between categorical variables. tally(~y | x, data, format, margin)","title":"Lab1G"},{"location":"lab_code/unit1/#lab1h","text":"Objective: Students will learn how to format the timeuse data such that each row represents a typical day for each person in the group and combine all of the ideas and topics they have learned to find out how their class spends their time. timeuse_format(_)","title":"Lab1H"},{"location":"lab_code/unit2/","text":"Unit 2 Lab Ojectives and Functions: The documentation below lists the new functions that will appear in each lab. The lab itself will explain how to use each function. For more examples, refer to the RStudio Lab Codes and Functions file. Lab2A Objective: Students will learn to use R to answer statistical questions that can be answered by calculating the mean, median and mean absolute deviation (MAD). mean(~x, data) median(~x, data) add_line(vline) MAD(~x, data) Lab2B Objective: Students will learn learn how to calculate and interpret additional summaries of distributions such as: minimums, maximums, ranges, quartiles and IQRs. They will also write a custom function. min(~x, data) max(~x, data) range(~x, data) quantile(~x, data, p) iqr(~x, data) bwplot(y ~ x, data) favstats(~x, data) diff(x) abs(x) Lab2C Objective: Students will learn how to simulate a playlist in R using the rep() function, randomly choose a song from the playlist using the sample() function, create multiple repetitions using the do() function and create the same random event using set.seed(). rep(x, times) sample(x, size, replace) do(x) set.seed(x) Lab2D Objective: Students will learn how to Use the mutate() and rowSums() functions to calculate the probability of more complex events. rowSums(x) / Lab2E Objective: Students will learn how to use permuted data to gauge how likely an event occurs by chance and investigate the question: Are women in slasher films more likely to survive until the end of the film than men? No new functions Lab2F Objective: Students will learn how to use permuted data to gauge whether a numerical summary of the data is significant or occurred by chance and investigate the question: Is there any evidence to suggest that those who survived paid a higher fare than those who died on the Titanic? No new functions Lab2G Objective: Students will learn how to use the merge() function to put data together from multiple datasets in order to investigate the question: How do people\u2019s personality colors and/or sports participation affect their stress levels? merge(x, y, by) Lab2H Objective: Students will learn how mean and standard deviation affect the shape of a normal curve and recognize when a normal model seems appropriate for real data. plotDist(\u2018norm\u2019, mean, sd) histogram(x, fit = \u201cnormal\u201d) Lab2I Objective: Students will use pnorm() to calculate probabilities from a normal distribution, rnorm() to simulate random draws from a normal distribution and use qnorm() to calculate quantiles from a normal distribution. pnorm(q, mean, sd) rnorm(n, mean, sd) qnorm(p, mean, sd)","title":"Unit 2 Lab Code"},{"location":"lab_code/unit2/#lab2a","text":"Objective: Students will learn to use R to answer statistical questions that can be answered by calculating the mean, median and mean absolute deviation (MAD). mean(~x, data) median(~x, data) add_line(vline) MAD(~x, data)","title":"Lab2A"},{"location":"lab_code/unit2/#lab2b","text":"Objective: Students will learn learn how to calculate and interpret additional summaries of distributions such as: minimums, maximums, ranges, quartiles and IQRs. They will also write a custom function. min(~x, data) max(~x, data) range(~x, data) quantile(~x, data, p) iqr(~x, data) bwplot(y ~ x, data) favstats(~x, data) diff(x) abs(x)","title":"Lab2B"},{"location":"lab_code/unit2/#lab2c","text":"Objective: Students will learn how to simulate a playlist in R using the rep() function, randomly choose a song from the playlist using the sample() function, create multiple repetitions using the do() function and create the same random event using set.seed(). rep(x, times) sample(x, size, replace) do(x) set.seed(x)","title":"Lab2C"},{"location":"lab_code/unit2/#lab2d","text":"Objective: Students will learn how to Use the mutate() and rowSums() functions to calculate the probability of more complex events. rowSums(x) /","title":"Lab2D"},{"location":"lab_code/unit2/#lab2e","text":"Objective: Students will learn how to use permuted data to gauge how likely an event occurs by chance and investigate the question: Are women in slasher films more likely to survive until the end of the film than men? No new functions","title":"Lab2E"},{"location":"lab_code/unit2/#lab2f","text":"Objective: Students will learn how to use permuted data to gauge whether a numerical summary of the data is significant or occurred by chance and investigate the question: Is there any evidence to suggest that those who survived paid a higher fare than those who died on the Titanic? No new functions","title":"Lab2F"},{"location":"lab_code/unit2/#lab2g","text":"Objective: Students will learn how to use the merge() function to put data together from multiple datasets in order to investigate the question: How do people\u2019s personality colors and/or sports participation affect their stress levels? merge(x, y, by)","title":"Lab2G"},{"location":"lab_code/unit2/#lab2h","text":"Objective: Students will learn how mean and standard deviation affect the shape of a normal curve and recognize when a normal model seems appropriate for real data. plotDist(\u2018norm\u2019, mean, sd) histogram(x, fit = \u201cnormal\u201d)","title":"Lab2H"},{"location":"lab_code/unit2/#lab2i","text":"Objective: Students will use pnorm() to calculate probabilities from a normal distribution, rnorm() to simulate random draws from a normal distribution and use qnorm() to calculate quantiles from a normal distribution. pnorm(q, mean, sd) rnorm(n, mean, sd) qnorm(p, mean, sd)","title":"Lab2I"},{"location":"lab_code/unit3/","text":"Unit 3 Lab Ojectives and Functions: The documentation below lists the new functions that will appear in each lab. The lab itself will explain how to use each function. For more examples, refer to the RStudio Lab Codes and Functions file. Lab3A Objective: Students will investigate the question: Does the stimulus your class chose change people\u2019s perception of time? and analyze their experimental data using plots, numerical summaries, and shuffling. No new functions Lab3B Objective: Students will understand the difference between experiments and observational studies and investigate the question: What is the effect of childhood smoking on lung health? No new functions Lab3C Objective: Students will understand the difference between population data and sample data and learn how sampling methods affect how representative a sample is of a population. slice(x, from:to) Lab3D Objective: Students will learn how to calculate margins of error using the bootstrap and investigate the question: What is the mean age of people older than 15 living in the United States? No new functions Lab3E Objective: Students will learn how to scrape data by gathering information from the web and cleaning it. readHTMLTable(url, which) length(x) Lab3F Objective: Students will learn how to to create an interactive map of the mountains data scraped in lab 3e. load(file) leaflet() addTiles() addMarkers(map, lng, lat, popup) colorize() addCircleMarkers() addLegend(x, colors, labels)","title":"Unit 3 Lab Code"},{"location":"lab_code/unit3/#lab3a","text":"Objective: Students will investigate the question: Does the stimulus your class chose change people\u2019s perception of time? and analyze their experimental data using plots, numerical summaries, and shuffling. No new functions","title":"Lab3A"},{"location":"lab_code/unit3/#lab3b","text":"Objective: Students will understand the difference between experiments and observational studies and investigate the question: What is the effect of childhood smoking on lung health? No new functions","title":"Lab3B"},{"location":"lab_code/unit3/#lab3c","text":"Objective: Students will understand the difference between population data and sample data and learn how sampling methods affect how representative a sample is of a population. slice(x, from:to)","title":"Lab3C"},{"location":"lab_code/unit3/#lab3d","text":"Objective: Students will learn how to calculate margins of error using the bootstrap and investigate the question: What is the mean age of people older than 15 living in the United States? No new functions","title":"Lab3D"},{"location":"lab_code/unit3/#lab3e","text":"Objective: Students will learn how to scrape data by gathering information from the web and cleaning it. readHTMLTable(url, which) length(x)","title":"Lab3E"},{"location":"lab_code/unit3/#lab3f","text":"Objective: Students will learn how to to create an interactive map of the mountains data scraped in lab 3e. load(file) leaflet() addTiles() addMarkers(map, lng, lat, popup) colorize() addCircleMarkers() addLegend(x, colors, labels)","title":"Lab3F"},{"location":"lab_code/unit4/","text":"Unit 4 Lab Ojectives and Functions: The documentation below lists the functions that will appear in each lab. The lab itself will explain how to use each function. For more examples, refer to the RStudio Lab Codes and Functions file. Lab4A Type these two commands into your console: data(cdc) View(cdc) Lab4B Type these two commands into your console: data(cdc) View(cdc)","title":"Unit 4 Lab Code"},{"location":"lab_code/unit4/#lab4a","text":"Type these two commands into your console: data(cdc) View(cdc)","title":"Lab4A"},{"location":"lab_code/unit4/#lab4b","text":"Type these two commands into your console: data(cdc) View(cdc)","title":"Lab4B"},{"location":"unit1/campaign1/","text":"Campaign Guidelines - Food Habits 1. The Issue: Although we might take its existence for granted today, the Nutrition Facts label was not always required to be on food packages. It wasn't until 1990 that the Nutrition Labeling Education Act mandated food companies to provide information on food labels to help consumers make wiser choices about what they eat. This raises some interesting questions: 1) Does knowing nutritional information about my snacks help me change my habits? 2) What is my snacking pattern? 3) Do I tend to eat healthy? How do I compare to my class? How does my class compare to the rest of the country? 2. Objectives: Upon completing this campaign, students will have the enduring understanding that interpreting graphs provides useful information about the real world as viewed through the data represented in the graphs. We can explore the relationship between two variables, and if there is a relationship, it is driven by the change in the independent variable, x, which causes a change in the dependent variable, y. 3. Survey Questions: (students will enter data about the snacks they consume): Consider Data: Before students submit a survey for their first snack, a class consensus of the meaning of the variables must be reached so that proper analysis and interpretations can be made. Two examples are listed below: when - If students have different definitions of \" evening \", it might make it hard to compare snacking patterns across students. As a class, come to a consensus about what time intervals are considered morning , afternoon , evening and night . cost - If a student has a bowl of cereal as a snack, are they going to include the cost of the entire box or are they going to calculate the unit cost for one serving? This needs to be a class decision. Prompt Variable Data Type What is the name of the snack? name text When did you eat the snack? when categorical morning afternoon evening night Is the snack salty or sweet? salty_sweet categorical Salty Sweet How healthy is the snack? (1 = Very unhealthy, 5 = Very healthy) healthy_level numerical 1 2 3 4 5 How many calories per serving? calories numerical How many grams of protein per serving? protein numerical How many grams of sugar per serving? sugar numerical How many milligrams of sodium per serving? sodium numerical How many ingredients are in your snack? ingredients numerical Why are you eating the snack? why categorical availability craving emotional energy hungry/thirsty social other How much does the snack cost (in dollars)? cost categorical $0 to < $1 $1 to < $3 $3 to < $7 $7 or more Take a picture (optional). snack_image photo AUTOMATIC location lat, long AUTOMATIC time time AUTOMATIC date date AUTOMATIC user user id When should you take the survey? If possible, take the survey every time you eat a snack or at the end of the day. Reminders can be set to ensure survey completion. How long should the campaign last? About nine days. Ideally, two of these days will include a weekend. 4. Motivation: As a class, come to an agreement about how many surveys each student should submit. All students should submit roughly the same number of surveys, and each student should submit at least four surveys. After the first day, use the Campaign Monitoring tool to see who has collected data. After two to three days, direct students\u2019 attention to the Total Responses by Day plot and comment on any patterns. For example, if they see a plot like the one below, ask \"What story does this tell us about our data collection?\u201d Story: They collected a lot of data together in class. Data collection increased every day from Wednesday to Friday. There was little to no data collection over the weekend. Data collection peaked on Monday - there were over 180 responses! Discuss data collection issues. What makes it hard? Does this affect the quality of data? What sort of snacks are you less likely to enter? 5. Technical Analysis: Students will use the Dashboard and Plot App as well as RStudio. 6. Guiding Questions: a. At what time of day do we eat the healthiest snacks? b. When did you snack? How does this compare to the rest of the class? c. Typically, how healthy were your snacks? How does this compare to the class as a whole? d. What were some of the characteristics of healthy snacks? What about unhealthy snacks? 7. Report: Students will complete a practicum in which they answer a statistical investigative question based on the Food Habits data collected.","title":"Campaign Guidelines \u2013 Food Habits"},{"location":"unit1/campaign1/#campaign-guidelines-food-habits","text":"","title":"Campaign Guidelines - Food Habits"},{"location":"unit1/campaign1/#1-the-issue","text":"Although we might take its existence for granted today, the Nutrition Facts label was not always required to be on food packages. It wasn't until 1990 that the Nutrition Labeling Education Act mandated food companies to provide information on food labels to help consumers make wiser choices about what they eat. This raises some interesting questions: 1) Does knowing nutritional information about my snacks help me change my habits? 2) What is my snacking pattern? 3) Do I tend to eat healthy? How do I compare to my class? How does my class compare to the rest of the country?","title":"1. The Issue:"},{"location":"unit1/campaign1/#2-objectives","text":"Upon completing this campaign, students will have the enduring understanding that interpreting graphs provides useful information about the real world as viewed through the data represented in the graphs. We can explore the relationship between two variables, and if there is a relationship, it is driven by the change in the independent variable, x, which causes a change in the dependent variable, y.","title":"2. Objectives:"},{"location":"unit1/campaign1/#3-survey-questions-students-will-enter-data-about-the-snacks-they-consume","text":"Consider Data: Before students submit a survey for their first snack, a class consensus of the meaning of the variables must be reached so that proper analysis and interpretations can be made. Two examples are listed below: when - If students have different definitions of \" evening \", it might make it hard to compare snacking patterns across students. As a class, come to a consensus about what time intervals are considered morning , afternoon , evening and night . cost - If a student has a bowl of cereal as a snack, are they going to include the cost of the entire box or are they going to calculate the unit cost for one serving? This needs to be a class decision. Prompt Variable Data Type What is the name of the snack? name text When did you eat the snack? when categorical morning afternoon evening night Is the snack salty or sweet? salty_sweet categorical Salty Sweet How healthy is the snack? (1 = Very unhealthy, 5 = Very healthy) healthy_level numerical 1 2 3 4 5 How many calories per serving? calories numerical How many grams of protein per serving? protein numerical How many grams of sugar per serving? sugar numerical How many milligrams of sodium per serving? sodium numerical How many ingredients are in your snack? ingredients numerical Why are you eating the snack? why categorical availability craving emotional energy hungry/thirsty social other How much does the snack cost (in dollars)? cost categorical $0 to < $1 $1 to < $3 $3 to < $7 $7 or more Take a picture (optional). snack_image photo AUTOMATIC location lat, long AUTOMATIC time time AUTOMATIC date date AUTOMATIC user user id When should you take the survey? If possible, take the survey every time you eat a snack or at the end of the day. Reminders can be set to ensure survey completion. How long should the campaign last? About nine days. Ideally, two of these days will include a weekend.","title":"3. Survey Questions: (students will enter data about the snacks they consume):"},{"location":"unit1/campaign1/#4-motivation","text":"As a class, come to an agreement about how many surveys each student should submit. All students should submit roughly the same number of surveys, and each student should submit at least four surveys. After the first day, use the Campaign Monitoring tool to see who has collected data. After two to three days, direct students\u2019 attention to the Total Responses by Day plot and comment on any patterns. For example, if they see a plot like the one below, ask \"What story does this tell us about our data collection?\u201d Story: They collected a lot of data together in class. Data collection increased every day from Wednesday to Friday. There was little to no data collection over the weekend. Data collection peaked on Monday - there were over 180 responses! Discuss data collection issues. What makes it hard? Does this affect the quality of data? What sort of snacks are you less likely to enter?","title":"4. Motivation:"},{"location":"unit1/campaign1/#5-technical-analysis","text":"Students will use the Dashboard and Plot App as well as RStudio.","title":"5. Technical Analysis:"},{"location":"unit1/campaign1/#6-guiding-questions","text":"a. At what time of day do we eat the healthiest snacks? b. When did you snack? How does this compare to the rest of the class? c. Typically, how healthy were your snacks? How does this compare to the class as a whole? d. What were some of the characteristics of healthy snacks? What about unhealthy snacks?","title":"6. Guiding Questions:"},{"location":"unit1/campaign1/#7-report","text":"Students will complete a practicum in which they answer a statistical investigative question based on the Food Habits data collected.","title":"7. Report:"},{"location":"unit1/campaign2/","text":"Campaign Guidelines - Time Use 1. The Issue: There have been many reports lately about people spending a large amount of time interacting with technology and the Internet. This raises some questions about time use: 1) How do I spend my time? 2) Are there groups that spend more time on certain activities than other groups? 3) How is my time use similar or different to other Americans? 2. Objectives: Upon completing this campaign, students will have compared themselves to the U.S. population to find whether they are similar in some ways and different in other ways. They will use single and multivariable plots, summary statistics, and frequency tables to find similarities and differences between groups of students and between students and other residents of the United States. 3. Survey Questions: (students will enter data for the activities in which they participated): Consider Data: The categories below are similar to the categories found in the American Time Use Survey (ATUS), which provides nationally representative estimates of how Americans spend their time. Having similar variables allows students to compare the way they spend their time to the official ATUS dataset. Before students begin collecting data, it is important to discuss different activities in their day and how they might be classified. A class consensus of the meaning of the variables must be reached so that proper analysis and interpretations can be made. Note: Students cannot double dip their time. For example, if they read during class, then those minutes spent reading do not count towards \u201cread\u201d but instead toward \u201cschool\u201d. Below are the definitions of some of the variables in the ATUS documentation. socialize - This category includes face-to-face social communication and hosting or attending social functions. consumer purchases - This includes time spent purchasing or renting consumer goods (regardless of whether it was in person, online, via telephone, at home, or in store). Note: The ATUS variable leisure combines many activities in which people might participate (such as watching television, reading, relaxing or thinking, playing computer, board, or card games, using a computer or the Internet for personal interest, playing or listening to music, and attending arts, cultural, and entertainment events). We have opted to list specific leisure activities that high school students might be more likely to engage in and made them separate variables. Prompt Variable Data Type For which day are you collecting data? day ordinal category (integers 1-10) What activities did you participate in? activities n/a a. How many MINUTES did you sleep? sleep number b. How many MINUTES did you spend eating/drinking? meals number c. How many MINUTES did you spend in classes at school? school number d. How many MINUTES did you spend doing homework? homework number e. How many MINUTES did you spend working at a job? work number f. How many MINUTES did you spend grooming yourself? grooming number g. How many MINUTES did you spend traveling/commuting? travel number h. How many MINUTES did you spend doing household chores? chores number i. How many MINUTES did you spend watching television (including streaming)? television number j. How many MINUTES did you spend playing video games? videogames number k. How many MINUTES did you spend participating in sports/exercise/physical activity? sports number l. How many MINUTES did you spend reading (not for class)? read number m. How many MINUTES did you spend communicating (includes texting, emails, video and voice calls)? communicate number n. How many MINUTES did you spend socializing (outside of class, in person)? socialize o. How many MINUTES did you spend on a spiritual activity? spiritual p. How many MINUTES did you spend purchasing items online or in a store? purchases q. How many MINUTES did you spend on hobbies/volunteering/leisure/extra-curricular activities (excluding sports and physical activity)? extra r. How many MINUTES did you spend on social media? social_media AUTOMATIC location lat, long AUTOMATIC time time AUTOMATIC date date When should you take the survey? It is recommended that students keep a log of their time and submit one survey at the end of each day, accounting for every minute of each day of the campaign. It might be helpful to split the log into three intervals where students pause and think about what they did before school, after school and in the evening. Once the log is complete and accounts for all 1,440 minutes of their day, students should then submit the survey corresponding to that day. How long should the campaign last? At least five days (maximum of ten days). Ideally, two of these days would include a weekend. 4. Motivation: Use the interactive graphic titled How Men and Women Spend Their Time found at: https://flowingdata.com/2021/09/21/how-men-and-women-spend-their-days/ After the first day, monitor the data collection and ensure that each student has submitted a survey for Day 1. Continue monitoring for the remainder of the campaign. Discuss data collection issues. What makes it hard? Does this affect the quality of data? 5. Technical Analysis: RStudio and American time use interactive graphic. Single/Multivariable plots: histograms, bar graphs, scatterplots, etc. Numerical summaries: mean, median, MAD, standard deviation. Frequency tables: One and two-way tables. 6. Guiding Questions: 1) On average, how long do students think they spend on homework? 2) Are there certain activities that take up most of the time in our day? 3) Are there groups of students who spend their time similarly to one another? 7. Report: Students will complete a practicum in which they answer a statistical investigative question based on the time-use data collected.","title":"Campaign Guidelines \u2013 Time Use"},{"location":"unit1/campaign2/#campaign-guidelines-time-use","text":"","title":"Campaign Guidelines - Time Use"},{"location":"unit1/campaign2/#1-the-issue","text":"There have been many reports lately about people spending a large amount of time interacting with technology and the Internet. This raises some questions about time use: 1) How do I spend my time? 2) Are there groups that spend more time on certain activities than other groups? 3) How is my time use similar or different to other Americans?","title":"1. The Issue:"},{"location":"unit1/campaign2/#2-objectives","text":"Upon completing this campaign, students will have compared themselves to the U.S. population to find whether they are similar in some ways and different in other ways. They will use single and multivariable plots, summary statistics, and frequency tables to find similarities and differences between groups of students and between students and other residents of the United States.","title":"2. Objectives:"},{"location":"unit1/campaign2/#3-survey-questions-students-will-enter-data-for-the-activities-in-which-they-participated","text":"Consider Data: The categories below are similar to the categories found in the American Time Use Survey (ATUS), which provides nationally representative estimates of how Americans spend their time. Having similar variables allows students to compare the way they spend their time to the official ATUS dataset. Before students begin collecting data, it is important to discuss different activities in their day and how they might be classified. A class consensus of the meaning of the variables must be reached so that proper analysis and interpretations can be made. Note: Students cannot double dip their time. For example, if they read during class, then those minutes spent reading do not count towards \u201cread\u201d but instead toward \u201cschool\u201d. Below are the definitions of some of the variables in the ATUS documentation. socialize - This category includes face-to-face social communication and hosting or attending social functions. consumer purchases - This includes time spent purchasing or renting consumer goods (regardless of whether it was in person, online, via telephone, at home, or in store). Note: The ATUS variable leisure combines many activities in which people might participate (such as watching television, reading, relaxing or thinking, playing computer, board, or card games, using a computer or the Internet for personal interest, playing or listening to music, and attending arts, cultural, and entertainment events). We have opted to list specific leisure activities that high school students might be more likely to engage in and made them separate variables. Prompt Variable Data Type For which day are you collecting data? day ordinal category (integers 1-10) What activities did you participate in? activities n/a a. How many MINUTES did you sleep? sleep number b. How many MINUTES did you spend eating/drinking? meals number c. How many MINUTES did you spend in classes at school? school number d. How many MINUTES did you spend doing homework? homework number e. How many MINUTES did you spend working at a job? work number f. How many MINUTES did you spend grooming yourself? grooming number g. How many MINUTES did you spend traveling/commuting? travel number h. How many MINUTES did you spend doing household chores? chores number i. How many MINUTES did you spend watching television (including streaming)? television number j. How many MINUTES did you spend playing video games? videogames number k. How many MINUTES did you spend participating in sports/exercise/physical activity? sports number l. How many MINUTES did you spend reading (not for class)? read number m. How many MINUTES did you spend communicating (includes texting, emails, video and voice calls)? communicate number n. How many MINUTES did you spend socializing (outside of class, in person)? socialize o. How many MINUTES did you spend on a spiritual activity? spiritual p. How many MINUTES did you spend purchasing items online or in a store? purchases q. How many MINUTES did you spend on hobbies/volunteering/leisure/extra-curricular activities (excluding sports and physical activity)? extra r. How many MINUTES did you spend on social media? social_media AUTOMATIC location lat, long AUTOMATIC time time AUTOMATIC date date When should you take the survey? It is recommended that students keep a log of their time and submit one survey at the end of each day, accounting for every minute of each day of the campaign. It might be helpful to split the log into three intervals where students pause and think about what they did before school, after school and in the evening. Once the log is complete and accounts for all 1,440 minutes of their day, students should then submit the survey corresponding to that day. How long should the campaign last? At least five days (maximum of ten days). Ideally, two of these days would include a weekend.","title":"3. Survey Questions: (students will enter data for the activities in which they participated):"},{"location":"unit1/campaign2/#4-motivation","text":"Use the interactive graphic titled How Men and Women Spend Their Time found at: https://flowingdata.com/2021/09/21/how-men-and-women-spend-their-days/ After the first day, monitor the data collection and ensure that each student has submitted a survey for Day 1. Continue monitoring for the remainder of the campaign. Discuss data collection issues. What makes it hard? Does this affect the quality of data?","title":"4. Motivation:"},{"location":"unit1/campaign2/#5-technical-analysis","text":"RStudio and American time use interactive graphic. Single/Multivariable plots: histograms, bar graphs, scatterplots, etc. Numerical summaries: mean, median, MAD, standard deviation. Frequency tables: One and two-way tables.","title":"5. Technical Analysis:"},{"location":"unit1/campaign2/#6-guiding-questions","text":"1) On average, how long do students think they spend on homework? 2) Are there certain activities that take up most of the time in our day? 3) Are there groups of students who spend their time similarly to one another?","title":"6. Guiding Questions:"},{"location":"unit1/campaign2/#7-report","text":"Students will complete a practicum in which they answer a statistical investigative question based on the time-use data collected.","title":"7. Report:"},{"location":"unit1/end/","text":"End of Unit Project and Oral Presentation: Analyzing Data to Evaluate Claims Objective: Students will apply their learning of the first unit in the curriculum by completing an End of Unit Project. Materials: IDS Unit 1\u2014End of Unit Project ( LMR_U1_End of Unit Project ) End of Unit 1 Project and Oral Presentation: Analyzing Data to Evaluate Claims Congratulations! You are on your way to becoming a Data Scientist. You have now learned some basic statistics concepts - along with RStudio skills - to help you analyze and interpret data. It is time to apply what you have learned so far. You will apply what you have learned by engaging in the following: Use an article from the list provided below, or find an article, report, blog post, etc., in a magazine, newspaper, or other media related to the topic of nutrition or time use that makes a claim. Use an article we have not used in class. a. How Americans Eat Today: http://www.cbsnews.com/news/how-americans-eat-today/ b. Why do we still eat this way?: https://www.washingtonpost.com/news/to-your-health/wp/2014/08/04/why-do-we-still-eat-this-way c. Americans Snack Differently Than Other Nations: http://www.usatoday.com/story/money/business/2014/09/29/snacking-consumer-eating-habits-nielsen/16263375/?siteID=je6NUbpObpQ-3jFHwYITZ99FE23ytK_q9g d. The Surprising Amount of Time Kids Spend Looking at a Screen http://www.theatlantic.com/education/archive/2015/01/the-surprising-amount-of-time-kidsspend-looking-at-screens/384737/ e. Youths Spend 7+ Hours/Day Consuming Media: http://www.cbsnews.com/news/youths-spend-7-plus-hours-day-consuming-media/ Analyze the article or report based on the following questions: a. What claim(s) did the article make? b. What statistical questions were they trying to answer? c. Does the article cite data? If so: i. Who was observed and what were the variables observed? ii. Who collected the data? iii. How was the data collected? iv. What are some statistics that the article used to make the claim(s)? d. If there was no data, how did the article justify its claim? Determine whether the class\u2019s Food Habits or Time Use campaign data supports, refutes, or is inconclusive of the claim(s) made in the article. Use RStudio to do your analysis using either the Food Habits or Time Use campaign data, and create graphics/plots that support your reasoning. Generate other statistical questions that you would like to investigate further after you reach your conclusion. Write a summary of your analysis that is no more than 4 pages long. Include graphics/plots/tables that provide evidence to support your reasoning. Be sure to include everything in items 1-5. Prepare a 2-minute presentation of your report. Make sure you refer to your graphics/plots/tables during your presentation.","title":"End of Unit Project and Oral Presentation: Analyzing Data to Evaluate Claims"},{"location":"unit1/end/#end-of-unit-project-and-oral-presentation-analyzing-data-to-evaluate-claims","text":"","title":"End of Unit Project and Oral Presentation: Analyzing Data to Evaluate Claims"},{"location":"unit1/end/#objective","text":"Students will apply their learning of the first unit in the curriculum by completing an End of Unit Project.","title":"Objective:"},{"location":"unit1/end/#materials","text":"IDS Unit 1\u2014End of Unit Project ( LMR_U1_End of Unit Project ) End of Unit 1 Project and Oral Presentation: Analyzing Data to Evaluate Claims Congratulations! You are on your way to becoming a Data Scientist. You have now learned some basic statistics concepts - along with RStudio skills - to help you analyze and interpret data. It is time to apply what you have learned so far. You will apply what you have learned by engaging in the following: Use an article from the list provided below, or find an article, report, blog post, etc., in a magazine, newspaper, or other media related to the topic of nutrition or time use that makes a claim. Use an article we have not used in class. a. How Americans Eat Today: http://www.cbsnews.com/news/how-americans-eat-today/ b. Why do we still eat this way?: https://www.washingtonpost.com/news/to-your-health/wp/2014/08/04/why-do-we-still-eat-this-way c. Americans Snack Differently Than Other Nations: http://www.usatoday.com/story/money/business/2014/09/29/snacking-consumer-eating-habits-nielsen/16263375/?siteID=je6NUbpObpQ-3jFHwYITZ99FE23ytK_q9g d. The Surprising Amount of Time Kids Spend Looking at a Screen http://www.theatlantic.com/education/archive/2015/01/the-surprising-amount-of-time-kidsspend-looking-at-screens/384737/ e. Youths Spend 7+ Hours/Day Consuming Media: http://www.cbsnews.com/news/youths-spend-7-plus-hours-day-consuming-media/ Analyze the article or report based on the following questions: a. What claim(s) did the article make? b. What statistical questions were they trying to answer? c. Does the article cite data? If so: i. Who was observed and what were the variables observed? ii. Who collected the data? iii. How was the data collected? iv. What are some statistics that the article used to make the claim(s)? d. If there was no data, how did the article justify its claim? Determine whether the class\u2019s Food Habits or Time Use campaign data supports, refutes, or is inconclusive of the claim(s) made in the article. Use RStudio to do your analysis using either the Food Habits or Time Use campaign data, and create graphics/plots that support your reasoning. Generate other statistical questions that you would like to investigate further after you reach your conclusion. Write a summary of your analysis that is no more than 4 pages long. Include graphics/plots/tables that provide evidence to support your reasoning. Be sure to include everything in items 1-5. Prepare a 2-minute presentation of your report. Make sure you refer to your graphics/plots/tables during your presentation.","title":"Materials:"},{"location":"unit1/essential/","text":"IDS Unit 1: Essential Concepts Lesson 1: Data Trails Data are a collection of recorded observations. Data are gathered by people and by sensors. Patterns in data can reveal previously unknown patterns in our world. Data play a large, and sometimes invisible, role in our lives. Lesson 2: Stick Figures Data consist of records of particular characteristics of people or objects. Data can be organized in many different ways, and some ways make it easier than others for achieving particular purposes. Lesson 3: Data Structures Variables record values that vary. By organizing data into rectangular format, we can easily see the characteristics of observations by reading across a row, or we can see the variability in a variable by reading down the column. Computers can easily process data when it is in rectangular format. Lesson 4: The Data Cycle A statistical investigation consists of cycling through the four stages of the Data Cycle; statistical investigative questions are questions that address variability and are productive in that they motivate data collection, analysis, and interpretation. The Data Collection phase might consist of collecting data through Participatory Sensing or some other means, or it might consist of examining previously collected data to determine the quality of the data for answering the statistical questions. Data Analysis is almost always done on the computer and consists of creating relevant graphics and numerical summaries of the data. Data Interpretation is involved with using the analysis to answer the statistical questions. Lesson 5: So Many Questions Statistical investigative questions typically begin with a vague general question, then develop into a precise question. The process of developing or creating a good investigative question is iterative and requires time and effort to get right. Lesson 6: What Do I Eat? [The Data Cycle: Consider Data] After raising statistical investigative questions, we examine and record data to see if the questions are appropriate. Lesson 7: Setting the Stage [The Data Cycle: Collect Data] In Participatory Sensing, we humans behave as if we are robot sensors, collecting data whenever a \"trigger\" event occurs. Our ability to learn about the patterns in our life through these data depends on our being reliable data collectors. Lesson 8: Tangible Plots [The Data Cycle: Analyze Data] Distributions organize data for us by telling us (a) which values of a variable were observed, and (b) how many times the values were observed (their frequency). Lesson 9: What Is Typical? The \u201ccenter\u201d of a distribution is a deliberately vague term, but it is one way to answer the subjective question \"what is a typical value?\" The center could be the perceived balancing point or the value that approximately cuts the area of the distribution in half. Lesson 10: Making Histograms Histograms can be created through the use of an algorithm. The distributions displayed in a histogram can be classified using the technical terms for the shapes of distributions. Learning to describe routine tasks through an algorithm is an important component of computational thinking. Lesson 11: What Shape Are You In? Identifying the shape of a histogram is part of the interpret step of the Data Cycle. Lesson 12: Exploring Food Habits Once Participatory Sensing data has been collected, the Dashboard and PlotApp perform the analysis step of the Data Cycle, though humans need to tell the computer which plots to examine. Lesson 13: RStudio Basics The computer has a syntax, and it can only understand if you speak its language. Lesson 14: Variables, Variables, Variables To examine whether two (or more) variables are related, we can plot their distributions on the same graph. Lesson 15: Americans\u2019 Time on Task Learning to examine other analyses is an important part of statistical thinking. Lesson 16: Categorical Associations A two-way table is a summary of the association/relationship between two categorical variables. Joint relative frequencies answer questions of the form \"what proportion of the people/objects had this value on the first variable and this value on the second?\" Lesson 17: Interpreting Two-Way Tables Marginal (relative) frequencies tell us about the distribution of a single variable. Conditional relative frequencies tell us about the distribution of one variable when \"subsetting\" the other.","title":"Essential Concepts"},{"location":"unit1/essential/#ids-unit-1-essential-concepts","text":"","title":"IDS Unit 1: Essential Concepts"},{"location":"unit1/essential/#lesson-1-data-trails","text":"Data are a collection of recorded observations. Data are gathered by people and by sensors. Patterns in data can reveal previously unknown patterns in our world. Data play a large, and sometimes invisible, role in our lives.","title":"Lesson 1: Data Trails"},{"location":"unit1/essential/#lesson-2-stick-figures","text":"Data consist of records of particular characteristics of people or objects. Data can be organized in many different ways, and some ways make it easier than others for achieving particular purposes.","title":"Lesson 2: Stick Figures"},{"location":"unit1/essential/#lesson-3-data-structures","text":"Variables record values that vary. By organizing data into rectangular format, we can easily see the characteristics of observations by reading across a row, or we can see the variability in a variable by reading down the column. Computers can easily process data when it is in rectangular format.","title":"Lesson 3: Data Structures"},{"location":"unit1/essential/#lesson-4-the-data-cycle","text":"A statistical investigation consists of cycling through the four stages of the Data Cycle; statistical investigative questions are questions that address variability and are productive in that they motivate data collection, analysis, and interpretation. The Data Collection phase might consist of collecting data through Participatory Sensing or some other means, or it might consist of examining previously collected data to determine the quality of the data for answering the statistical questions. Data Analysis is almost always done on the computer and consists of creating relevant graphics and numerical summaries of the data. Data Interpretation is involved with using the analysis to answer the statistical questions.","title":"Lesson 4: The Data Cycle"},{"location":"unit1/essential/#lesson-5-so-many-questions","text":"Statistical investigative questions typically begin with a vague general question, then develop into a precise question. The process of developing or creating a good investigative question is iterative and requires time and effort to get right.","title":"Lesson 5: So Many Questions"},{"location":"unit1/essential/#lesson-6-what-do-i-eat-the-data-cycle-consider-data","text":"After raising statistical investigative questions, we examine and record data to see if the questions are appropriate.","title":"Lesson 6: What Do I Eat? [The Data Cycle: Consider Data]"},{"location":"unit1/essential/#lesson-7-setting-the-stage-the-data-cycle-collect-data","text":"In Participatory Sensing, we humans behave as if we are robot sensors, collecting data whenever a \"trigger\" event occurs. Our ability to learn about the patterns in our life through these data depends on our being reliable data collectors.","title":"Lesson 7: Setting the Stage [The Data Cycle: Collect Data]"},{"location":"unit1/essential/#lesson-8-tangible-plots-the-data-cycle-analyze-data","text":"Distributions organize data for us by telling us (a) which values of a variable were observed, and (b) how many times the values were observed (their frequency).","title":"Lesson 8: Tangible Plots [The Data Cycle: Analyze Data]"},{"location":"unit1/essential/#lesson-9-what-is-typical","text":"The \u201ccenter\u201d of a distribution is a deliberately vague term, but it is one way to answer the subjective question \"what is a typical value?\" The center could be the perceived balancing point or the value that approximately cuts the area of the distribution in half.","title":"Lesson 9: What Is Typical?"},{"location":"unit1/essential/#lesson-10-making-histograms","text":"Histograms can be created through the use of an algorithm. The distributions displayed in a histogram can be classified using the technical terms for the shapes of distributions. Learning to describe routine tasks through an algorithm is an important component of computational thinking.","title":"Lesson 10: Making Histograms"},{"location":"unit1/essential/#lesson-11-what-shape-are-you-in","text":"Identifying the shape of a histogram is part of the interpret step of the Data Cycle.","title":"Lesson 11: What Shape Are You In?"},{"location":"unit1/essential/#lesson-12-exploring-food-habits","text":"Once Participatory Sensing data has been collected, the Dashboard and PlotApp perform the analysis step of the Data Cycle, though humans need to tell the computer which plots to examine.","title":"Lesson 12: Exploring Food Habits"},{"location":"unit1/essential/#lesson-13-rstudio-basics","text":"The computer has a syntax, and it can only understand if you speak its language.","title":"Lesson 13: RStudio Basics"},{"location":"unit1/essential/#lesson-14-variables-variables-variables","text":"To examine whether two (or more) variables are related, we can plot their distributions on the same graph.","title":"Lesson 14: Variables, Variables, Variables"},{"location":"unit1/essential/#lesson-15-americans-time-on-task","text":"Learning to examine other analyses is an important part of statistical thinking.","title":"Lesson 15: Americans\u2019 Time on Task"},{"location":"unit1/essential/#lesson-16-categorical-associations","text":"A two-way table is a summary of the association/relationship between two categorical variables. Joint relative frequencies answer questions of the form \"what proportion of the people/objects had this value on the first variable and this value on the second?\"","title":"Lesson 16: Categorical Associations"},{"location":"unit1/essential/#lesson-17-interpreting-two-way-tables","text":"Marginal (relative) frequencies tell us about the distribution of a single variable. Conditional relative frequencies tell us about the distribution of one variable when \"subsetting\" the other.","title":"Lesson 17: Interpreting Two-Way Tables"},{"location":"unit1/lab1a/","text":"Lab 1A - Data, Code & RStudio Directions: Follow along with the slides, completing the questions in blue on your computer, and answering the questions in red in your journal. Welcome to the labs! Throughout the year, you'll be putting your data science skills to work by completing the labs. You'll learn how to program in the R programming language. \u2013 The programming language used by actual data scientists. Your code will be written in RStudio which is an easy to use interface for coding using R . So let's get started! The data for our first few labs comes from the Centers for Disease Control (CDC). \u2013 The CDC is a federal institution that studies public health. Type these two commands into your console: data(cdc) View(cdc) Describe the data that appeared after running View(cdc) : \u2013 Who is the information about? \u2013 What sorts of information about them was collected? To find out more information about the cdc data, type the command below into your console. \u2013 To get back to the slides find and click on the Viewer tab. ?cdc Data: Variables & Observations Data can be broken up into two parts. `1. Observations `2. Variables \u2013 Observations are the who or what we are collecting data from/about. \u2013 Variables are the measurements or characteristics about our observations . If need be, re-type the command you used to View your data. Then answer the following: \u2013 Based on the data, describe a few characteristics about the first observation. \u2013 What does the first column tell us about our observations? In order to describe the first observation, notice that you had to look at the first row of the spreadsheet. Each row, in this case, describes a person. The columns of the spreadsheet represent variables. Uncovering our Data's Structure Now that we've looked at our data, let's look at how RStudio is organized. RStudio's main window is composed of four panes Find the pane that has a tab titled Environment and click on the tab . \u2013 This pane contains a list of everything that's currently available for R to use. \u2013 Notice that R knows we have our cdc data loaded. How many students are in our cdc data set? How many variables were measured for each student? Some New Functions Type the following commands into the console: dim(cdc) nrow(cdc) ncol(cdc) names(cdc) Which of these functions tell us the number of observations in our data? Which of these functions tell us the number of variables? First Steps Typing commands into the console is your first step into the larger world of programming or coding (terms which are often used interchangeably). Coding is all about learning how to send instructions to your computer. \u2013 The way we speak to the computer, using a coding language, syntax . R is one of many coding languages. Each coding language is slightly different, and these differences are reflected in the syntax. Capitalization , spelling and punctuation are REALLY important. Syntax matters Run the following commands. What happens after each command? Names(cdc) NAMES(cdc) names(cdc) names(CDC) Which does R understand? R's most important syntax Most of the commands you will be using follow the syntax below: function (y ~ x, data = ____ ) To create graphs or plots you need to provide R with the following: \u2013 The name of the R function, often the plot\u2019s name, that tells the computer how to create your graph. The variable(s) containing the information we want the function to use. The data set containing the variables. Notice that when we analyze a single variable the value for y is left blank. bargraph(~grade, data = cdc) Later on, we\u2019ll see we can use this syntax to do more than create graphs. Syntax in action Search through the different panes. Find and then click on the Plots tab. To get back to the slides, find and then click on the Viewer tab. Which one of these plots would be useful for answering the question: Is it unusual for students in the CDC dataset to be taller than 1.8 meters? Run the three commands below then answer the question that follows. histogram(~height, data = cdc) bargraph(~drive_text, data = cdc) xyplot(weight~height, data = cdc) Do you think it\u2019s unusual for students in the data to be taller than 1.8 meters? Why or why not? Hint: Use the arrow keys on the Plots tab to toggle between the plots. On your own: After completing the lab, answer the following questions: \u2013 What is public health and do we collect data about it? \u2013 How do you think our data was collected? Does it include every high school aged student in the US? \u2013 How might the CDC use this data? Who else could benefit from using this data? \u2013 Write the code to visualize the distribution of weights of the students in the CDC data with a histogram . What is the typical weight? \u2013 Write the code to create a bargraph to visualize the distribution of how often students ate fruit. About how many students did not eat fruit over the previous 7 days?","title":"Lab 1A - Data, Code & RStudio"},{"location":"unit1/lab1a/#lab-1a-data-code-rstudio","text":"Directions: Follow along with the slides, completing the questions in blue on your computer, and answering the questions in red in your journal.","title":"Lab 1A - Data, Code &amp; RStudio"},{"location":"unit1/lab1a/#welcome-to-the-labs","text":"Throughout the year, you'll be putting your data science skills to work by completing the labs. You'll learn how to program in the R programming language. \u2013 The programming language used by actual data scientists. Your code will be written in RStudio which is an easy to use interface for coding using R .","title":"Welcome to the labs!"},{"location":"unit1/lab1a/#so-lets-get-started","text":"The data for our first few labs comes from the Centers for Disease Control (CDC). \u2013 The CDC is a federal institution that studies public health. Type these two commands into your console: data(cdc) View(cdc) Describe the data that appeared after running View(cdc) : \u2013 Who is the information about? \u2013 What sorts of information about them was collected? To find out more information about the cdc data, type the command below into your console. \u2013 To get back to the slides find and click on the Viewer tab. ?cdc","title":"So let's get started!"},{"location":"unit1/lab1a/#data-variables-observations","text":"Data can be broken up into two parts. `1. Observations `2. Variables \u2013 Observations are the who or what we are collecting data from/about. \u2013 Variables are the measurements or characteristics about our observations . If need be, re-type the command you used to View your data. Then answer the following: \u2013 Based on the data, describe a few characteristics about the first observation. \u2013 What does the first column tell us about our observations? In order to describe the first observation, notice that you had to look at the first row of the spreadsheet. Each row, in this case, describes a person. The columns of the spreadsheet represent variables.","title":"Data: Variables &amp; Observations"},{"location":"unit1/lab1a/#uncovering-our-datas-structure","text":"Now that we've looked at our data, let's look at how RStudio is organized. RStudio's main window is composed of four panes Find the pane that has a tab titled Environment and click on the tab . \u2013 This pane contains a list of everything that's currently available for R to use. \u2013 Notice that R knows we have our cdc data loaded. How many students are in our cdc data set? How many variables were measured for each student?","title":"Uncovering our Data's Structure"},{"location":"unit1/lab1a/#some-new-functions","text":"Type the following commands into the console: dim(cdc) nrow(cdc) ncol(cdc) names(cdc) Which of these functions tell us the number of observations in our data? Which of these functions tell us the number of variables?","title":"Some New Functions"},{"location":"unit1/lab1a/#first-steps","text":"Typing commands into the console is your first step into the larger world of programming or coding (terms which are often used interchangeably). Coding is all about learning how to send instructions to your computer. \u2013 The way we speak to the computer, using a coding language, syntax . R is one of many coding languages. Each coding language is slightly different, and these differences are reflected in the syntax. Capitalization , spelling and punctuation are REALLY important.","title":"First Steps"},{"location":"unit1/lab1a/#syntax-matters","text":"Run the following commands. What happens after each command? Names(cdc) NAMES(cdc) names(cdc) names(CDC) Which does R understand?","title":"Syntax matters"},{"location":"unit1/lab1a/#rs-most-important-syntax","text":"Most of the commands you will be using follow the syntax below: function (y ~ x, data = ____ ) To create graphs or plots you need to provide R with the following: \u2013 The name of the R function, often the plot\u2019s name, that tells the computer how to create your graph. The variable(s) containing the information we want the function to use. The data set containing the variables. Notice that when we analyze a single variable the value for y is left blank. bargraph(~grade, data = cdc) Later on, we\u2019ll see we can use this syntax to do more than create graphs.","title":"R's most important syntax"},{"location":"unit1/lab1a/#syntax-in-action","text":"Search through the different panes. Find and then click on the Plots tab. To get back to the slides, find and then click on the Viewer tab. Which one of these plots would be useful for answering the question: Is it unusual for students in the CDC dataset to be taller than 1.8 meters? Run the three commands below then answer the question that follows. histogram(~height, data = cdc) bargraph(~drive_text, data = cdc) xyplot(weight~height, data = cdc) Do you think it\u2019s unusual for students in the data to be taller than 1.8 meters? Why or why not? Hint: Use the arrow keys on the Plots tab to toggle between the plots.","title":"Syntax in action"},{"location":"unit1/lab1a/#on-your-own","text":"After completing the lab, answer the following questions: \u2013 What is public health and do we collect data about it? \u2013 How do you think our data was collected? Does it include every high school aged student in the US? \u2013 How might the CDC use this data? Who else could benefit from using this data? \u2013 Write the code to visualize the distribution of weights of the students in the CDC data with a histogram . What is the typical weight? \u2013 Write the code to create a bargraph to visualize the distribution of how often students ate fruit. About how many students did not eat fruit over the previous 7 days?","title":"On your own:"},{"location":"unit1/lab1b/","text":"Lab 1B - Get the picture? Directions: Follow along with the slides, completing the questions in blue on your computer, and answering the questions in red in your journal. Where'd we leave off ... In the previous lab, we started to get acquainted with the layout of RStudio and some of the commands. In this lab, we'll learn about different types of variables. \u2013 Such as those that are measured by numbers and others that have values that are categories. We'll also look at ways to visualize these different types of data using plots (a word data scientists use interchangeably with the word graph ). Find the History tab in RStudio and click on it. Figure out how to use the information to reload the cdc data. Variable Types Numerical variables have values that are measured in units. Categorical Variables have values that describe or categorize our observations. View your cdc data and find the columns for height and gender (use the History pane again if you need help to View your data). \u2013 Is height a numerical or categorical variable? Why? \u2013 Is gender a numerical or categorical variable? Why? \u2013 List either the different categories or what you think the measured units are for height and gender . Which is which? Run the code you used in the previous lab to display the names of your cdc data's variables (Use the code displayed in the History pane to resubmit previously typed commands). Use the code's output to help you complete the following: \u2013 Write down 3 variables that you think are categorical variables and why. \u2013 Write down 3 variables that you think are numerical variables and why. Data Structures One way to get a good summary of your data is to look at the data's structure . \u2013 One way to view this info would be to click on the little blue arrow next to cdc in the Environment pane. \u2013 Another way would be to run the following in the console: str(cdc) Look at the str ucture of your cdc data and answer: What information does the str function output? Were you able to correctly guess which variables were categorical and numeric? Which ones did you mislabel? Visualizing data Visualizing data is a really helpful way to learn about our variables. Choose one numeric and one categorical variable from the data and create both a bargraph and a histogram for each variable. \u2013 Which function, either bargraph or histogram , is better at visualizing categorical variables? Which is better at visualizing numerical variables? We have options Make a graph that shows the distribution of people\u2019s weight . \u2013 Describe the distribution of weight . Make sure to describe the shape , center and spread of the distribution. Options can be added to plotting functions to change their appearance. The code below includes the nint option which controls the number of intervals in a numerical plot. \u2013 Options, also known as arguments , are additional pieces of information you provide to a function, and separated by commas. Type the command below on your console and then answer the questions that follow: histogram(~weight, data = cdc, nint = 3) How did including the option nint = 3 change the histogram ? Does setting nint = 3 impact how you would describe the shape, center and spread? Try other values for nint . What value produced the best graph? Why? How often do people text & drive? Make a graph that shows how often people in our data texted while driving. \u2013 What does the y-axis represent? \u2013 What does the x-axis tell us? \u2013 Would you say that most people never texted while driving? What does the word most mean? \u2013 Approximately what percent of the people texted while driving for 20 or more days? (Hint: There's 13,677 students in our data.) Does texting and driving differ by gender? Fill in the blanks with the correct variables to create a side-by-side bargraph: bargraph (~ ____ , data = ____ , groups = ____ ) Write a sentence explaining how boys and girls differ when it comes to texting while driving. Would you say that most girls never text and drive? Would you say that most boys never text and drive? How did including the groups argument in your code change the graph? Do males and females have similar heights? To answer this, what we'd like to do is visualize the distributions of heights, separately, for males and females. \u2013 This way, we can easily compare them. Use the groups argument to create a histogram for the height of males and females. \u2013 Can you use this graphic to answer the question at the top of the slide? Why or why not? \u2013 Is grouping numeric values, such as heights, as helpful as grouping categorical variables, such as texting & driving? Do males and females have similar heights?, continued Why does this work for bargraphs but not histograms? \u2013 The groups argument uses color to differentiate between groups. - With bargraphs, each group is split with bars next to each other on teh x-axis. - With histograms, the x-axis is a continuous set of numbers so the bars overlap making it difficult to compare center and spread. Fill in the blanks with the correct variables to create a split histogram to answer the questions below: histogram (~ ____ | ____ , data = ____ ) Do you think males and females have similar heights? Use the plot you create to justify your answer. Just like we did for the histogram , is it possible to create a split bargraph ? Try to create a bargraph of drive_text that's split by gender to find out. On your own: In this lab, we looked at the texting & driving habits of boys and girls. What other factors do you think might affect how often people text and drive? \u2013 Choose one variable from the cdc data, make a graph, and use the graph to describe how drive_text use differs with this variable.","title":"Lab 1B: Get the Picture?"},{"location":"unit1/lab1b/#lab-1b-get-the-picture","text":"Directions: Follow along with the slides, completing the questions in blue on your computer, and answering the questions in red in your journal.","title":"Lab 1B - Get the picture?"},{"location":"unit1/lab1b/#whered-we-leave-off","text":"In the previous lab, we started to get acquainted with the layout of RStudio and some of the commands. In this lab, we'll learn about different types of variables. \u2013 Such as those that are measured by numbers and others that have values that are categories. We'll also look at ways to visualize these different types of data using plots (a word data scientists use interchangeably with the word graph ). Find the History tab in RStudio and click on it. Figure out how to use the information to reload the cdc data.","title":"Where'd we leave off ..."},{"location":"unit1/lab1b/#variable-types","text":"Numerical variables have values that are measured in units. Categorical Variables have values that describe or categorize our observations. View your cdc data and find the columns for height and gender (use the History pane again if you need help to View your data). \u2013 Is height a numerical or categorical variable? Why? \u2013 Is gender a numerical or categorical variable? Why? \u2013 List either the different categories or what you think the measured units are for height and gender .","title":"Variable Types"},{"location":"unit1/lab1b/#which-is-which","text":"Run the code you used in the previous lab to display the names of your cdc data's variables (Use the code displayed in the History pane to resubmit previously typed commands). Use the code's output to help you complete the following: \u2013 Write down 3 variables that you think are categorical variables and why. \u2013 Write down 3 variables that you think are numerical variables and why.","title":"Which is which?"},{"location":"unit1/lab1b/#data-structures","text":"One way to get a good summary of your data is to look at the data's structure . \u2013 One way to view this info would be to click on the little blue arrow next to cdc in the Environment pane. \u2013 Another way would be to run the following in the console: str(cdc) Look at the str ucture of your cdc data and answer: What information does the str function output? Were you able to correctly guess which variables were categorical and numeric? Which ones did you mislabel?","title":"Data Structures"},{"location":"unit1/lab1b/#visualizing-data","text":"Visualizing data is a really helpful way to learn about our variables. Choose one numeric and one categorical variable from the data and create both a bargraph and a histogram for each variable. \u2013 Which function, either bargraph or histogram , is better at visualizing categorical variables? Which is better at visualizing numerical variables?","title":"Visualizing data"},{"location":"unit1/lab1b/#we-have-options","text":"Make a graph that shows the distribution of people\u2019s weight . \u2013 Describe the distribution of weight . Make sure to describe the shape , center and spread of the distribution. Options can be added to plotting functions to change their appearance. The code below includes the nint option which controls the number of intervals in a numerical plot. \u2013 Options, also known as arguments , are additional pieces of information you provide to a function, and separated by commas. Type the command below on your console and then answer the questions that follow: histogram(~weight, data = cdc, nint = 3) How did including the option nint = 3 change the histogram ? Does setting nint = 3 impact how you would describe the shape, center and spread? Try other values for nint . What value produced the best graph? Why?","title":"We have options"},{"location":"unit1/lab1b/#how-often-do-people-text-drive","text":"Make a graph that shows how often people in our data texted while driving. \u2013 What does the y-axis represent? \u2013 What does the x-axis tell us? \u2013 Would you say that most people never texted while driving? What does the word most mean? \u2013 Approximately what percent of the people texted while driving for 20 or more days? (Hint: There's 13,677 students in our data.)","title":"How often do people text &amp; drive?"},{"location":"unit1/lab1b/#does-texting-and-driving-differ-by-gender","text":"Fill in the blanks with the correct variables to create a side-by-side bargraph: bargraph (~ ____ , data = ____ , groups = ____ ) Write a sentence explaining how boys and girls differ when it comes to texting while driving. Would you say that most girls never text and drive? Would you say that most boys never text and drive? How did including the groups argument in your code change the graph?","title":"Does texting and driving differ by gender?"},{"location":"unit1/lab1b/#do-males-and-females-have-similar-heights","text":"To answer this, what we'd like to do is visualize the distributions of heights, separately, for males and females. \u2013 This way, we can easily compare them. Use the groups argument to create a histogram for the height of males and females. \u2013 Can you use this graphic to answer the question at the top of the slide? Why or why not? \u2013 Is grouping numeric values, such as heights, as helpful as grouping categorical variables, such as texting & driving?","title":"Do males and females have similar heights?"},{"location":"unit1/lab1b/#do-males-and-females-have-similar-heights-continued","text":"Why does this work for bargraphs but not histograms? \u2013 The groups argument uses color to differentiate between groups. - With bargraphs, each group is split with bars next to each other on teh x-axis. - With histograms, the x-axis is a continuous set of numbers so the bars overlap making it difficult to compare center and spread. Fill in the blanks with the correct variables to create a split histogram to answer the questions below: histogram (~ ____ | ____ , data = ____ ) Do you think males and females have similar heights? Use the plot you create to justify your answer. Just like we did for the histogram , is it possible to create a split bargraph ? Try to create a bargraph of drive_text that's split by gender to find out.","title":"Do males and females have similar heights?, continued"},{"location":"unit1/lab1b/#on-your-own","text":"In this lab, we looked at the texting & driving habits of boys and girls. What other factors do you think might affect how often people text and drive? \u2013 Choose one variable from the cdc data, make a graph, and use the graph to describe how drive_text use differs with this variable.","title":"On your own:"},{"location":"unit1/lab1c/","text":"Lab 1C - Export, Upload, Import Directions: Follow along with the slides, completing the questions in blue on your computer, and answering the questions in red in your journal. Whose data? Our data. Throughout the previous labs, we've been using data that was already loaded in RStudio. \u2013 But what if we want to analyze our own data? This lab is all about learning how to load our own participatory sensing data into RStudio. Export, upload, import Before we can perform any analysis, we have to load data into R . When we want to get our participatory sensing data into RStudio, we: \u2013 Export the data from your class\u2019 campaign page. \u2013 Upload data to RStudio server \u2013 Import the data into R's working memory Note: You can watch the following video for a step-by-step walk-through of the process: https://www.youtube.com/embed/4mChtv5qy1g Exporting To begin, go to the IDS Tools page \u2013 Click on the Campaign Manager \u2013 Fill in your username and password and click \"Sign in\". If you forget your username or password, ask your teacher to remind you. Campaign Manager After logging in, your screen should look similar to this. Click on the dropdown arrow for the campaign you are interested in downloading. \u2013 At this point in the course, it will most likely be the Food Habits campaign. Dropdown Arrow The options for the dropdown menu will look like this: Click on the option labeled Export Data. \u2013 Remember where you save your file! Exporting When you clicked the Export link, a .csv file was saved on your computer. Now that the file is on your computer, we need to upload it into RStudio. Uploading Look at the four different panes in RStudio. \u2013 Find the pane with a Files tab. \u2013 Click it! Click the button on the Files pane that says \"Upload\". \u2013 Click on \"Choose File\" and find the SurveyResponses.csv file you saved to your computer. \u2013 Hit the OK button. Voila! \u2013 If you look in the Files pane, you should be able to find your data! Upload vs. Import By Uploading your data into RStudio you've really only given yourself access to it. \u2013 Don't believe me? Look at the Environment pane ... where's your data? To actually use the data we need to Import it into your computer's memory. To compute more quickly and efficiently, R will only keep a few data sets stored in its memory at a time. \u2013 By importing data, you are telling R that this is a data set that is important to store it in its memory so you can use it. Importing On the Files pane, find the data you want to import . Click on the name of the file and choose the option \"Import Data set...\" Data Preview You can give your data a name using the Name: field in the lower left corner. What's in a name? The name you give your data is what you will use when you write code to analyze your data. \u2013 Good names are short and descriptive. \u2013 For your food habits campaign, some good names to use would be \"foodhabits\" or even just \"food\". When you're ready, click the Import button. read.csv() After you click Import you might notice something appeared in your console. data.file <- read_csv(\"~/SurveyResponse.csv\") View(data.file) This is the actual code RStudio uses to read your data when you clicked the Import button. \u2013 So instead of using the RStudio buttons, we can actually Import by writing code similar to what was output into the console! \u2013 This will come in handy later in the course. A word on staying organized... The Files tab has a few other features to help keep you organized. \u2013 SurveyResponse probably isn't the best name for your data. Click Rename to give it a clearer name. \u2013 Often, it\u2019s helpful to give your data file the same name as when you import your data. \u2013 So in this case, we could name our data file food.csv . Analysis Time After you Export , Upload , Import your data, you're ready to analyze. View your data, then select a variable and try to make an appropriate plot for that variable.","title":"Lab 1C: Export, Upload, Import"},{"location":"unit1/lab1c/#lab-1c-export-upload-import","text":"Directions: Follow along with the slides, completing the questions in blue on your computer, and answering the questions in red in your journal.","title":"Lab 1C - Export, Upload, Import"},{"location":"unit1/lab1c/#whose-data-our-data","text":"Throughout the previous labs, we've been using data that was already loaded in RStudio. \u2013 But what if we want to analyze our own data? This lab is all about learning how to load our own participatory sensing data into RStudio.","title":"Whose data? Our data."},{"location":"unit1/lab1c/#export-upload-import","text":"Before we can perform any analysis, we have to load data into R . When we want to get our participatory sensing data into RStudio, we: \u2013 Export the data from your class\u2019 campaign page. \u2013 Upload data to RStudio server \u2013 Import the data into R's working memory Note: You can watch the following video for a step-by-step walk-through of the process: https://www.youtube.com/embed/4mChtv5qy1g","title":"Export, upload, import"},{"location":"unit1/lab1c/#exporting","text":"To begin, go to the IDS Tools page \u2013 Click on the Campaign Manager \u2013 Fill in your username and password and click \"Sign in\". If you forget your username or password, ask your teacher to remind you.","title":"Exporting"},{"location":"unit1/lab1c/#campaign-manager","text":"After logging in, your screen should look similar to this. Click on the dropdown arrow for the campaign you are interested in downloading. \u2013 At this point in the course, it will most likely be the Food Habits campaign.","title":"Campaign Manager"},{"location":"unit1/lab1c/#dropdown-arrow","text":"The options for the dropdown menu will look like this: Click on the option labeled Export Data. \u2013 Remember where you save your file!","title":"Dropdown Arrow"},{"location":"unit1/lab1c/#exporting_1","text":"When you clicked the Export link, a .csv file was saved on your computer. Now that the file is on your computer, we need to upload it into RStudio.","title":"Exporting"},{"location":"unit1/lab1c/#uploading","text":"Look at the four different panes in RStudio. \u2013 Find the pane with a Files tab. \u2013 Click it! Click the button on the Files pane that says \"Upload\". \u2013 Click on \"Choose File\" and find the SurveyResponses.csv file you saved to your computer. \u2013 Hit the OK button. Voila! \u2013 If you look in the Files pane, you should be able to find your data!","title":"Uploading"},{"location":"unit1/lab1c/#upload-vs-import","text":"By Uploading your data into RStudio you've really only given yourself access to it. \u2013 Don't believe me? Look at the Environment pane ... where's your data? To actually use the data we need to Import it into your computer's memory. To compute more quickly and efficiently, R will only keep a few data sets stored in its memory at a time. \u2013 By importing data, you are telling R that this is a data set that is important to store it in its memory so you can use it.","title":"Upload vs. Import"},{"location":"unit1/lab1c/#importing","text":"On the Files pane, find the data you want to import . Click on the name of the file and choose the option \"Import Data set...\"","title":"Importing"},{"location":"unit1/lab1c/#data-preview","text":"You can give your data a name using the Name: field in the lower left corner.","title":"Data Preview"},{"location":"unit1/lab1c/#whats-in-a-name","text":"The name you give your data is what you will use when you write code to analyze your data. \u2013 Good names are short and descriptive. \u2013 For your food habits campaign, some good names to use would be \"foodhabits\" or even just \"food\". When you're ready, click the Import button.","title":"What's in a name?"},{"location":"unit1/lab1c/#readcsv","text":"After you click Import you might notice something appeared in your console. data.file <- read_csv(\"~/SurveyResponse.csv\") View(data.file) This is the actual code RStudio uses to read your data when you clicked the Import button. \u2013 So instead of using the RStudio buttons, we can actually Import by writing code similar to what was output into the console! \u2013 This will come in handy later in the course.","title":"read.csv()"},{"location":"unit1/lab1c/#a-word-on-staying-organized","text":"The Files tab has a few other features to help keep you organized. \u2013 SurveyResponse probably isn't the best name for your data. Click Rename to give it a clearer name. \u2013 Often, it\u2019s helpful to give your data file the same name as when you import your data. \u2013 So in this case, we could name our data file food.csv .","title":"A word on staying organized..."},{"location":"unit1/lab1c/#analysis-time","text":"After you Export , Upload , Import your data, you're ready to analyze. View your data, then select a variable and try to make an appropriate plot for that variable.","title":"Analysis Time"},{"location":"unit1/lab1d/","text":"Lab 1D - Zooming Through Data Directions: Follow along with the slides, completing the questions in blue on your computer, and answering the questions in red in your journal. Data with Clarity Previously, we've looked at graphs of entire variables (by looking at all of their values). \u2013 Doing this is helpful to get a big picture idea of our data. In this lab, we'll learn how to zoom in on our data by learning how to subset. \u2013 We'll also learn a few ways to manipulate the plots we've been making to make them easier to use for analyses. Import the data from your class' Food Habits campaign and name it food . Another plotting function A dotPlot is another plot that can be used to analyze a numerical variable. \u2013 Dotplots are better suited for smaller datasets. If datasets are too large, the dots become too small to see. \u2013 Similarly, distributions with a large spread might impact the readability of the plot. Use the dotPlot() function to create a dotPlot of the amount of sugar in our food data. \u2013 The code to create a dotPlot is exactly like you\u2019d use to make a histogram . \u2013 Make sure to use a capital P in dotPlot . More options While a dotPlot should conserve the exact value of each data point, sometimes it behaves like a histogram in that it lumps values together. Create a more accurate dotPlot by including the nint option. \u2013 Set nint equal to the maximum value for sugar minus the minimum value for sugar plus one. On your food data spreadsheet, click on the sugar header to sort in ascending order (to obtain minimum). Click on the sugar header again to sort in descending order (to obtain maximum). \u2013 Use your History pane to see how we included the option nint with the histogram function. Pro-tip: If the dotPlot comes out looking wonky, try changing the value of the character expansion option, cex . \u2013 The default value is 1 . Try a few values between 0 and 1 and a few more values larger than 1 . Splitting data sets In lab 1B , we learned that we can facet (or split) our data based on a categorical variable. Split the dotPlot displaying the distribution of grams of sugar in two, by faceting on our observations\u2019 salty_sweet variable. \u2013 Describe how R decides which observations go into the left or right plot. \u2013 What does each dot in the plot represent? Altering the layout It would be much easier to compare the sugar levels of salty and sweet snacks if the dotPlots were stacked on top of one another. We can change the layout of our separated plots by including the layout option in our dotPlot function. \u2013 **Add the following option to the code you used to create the dotPlot split by salty_sweet . layout = c(1,2) Hint: Use a similar syntax used with the nint option to add the layout option to the dotPlot function. Subsetting Subsetting is a term we use to describe the process of looking at only the data that conforms to some set of rules: \u2013 Geologists may subset earthquake data by looking at only large earthquakes. \u2013 Stock market traders may subset their trading data by looking only at the previous day's trades. There's many ways to subset data using RStudio, we'll focus on learning the most common methods. The filter function Creating two plots, one for Salty and one for Sweet is useful for comparing Salty and Sweet . What if we want to examine one group by itself? The line of code below creates a subset of the food dataset containing only Salty snacks. We will break it down piece by piece in the next few slides. Run the line of code below: food_salty <- filter(____ , ____ == \"Salty\") View food_salty and write down the number of observations in it. So what's really going on? Coding in R is really just about supplying directions in a way that R understands. \u2013 We'll start by focusing on everything to the right of the \"<-\" symbol food_salty <- filter(____ , ____ == \"Salty\") filter() tells R that we're going to look at only the values in our data that follow a rule . The first blank should be the data we're going to filter down into a smaller set (Based on our rule). salty_sweet == \"Salty\" is the rule to follow. 3 parts of defining rules We can decompose our rule, salty_sweet == \"Salty\" , into 3 parts: (1) salty_sweet , is the particular variable we want to use to select our subset. (2) \"Salty\" is the value of the variable that we want to select. We only want to see data with the value Salty for the variable salty_sweet . (3) == describes how we want to relate our variable ( salty_sweet ) to our value ( \"Salty\" ). In this case, we want values of salty_sweet that are exactly equal to \"Salty\" . Notice: Values (that are also words) have quotation marks around them. Variables do not. More on == We can use the head() function to help us see what's happening when we write salty_sweet == \"Salty\" . \u2013 head() returns the values of the first 6 observations. \u2013 The tail() function returns the last 6 observations. Run the following code and answer the question below: head(~salty_sweet == \"Salty\", data = food) What do the values TRUE and FALSE tell us about how our rule applies to the first six snacks in our data? Which of the first six observations were Salty ? Saving values To use our subset data we need to save it first. \u2013 When we save something in R what we are really doing is giving a value, or set of values, a specific name for us to use later. The arrow <- is called the \"assignment\" operator. It assigns names (on the left) to values (on the right) \u2013 We now focus on everything to the left of, and including, the \"<-\" symbol food_salty <- filter(____ , ____ == \"Salty\") Saving our subset food_salty <- filter(____ , ____ == \"Salty\") This code then: \u2013 takes our subset data, (everything to the right of \"<-\") ... \u2013 and assigns the subset data, by using the arrow \"<-\" ... \u2013 the name food_salty . We can now use food_salty to do anything we could do with the regular food data ... \u2013 but only including those snacks who reported being Salty . As a result of assigning the subset data to food_salty , food_salty now appears in the Environment pane. Whenever data is assigned to a variable name, that variable name will appear in the Environment pane. Use food_salty to make a dotPlot of the sodium in our Salty snacks. Including more filters We often want to filter our data based on multiple rules. \u2013 For instance, we might want to filter our food data based on the food being salty AND having less than 200 calories. We can include multiple filters to our subsets by separating each rule with a comma like so: my_sub <- filter(food , salty_sweet == \"Salty\", calories < 200) View the my_sub data we filtered in the above line of code and verify that it only includes salty snacks that have less than 200 calories. Put it all together Use an appropriate dotPlot to answer each of the following questions: \u2013 About how much sugar does the typical sweet snack have? \u2013 How does the typical amount of sugar compare when healthy_level < 3 and when healthy_level > 3 ? Because you are now working with subsets of data, it is important to label our plots and make this distinction. \u2013 We can use the main option to add a title to our plots Add the following option to the code you used to create the dotPlot of the sugar in Sweet snacks. main = \"Distribution of sugar in sweet snacks\"","title":"Lab 1D: Zooming Through Data"},{"location":"unit1/lab1d/#lab-1d-zooming-through-data","text":"Directions: Follow along with the slides, completing the questions in blue on your computer, and answering the questions in red in your journal.","title":"Lab 1D - Zooming Through Data"},{"location":"unit1/lab1d/#data-with-clarity","text":"Previously, we've looked at graphs of entire variables (by looking at all of their values). \u2013 Doing this is helpful to get a big picture idea of our data. In this lab, we'll learn how to zoom in on our data by learning how to subset. \u2013 We'll also learn a few ways to manipulate the plots we've been making to make them easier to use for analyses. Import the data from your class' Food Habits campaign and name it food .","title":"Data with Clarity"},{"location":"unit1/lab1d/#another-plotting-function","text":"A dotPlot is another plot that can be used to analyze a numerical variable. \u2013 Dotplots are better suited for smaller datasets. If datasets are too large, the dots become too small to see. \u2013 Similarly, distributions with a large spread might impact the readability of the plot. Use the dotPlot() function to create a dotPlot of the amount of sugar in our food data. \u2013 The code to create a dotPlot is exactly like you\u2019d use to make a histogram . \u2013 Make sure to use a capital P in dotPlot .","title":"Another plotting function"},{"location":"unit1/lab1d/#more-options","text":"While a dotPlot should conserve the exact value of each data point, sometimes it behaves like a histogram in that it lumps values together. Create a more accurate dotPlot by including the nint option. \u2013 Set nint equal to the maximum value for sugar minus the minimum value for sugar plus one. On your food data spreadsheet, click on the sugar header to sort in ascending order (to obtain minimum). Click on the sugar header again to sort in descending order (to obtain maximum). \u2013 Use your History pane to see how we included the option nint with the histogram function. Pro-tip: If the dotPlot comes out looking wonky, try changing the value of the character expansion option, cex . \u2013 The default value is 1 . Try a few values between 0 and 1 and a few more values larger than 1 .","title":"More options"},{"location":"unit1/lab1d/#splitting-data-sets","text":"In lab 1B , we learned that we can facet (or split) our data based on a categorical variable. Split the dotPlot displaying the distribution of grams of sugar in two, by faceting on our observations\u2019 salty_sweet variable. \u2013 Describe how R decides which observations go into the left or right plot. \u2013 What does each dot in the plot represent?","title":"Splitting data sets"},{"location":"unit1/lab1d/#altering-the-layout","text":"It would be much easier to compare the sugar levels of salty and sweet snacks if the dotPlots were stacked on top of one another. We can change the layout of our separated plots by including the layout option in our dotPlot function. \u2013 **Add the following option to the code you used to create the dotPlot split by salty_sweet . layout = c(1,2) Hint: Use a similar syntax used with the nint option to add the layout option to the dotPlot function.","title":"Altering the layout"},{"location":"unit1/lab1d/#subsetting","text":"Subsetting is a term we use to describe the process of looking at only the data that conforms to some set of rules: \u2013 Geologists may subset earthquake data by looking at only large earthquakes. \u2013 Stock market traders may subset their trading data by looking only at the previous day's trades. There's many ways to subset data using RStudio, we'll focus on learning the most common methods.","title":"Subsetting"},{"location":"unit1/lab1d/#the-filter-function","text":"Creating two plots, one for Salty and one for Sweet is useful for comparing Salty and Sweet . What if we want to examine one group by itself? The line of code below creates a subset of the food dataset containing only Salty snacks. We will break it down piece by piece in the next few slides. Run the line of code below: food_salty <- filter(____ , ____ == \"Salty\") View food_salty and write down the number of observations in it.","title":"The filter function"},{"location":"unit1/lab1d/#so-whats-really-going-on","text":"Coding in R is really just about supplying directions in a way that R understands. \u2013 We'll start by focusing on everything to the right of the \"<-\" symbol food_salty <- filter(____ , ____ == \"Salty\") filter() tells R that we're going to look at only the values in our data that follow a rule . The first blank should be the data we're going to filter down into a smaller set (Based on our rule). salty_sweet == \"Salty\" is the rule to follow.","title":"So what's really going on?"},{"location":"unit1/lab1d/#3-parts-of-defining-rules","text":"We can decompose our rule, salty_sweet == \"Salty\" , into 3 parts: (1) salty_sweet , is the particular variable we want to use to select our subset. (2) \"Salty\" is the value of the variable that we want to select. We only want to see data with the value Salty for the variable salty_sweet . (3) == describes how we want to relate our variable ( salty_sweet ) to our value ( \"Salty\" ). In this case, we want values of salty_sweet that are exactly equal to \"Salty\" . Notice: Values (that are also words) have quotation marks around them. Variables do not.","title":"3 parts of defining rules"},{"location":"unit1/lab1d/#more-on","text":"We can use the head() function to help us see what's happening when we write salty_sweet == \"Salty\" . \u2013 head() returns the values of the first 6 observations. \u2013 The tail() function returns the last 6 observations. Run the following code and answer the question below: head(~salty_sweet == \"Salty\", data = food) What do the values TRUE and FALSE tell us about how our rule applies to the first six snacks in our data? Which of the first six observations were Salty ?","title":"More on =="},{"location":"unit1/lab1d/#saving-values","text":"To use our subset data we need to save it first. \u2013 When we save something in R what we are really doing is giving a value, or set of values, a specific name for us to use later. The arrow <- is called the \"assignment\" operator. It assigns names (on the left) to values (on the right) \u2013 We now focus on everything to the left of, and including, the \"<-\" symbol food_salty <- filter(____ , ____ == \"Salty\")","title":"Saving values"},{"location":"unit1/lab1d/#saving-our-subset","text":"food_salty <- filter(____ , ____ == \"Salty\") This code then: \u2013 takes our subset data, (everything to the right of \"<-\") ... \u2013 and assigns the subset data, by using the arrow \"<-\" ... \u2013 the name food_salty . We can now use food_salty to do anything we could do with the regular food data ... \u2013 but only including those snacks who reported being Salty . As a result of assigning the subset data to food_salty , food_salty now appears in the Environment pane. Whenever data is assigned to a variable name, that variable name will appear in the Environment pane. Use food_salty to make a dotPlot of the sodium in our Salty snacks.","title":"Saving our subset"},{"location":"unit1/lab1d/#including-more-filters","text":"We often want to filter our data based on multiple rules. \u2013 For instance, we might want to filter our food data based on the food being salty AND having less than 200 calories. We can include multiple filters to our subsets by separating each rule with a comma like so: my_sub <- filter(food , salty_sweet == \"Salty\", calories < 200) View the my_sub data we filtered in the above line of code and verify that it only includes salty snacks that have less than 200 calories.","title":"Including more filters"},{"location":"unit1/lab1d/#put-it-all-together","text":"Use an appropriate dotPlot to answer each of the following questions: \u2013 About how much sugar does the typical sweet snack have? \u2013 How does the typical amount of sugar compare when healthy_level < 3 and when healthy_level > 3 ? Because you are now working with subsets of data, it is important to label our plots and make this distinction. \u2013 We can use the main option to add a title to our plots Add the following option to the code you used to create the dotPlot of the sugar in Sweet snacks. main = \"Distribution of sugar in sweet snacks\"","title":"Put it all together"},{"location":"unit1/lab1e/","text":"Lab 1E - What's the Relationship? Directions: Follow along with the slides, completing the questions in blue on your computer, and answering the questions in red in your journal. Finding patterns in data. To discover ( really ) interesting observations or relationships in data, we need to find them! \u2013 Which is difficult if we only look at the raw data. The best tool for finding patterns is often ... your own eyes. \u2013 Plots are an excellent way to help your eye search for patterns. In this lab, we'll learn how to include more variables in our plots to make them more informative. Import the data from your class' Food Habits campaign and name it food . Where's the variables? How many variables were used to create this plot? Which variables were used and how were they used? Multiple variable plots The previous graph is an example of a multiple variable plot , which means that more than a single variable was used. In this case: Variable 1: height Variable 2: gender Multiple variable plots are tools for finding relationships between data. Let's take our food data and make some new multiple variable plots you haven't created before! Scatterplots Scatterplots are useful for viewing how one numerical variable relates to another numerical variable. Creating scatterplots Fill in the blanks to create a scatterplot with sodium on the y-axis and sugar on the x-axis. xyplot(____ ~ ____, data = food) Scatterplots in action Use a scatterplot to answer the following questions: \u2013 Do snacks that have more protein also have more calories ? Why do you think that? \u2013 What happens if you swap the protein and calories variables in your code? Does the relationship between the variables change? \u2013 Does the relationship between protein and calories change when the snack is either Salty or Sweet ? Write down the code you used to answer this question. 4-variable scatterplots When we make scatterplots, we can include: \u2013 1 numerical variable on the x-axis \u2013 1 numerical variable on the y-axis \u2013 Use 1 categorical variable to facet our scatterplot \u2013 Change the color of the points based on another categorical variable To change the color of our points, we can include the groups argument much like we did for bargraphs (use the search feature in the History pane if you need help). Create a scatterplot that uses these 4 variables: sodium , sugar , cost , salty_sweet . Multiple facets It can sometimes be helpful to facet on more than 1 variable. \u2013 Splitting the data using 2 facets can give us additional insights that might otherwise be hidden. Create a dotPlot or histogram of the calories variable, but facet the data using: healthy_level + salty_sweet How does the healthy_level of a Salty or Sweet snack impact the number of calories in the snack? Although we are treating healthy_level as a categorical variable, R recongizes it as a numerical variable. \u2013 Verify this using the str function. \u2013 Notice that the faceted histograms or dotPlots do not have labels but rather tick-marks. \u2013 You will have the opportunity to convert the healthy_level variable into a factor later on. Faceting your data on a numerical variable is NOT recommended. \u2013 Numerical variables often have so many different values that they overwhelm the plot and make it hard to read. On your own Answer the following questions by creating an appropriate graph or graphs: \u2013 Do healthier snacks have more or less ingredients than less healthy snacks? \u2013 What other variables seem to be related to the number of ingredients of a snack? Describe their relationships.","title":"Lab 1E: What\u2019s the Relationship?"},{"location":"unit1/lab1e/#lab-1e-whats-the-relationship","text":"Directions: Follow along with the slides, completing the questions in blue on your computer, and answering the questions in red in your journal.","title":"Lab 1E - What's the Relationship?"},{"location":"unit1/lab1e/#finding-patterns-in-data","text":"To discover ( really ) interesting observations or relationships in data, we need to find them! \u2013 Which is difficult if we only look at the raw data. The best tool for finding patterns is often ... your own eyes. \u2013 Plots are an excellent way to help your eye search for patterns. In this lab, we'll learn how to include more variables in our plots to make them more informative. Import the data from your class' Food Habits campaign and name it food .","title":"Finding patterns in data."},{"location":"unit1/lab1e/#wheres-the-variables","text":"How many variables were used to create this plot? Which variables were used and how were they used?","title":"Where's the variables?"},{"location":"unit1/lab1e/#multiple-variable-plots","text":"The previous graph is an example of a multiple variable plot , which means that more than a single variable was used. In this case: Variable 1: height Variable 2: gender Multiple variable plots are tools for finding relationships between data. Let's take our food data and make some new multiple variable plots you haven't created before!","title":"Multiple variable plots"},{"location":"unit1/lab1e/#scatterplots","text":"Scatterplots are useful for viewing how one numerical variable relates to another numerical variable.","title":"Scatterplots"},{"location":"unit1/lab1e/#creating-scatterplots","text":"Fill in the blanks to create a scatterplot with sodium on the y-axis and sugar on the x-axis. xyplot(____ ~ ____, data = food)","title":"Creating scatterplots"},{"location":"unit1/lab1e/#scatterplots-in-action","text":"Use a scatterplot to answer the following questions: \u2013 Do snacks that have more protein also have more calories ? Why do you think that? \u2013 What happens if you swap the protein and calories variables in your code? Does the relationship between the variables change? \u2013 Does the relationship between protein and calories change when the snack is either Salty or Sweet ? Write down the code you used to answer this question.","title":"Scatterplots in action"},{"location":"unit1/lab1e/#4-variable-scatterplots","text":"When we make scatterplots, we can include: \u2013 1 numerical variable on the x-axis \u2013 1 numerical variable on the y-axis \u2013 Use 1 categorical variable to facet our scatterplot \u2013 Change the color of the points based on another categorical variable To change the color of our points, we can include the groups argument much like we did for bargraphs (use the search feature in the History pane if you need help). Create a scatterplot that uses these 4 variables: sodium , sugar , cost , salty_sweet .","title":"4-variable scatterplots"},{"location":"unit1/lab1e/#multiple-facets","text":"It can sometimes be helpful to facet on more than 1 variable. \u2013 Splitting the data using 2 facets can give us additional insights that might otherwise be hidden. Create a dotPlot or histogram of the calories variable, but facet the data using: healthy_level + salty_sweet How does the healthy_level of a Salty or Sweet snack impact the number of calories in the snack? Although we are treating healthy_level as a categorical variable, R recongizes it as a numerical variable. \u2013 Verify this using the str function. \u2013 Notice that the faceted histograms or dotPlots do not have labels but rather tick-marks. \u2013 You will have the opportunity to convert the healthy_level variable into a factor later on. Faceting your data on a numerical variable is NOT recommended. \u2013 Numerical variables often have so many different values that they overwhelm the plot and make it hard to read.","title":"Multiple facets"},{"location":"unit1/lab1e/#on-your-own","text":"Answer the following questions by creating an appropriate graph or graphs: \u2013 Do healthier snacks have more or less ingredients than less healthy snacks? \u2013 What other variables seem to be related to the number of ingredients of a snack? Describe their relationships.","title":"On your own"},{"location":"unit1/lab1f/","text":"Lab 1F - A Diamond in the Rough Directions: Follow along with the slides, completing the questions in blue on your computer, and answering the questions in red in your journal. Messy data? Get used to it Since lab 1, the data we've been using has been pretty clean . Why do we call it clean ? \u2013 Variables were named so we could understand what they were about. \u2013 There didn't seem to be any typos in the values. \u2013 Numerical variables were considered numbers. \u2013 Categorical variables were composed of categories. Unfortunately, more often than not, data is messy until YOU clean it. In this lab, we'll learn a few essentials for cleaning dirty data. Messy data? What do we mean by messy data? Variables might have non-descriptive names \u2013 Var01 , V2 , a , ... Categorical variables might have misspelled categories \u2013 \"blue\" , \"Blue\" , \"blu\" , ... Numerical variables might have been input incorrectly . For example, if we're talking about people's height in inches: \u2013 64.7 , 6.86 , 676 , ... Numerical variables might be incorrectly coded as categorical variables (or vice-versa) \u2013 \"64.7\", \"68.6\", \"67.6\" The American Time Use Survey To show you what dirty data looks like, we'll check out the American Time Use Survey , or ATU survey. What is ATU survey? \u2013 It's a survey conducted by the US government (Specifically the Bureau of Labor Statistics). \u2013 They survey thousands of people to find out exactly what activities they do throughout a single day. \u2013 These thousands of people combined together give an idea about how much time the typical person living in the US spends doing various activites. Load and go: Type the following commands into your console: data(atu_dirty) View(atu_dirty) Just by viewing the data, what parts of our ATU data do you think need cleaning? Description of ATU Variables The description of the actual variables: \u2013 caseid : Anonymous ID of survey taker. \u2013 V1 : The age of the respondent. \u2013 V2 : The gender of the respondent. \u2013 V3 : Whether the person is employed full-time or part-time. \u2013 V4 : Whether the person has a physical difficulty. \u2013 V5 : How long the person sleeps, in minutes. \u2013 V6 : How long the survey taker spent on homework, in minutes. \u2013 V7 : How long the respondent spent socializing, in minutes. New name, same old data To fix the variable names, we need to assign a new set of names in place of the old ones. \u2013 Below is an example of the rename function: atu_cleaner <- rename(atu_dirty, age = V1, gender = V2) Use the example code and the variable information on the previous slide to rename the rest of the variables in atu_dirty . Write down the new names you chose for the rest of the variables in atu_dirty . \u2013 Names should be short, contain no spaces and describe what the variable is related to. So use abbreviations to your heart's content. Next up: Strings In programming, a string is sort of like a word . \u2013 It's a value made up of characters (i.e. letters) The following are examples of strings. Notice that each string has quotes before and after. \"string\" \"A1B2c3\" \"Hot Cocoa\" \"0015\" Numbers are words? (Sometimes) In some cases, R will treat values that look like numbers as if they were strings . Sometimes we do this on purpose. \u2013 For example, we can code Yes/No variables as \"1\" / \"0\" . Sometimes we don't mean for this to happen. \u2013 The number of siblings a person has should not be a string. Look at the str ucture of your data and the variable descriptions from a few slides back: \u2013 Write down the variables that should be numeric but are improperly coded as strings or characters . Changing strings into numbers To fix this problem, we need to tell R to think of our \"numeric\" variables as numeric variables. We can do this with the as.numeric function. \u2013 An example using this function is below: as.numeric(\"3.14\") ## [1] 3.14 Notice: We started with a string, \"3.14\" , but as.numeric was able to turn it back into a number. Mutating in action Look at the variables you thought should be numeric and select one. Then fill in the blanks below to see how we can correctly code it as a number: atu_cleaner <- mutate(atu_cleaner, age = as.numeric(age), ___ = as.numeric(___)) Once you have this code working, use a similar line of code to correctly code the other numeric variables as numbers. Deciphering Categorical Variables We mentioned earlier that we sometimes code categorical variables as numbers. \u2013 For example, our gender variable uses \"01\" and \"02\" for \"Male\" and \"Female\" , respectively. It's often much easier to analyze and interpret when we use more descriptive categories, such as \"Male\" and \"Female\" . Factors and Levels R has a special name for categorical variables, called factors . R also has a special name for the different categories of a categorical variable. \u2013 The individual categories are called levels . To see the levels of gender and their counts type : tally(~gender, data = atu_cleaner) Use similar code as we used above to write down the levels for the three factors in our data. A level by any other name... If we know that '01' means 'Male' and '02' means 'Female' then we can use the following code to recode the levels of gender . Type the following command into your console: atu_cleaner <- mutate(atu_cleaner, gender = recode(gender, \"01\"=\"Male\", \"02\" = \"Female\")) This code is definitely a bit of a mouthful. Let's break it down. Allow me to explain atu_cleaner <- mutate(atu_cleaner, gender = recode(gender, \"01\"=\"Male\", \"02\" = \"Female\")) This code is saying: \u2013 Replace my current version of atu_cleaner ... \u2013 with a mutated one where ... \u2013 the gender variable's levels ... \u2013 have been recoded...\" \u2013 where \"01\" will now be \"Male\" ... \u2013 and \"02\" will now be \"Female\" . Finish it off! Recode the categorical variable about whether the person surveyed had a physical challenge or not. The coding is currently: \u2013 \"01\" : Person surveyed did not have a physical challenge. \u2013 \"02\" : Person surveyed did have a physical challenge. Write a script that: (1) Loads the atu_dirty data set (2) Cleans the the data as we have in this lab (3) Saves a copy of the cleaned data (see next slide). NOTE: You can watch this video to learn about RScripts: https://www.youtube.com/embed/OPqjL9Azmk The final lines The last few lines of your script are extremely important because they will save all of your work. Be sure to View your data and check its str ucture to make sure it looks clean and tidy before saving. Run the code below: atu_clean <- atu_cleaner This code will create a new data frame in your Environment called atu_clean which is a final copy of atu_cleaner . \u2013 If atu_clean is swept from your Environment all of the changes you made will NOT be saved. \u2013 You would need to re-run the script to clean the data again To permanently save your changes you need to save the file as an R data file or .Rda Run the code below: save(atu_clean, file = \"atu_clean.Rda\") Look in your Files pane for the atu_clean.Rda file \u2013 This is as permanent copy of your clean atu data \u2013 To load the data onto your Environment click on the file \u2013 A pop-up window confirming the upload will appear Flex your skills Now that you have learned some cleaning data basics, it\u2019s time to revisit the food data. Run the code below: histogram(~calories | healthy_level, data = food) Use the as.factor() function to convert healthy_level into a categorical variable and re-run the histogram function. \u2013 Notice that the healthy_level categories are now numbers as opposed to tick-marks. This is an improvement but an even better solution would be to recode the categories. Recode the healthy_level categories and re-run the histogram function. \u2013 \"1\" = \"Very Unhealthy\" \u2013 \"2\" = \"Unhealthy\" \u2013 \"3\" = \"Neutral\" \u2013 \"4\" = \"Healthy\" \u2013 \"5\" = \"Very Healthy\" If your food data is cleared from your Environment , the changes that you made to the healthy_level variable will not be saved. To save your changes permanently save your food file as an R data file.","title":"Lab 1F: A Diamond in the Rough"},{"location":"unit1/lab1f/#lab-1f-a-diamond-in-the-rough","text":"Directions: Follow along with the slides, completing the questions in blue on your computer, and answering the questions in red in your journal.","title":"Lab 1F - A Diamond in the Rough"},{"location":"unit1/lab1f/#messy-data-get-used-to-it","text":"Since lab 1, the data we've been using has been pretty clean . Why do we call it clean ? \u2013 Variables were named so we could understand what they were about. \u2013 There didn't seem to be any typos in the values. \u2013 Numerical variables were considered numbers. \u2013 Categorical variables were composed of categories. Unfortunately, more often than not, data is messy until YOU clean it. In this lab, we'll learn a few essentials for cleaning dirty data.","title":"Messy data? Get used to it"},{"location":"unit1/lab1f/#messy-data","text":"What do we mean by messy data? Variables might have non-descriptive names \u2013 Var01 , V2 , a , ... Categorical variables might have misspelled categories \u2013 \"blue\" , \"Blue\" , \"blu\" , ... Numerical variables might have been input incorrectly . For example, if we're talking about people's height in inches: \u2013 64.7 , 6.86 , 676 , ... Numerical variables might be incorrectly coded as categorical variables (or vice-versa) \u2013 \"64.7\", \"68.6\", \"67.6\"","title":"Messy data?"},{"location":"unit1/lab1f/#the-american-time-use-survey","text":"To show you what dirty data looks like, we'll check out the American Time Use Survey , or ATU survey. What is ATU survey? \u2013 It's a survey conducted by the US government (Specifically the Bureau of Labor Statistics). \u2013 They survey thousands of people to find out exactly what activities they do throughout a single day. \u2013 These thousands of people combined together give an idea about how much time the typical person living in the US spends doing various activites.","title":"The American Time Use Survey"},{"location":"unit1/lab1f/#load-and-go","text":"Type the following commands into your console: data(atu_dirty) View(atu_dirty) Just by viewing the data, what parts of our ATU data do you think need cleaning?","title":"Load and go:"},{"location":"unit1/lab1f/#description-of-atu-variables","text":"The description of the actual variables: \u2013 caseid : Anonymous ID of survey taker. \u2013 V1 : The age of the respondent. \u2013 V2 : The gender of the respondent. \u2013 V3 : Whether the person is employed full-time or part-time. \u2013 V4 : Whether the person has a physical difficulty. \u2013 V5 : How long the person sleeps, in minutes. \u2013 V6 : How long the survey taker spent on homework, in minutes. \u2013 V7 : How long the respondent spent socializing, in minutes.","title":"Description of ATU Variables"},{"location":"unit1/lab1f/#new-name-same-old-data","text":"To fix the variable names, we need to assign a new set of names in place of the old ones. \u2013 Below is an example of the rename function: atu_cleaner <- rename(atu_dirty, age = V1, gender = V2) Use the example code and the variable information on the previous slide to rename the rest of the variables in atu_dirty . Write down the new names you chose for the rest of the variables in atu_dirty . \u2013 Names should be short, contain no spaces and describe what the variable is related to. So use abbreviations to your heart's content.","title":"New name, same old data"},{"location":"unit1/lab1f/#next-up-strings","text":"In programming, a string is sort of like a word . \u2013 It's a value made up of characters (i.e. letters) The following are examples of strings. Notice that each string has quotes before and after. \"string\" \"A1B2c3\" \"Hot Cocoa\" \"0015\"","title":"Next up: Strings"},{"location":"unit1/lab1f/#numbers-are-words-sometimes","text":"In some cases, R will treat values that look like numbers as if they were strings . Sometimes we do this on purpose. \u2013 For example, we can code Yes/No variables as \"1\" / \"0\" . Sometimes we don't mean for this to happen. \u2013 The number of siblings a person has should not be a string. Look at the str ucture of your data and the variable descriptions from a few slides back: \u2013 Write down the variables that should be numeric but are improperly coded as strings or characters .","title":"Numbers are words? (Sometimes)"},{"location":"unit1/lab1f/#changing-strings-into-numbers","text":"To fix this problem, we need to tell R to think of our \"numeric\" variables as numeric variables. We can do this with the as.numeric function. \u2013 An example using this function is below: as.numeric(\"3.14\") ## [1] 3.14 Notice: We started with a string, \"3.14\" , but as.numeric was able to turn it back into a number.","title":"Changing strings into numbers"},{"location":"unit1/lab1f/#mutating-in-action","text":"Look at the variables you thought should be numeric and select one. Then fill in the blanks below to see how we can correctly code it as a number: atu_cleaner <- mutate(atu_cleaner, age = as.numeric(age), ___ = as.numeric(___)) Once you have this code working, use a similar line of code to correctly code the other numeric variables as numbers.","title":"Mutating in action"},{"location":"unit1/lab1f/#deciphering-categorical-variables","text":"We mentioned earlier that we sometimes code categorical variables as numbers. \u2013 For example, our gender variable uses \"01\" and \"02\" for \"Male\" and \"Female\" , respectively. It's often much easier to analyze and interpret when we use more descriptive categories, such as \"Male\" and \"Female\" .","title":"Deciphering Categorical Variables"},{"location":"unit1/lab1f/#factors-and-levels","text":"R has a special name for categorical variables, called factors . R also has a special name for the different categories of a categorical variable. \u2013 The individual categories are called levels . To see the levels of gender and their counts type : tally(~gender, data = atu_cleaner) Use similar code as we used above to write down the levels for the three factors in our data.","title":"Factors and Levels"},{"location":"unit1/lab1f/#a-level-by-any-other-name","text":"If we know that '01' means 'Male' and '02' means 'Female' then we can use the following code to recode the levels of gender . Type the following command into your console: atu_cleaner <- mutate(atu_cleaner, gender = recode(gender, \"01\"=\"Male\", \"02\" = \"Female\")) This code is definitely a bit of a mouthful. Let's break it down.","title":"A level by any other name..."},{"location":"unit1/lab1f/#allow-me-to-explain","text":"atu_cleaner <- mutate(atu_cleaner, gender = recode(gender, \"01\"=\"Male\", \"02\" = \"Female\")) This code is saying: \u2013 Replace my current version of atu_cleaner ... \u2013 with a mutated one where ... \u2013 the gender variable's levels ... \u2013 have been recoded...\" \u2013 where \"01\" will now be \"Male\" ... \u2013 and \"02\" will now be \"Female\" .","title":"Allow me to explain"},{"location":"unit1/lab1f/#finish-it-off","text":"Recode the categorical variable about whether the person surveyed had a physical challenge or not. The coding is currently: \u2013 \"01\" : Person surveyed did not have a physical challenge. \u2013 \"02\" : Person surveyed did have a physical challenge. Write a script that: (1) Loads the atu_dirty data set (2) Cleans the the data as we have in this lab (3) Saves a copy of the cleaned data (see next slide). NOTE: You can watch this video to learn about RScripts: https://www.youtube.com/embed/OPqjL9Azmk","title":"Finish it off!"},{"location":"unit1/lab1f/#the-final-lines","text":"The last few lines of your script are extremely important because they will save all of your work. Be sure to View your data and check its str ucture to make sure it looks clean and tidy before saving. Run the code below: atu_clean <- atu_cleaner This code will create a new data frame in your Environment called atu_clean which is a final copy of atu_cleaner . \u2013 If atu_clean is swept from your Environment all of the changes you made will NOT be saved. \u2013 You would need to re-run the script to clean the data again To permanently save your changes you need to save the file as an R data file or .Rda Run the code below: save(atu_clean, file = \"atu_clean.Rda\") Look in your Files pane for the atu_clean.Rda file \u2013 This is as permanent copy of your clean atu data \u2013 To load the data onto your Environment click on the file \u2013 A pop-up window confirming the upload will appear","title":"The final lines"},{"location":"unit1/lab1f/#flex-your-skills","text":"Now that you have learned some cleaning data basics, it\u2019s time to revisit the food data. Run the code below: histogram(~calories | healthy_level, data = food) Use the as.factor() function to convert healthy_level into a categorical variable and re-run the histogram function. \u2013 Notice that the healthy_level categories are now numbers as opposed to tick-marks. This is an improvement but an even better solution would be to recode the categories. Recode the healthy_level categories and re-run the histogram function. \u2013 \"1\" = \"Very Unhealthy\" \u2013 \"2\" = \"Unhealthy\" \u2013 \"3\" = \"Neutral\" \u2013 \"4\" = \"Healthy\" \u2013 \"5\" = \"Very Healthy\" If your food data is cleared from your Environment , the changes that you made to the healthy_level variable will not be saved. To save your changes permanently save your food file as an R data file.","title":"Flex your skills"},{"location":"unit1/lab1g/","text":"Lab 1G - What's the FREQ? Directions: Follow along with the slides, completing the questions in blue on your computer, and answering the questions in red in your journal. Clean it up! In Lab 1F , we saw how we could clean data to make it easier to use and analyze. \u2013 You cleaned a small set of variables from the American Time Use (ATU) survey. \u2013 The process of cleaning and then analyzing data is very common in Data Science. In this lab, we'll learn how we can create frequency tables to detect relationships between categorical variables. \u2013 For the sake of consistency, rather than using the data that you cleaned, you will use the pre-loaded ATU data. \u2013 Use the data() function to load the atu_clean data file to use in this lab. How do we summarize categorical variables? When we're dealing with categorical variables, we can't just calculate an average to describe a typical value. \u2013 (Honestly, what's the average of categories orange , apple and banana , for instance?) When trying to describe categorical variables with numbers, we calculate frequency tables Frequency tables? When it comes to categories, about all you can do is count or tally how often each category comes up in the data. Fill in the blanks below to answer the following: How many more females than males are there in our ATU data? tally(~ ____, data = ____) 2-way Frequency Tables Counting the categories of a single variable is nice, but often times we want to make comparisons. For example, what if we wanted to answer the question: \u2013 Does one gender seem to have a higher occurrence of physical challenges than the other? We could use the following plot to try and answer this question: bargraph(~phys_challenge | gender, data = atu_clean) The split bargraph helps us get an idea of the answer to the question, but we need to provide precise values. Use a line of code, that\u2019s similar to how we facet plots, to obtain a tally of the number of people with physical challenges and their genders. Write down the resulting table. Interpreting 2-way frequency tables Recall that there were 1153 more women than men in our data set. \u2013 If there are more women, then we might expect women to have more physical challenges (compared to men). Instead of using counts we use percentages . Include: format = \"percent\" as an option to the code you used to make your 2-way frequency table. \u2013 Does one gender seem to have a higher occurrence of physical challenges than the other? If so, which one and explain your reasoning? It\u2019s often helpful to display totals in our 2-way frequency tables. \u2013 To include them, include margins = TRUE as an option in the tally function. Conditional Relative Frequencies There is as difference between phys_challenge | gender and gender | phys_challenge ! tally(~phys_challenge | gender, data = atu_clean, margin = TRUE) ## gender ## phys_challenge Male Female ## No difficulty 4140 5048 ## Has difficulty 530 775 ## Total 4670 5823 tally(~gender | phys_challenge, data = atu_clean, margin = TRUE) ## phys_challenge ## gender No difficulty Has difficulty ## Male 4140 530 ## Female 5048 775 ## Total 9188 1305 Conditional Relative Frequencies, continued At first glance, the two-way frequency tables might look similar (especially when the margin option is excluded). Notice, however, that the totals are different. The totals are telling us that R calculates conditional frequencies by column! What does this mean? \u2013 In the first two-way frequency table the groups being compared are Male and Female on the distribution of physical challenges. \u2013 In the second two-way frequency table the groups being compared are the people with No difficulty and those that Has difficulty on the distribution of gender. Add the option format = \"percent\" to the first tally function. How were the percents calculated? Interpret what they mean. On your own Describe what happens if you create a 2-way frequency table with a numerical variable and a categorical variable. How are the types of statistical investigative questions that 2-way frequency tables can answer different than 1-way frequency tables? Which gender has a higher rate of part time employment ?","title":"Lab 1G: What\u2019s the FREQ?"},{"location":"unit1/lab1g/#lab-1g-whats-the-freq","text":"Directions: Follow along with the slides, completing the questions in blue on your computer, and answering the questions in red in your journal.","title":"Lab 1G - What's the FREQ?"},{"location":"unit1/lab1g/#clean-it-up","text":"In Lab 1F , we saw how we could clean data to make it easier to use and analyze. \u2013 You cleaned a small set of variables from the American Time Use (ATU) survey. \u2013 The process of cleaning and then analyzing data is very common in Data Science. In this lab, we'll learn how we can create frequency tables to detect relationships between categorical variables. \u2013 For the sake of consistency, rather than using the data that you cleaned, you will use the pre-loaded ATU data. \u2013 Use the data() function to load the atu_clean data file to use in this lab.","title":"Clean it up!"},{"location":"unit1/lab1g/#how-do-we-summarize-categorical-variables","text":"When we're dealing with categorical variables, we can't just calculate an average to describe a typical value. \u2013 (Honestly, what's the average of categories orange , apple and banana , for instance?) When trying to describe categorical variables with numbers, we calculate frequency tables","title":"How do we summarize categorical variables?"},{"location":"unit1/lab1g/#frequency-tables","text":"When it comes to categories, about all you can do is count or tally how often each category comes up in the data. Fill in the blanks below to answer the following: How many more females than males are there in our ATU data? tally(~ ____, data = ____)","title":"Frequency tables?"},{"location":"unit1/lab1g/#2-way-frequency-tables","text":"Counting the categories of a single variable is nice, but often times we want to make comparisons. For example, what if we wanted to answer the question: \u2013 Does one gender seem to have a higher occurrence of physical challenges than the other? We could use the following plot to try and answer this question: bargraph(~phys_challenge | gender, data = atu_clean) The split bargraph helps us get an idea of the answer to the question, but we need to provide precise values. Use a line of code, that\u2019s similar to how we facet plots, to obtain a tally of the number of people with physical challenges and their genders. Write down the resulting table.","title":"2-way Frequency Tables"},{"location":"unit1/lab1g/#interpreting-2-way-frequency-tables","text":"Recall that there were 1153 more women than men in our data set. \u2013 If there are more women, then we might expect women to have more physical challenges (compared to men). Instead of using counts we use percentages . Include: format = \"percent\" as an option to the code you used to make your 2-way frequency table. \u2013 Does one gender seem to have a higher occurrence of physical challenges than the other? If so, which one and explain your reasoning? It\u2019s often helpful to display totals in our 2-way frequency tables. \u2013 To include them, include margins = TRUE as an option in the tally function.","title":"Interpreting 2-way frequency tables"},{"location":"unit1/lab1g/#conditional-relative-frequencies","text":"There is as difference between phys_challenge | gender and gender | phys_challenge ! tally(~phys_challenge | gender, data = atu_clean, margin = TRUE) ## gender ## phys_challenge Male Female ## No difficulty 4140 5048 ## Has difficulty 530 775 ## Total 4670 5823 tally(~gender | phys_challenge, data = atu_clean, margin = TRUE) ## phys_challenge ## gender No difficulty Has difficulty ## Male 4140 530 ## Female 5048 775 ## Total 9188 1305","title":"Conditional Relative Frequencies"},{"location":"unit1/lab1g/#conditional-relative-frequencies-continued","text":"At first glance, the two-way frequency tables might look similar (especially when the margin option is excluded). Notice, however, that the totals are different. The totals are telling us that R calculates conditional frequencies by column! What does this mean? \u2013 In the first two-way frequency table the groups being compared are Male and Female on the distribution of physical challenges. \u2013 In the second two-way frequency table the groups being compared are the people with No difficulty and those that Has difficulty on the distribution of gender. Add the option format = \"percent\" to the first tally function. How were the percents calculated? Interpret what they mean.","title":"Conditional Relative Frequencies, continued"},{"location":"unit1/lab1g/#on-your-own","text":"Describe what happens if you create a 2-way frequency table with a numerical variable and a categorical variable. How are the types of statistical investigative questions that 2-way frequency tables can answer different than 1-way frequency tables? Which gender has a higher rate of part time employment ?","title":"On your own"},{"location":"unit1/lab1h/","text":"Lab 1H - Our Time Directions: Follow along with the slides, completing the questions in blue on your computer, and answering the questions in red in your journal. We've come a long way The labs until now have covered a huge range of topics: \u2013 We've learned how to make plots for different types of variables. \u2013 We know how to subset our data to get a more refined view of our data. \u2013 We've covered cleaning data and making two-way frequency tables. In this lab, we're going to combine all of these ideas and topics together to find out how we spend out time. First steps first. Export, Upload, Import the data from your class\u2019 Time Use campaign. The data, as-is, is very messy and hard to interpret/analyze. \u2013 Fill in the blank with the name of your imported data to format it: timeuse <- timeuse_format( ______ ) This function formats/cleans the data so that each row represents a typical day for each student in the class Hint: Search your History tab for the code to save your formatted timeuse data as an R data file (.Rda) timeuse_format specifics In case you're wondering, the timeuse_format function: \u2013 Takes each student's daily data and adds up all of the time spent doing each activity for each day. \u2013 The time spent on each activity for each day is then averaged together to create a typical day in the life of each student. Exploring your data Start by getting familiar with your timeuse data: \u2013 How many observations and variables are there? \u2013 What are the names of the variables? \u2013 Which row represents YOUR typical day ? How do we spend our time? We would like to investigate the research question : \"How did our class spend our time?\" \u2013 To do this, we'll perform a statistical investigation. State and answer two statistical questions based on our research question . \u2013 Also, state one way in which your personal data is typical and one way that it differs from the rest of the class. Justify your answers by using appropriate statistical graphics and summary tables. \u2013 If you subset your data, explain why and how it benefited your analysis.","title":"Lab 1H: Our Time"},{"location":"unit1/lab1h/#lab-1h-our-time","text":"Directions: Follow along with the slides, completing the questions in blue on your computer, and answering the questions in red in your journal.","title":"Lab 1H - Our Time"},{"location":"unit1/lab1h/#weve-come-a-long-way","text":"The labs until now have covered a huge range of topics: \u2013 We've learned how to make plots for different types of variables. \u2013 We know how to subset our data to get a more refined view of our data. \u2013 We've covered cleaning data and making two-way frequency tables. In this lab, we're going to combine all of these ideas and topics together to find out how we spend out time.","title":"We've come a long way"},{"location":"unit1/lab1h/#first-steps-first","text":"Export, Upload, Import the data from your class\u2019 Time Use campaign. The data, as-is, is very messy and hard to interpret/analyze. \u2013 Fill in the blank with the name of your imported data to format it: timeuse <- timeuse_format( ______ ) This function formats/cleans the data so that each row represents a typical day for each student in the class Hint: Search your History tab for the code to save your formatted timeuse data as an R data file (.Rda)","title":"First steps first."},{"location":"unit1/lab1h/#timeuse_format-specifics","text":"In case you're wondering, the timeuse_format function: \u2013 Takes each student's daily data and adds up all of the time spent doing each activity for each day. \u2013 The time spent on each activity for each day is then averaged together to create a typical day in the life of each student.","title":"timeuse_format specifics"},{"location":"unit1/lab1h/#exploring-your-data","text":"Start by getting familiar with your timeuse data: \u2013 How many observations and variables are there? \u2013 What are the names of the variables? \u2013 Which row represents YOUR typical day ?","title":"Exploring your data"},{"location":"unit1/lab1h/#how-do-we-spend-our-time","text":"We would like to investigate the research question : \"How did our class spend our time?\" \u2013 To do this, we'll perform a statistical investigation. State and answer two statistical questions based on our research question . \u2013 Also, state one way in which your personal data is typical and one way that it differs from the rest of the class. Justify your answers by using appropriate statistical graphics and summary tables. \u2013 If you subset your data, explain why and how it benefited your analysis.","title":"How do we spend our time?"},{"location":"unit1/lesson1/","text":"Lesson 1: Data Trails Objective: Students will understand what are data, how they are collected, and possible effects of sharing data. Materials: Video: The Target Story found at: https://www.youtube.com/watch?v=XvSA-6BJkx4 https://www.youtube.com/embed/XvSA-6BJkx4 Data Science (DS) journal (quad-ruled composition book or similar); MUST be available for every lesson Data Diary handout ( LMR_1.1_Data Diary ) Video: Terms and Conditions found at: https://www.youtube.com/watch?v=ZcjtEKNP05c https://www.youtube.com/embed/ZcjtEKNP05c Vocabulary: data observations data trails privacy Essential Concepts: Essential Concepts: Data are a collection of recorded observations. Data are gathered by people and by sensors. Patterns in data can reveal previously unknown patterns in our world. Data play a large, and sometimes invisible, role in our lives. Lesson: Before implementing the IDS curriculum, ensure that: a) Students have been placed in teams and each student understands his or her role in the team. b) Each student knows who his/her partner is within each team. c) Expectations regarding collaborative teamwork are discussed and understood (see Team Roles in Teacher Resources ). Introduce the lesson by showing The Target Story video: https://www.youtube.com/watch?v=XvSA-6BJkx4 In pairs, ask students to discuss the following question using the TPS strategy (see Instructional Strategies in Teacher Resources ): How do you think Target knew about the daughter? In other words, how did Target know the daughter was pregnant before her father did? Target used the information gathered from the daughter\u2019s Red Card and compared it to information about other shoppers. Typically, women who bought those particular products were pregnant. After students have had time to share their responses, engage in a whole class discussion regarding: What are data ? Data are information, or observations , that have been gathered and recorded. Where do data come from? Data can come from a variety of places. Some examples might include: cell phones, computers, school records, surveys, etc. Give an example of data. Answers will vary. One example might be information about a person \u2013 including their age, height, weight, eye color, etc. Give an example of something that is not data (e.g., something that was never written down). Answers will vary. One example might be just watching an event happen. If it wasn\u2019t recorded in some way, it cannot be counted as data. Explain to the students that we create \"data trails\" as we go through life. A data trail is the data collected about us as individuals that could be used to see the patterns in our personal lives. Inform the students that they will learn about their own data trails by keeping a data diary and logging entries over the next 24 hours. It is likely that students do not realize how often they leave a data trail or what information is being collected about them on a regular basis. Distribute the Data Diary handout ( LMR_1.1 ) and be sure to go over the instructions, along with the first example to give the students an idea of how to proceed. LMR_1.1 Inform the students that you will collect the handouts during the next class in order to assess their understanding of data. To get students thinking about what happens to their data, show the Terms and Conditions video: https://www.youtube.com/watch?v=ZcjtEKNP05c Engage the students in a whole class discussion about the video, particularly noting: What terms in the privacy statements were concerning or worrisome? Do you read the agreements when you download phone apps? Class Scribes: One team of students will give a brief talk to discuss what they think the 3 most important topics of the day were. Be prepared to facilitate a good discussion and to ask probing questions in order for students to elaborate on their thinking so that vague responses such as \u201cwe learned about data\u201d can be avoided. Homework Students will complete the Data Diary handout. When grading the homework, be aware of whether the data really could be collected, and whether the students' ideas about how the data might be used are reasonable. For instance, students will often imagine that there is a \"spy\" watching them; this is not what we are after. We are after actual instances in which sensors or electronic surveillance records their actions or records information about them. For example, \"someone saw me going into the store\" is not valid data for this exercise, but \"a camera recorded me entering the store\" is valid data.","title":"Lesson 1: Data Trails"},{"location":"unit1/lesson1/#lesson-1-data-trails","text":"","title":"Lesson 1: Data Trails"},{"location":"unit1/lesson1/#objective","text":"Students will understand what are data, how they are collected, and possible effects of sharing data.","title":"Objective:"},{"location":"unit1/lesson1/#materials","text":"Video: The Target Story found at: https://www.youtube.com/watch?v=XvSA-6BJkx4 https://www.youtube.com/embed/XvSA-6BJkx4 Data Science (DS) journal (quad-ruled composition book or similar); MUST be available for every lesson Data Diary handout ( LMR_1.1_Data Diary ) Video: Terms and Conditions found at: https://www.youtube.com/watch?v=ZcjtEKNP05c https://www.youtube.com/embed/ZcjtEKNP05c","title":"Materials:"},{"location":"unit1/lesson1/#vocabulary","text":"data observations data trails privacy","title":"Vocabulary:"},{"location":"unit1/lesson1/#essential-concepts","text":"Essential Concepts: Data are a collection of recorded observations. Data are gathered by people and by sensors. Patterns in data can reveal previously unknown patterns in our world. Data play a large, and sometimes invisible, role in our lives.","title":"Essential Concepts:"},{"location":"unit1/lesson1/#lesson","text":"Before implementing the IDS curriculum, ensure that: a) Students have been placed in teams and each student understands his or her role in the team. b) Each student knows who his/her partner is within each team. c) Expectations regarding collaborative teamwork are discussed and understood (see Team Roles in Teacher Resources ). Introduce the lesson by showing The Target Story video: https://www.youtube.com/watch?v=XvSA-6BJkx4 In pairs, ask students to discuss the following question using the TPS strategy (see Instructional Strategies in Teacher Resources ): How do you think Target knew about the daughter? In other words, how did Target know the daughter was pregnant before her father did? Target used the information gathered from the daughter\u2019s Red Card and compared it to information about other shoppers. Typically, women who bought those particular products were pregnant. After students have had time to share their responses, engage in a whole class discussion regarding: What are data ? Data are information, or observations , that have been gathered and recorded. Where do data come from? Data can come from a variety of places. Some examples might include: cell phones, computers, school records, surveys, etc. Give an example of data. Answers will vary. One example might be information about a person \u2013 including their age, height, weight, eye color, etc. Give an example of something that is not data (e.g., something that was never written down). Answers will vary. One example might be just watching an event happen. If it wasn\u2019t recorded in some way, it cannot be counted as data. Explain to the students that we create \"data trails\" as we go through life. A data trail is the data collected about us as individuals that could be used to see the patterns in our personal lives. Inform the students that they will learn about their own data trails by keeping a data diary and logging entries over the next 24 hours. It is likely that students do not realize how often they leave a data trail or what information is being collected about them on a regular basis. Distribute the Data Diary handout ( LMR_1.1 ) and be sure to go over the instructions, along with the first example to give the students an idea of how to proceed. LMR_1.1 Inform the students that you will collect the handouts during the next class in order to assess their understanding of data. To get students thinking about what happens to their data, show the Terms and Conditions video: https://www.youtube.com/watch?v=ZcjtEKNP05c Engage the students in a whole class discussion about the video, particularly noting: What terms in the privacy statements were concerning or worrisome? Do you read the agreements when you download phone apps?","title":"Lesson:"},{"location":"unit1/lesson1/#class-scribes","text":"One team of students will give a brief talk to discuss what they think the 3 most important topics of the day were. Be prepared to facilitate a good discussion and to ask probing questions in order for students to elaborate on their thinking so that vague responses such as \u201cwe learned about data\u201d can be avoided.","title":"Class Scribes:"},{"location":"unit1/lesson1/#homework","text":"Students will complete the Data Diary handout. When grading the homework, be aware of whether the data really could be collected, and whether the students' ideas about how the data might be used are reasonable. For instance, students will often imagine that there is a \"spy\" watching them; this is not what we are after. We are after actual instances in which sensors or electronic surveillance records their actions or records information about them. For example, \"someone saw me going into the store\" is not valid data for this exercise, but \"a camera recorded me entering the store\" is valid data.","title":"Homework"},{"location":"unit1/lesson10/","text":"Lesson 10: Making Histograms Objective: Students will understand that a histogram represents observations grouped into bins, and that bars are drawn to show how many observations (or what proportion of the observations) lie in each bin, rather than representing individual observations, as in a dotplot. Materials: Peanut butter Jelly Loaf of sliced bread Butter knife Plate Sleep dotplots (from lesson 9 ) Poster paper Markers Vocabulary: algorithm histogram bin(s) bin widths output input left-hand rule right-hand rule Essential Concepts: Essential Concepts: Histograms can be created through the use of an algorithm. The distributions displayed in a histogram can be classified using the technical terms for the shapes of distributions. Learning to describe routine tasks through an algorithm is an important component of computational thinking. Lesson: Inform students that they will be telling us how to make a sandwich today. Giving clear, concise instructions to others is an important skill for students to learn. In this activity, students will practice using descriptive vocabulary, communicating ideas to others, recognizing steps in a process, and recognizing the importance of the use of clear language. Prepare for this task by gathering the necessary materials for making a peanut butter and jelly sandwich and arranging them in a way that makes them easy to use. You may want to wear an apron and have a trash bag smock \u2014this can get messy but that\u2019s most of the fun! Note: Be aware of peanut allergies! If any of your students are allergic to peanut butter, DO NOT ALLOW STUDENTS TO HANDLE THE PEANUT BUTTER! Peanut allergies can be very serious and children can have reactions without even eating it. So be aware and be careful! Ask your students if they have ever followed a recipe before. What kinds of things have they made? Does anyone know how to make a peanut butter and jelly sandwich? Would they teach you how? Would they give you all the steps to make a sandwich? Show your students the materials you have for making a sandwich. Have students take out paper and pencils and ask each student (or pair of students) to write down their instructions for making a peanut butter and jelly sandwich. We can also call these instructions an algorithm . Explain to students that precise instructions for any process are like a formula to follow in order to get the same results each time. Also, an algorithm is how we communicate with the computer. The teacher will function as the computer. Your job is to give him/her rules so that he/she can carry out and successfully make a PB&J sandwich. Every algorithm needs input and produces output. The output here will be a PB&J sandwich. What is the input? Steps, or actions to follow. Tell students that when they are done you will select someone to share their instructions and you will make a sandwich following the instructions. Select a student to read their instructions, and do EXACTLY what it says. For example, if it says \u201cput the peanut butter on the bread,\u201d you can literally put the jar of peanut butter on the bag of bread. There was no instruction to open the bread or the jar of peanut butter, no instruction to use the knife in any way, etc. Listen for other examples of unclear instructions and think of how you might act them out. If students are not clear about where to spread the peanut butter, put it on the crust. The more literal you are by doing exactly what the instructions say, the funnier the activity will be and the more likely you are to get your point across about the importance of clear instructions. After your first sandwich, ask your students if they think their instructions were clear or not. What are some things they might have done differently? Select another student to read his/her instructions. They will be sure to use clarifications of the instructions you acted upon before - this is a good thing! After you finish the sandwich, ask your students if they think clear instructions are important. Why? Let students know that they will now develop an algorithm for building a histogram to represent the sleep dotplots they created in the previous lesson. Explain that a histogram , rather than showing the frequency for each value, shows the frequency (or percent, but we will focus on frequency) of all the values that fall in a certain range, called a bin . For example, we might choose bins that go from 0-5, 5-10, 10-15, 15-20, 20-25. Bin widths will vary by class. Model how to create a histogram using the data from the dotplot \u201chours of sleep last night\u201d. On a blank chart, create the x-axis with bin widths 0-3, 3-6, 6-9, etc. and place marks on the plot at those intervals and ask students: \u201cWhat are the frequencies in each bin?\u201d Notice that multiples of three appear in more than one bin. Let\u2019s take the value of 6 hours as an example. Should those observations be included in the second bin (3-6) or the third bin (6-9)? If students include the values of 6 hours in the second bin then they are using the left-hand rule . If students include the values of 6 hours in the third bin then they are using the right-hand rule . Once the frequencies have been determined, draw the bars with corresponding heights. Do not include spaces between the bars as time is a continuous variable. Next, student teams will create an algorithm that gives directions for how to construct a histogram for the data from the dotplot for \u201chours of sleep they hope to get on Saturday.\u201d Remember, an algorithm is a set of rules that can always be applied. Similar to the way they wrote a process for making a PB&J sandwich, students will write a process for creating a histogram. Tell students to continue thinking of the process to transform the data in the dotplot to create a histogram. The algorithm will produce an output , which will be a histogram. What's the input ? Data, or maybe the dotplot. Inform the students that you will provide a piece of input: how wide the bin will be. For instance, it might be 5 hours, it might be 1 hour, or it might be 10 hours (or half an hour!). Whatever it is, their algorithm should work for any input value. Let students work for a bit. They should write out Step 1, Step 2, etc. Then choose a group and ask them to get you started. Give them a bin width of 4. Teachers should sketch the histogram on the board or chart paper as students read their algorithms. Again, teachers should take things very literally. For example, if they do not tell you exactly where the bins should start, start one way off to the left. If they are vague and say \"divide the number line into groups of 10,\u201d then make them arbitrary sizes. If they have to be the same size, ask them how to do that. Points to consider: Where do we start drawing the bins? Always at the location of the smallest dotplot? Always at the greatest? A little to the left? What do we do with points that fall exactly on a boundary? Do they go to the bin on the left or on the right? Does it matter? No. Can we do it differently every time? No. We need to be consistent. This is called either the left-hand rule or the right-hand rule, depending on which is chosen. After following 2 or 3 algorithms, ask students if they feel their algorithm is precise enough. Allow students time to revise their algorithms. Have a class discussion about the similarities and differences between the original dotplot and a histogram. Ask: What have we gained from the histogram? We now can see the shape of the distribution as a whole. What have we lost? We lost each individual observation by grouping them into bins. Class Scribes: One team of students will give a brief talk to discuss what they think the 3 most important topics of the day were. Homework Students should continue to collect nutritional facts data using the Food Habits Participatory Sensing campaign on their smart devices or via web browser.","title":"Lesson 10: Making Histograms"},{"location":"unit1/lesson10/#lesson-10-making-histograms","text":"","title":"Lesson 10: Making Histograms"},{"location":"unit1/lesson10/#objective","text":"Students will understand that a histogram represents observations grouped into bins, and that bars are drawn to show how many observations (or what proportion of the observations) lie in each bin, rather than representing individual observations, as in a dotplot.","title":"Objective:"},{"location":"unit1/lesson10/#materials","text":"Peanut butter Jelly Loaf of sliced bread Butter knife Plate Sleep dotplots (from lesson 9 ) Poster paper Markers","title":"Materials:"},{"location":"unit1/lesson10/#vocabulary","text":"algorithm histogram bin(s) bin widths output input left-hand rule right-hand rule","title":"Vocabulary:"},{"location":"unit1/lesson10/#essential-concepts","text":"Essential Concepts: Histograms can be created through the use of an algorithm. The distributions displayed in a histogram can be classified using the technical terms for the shapes of distributions. Learning to describe routine tasks through an algorithm is an important component of computational thinking.","title":"Essential Concepts:"},{"location":"unit1/lesson10/#lesson","text":"Inform students that they will be telling us how to make a sandwich today. Giving clear, concise instructions to others is an important skill for students to learn. In this activity, students will practice using descriptive vocabulary, communicating ideas to others, recognizing steps in a process, and recognizing the importance of the use of clear language. Prepare for this task by gathering the necessary materials for making a peanut butter and jelly sandwich and arranging them in a way that makes them easy to use. You may want to wear an apron and have a trash bag smock \u2014this can get messy but that\u2019s most of the fun! Note: Be aware of peanut allergies! If any of your students are allergic to peanut butter, DO NOT ALLOW STUDENTS TO HANDLE THE PEANUT BUTTER! Peanut allergies can be very serious and children can have reactions without even eating it. So be aware and be careful! Ask your students if they have ever followed a recipe before. What kinds of things have they made? Does anyone know how to make a peanut butter and jelly sandwich? Would they teach you how? Would they give you all the steps to make a sandwich? Show your students the materials you have for making a sandwich. Have students take out paper and pencils and ask each student (or pair of students) to write down their instructions for making a peanut butter and jelly sandwich. We can also call these instructions an algorithm . Explain to students that precise instructions for any process are like a formula to follow in order to get the same results each time. Also, an algorithm is how we communicate with the computer. The teacher will function as the computer. Your job is to give him/her rules so that he/she can carry out and successfully make a PB&J sandwich. Every algorithm needs input and produces output. The output here will be a PB&J sandwich. What is the input? Steps, or actions to follow. Tell students that when they are done you will select someone to share their instructions and you will make a sandwich following the instructions. Select a student to read their instructions, and do EXACTLY what it says. For example, if it says \u201cput the peanut butter on the bread,\u201d you can literally put the jar of peanut butter on the bag of bread. There was no instruction to open the bread or the jar of peanut butter, no instruction to use the knife in any way, etc. Listen for other examples of unclear instructions and think of how you might act them out. If students are not clear about where to spread the peanut butter, put it on the crust. The more literal you are by doing exactly what the instructions say, the funnier the activity will be and the more likely you are to get your point across about the importance of clear instructions. After your first sandwich, ask your students if they think their instructions were clear or not. What are some things they might have done differently? Select another student to read his/her instructions. They will be sure to use clarifications of the instructions you acted upon before - this is a good thing! After you finish the sandwich, ask your students if they think clear instructions are important. Why? Let students know that they will now develop an algorithm for building a histogram to represent the sleep dotplots they created in the previous lesson. Explain that a histogram , rather than showing the frequency for each value, shows the frequency (or percent, but we will focus on frequency) of all the values that fall in a certain range, called a bin . For example, we might choose bins that go from 0-5, 5-10, 10-15, 15-20, 20-25. Bin widths will vary by class. Model how to create a histogram using the data from the dotplot \u201chours of sleep last night\u201d. On a blank chart, create the x-axis with bin widths 0-3, 3-6, 6-9, etc. and place marks on the plot at those intervals and ask students: \u201cWhat are the frequencies in each bin?\u201d Notice that multiples of three appear in more than one bin. Let\u2019s take the value of 6 hours as an example. Should those observations be included in the second bin (3-6) or the third bin (6-9)? If students include the values of 6 hours in the second bin then they are using the left-hand rule . If students include the values of 6 hours in the third bin then they are using the right-hand rule . Once the frequencies have been determined, draw the bars with corresponding heights. Do not include spaces between the bars as time is a continuous variable. Next, student teams will create an algorithm that gives directions for how to construct a histogram for the data from the dotplot for \u201chours of sleep they hope to get on Saturday.\u201d Remember, an algorithm is a set of rules that can always be applied. Similar to the way they wrote a process for making a PB&J sandwich, students will write a process for creating a histogram. Tell students to continue thinking of the process to transform the data in the dotplot to create a histogram. The algorithm will produce an output , which will be a histogram. What's the input ? Data, or maybe the dotplot. Inform the students that you will provide a piece of input: how wide the bin will be. For instance, it might be 5 hours, it might be 1 hour, or it might be 10 hours (or half an hour!). Whatever it is, their algorithm should work for any input value. Let students work for a bit. They should write out Step 1, Step 2, etc. Then choose a group and ask them to get you started. Give them a bin width of 4. Teachers should sketch the histogram on the board or chart paper as students read their algorithms. Again, teachers should take things very literally. For example, if they do not tell you exactly where the bins should start, start one way off to the left. If they are vague and say \"divide the number line into groups of 10,\u201d then make them arbitrary sizes. If they have to be the same size, ask them how to do that. Points to consider: Where do we start drawing the bins? Always at the location of the smallest dotplot? Always at the greatest? A little to the left? What do we do with points that fall exactly on a boundary? Do they go to the bin on the left or on the right? Does it matter? No. Can we do it differently every time? No. We need to be consistent. This is called either the left-hand rule or the right-hand rule, depending on which is chosen. After following 2 or 3 algorithms, ask students if they feel their algorithm is precise enough. Allow students time to revise their algorithms. Have a class discussion about the similarities and differences between the original dotplot and a histogram. Ask: What have we gained from the histogram? We now can see the shape of the distribution as a whole. What have we lost? We lost each individual observation by grouping them into bins.","title":"Lesson:"},{"location":"unit1/lesson10/#class-scribes","text":"One team of students will give a brief talk to discuss what they think the 3 most important topics of the day were.","title":"Class Scribes:"},{"location":"unit1/lesson10/#homework","text":"Students should continue to collect nutritional facts data using the Food Habits Participatory Sensing campaign on their smart devices or via web browser.","title":"Homework"},{"location":"unit1/lesson11/","text":"Lesson 11: What Shape Are You In? Objective: Students will learn to classify distributions in terms of shape, and can suggest theories for why a distribution might be one shape or another. Materials: Sorting Histograms handout ( LMR_1.10_Sorting Histograms ) - one copy per group of 4 students. (This activity comes from the AIMS project, University of Minnesota, J. Garfield.) Advanced preparation required (see step 1 below) Vocabulary: symmetric left-skewed right-skewed unimodal bimodal Essential Concepts: Essential Concepts: Identifying the shape of a histogram is part of the interpret step of the Data Cycle. Lesson: Distribute the cutouts from the Sorting Histograms handout ( LMR_1.10 ). Give each student team all of the 24 histograms (can be paper clipped together or put in small zippered bags). Advanced preparation required: Print the Sorting Histogram s file ( LMR_1.10 ). Cut each histogram so that it is on its own piece of paper. Create enough sets for each team to have all 24 histograms. They can be paper clipped together, or put in small zippered bags. LMR_1.10 Inform students that the type of data being measured is indicated on the horizontal axis, and the vertical axis represents how many observations are in each bar. The students will then sort their stack of plots into different piles according to their shapes. Histograms that have similar shapes should be sorted into the same stack. Once the student teams have agreed upon the histogram shape groupings, they should discuss and write down answers to the following in their DS journals: Describe what\u2019s similar about the plots in each group. Answers will vary, but should be grouped by the overall shape of the distribution. For example, plots with a higher density of bars on the right side of the plot should all be in the same group. Pick one graph in each group that is the best example of that group. Answers will vary. Give the group a name that you think describes the general shape. Answers will vary. If there are graphs that do not fit into any group, try to determine why it was impossible to place them. What is different or confusing about them? Answers will vary. After each team has had time to discuss and write down their observations, have a class discussion about the histogram groupings. Do the students agree about the general shapes? In statistics, we use specific terminology when discussing the shapes of distributions, such as symmetric , right-skewed , left-skewed , unimodal , bimodal , etc. Did any of the teams use these terms? If not, introduce each one and ask which of the 24 histograms could be classified as that shape. Next, introduce the following scenarios and ask students to determine what a corresponding histogram might look like. They should use statistical terms to describe their answer. The grades on an easy test. Left-skewed, unimodal The grades on a difficult test. Right-skewed, unimodal The number of times IDS students study during the first week of class. Answers will vary. The age of cars on a used car lot. Right-skewed, probably unimodal The amount of time spent by students on a difficult test (max time allowed is 50 mins). Left-skewed, but may also just be one bar with all observations at 50 mins, unimodal The heights of students in your high school band. Symmetric, bimodal The salaries of all persons employed at the Los Angeles Unified School District. Right-skewed, potentially bimodal (teachers vs. LAUSD administrators) Class Scribes: One team of students will give a brief talk to discuss what they think the 3 most important topics of the day were. Homework Students should continue to collect nutritional facts data using the Food Habits Participatory Sensing campaign on their smart devices or via web browser.","title":"Lesson 11: What Shape Are You In?"},{"location":"unit1/lesson11/#lesson-11-what-shape-are-you-in","text":"","title":"Lesson 11: What Shape Are You In?"},{"location":"unit1/lesson11/#objective","text":"Students will learn to classify distributions in terms of shape, and can suggest theories for why a distribution might be one shape or another.","title":"Objective:"},{"location":"unit1/lesson11/#materials","text":"Sorting Histograms handout ( LMR_1.10_Sorting Histograms ) - one copy per group of 4 students. (This activity comes from the AIMS project, University of Minnesota, J. Garfield.) Advanced preparation required (see step 1 below)","title":"Materials:"},{"location":"unit1/lesson11/#vocabulary","text":"symmetric left-skewed right-skewed unimodal bimodal","title":"Vocabulary:"},{"location":"unit1/lesson11/#essential-concepts","text":"Essential Concepts: Identifying the shape of a histogram is part of the interpret step of the Data Cycle.","title":"Essential Concepts:"},{"location":"unit1/lesson11/#lesson","text":"Distribute the cutouts from the Sorting Histograms handout ( LMR_1.10 ). Give each student team all of the 24 histograms (can be paper clipped together or put in small zippered bags). Advanced preparation required: Print the Sorting Histogram s file ( LMR_1.10 ). Cut each histogram so that it is on its own piece of paper. Create enough sets for each team to have all 24 histograms. They can be paper clipped together, or put in small zippered bags. LMR_1.10 Inform students that the type of data being measured is indicated on the horizontal axis, and the vertical axis represents how many observations are in each bar. The students will then sort their stack of plots into different piles according to their shapes. Histograms that have similar shapes should be sorted into the same stack. Once the student teams have agreed upon the histogram shape groupings, they should discuss and write down answers to the following in their DS journals: Describe what\u2019s similar about the plots in each group. Answers will vary, but should be grouped by the overall shape of the distribution. For example, plots with a higher density of bars on the right side of the plot should all be in the same group. Pick one graph in each group that is the best example of that group. Answers will vary. Give the group a name that you think describes the general shape. Answers will vary. If there are graphs that do not fit into any group, try to determine why it was impossible to place them. What is different or confusing about them? Answers will vary. After each team has had time to discuss and write down their observations, have a class discussion about the histogram groupings. Do the students agree about the general shapes? In statistics, we use specific terminology when discussing the shapes of distributions, such as symmetric , right-skewed , left-skewed , unimodal , bimodal , etc. Did any of the teams use these terms? If not, introduce each one and ask which of the 24 histograms could be classified as that shape. Next, introduce the following scenarios and ask students to determine what a corresponding histogram might look like. They should use statistical terms to describe their answer. The grades on an easy test. Left-skewed, unimodal The grades on a difficult test. Right-skewed, unimodal The number of times IDS students study during the first week of class. Answers will vary. The age of cars on a used car lot. Right-skewed, probably unimodal The amount of time spent by students on a difficult test (max time allowed is 50 mins). Left-skewed, but may also just be one bar with all observations at 50 mins, unimodal The heights of students in your high school band. Symmetric, bimodal The salaries of all persons employed at the Los Angeles Unified School District. Right-skewed, potentially bimodal (teachers vs. LAUSD administrators)","title":"Lesson:"},{"location":"unit1/lesson11/#class-scribes","text":"One team of students will give a brief talk to discuss what they think the 3 most important topics of the day were.","title":"Class Scribes:"},{"location":"unit1/lesson11/#homework","text":"Students should continue to collect nutritional facts data using the Food Habits Participatory Sensing campaign on their smart devices or via web browser.","title":"Homework"},{"location":"unit1/lesson12/","text":"Lesson 12: Exploring Food Habits Objective: Students will experience the full Data Cycle, and for the first time will do so with data they have collected. They will use the Dashboard and PlotApp, tools that are easy to learn. This first time, the teacher will \u201cnavigate and steer\" so that students can focus on asking questions and interpreting the plots. Materials: Computers Projector Food Habits Check-In handout ( LMR_1.11_Food Habits Check-In ) Exploring Our Food Habits handout ( LMR_1.12_Exploring Our Food Habits ) Essential Concepts: Essential Concepts: Once Participatory Sensing data has been collected, the Dashboard and PlotApp perform the analysis step of the Data Cycle, though humans need to tell the computer which plots we wish to examine. In preparation for this lesson, watch these two videos: a) Navigating the Dashboard b) Navigating the Plot App Lesson: Ask students to reflect about their experience so far with the Food Habits Participatory Sensing campaign by completing the Food Habits Check-In handout ( LMR_1.11 ). LMR_1.11 Demonstrate how to access the IDS Homepage found at https://portal.idsucla.org Explain to students that all of the IDS web tools can be accessed through this page. For this lesson, students will need to observe the teacher using the Campaign Manager , the Dashboard , and the PlotApp . Click on the Campaign Manager. Explain that selecting any of the web tools on the IDS page without logging in first will redirect them to the login prompt. Demonstrate how to log in to access the IDS software suite. Inform students that they will use the same login information they have used to collect data on their mobile devices or the browser-based version. Inform students that the Campaign Manager is the place where they will access, download, and export their campaign data in subsequent lessons. It also provides shortcuts to the Dashboard and PlotApp. The Campaign Manager allows them to view and learn about the campaigns in which they are participating, and to edit the campaigns they will be creating later in this course. Show them the drop-down menu on the right-hand side, and explain that they will only be concerned with the Responses tab for this lesson. Explain that the Responses tab allows them to view, delete, or share their data, and to view shared data contributed by other users of the campaign. Distribute the Exploring Our Food Habits handout ( LMR_1.12 ). LMR_1.12 Inform students that the teacher will be navigating through the IDS website as the students follow along in the Exploring Our Food Habits handout ( LMR_1.12 ). Once completed, students should turn in the handout for assessment. Class Scribes: One team of students will give a brief talk to discuss what they think the 3 most important topics of the day were. Homework Students should continue to collect nutritional facts data using the Food Habits Participatory Sensing campaign on their smart devices or via web browser.","title":"Lesson 12: Exploring Food Habits"},{"location":"unit1/lesson12/#lesson-12-exploring-food-habits","text":"","title":"Lesson 12: Exploring Food Habits"},{"location":"unit1/lesson12/#objective","text":"Students will experience the full Data Cycle, and for the first time will do so with data they have collected. They will use the Dashboard and PlotApp, tools that are easy to learn. This first time, the teacher will \u201cnavigate and steer\" so that students can focus on asking questions and interpreting the plots.","title":"Objective:"},{"location":"unit1/lesson12/#materials","text":"Computers Projector Food Habits Check-In handout ( LMR_1.11_Food Habits Check-In ) Exploring Our Food Habits handout ( LMR_1.12_Exploring Our Food Habits )","title":"Materials:"},{"location":"unit1/lesson12/#essential-concepts","text":"Essential Concepts: Once Participatory Sensing data has been collected, the Dashboard and PlotApp perform the analysis step of the Data Cycle, though humans need to tell the computer which plots we wish to examine. In preparation for this lesson, watch these two videos: a) Navigating the Dashboard b) Navigating the Plot App","title":"Essential Concepts:"},{"location":"unit1/lesson12/#lesson","text":"Ask students to reflect about their experience so far with the Food Habits Participatory Sensing campaign by completing the Food Habits Check-In handout ( LMR_1.11 ). LMR_1.11 Demonstrate how to access the IDS Homepage found at https://portal.idsucla.org Explain to students that all of the IDS web tools can be accessed through this page. For this lesson, students will need to observe the teacher using the Campaign Manager , the Dashboard , and the PlotApp . Click on the Campaign Manager. Explain that selecting any of the web tools on the IDS page without logging in first will redirect them to the login prompt. Demonstrate how to log in to access the IDS software suite. Inform students that they will use the same login information they have used to collect data on their mobile devices or the browser-based version. Inform students that the Campaign Manager is the place where they will access, download, and export their campaign data in subsequent lessons. It also provides shortcuts to the Dashboard and PlotApp. The Campaign Manager allows them to view and learn about the campaigns in which they are participating, and to edit the campaigns they will be creating later in this course. Show them the drop-down menu on the right-hand side, and explain that they will only be concerned with the Responses tab for this lesson. Explain that the Responses tab allows them to view, delete, or share their data, and to view shared data contributed by other users of the campaign. Distribute the Exploring Our Food Habits handout ( LMR_1.12 ). LMR_1.12 Inform students that the teacher will be navigating through the IDS website as the students follow along in the Exploring Our Food Habits handout ( LMR_1.12 ). Once completed, students should turn in the handout for assessment.","title":"Lesson:"},{"location":"unit1/lesson12/#class-scribes","text":"One team of students will give a brief talk to discuss what they think the 3 most important topics of the day were.","title":"Class Scribes:"},{"location":"unit1/lesson12/#homework","text":"Students should continue to collect nutritional facts data using the Food Habits Participatory Sensing campaign on their smart devices or via web browser.","title":"Homework"},{"location":"unit1/lesson13/","text":"Lesson 13: RStudio Basics Objective: Students will learn RStudio/Posit Cloud\u2019s interface, as well as a few basic commands to discover the structure behind a dataset. Materials: Computer Projector RStudio: https://portal.idsucla.org Video showing how to log into RStudio/Posit Cloud for the first time found here . Vocabulary: pane preview console plot environment RStudio Commands: data( ), View( ), names( ), help( ), dim( ), tally( ), load_labs( ) Essential Concepts: Essential Concepts: The computer has a syntax, and it can only understand if you speak its language. Before inviting students to your RStudio/Posit Cloud Teacher Space, ensure that: a) Students sign-in to RStudio/Posit Cloud using the \"Log In With Google\" option using their school email address. b) Each student watches the video showing how to log into RStudio/Posit Cloud for the first time found here . c) You are familiar with managing your RStudio/Posit Teacher Space. See video here . In preparation for this lesson, watch this video: RStudio Basics Lesson: Inform students that the Dashboard and PlotApp are data visualization tools that are coded in R, the statistical programming software that academics and professional statisticians use. The Introduction to Data Science course will utilize RStudio, which also runs on R. They will learn the programming language of RStudio for data analysis. Demonstrate how to access RStudio/Posit Cloud by projecting the URL: https://portal.idsucla.org on a screen. Then, click on the RStudio (Posit Cloud) icon on the page. Inform students that they will log into RStudio/Posit Cloud using the \"Log In with Google\" option. Note that this is not the same as their IDS App & IDS Homepage login. Once logged in, show each pane , or rectangular area, of the RStudio/Posit Cloud interface: preview (spreadsheet) - where they will be able to see the variables and observations (index); rows and columns of data console - where they will be entering their code plot - where their plots/graphs/visualizations will be generated environment - where they will see values and objects Inform students that they will be looking at a dataset from The Centers for Disease Control and Prevention (CDC), a government agency that collects data about teenagers on a variety of topics. Demonstrate how to load and view the CDC data file to the workspace by typing the following command in the console: >data(cdc) >View(cdc) Examine the environment pane. Ask a student to describe how the data are displayed. The data are displayed in rows and columns. Demonstrate how to list the variables found in the CDC dataset. Students may take notes and write down commands in their DS journals: >names(cdc) Ask: What do you notice? What is one variable of this dataset? How many variables are there? How does this output compare to the information in the preview pane? This command lists the names of each variable in the dataset. Demonstrate how to obtain more detailed information about the dataset by typing the following command in the console >help(cdc) Ask: What unit of measurement is height reported in? Height was reported in meters. Demonstrate how to find the number of rows and columns in the dataset. >dim(cdc) Ask: Which number do you think represents the rows? Which one represents the columns? How does this output compare to the information in the preview and environment panes? How many observations are there in the dataset? How many variables does this dataset contain? There are 13,677 rows, or 13,677 observations; and there are 30 columns, or 30 variables. This information is also visible in the preview pane. Next, show students how to access the number of observations of a specific variable. >tally(~seat_belt, data = cdc) Ask: What do you notice? Describe the output. Notice that six categories are displayed. Each category shows the number of observations contained in it. E.g,. \u201cNever\u201d has 294 observations, meaning 294 teens reported never wearing their seat belt as a passenger in a motor vehicle. <NA> = Not Available, represents teens that did not provide information about their seat belt habits. Change the variable to height. >tally(~height, data = cdc) Ask: What do you notice? Describe the output. The levels are missing. It happened because the variable height contains numbers, not categories. Let\u2019s take a closer look at the variables seat_belt and height. Maximize the console. Ask teams to discuss the following question: What is the difference between the data from the variables seat_belt and height? The data from the seat_belt variable is categorical, which means it consists of groupings. The data from the variable height is numerical, which means it consists of numbers. Summarize: In data science, the variable seat_belt is what we call a categorical variable , and the variable height is what we call a numerical variable . Let\u2019s look at the other variables in this dataset. In pairs, categorize each variable as categorical or numerical: eat_fruit (categorical) weight (numerical) grade (categorical) gender (categorical) Inform students that they will be learning RStudio code to work with data. They will be completing RStudio labs throughout the course. Demonstrate how to load the menu of labs by typing the following code: >load_labs( ) The load labs command displays a list of available labs and a selection prompt. To select Lab 1A, type number 1 after the selection prompt. Next, direct students\u2019 attention to the plot pane. Show them the location of Lab 1A\u2019s presentation. Click on the arrows at the bottom right-hand side of the presentation to view each slide. Pause on a slide titled \u201cR\u2019s most important syntax.\u201d There are 3 boxes, each containing a line of code. Explain that every time they see a grey box with a line of code, they are to type the code in the console. The output will appear either on the console itself or on the plot pane. Type in one of the lines of code. In this particular case, the output will be a plot. Show students the location of the plot and demonstrate how to toggle between the plots and presentation tabs. Inform students that they will be completing the first lab, 1A, the next day. Class Scribes: One team of students will give a brief talk to discuss what they think the 3 most important topics of the day were. Homework & Next 3 Days Students should continue to collect nutritional facts data using the Food Habits Participatory Sensing campaign on their smart devices or via web browser. Lab 1A: Data, Code & RStudio Lab 1B: Get the Picture? Lab 1C: Export, Upload, Import Complete Labs 1A , 1B and 1C prior to Lesson 14","title":"Lesson 13: RStudio Basics"},{"location":"unit1/lesson13/#lesson-13-rstudio-basics","text":"","title":"Lesson 13: RStudio Basics"},{"location":"unit1/lesson13/#objective","text":"Students will learn RStudio/Posit Cloud\u2019s interface, as well as a few basic commands to discover the structure behind a dataset.","title":"Objective:"},{"location":"unit1/lesson13/#materials","text":"Computer Projector RStudio: https://portal.idsucla.org Video showing how to log into RStudio/Posit Cloud for the first time found here .","title":"Materials:"},{"location":"unit1/lesson13/#vocabulary","text":"pane preview console plot environment","title":"Vocabulary:"},{"location":"unit1/lesson13/#rstudio-commands","text":"data( ), View( ), names( ), help( ), dim( ), tally( ), load_labs( )","title":"RStudio Commands:"},{"location":"unit1/lesson13/#essential-concepts","text":"Essential Concepts: The computer has a syntax, and it can only understand if you speak its language. Before inviting students to your RStudio/Posit Cloud Teacher Space, ensure that: a) Students sign-in to RStudio/Posit Cloud using the \"Log In With Google\" option using their school email address. b) Each student watches the video showing how to log into RStudio/Posit Cloud for the first time found here . c) You are familiar with managing your RStudio/Posit Teacher Space. See video here . In preparation for this lesson, watch this video: RStudio Basics","title":"Essential Concepts:"},{"location":"unit1/lesson13/#lesson","text":"Inform students that the Dashboard and PlotApp are data visualization tools that are coded in R, the statistical programming software that academics and professional statisticians use. The Introduction to Data Science course will utilize RStudio, which also runs on R. They will learn the programming language of RStudio for data analysis. Demonstrate how to access RStudio/Posit Cloud by projecting the URL: https://portal.idsucla.org on a screen. Then, click on the RStudio (Posit Cloud) icon on the page. Inform students that they will log into RStudio/Posit Cloud using the \"Log In with Google\" option. Note that this is not the same as their IDS App & IDS Homepage login. Once logged in, show each pane , or rectangular area, of the RStudio/Posit Cloud interface: preview (spreadsheet) - where they will be able to see the variables and observations (index); rows and columns of data console - where they will be entering their code plot - where their plots/graphs/visualizations will be generated environment - where they will see values and objects Inform students that they will be looking at a dataset from The Centers for Disease Control and Prevention (CDC), a government agency that collects data about teenagers on a variety of topics. Demonstrate how to load and view the CDC data file to the workspace by typing the following command in the console: >data(cdc) >View(cdc) Examine the environment pane. Ask a student to describe how the data are displayed. The data are displayed in rows and columns. Demonstrate how to list the variables found in the CDC dataset. Students may take notes and write down commands in their DS journals: >names(cdc) Ask: What do you notice? What is one variable of this dataset? How many variables are there? How does this output compare to the information in the preview pane? This command lists the names of each variable in the dataset. Demonstrate how to obtain more detailed information about the dataset by typing the following command in the console >help(cdc) Ask: What unit of measurement is height reported in? Height was reported in meters. Demonstrate how to find the number of rows and columns in the dataset. >dim(cdc) Ask: Which number do you think represents the rows? Which one represents the columns? How does this output compare to the information in the preview and environment panes? How many observations are there in the dataset? How many variables does this dataset contain? There are 13,677 rows, or 13,677 observations; and there are 30 columns, or 30 variables. This information is also visible in the preview pane. Next, show students how to access the number of observations of a specific variable. >tally(~seat_belt, data = cdc) Ask: What do you notice? Describe the output. Notice that six categories are displayed. Each category shows the number of observations contained in it. E.g,. \u201cNever\u201d has 294 observations, meaning 294 teens reported never wearing their seat belt as a passenger in a motor vehicle. <NA> = Not Available, represents teens that did not provide information about their seat belt habits. Change the variable to height. >tally(~height, data = cdc) Ask: What do you notice? Describe the output. The levels are missing. It happened because the variable height contains numbers, not categories. Let\u2019s take a closer look at the variables seat_belt and height. Maximize the console. Ask teams to discuss the following question: What is the difference between the data from the variables seat_belt and height? The data from the seat_belt variable is categorical, which means it consists of groupings. The data from the variable height is numerical, which means it consists of numbers. Summarize: In data science, the variable seat_belt is what we call a categorical variable , and the variable height is what we call a numerical variable . Let\u2019s look at the other variables in this dataset. In pairs, categorize each variable as categorical or numerical: eat_fruit (categorical) weight (numerical) grade (categorical) gender (categorical) Inform students that they will be learning RStudio code to work with data. They will be completing RStudio labs throughout the course. Demonstrate how to load the menu of labs by typing the following code: >load_labs( ) The load labs command displays a list of available labs and a selection prompt. To select Lab 1A, type number 1 after the selection prompt. Next, direct students\u2019 attention to the plot pane. Show them the location of Lab 1A\u2019s presentation. Click on the arrows at the bottom right-hand side of the presentation to view each slide. Pause on a slide titled \u201cR\u2019s most important syntax.\u201d There are 3 boxes, each containing a line of code. Explain that every time they see a grey box with a line of code, they are to type the code in the console. The output will appear either on the console itself or on the plot pane. Type in one of the lines of code. In this particular case, the output will be a plot. Show students the location of the plot and demonstrate how to toggle between the plots and presentation tabs. Inform students that they will be completing the first lab, 1A, the next day.","title":"Lesson:"},{"location":"unit1/lesson13/#class-scribes","text":"One team of students will give a brief talk to discuss what they think the 3 most important topics of the day were.","title":"Class Scribes:"},{"location":"unit1/lesson13/#homework-next-3-days","text":"Students should continue to collect nutritional facts data using the Food Habits Participatory Sensing campaign on their smart devices or via web browser. Lab 1A: Data, Code & RStudio Lab 1B: Get the Picture? Lab 1C: Export, Upload, Import Complete Labs 1A , 1B and 1C prior to Lesson 14","title":"Homework &amp; Next 3 Days"},{"location":"unit1/lesson14/","text":"Lesson 14: Variables, Variables, Variables Objective: Students will learn how to read and interpret multiple variable plots: bivariate scatterplots, multiple variable scatterplots, stacked bar plots, and side-by-side bar plots. They will summarize their learning about multiple variable plots using a four-fold graphic organizer. Materials: Scatterplot of Heights & Weights ( LMR_1.13 ) Scatterplot of Heights & Weights, Split by Gender ( LMR_1.14 ) Side-by-Side Bar Chart ( LMR_1.15 ) Faceted Histogram of Height by Gender ( LMR_1.16 ) Summarizing Multi-Variable Plots graphic organizer ( LMR_1.17 ) Vocabulary: scatterplot grouping side-by-side bar plot Essential Concepts: Essential Concepts: To examine whether two (or more) variables are related, we can plot their distributions on the same graph. Lesson: Begin by informing students that they will learn how to make visual displays using more than one variable, and by asking them to ponder the following questions: What do you think is the relation between people\u2019s heights and weights? Are taller people heavier? Always? Or is this just a tendency? What do you think is the relation between people's heights and weights? Are taller people heavier? Always? Or is this just a tendency? Let's look at some data. Display the following plot to the class ( LMR_1.13 ) so they can see some actual data: LMR_1.13 Ask students to individually answer the following questions about the plot on the handout ( LMR_1.13 ): What kind of plot is this and how will you remember its features? Scatterplot. How many variables are displayed in this plot? Name the variable(s) and identify the type of variable(s). Two variables. Weight in kilograms and height in meters. Numerical variables. What do the axes show? The x-axis shows the height of teens in meters, and the y-axis shows the weight of teens in kilograms. Do taller people weigh more? Not necessarily, but there is a tendency for this to be true. Discuss this plot with the class by eliciting students\u2019 responses to the questions. Students actively listen to the discussion by confirming, correcting, or adding to their own responses. Close the discussion by asking students: What questions might you have about this plot? What additional information would be helpful? Now, suppose we could see which of these dots represented girls and which represented boys. Where do you think most of the girls' dots would be relative to the boys? Display the following plot to the class ( LMR_1.14 ): LMR_1.14 Ask students to individually answer the following questions about the plot on the handout ( LMR_1.14 ): What kind of plot is this and how will you remember its features? Scatterplot. How many variables are displayed in this plot? Name the variable(s) and identify the type of variable(s). Three variables. Weight in kilograms and height in meters are numerical variables. Gender is categorical. What do the axes show? The x-axis shows the height of teens in meters, and the y-axis shows the weight of teens in kilograms. What questions can we ask that this graph might answer? Who is taller, boys or girls? Who weighs more? Is the association between height and weight the same for boys as it is for girls? Discuss this plot with the class by eliciting students\u2019 responses to the questions. Students actively listen to the discussion by confirming, correcting, or adding to their own responses. Follow-up discussion: when the data are split into categories, it is called grouping . Close the discussion by asking students: What questions might you have about this plot? What additional information would be helpful? Display the following plot to the class ( LMR_1.15 ): LMR_1.15 Ask students to individually answer the following questions on the handout ( LMR_1.15 ): What kind of plot is this and how will you remember its features? Side-by-side bar chart. How many variables are displayed in this plot? Name the variable(s). Two variables: whether or not someone is Hispanic, and how often they wear sunscreen. What are the x-axis and y-axis telling us? The x-axis shows how often a student wears sunscreen, and the y-axis shows the percentage of the total that fall into that category (broken into two bars, one for Hispanic and one for non-Hispanic). What statistical questions can you answer with this graph? Do Hispanics and non- Hispanics have different approaches to sunscreen? What percent of Hispanics always/never wear sunscreen? How does that compare to non-Hispanics? Discuss this plot with the class by eliciting students\u2019 responses to the questions. Students actively listen to the discussion by confirming, correcting, or adding to their own responses. Close the discussion by asking students: What questions might you have about this plot? What additional information would be helpful? Display the following plot to the class ( LMR_1.16 ) LMR_1.16 Ask students to individually answer the following questions on the handout ( LMR_1.16 ): What kind of plot is this and how will you remember its features? Split or faceted histogram. How many variables are displayed in this plot? Name the variable(s). Two variables: height and gender. What are the x-axis and y-axis telling us? The x-axis shows height in feet, and the y-axis shows the total that fall into a certain range of heights (broken into two histograms, one for males and one for females). What statistical questions can you answer with this graph? Do males and females differ in height? What is the typical female height? What is the typical male height? Discuss this plot with the class by eliciting students\u2019 responses to the questions. Students actively listen to the discussion by confirming, correcting, or adding to their own responses. Using the notes and sketches in their DS journals, students will summarize their learning of how to read and interpret basic multiple variable plots by completing the Multiple Variable Plots fourfold graphic organizer ( LMR_1.17 ): LMR_1.17 Class Scribes: One team of students will give a brief talk to discuss what they think the 3 most important topics of the day were. Next 2 Days LAB 1D: Zooming through Data LAB 1E: What\u2019s the Relationship? Complete Lab 1D and 1E prior to the Practicum.","title":"Lesson 14: Variables, Variables, Variables"},{"location":"unit1/lesson14/#lesson-14-variables-variables-variables","text":"","title":"Lesson 14: Variables, Variables, Variables"},{"location":"unit1/lesson14/#objective","text":"Students will learn how to read and interpret multiple variable plots: bivariate scatterplots, multiple variable scatterplots, stacked bar plots, and side-by-side bar plots. They will summarize their learning about multiple variable plots using a four-fold graphic organizer.","title":"Objective:"},{"location":"unit1/lesson14/#materials","text":"Scatterplot of Heights & Weights ( LMR_1.13 ) Scatterplot of Heights & Weights, Split by Gender ( LMR_1.14 ) Side-by-Side Bar Chart ( LMR_1.15 ) Faceted Histogram of Height by Gender ( LMR_1.16 ) Summarizing Multi-Variable Plots graphic organizer ( LMR_1.17 )","title":"Materials:"},{"location":"unit1/lesson14/#vocabulary","text":"scatterplot grouping side-by-side bar plot","title":"Vocabulary:"},{"location":"unit1/lesson14/#essential-concepts","text":"Essential Concepts: To examine whether two (or more) variables are related, we can plot their distributions on the same graph.","title":"Essential Concepts:"},{"location":"unit1/lesson14/#lesson","text":"Begin by informing students that they will learn how to make visual displays using more than one variable, and by asking them to ponder the following questions: What do you think is the relation between people\u2019s heights and weights? Are taller people heavier? Always? Or is this just a tendency? What do you think is the relation between people's heights and weights? Are taller people heavier? Always? Or is this just a tendency? Let's look at some data. Display the following plot to the class ( LMR_1.13 ) so they can see some actual data: LMR_1.13 Ask students to individually answer the following questions about the plot on the handout ( LMR_1.13 ): What kind of plot is this and how will you remember its features? Scatterplot. How many variables are displayed in this plot? Name the variable(s) and identify the type of variable(s). Two variables. Weight in kilograms and height in meters. Numerical variables. What do the axes show? The x-axis shows the height of teens in meters, and the y-axis shows the weight of teens in kilograms. Do taller people weigh more? Not necessarily, but there is a tendency for this to be true. Discuss this plot with the class by eliciting students\u2019 responses to the questions. Students actively listen to the discussion by confirming, correcting, or adding to their own responses. Close the discussion by asking students: What questions might you have about this plot? What additional information would be helpful? Now, suppose we could see which of these dots represented girls and which represented boys. Where do you think most of the girls' dots would be relative to the boys? Display the following plot to the class ( LMR_1.14 ): LMR_1.14 Ask students to individually answer the following questions about the plot on the handout ( LMR_1.14 ): What kind of plot is this and how will you remember its features? Scatterplot. How many variables are displayed in this plot? Name the variable(s) and identify the type of variable(s). Three variables. Weight in kilograms and height in meters are numerical variables. Gender is categorical. What do the axes show? The x-axis shows the height of teens in meters, and the y-axis shows the weight of teens in kilograms. What questions can we ask that this graph might answer? Who is taller, boys or girls? Who weighs more? Is the association between height and weight the same for boys as it is for girls? Discuss this plot with the class by eliciting students\u2019 responses to the questions. Students actively listen to the discussion by confirming, correcting, or adding to their own responses. Follow-up discussion: when the data are split into categories, it is called grouping . Close the discussion by asking students: What questions might you have about this plot? What additional information would be helpful? Display the following plot to the class ( LMR_1.15 ): LMR_1.15 Ask students to individually answer the following questions on the handout ( LMR_1.15 ): What kind of plot is this and how will you remember its features? Side-by-side bar chart. How many variables are displayed in this plot? Name the variable(s). Two variables: whether or not someone is Hispanic, and how often they wear sunscreen. What are the x-axis and y-axis telling us? The x-axis shows how often a student wears sunscreen, and the y-axis shows the percentage of the total that fall into that category (broken into two bars, one for Hispanic and one for non-Hispanic). What statistical questions can you answer with this graph? Do Hispanics and non- Hispanics have different approaches to sunscreen? What percent of Hispanics always/never wear sunscreen? How does that compare to non-Hispanics? Discuss this plot with the class by eliciting students\u2019 responses to the questions. Students actively listen to the discussion by confirming, correcting, or adding to their own responses. Close the discussion by asking students: What questions might you have about this plot? What additional information would be helpful? Display the following plot to the class ( LMR_1.16 ) LMR_1.16 Ask students to individually answer the following questions on the handout ( LMR_1.16 ): What kind of plot is this and how will you remember its features? Split or faceted histogram. How many variables are displayed in this plot? Name the variable(s). Two variables: height and gender. What are the x-axis and y-axis telling us? The x-axis shows height in feet, and the y-axis shows the total that fall into a certain range of heights (broken into two histograms, one for males and one for females). What statistical questions can you answer with this graph? Do males and females differ in height? What is the typical female height? What is the typical male height? Discuss this plot with the class by eliciting students\u2019 responses to the questions. Students actively listen to the discussion by confirming, correcting, or adding to their own responses. Using the notes and sketches in their DS journals, students will summarize their learning of how to read and interpret basic multiple variable plots by completing the Multiple Variable Plots fourfold graphic organizer ( LMR_1.17 ): LMR_1.17","title":"Lesson:"},{"location":"unit1/lesson14/#class-scribes","text":"One team of students will give a brief talk to discuss what they think the 3 most important topics of the day were.","title":"Class Scribes:"},{"location":"unit1/lesson14/#next-2-days","text":"LAB 1D: Zooming through Data LAB 1E: What\u2019s the Relationship? Complete Lab 1D and 1E prior to the Practicum.","title":"Next 2 Days"},{"location":"unit1/lesson15/","text":"Lesson 15: Americans' Time on Task Objective: Introduction to Time Use Campaign. Students will explore a multimedia graphic that incorporates data from the American Time Use Survey to spark their interest about how they spend their time. They will begin to learn how to evaluate reports that make claims based on data by reading The Washington Post article Teens Are Spending More Time Consuming Social Media, On Mobile Devices. Materials: Computers Data Collection Devices Interactive multimedia graphic titled How Men and Women Spend Their Days found at: https://flowingdata.com/2021/09/21/how-men-and-women-spend-their-days/ Article: The Washington Post\u2019s Teens Are Spending More Time Consuming Social Media, on Mobile Devices found at: ( LMR_Teens_Consuming_Media ) K-L-W Graphic Organizer ( LMR_TR_K-L-W Chart ) Vocabulary: evaluate claim Essential Concepts: Essential Concepts: Learning to examine other analyses is an important part of statistical thinking. Lesson: Become familiar with the Time-Use Campaign Guidelines (shown at the end of this lesson), particularly the big questions, to help guide students during the campaign (see Campaign Guidelines in Teacher Resources ). In pairs, ask students to make predictions based on the big questions in the Time-Use Campaign Guidelines . Next, inform students that The Bureau of Labor Statistics (BLS) collects data about Americans\u2019 daily time use and that they will be exploring time use through an interactive graphic. Ask students to go to the multimedia graphic at the following URL: https://flowingdata.com/2021/09/21/how-men-and-women-spend-their-days/ Students will spend 10 minutes exploring the interactive graphic. Their task is to answer the following questions (display questions to students): What variables are represented in this graphic? The variables represented are activities that Americans spend their time doing. These include sleeping, working, traveling, etc. Gender, employment status and weekday/weekend are also variables that are represented. Explain what the graphic is telling you. The graphic shows the proportion of people living in the United States, broken down by employment status and gender, who are engaged in different activities (Denoted by the colors) at different times of the day. Where did the data come from? The data come from thousands of Americans over the age of 15 who took a survey recalling every minute of a day in 2020. What are some interesting findings? Be prepared to share. Answers will vary. Ask students to share their findings in pairs. Each pair will agree on and select one finding to share with the class. In a Whip Around , ask each pair to share their finding. Inform students that they will continue to investigate Americans\u2019 daily time use. Using the KLW graphic organizer , read out loud the title of The Washington Post article: Teens Are Spending More Time Consuming Social Media , On Mobile Devices . Ask them to write what they know about the topic in the Know column. Note to Teacher: If this is the first time using KLW, please take time to provide an overview of the graphic organizer. Next, ask students to read the article individually: LMR_Teens_Consuming_Media As they read, students may complete the Learn column of the KLW graphic organizer . Ask students to complete the Want to Learn column when they finish reading the article. When reading a newspaper, magazine, or blog that includes statistical analysis, it is important to evaluate , or think carefully, about claims that these articles state as fact. Ask students to work in teams to evaluate the article based on the questions below: Who was observed and what were the variables observed? A group of 8 to 18-year-olds were observed, and the variables observed had to do with consuming media - watching TV, listening to music, surfing the Web, playing video games, and time spent on mobile devices. What statistical questions were they trying to answer? Possible statistical question: How much time per day does today\u2019s typical 8 to 18-year-old spend consuming media? Who collected the data? There were 3 sources cited. The Kaiser Family Foundation collected data in a 2010 study, the Pew Internet and American Life Project collected data in a 2011 study, and the Bureau of Labor Statistics collected data in 2011 with the American Time Use Survey. How was the data collected? Two were studies whose data collection method is not stated, and one was a survey. What claim(s) did the article make? Main claim: \u201cToday\u2019s teens spend more than 7.5 hours a day consuming media.\u201d What are some statistics that the article used to make the claim(s)? Examples include: Teens use their cellphones to send an average of 60 texts a day. On average, high school students spent less than one hour per weekday on sports, exercise, and recreation. Select a whole group share-out/discussion strategy from the Instructional Strategies Teacher Resource to discuss the answers to the evaluation questions. Inform students that they will engage in the Time-Use Participatory Sensing campaign and will begin to collect data about their own time use. Follow the Time-Use Guidelines . Reminder: Once logged into the app or the browser-based version, students may go to Campaigns to see the campaigns in which they are participating. They can then add the campaign by tapping the name of the campaign. If no campaigns are visible, ask them to click the refresh option. Emphasize that this data should be logged in a journal three times per day. Students should set reminders. Class Scribes: One team of students will give a brief talk to discuss what they think the 3 most important topics of the day were. Homework & Next Day For the next 5 days, students will collect data using the Time Use campaign on their smart devices or via web browser. LAB 1F: A Diamond in the Rough and Data Collection Monitoring Data Collection Monitoring: Display the IDS Campaign Monitoring Tool, found at https://portal.idsucla.org Click on Campaign Monitor and sign in. See User List and sort by Total . Ask: Who has collected the most data so far? Click on the pie chart. Ask: How many active users are there? How many inactive users are there? See Total Responses . How many responses have been submitted? Using TPS, ask students to think about what they can do to increase their data collection. Inform students that you will conduct another data collection check with the whole class in a couple of days, and that they will understand the private vs. shared data after they have completed the campaign collection. Complete Lab 1F prior to Lesson 16","title":"Lesson 15: Americans\u2019 Time on Task"},{"location":"unit1/lesson15/#lesson-15-americans-time-on-task","text":"","title":"Lesson 15: Americans' Time on Task"},{"location":"unit1/lesson15/#objective","text":"Introduction to Time Use Campaign. Students will explore a multimedia graphic that incorporates data from the American Time Use Survey to spark their interest about how they spend their time. They will begin to learn how to evaluate reports that make claims based on data by reading The Washington Post article Teens Are Spending More Time Consuming Social Media, On Mobile Devices.","title":"Objective:"},{"location":"unit1/lesson15/#materials","text":"Computers Data Collection Devices Interactive multimedia graphic titled How Men and Women Spend Their Days found at: https://flowingdata.com/2021/09/21/how-men-and-women-spend-their-days/ Article: The Washington Post\u2019s Teens Are Spending More Time Consuming Social Media, on Mobile Devices found at: ( LMR_Teens_Consuming_Media ) K-L-W Graphic Organizer ( LMR_TR_K-L-W Chart )","title":"Materials:"},{"location":"unit1/lesson15/#vocabulary","text":"evaluate claim","title":"Vocabulary:"},{"location":"unit1/lesson15/#essential-concepts","text":"Essential Concepts: Learning to examine other analyses is an important part of statistical thinking.","title":"Essential Concepts:"},{"location":"unit1/lesson15/#lesson","text":"Become familiar with the Time-Use Campaign Guidelines (shown at the end of this lesson), particularly the big questions, to help guide students during the campaign (see Campaign Guidelines in Teacher Resources ). In pairs, ask students to make predictions based on the big questions in the Time-Use Campaign Guidelines . Next, inform students that The Bureau of Labor Statistics (BLS) collects data about Americans\u2019 daily time use and that they will be exploring time use through an interactive graphic. Ask students to go to the multimedia graphic at the following URL: https://flowingdata.com/2021/09/21/how-men-and-women-spend-their-days/ Students will spend 10 minutes exploring the interactive graphic. Their task is to answer the following questions (display questions to students): What variables are represented in this graphic? The variables represented are activities that Americans spend their time doing. These include sleeping, working, traveling, etc. Gender, employment status and weekday/weekend are also variables that are represented. Explain what the graphic is telling you. The graphic shows the proportion of people living in the United States, broken down by employment status and gender, who are engaged in different activities (Denoted by the colors) at different times of the day. Where did the data come from? The data come from thousands of Americans over the age of 15 who took a survey recalling every minute of a day in 2020. What are some interesting findings? Be prepared to share. Answers will vary. Ask students to share their findings in pairs. Each pair will agree on and select one finding to share with the class. In a Whip Around , ask each pair to share their finding. Inform students that they will continue to investigate Americans\u2019 daily time use. Using the KLW graphic organizer , read out loud the title of The Washington Post article: Teens Are Spending More Time Consuming Social Media , On Mobile Devices . Ask them to write what they know about the topic in the Know column. Note to Teacher: If this is the first time using KLW, please take time to provide an overview of the graphic organizer. Next, ask students to read the article individually: LMR_Teens_Consuming_Media As they read, students may complete the Learn column of the KLW graphic organizer . Ask students to complete the Want to Learn column when they finish reading the article. When reading a newspaper, magazine, or blog that includes statistical analysis, it is important to evaluate , or think carefully, about claims that these articles state as fact. Ask students to work in teams to evaluate the article based on the questions below: Who was observed and what were the variables observed? A group of 8 to 18-year-olds were observed, and the variables observed had to do with consuming media - watching TV, listening to music, surfing the Web, playing video games, and time spent on mobile devices. What statistical questions were they trying to answer? Possible statistical question: How much time per day does today\u2019s typical 8 to 18-year-old spend consuming media? Who collected the data? There were 3 sources cited. The Kaiser Family Foundation collected data in a 2010 study, the Pew Internet and American Life Project collected data in a 2011 study, and the Bureau of Labor Statistics collected data in 2011 with the American Time Use Survey. How was the data collected? Two were studies whose data collection method is not stated, and one was a survey. What claim(s) did the article make? Main claim: \u201cToday\u2019s teens spend more than 7.5 hours a day consuming media.\u201d What are some statistics that the article used to make the claim(s)? Examples include: Teens use their cellphones to send an average of 60 texts a day. On average, high school students spent less than one hour per weekday on sports, exercise, and recreation. Select a whole group share-out/discussion strategy from the Instructional Strategies Teacher Resource to discuss the answers to the evaluation questions. Inform students that they will engage in the Time-Use Participatory Sensing campaign and will begin to collect data about their own time use. Follow the Time-Use Guidelines . Reminder: Once logged into the app or the browser-based version, students may go to Campaigns to see the campaigns in which they are participating. They can then add the campaign by tapping the name of the campaign. If no campaigns are visible, ask them to click the refresh option. Emphasize that this data should be logged in a journal three times per day. Students should set reminders.","title":"Lesson:"},{"location":"unit1/lesson15/#class-scribes","text":"One team of students will give a brief talk to discuss what they think the 3 most important topics of the day were.","title":"Class Scribes:"},{"location":"unit1/lesson15/#homework-next-day","text":"For the next 5 days, students will collect data using the Time Use campaign on their smart devices or via web browser. LAB 1F: A Diamond in the Rough and Data Collection Monitoring Data Collection Monitoring: Display the IDS Campaign Monitoring Tool, found at https://portal.idsucla.org Click on Campaign Monitor and sign in. See User List and sort by Total . Ask: Who has collected the most data so far? Click on the pie chart. Ask: How many active users are there? How many inactive users are there? See Total Responses . How many responses have been submitted? Using TPS, ask students to think about what they can do to increase their data collection. Inform students that you will conduct another data collection check with the whole class in a couple of days, and that they will understand the private vs. shared data after they have completed the campaign collection. Complete Lab 1F prior to Lesson 16","title":"Homework &amp; Next Day"},{"location":"unit1/lesson16/","text":"Lesson 16: Categorical Associations Objective: Students will learn to construct, interpret, and calculate the joint relative frequencies of two-way frequency tables. Vocabulary: two-way frequency table joint relative frequency Essential Concepts: Essential Concepts: A two-way table is a summary of the association/relationship between two categorical variables. Joint relative frequencies answer questions of the form \"what proportion of the people/objects had this value on the first variable and this value on the second.\" Lesson: Launch the lesson by displaying the following scenario: Rosa has a theory that cat owners are also musical. To find out, she decided to collect data that would help her understand the relationship between cat ownership and instrument playing among the students in her art class. She conducted a survey and found that out of the 35 students in her art class, 16 owned a cat and out of those that owned a cat, 7 played an instrument. She also discovered that 9 owned a cat, but did not play an instrument. There were also 9 students who neither owned a cat nor played an instrument. Inform students that Rosa asked two questions that provided the data for her two-way frequency table. What could those two questions be? Possible Answer: Question 1\u2014Do you play an instrument? Question 2\u2014Do you own a cat? What variables did Rosa collect? What were the values of those variables? In pairs, write out on paper what the original data must have looked like. Answer: Variable 1: Owns Cat. Variable 2: Plays Instrument Owns Cat Plays Instrument Yes Yes (There are 7 of these) Yes No (9 of these) No Yes (10 of these) No No (9 of these) Inform students that today they will be looking at associations in categorical variables. Note: If necessary, review the difference between questions for categorical and numerical variables. Explain that their task is to summarize Rosa\u2019s findings in one table that shows totals. Remind students to use their knowledge of data structures from Lesson 2 , especially organizing in rows and columns. Allow time for teams to wrestle with how to organize their data in one table. As teams work, walk around monitoring their data tables. Select a few data tables to display and share with the entire class. Explain the Anonymous Author strategy to students (see Instructional Strategies in Teacher Resources ). Display the data tables and ask students to engage in the Anonymous Author strategy. You may want to start the discussion by asking about the total number of students Rosa surveyed. Make sure the last data table you display correctly shows a two-way frequency table . A two-way frequency table displays the data that pertains to two categories from one group. One category is represented in rows and the other is represented in columns. In this exercise, the group is a class of art students. Cat Ownership and Instruments Plays an instrument Does not play instrument Total Owns a cat 7 9 16 No cats 10 9 19 Total 17 18 35 Based on the Cat Ownership and Instruments table, ask student teams to generate questions that can be asked and answered by the data. In a Whip Around , ask student teams to share one of their questions. Explain that a two-way frequency table can show relative frequencies. A relative frequency is how often something occurs in relation to the total number of occurrences, and is expressed as a proportion or percentage of a total. For example, what is the relative frequency of those who own a cat and play an instrument? Answer: 7/35 or 0.2 or 20%. Note: Review how to write a proportion and how to express a proportion as a percent. Ask students to calculate the relative frequencies for the entire table. They may check their calculations with a partner. Cat Ownership and Instruments Relative Frequencies Plays an instrument Does not play instrument Total Owns a cat 7/35=0.20 9/35 \u2248 0.26 16/35 \u2248 0.46 No cats 10/35 \u2248 0.29 9/35 \u2248 0.26 19/35 \u2248 0.54 Total 17/35 \u2248 0.49 18/35 \u2248 0.51 35/35 = 1.00 In teams, students will generate 2 questions about 2 categorical variables. Allow teams 3-5 minutes to generate their questions. Students should not choose two random categorical variables. Rather, they should choose two categorical variables that they predict might be associated. The team will create a two-way table that corresponds to their categorical variables. Class Scribes: One team of students will give a brief talk to discuss what they think the 3 most important topics of the day were. Homework Rosa posed this statistical question: What proportion of students did not play an instrument and did not own a cat? Use what you know about two-way tables to answer her question.","title":"Lesson 16: Categorical Associations"},{"location":"unit1/lesson16/#lesson-16-categorical-associations","text":"","title":"Lesson 16: Categorical Associations"},{"location":"unit1/lesson16/#objective","text":"Students will learn to construct, interpret, and calculate the joint relative frequencies of two-way frequency tables.","title":"Objective:"},{"location":"unit1/lesson16/#vocabulary","text":"two-way frequency table joint relative frequency","title":"Vocabulary:"},{"location":"unit1/lesson16/#essential-concepts","text":"Essential Concepts: A two-way table is a summary of the association/relationship between two categorical variables. Joint relative frequencies answer questions of the form \"what proportion of the people/objects had this value on the first variable and this value on the second.\"","title":"Essential Concepts:"},{"location":"unit1/lesson16/#lesson","text":"Launch the lesson by displaying the following scenario: Rosa has a theory that cat owners are also musical. To find out, she decided to collect data that would help her understand the relationship between cat ownership and instrument playing among the students in her art class. She conducted a survey and found that out of the 35 students in her art class, 16 owned a cat and out of those that owned a cat, 7 played an instrument. She also discovered that 9 owned a cat, but did not play an instrument. There were also 9 students who neither owned a cat nor played an instrument. Inform students that Rosa asked two questions that provided the data for her two-way frequency table. What could those two questions be? Possible Answer: Question 1\u2014Do you play an instrument? Question 2\u2014Do you own a cat? What variables did Rosa collect? What were the values of those variables? In pairs, write out on paper what the original data must have looked like. Answer: Variable 1: Owns Cat. Variable 2: Plays Instrument Owns Cat Plays Instrument Yes Yes (There are 7 of these) Yes No (9 of these) No Yes (10 of these) No No (9 of these) Inform students that today they will be looking at associations in categorical variables. Note: If necessary, review the difference between questions for categorical and numerical variables. Explain that their task is to summarize Rosa\u2019s findings in one table that shows totals. Remind students to use their knowledge of data structures from Lesson 2 , especially organizing in rows and columns. Allow time for teams to wrestle with how to organize their data in one table. As teams work, walk around monitoring their data tables. Select a few data tables to display and share with the entire class. Explain the Anonymous Author strategy to students (see Instructional Strategies in Teacher Resources ). Display the data tables and ask students to engage in the Anonymous Author strategy. You may want to start the discussion by asking about the total number of students Rosa surveyed. Make sure the last data table you display correctly shows a two-way frequency table . A two-way frequency table displays the data that pertains to two categories from one group. One category is represented in rows and the other is represented in columns. In this exercise, the group is a class of art students. Cat Ownership and Instruments Plays an instrument Does not play instrument Total Owns a cat 7 9 16 No cats 10 9 19 Total 17 18 35 Based on the Cat Ownership and Instruments table, ask student teams to generate questions that can be asked and answered by the data. In a Whip Around , ask student teams to share one of their questions. Explain that a two-way frequency table can show relative frequencies. A relative frequency is how often something occurs in relation to the total number of occurrences, and is expressed as a proportion or percentage of a total. For example, what is the relative frequency of those who own a cat and play an instrument? Answer: 7/35 or 0.2 or 20%. Note: Review how to write a proportion and how to express a proportion as a percent. Ask students to calculate the relative frequencies for the entire table. They may check their calculations with a partner. Cat Ownership and Instruments Relative Frequencies Plays an instrument Does not play instrument Total Owns a cat 7/35=0.20 9/35 \u2248 0.26 16/35 \u2248 0.46 No cats 10/35 \u2248 0.29 9/35 \u2248 0.26 19/35 \u2248 0.54 Total 17/35 \u2248 0.49 18/35 \u2248 0.51 35/35 = 1.00 In teams, students will generate 2 questions about 2 categorical variables. Allow teams 3-5 minutes to generate their questions. Students should not choose two random categorical variables. Rather, they should choose two categorical variables that they predict might be associated. The team will create a two-way table that corresponds to their categorical variables.","title":"Lesson:"},{"location":"unit1/lesson16/#class-scribes","text":"One team of students will give a brief talk to discuss what they think the 3 most important topics of the day were.","title":"Class Scribes:"},{"location":"unit1/lesson16/#homework","text":"Rosa posed this statistical question: What proportion of students did not play an instrument and did not own a cat? Use what you know about two-way tables to answer her question.","title":"Homework"},{"location":"unit1/lesson17/","text":"Lesson 17: Interpreting Two-Way Tables Objective: Students will calculate conditional, marginal, and joint frequencies and explain what they mean in the context of the data. Materials: Poster paper Markers Analyzing Categorical Variables ( LMR_1.18 ) Advanced preparation required (see step 19 below) Interpreting Categorical Variables ( LMR_1.19 ) Vocabulary: marginal frequency joint frequency conditional relative frequency Essential Concepts: Essential Concepts: Marginal (relative) frequencies tell us about the distribution of a single variable. Conditional relative frequencies tell us about the distribution of one variable when \"subsetting\" the other. Lesson: Time Use Campaign Data Collection Monitoring: Display the IDS Campaign Monitoring Tool, found at https://portal.idsucla.org Click on Campaign Monitor and sign in. Inform students that you will be monitoring their data collection again today. See User List and sort by Total . Ask: Who has collected the most data so far? Click on the pie chart. Ask: How many active users are there? How many inactive users are there? See Total Responses . How many responses have been submitted? Using TPS, ask students to think about what can they do to increase their data collection. Remind students that this is the last day to collect data. Ask student teams to take out the 2 questions and the two-way table that they created in the previous day\u2019s lesson. Before teams ask the class their questions, ask them to strategize about how they will collect and record their data, because they can only ask the 2 questions. Students in the class will respond to each question by raising their hands. In a Whip Around , have each team ask their 2 questions. Pause briefly between teams so that the asking team has time to collect and record their data. Students will use their frequency tables before the end of the lesson. Recall that in the previous lesson, students learned to calculate relative frequencies. Now it\u2019s time to look at other ways of understanding a two-way frequency table. Display the Cat Ownership and Instruments table: Cat Ownership and Instruments Plays an instrument Does not play instrument Total Owns a cat 7 9 16 No cats 10 9 19 Total 17 18 35 Suppose that we want to know the following information (display questions): How many students own a cat? 16 What is the proportion of students who own a cat? 16/35 \u2248 0.46 What is the proportion of students who do not play an instrument? 18/35 \u2248 0.51 In teams, discuss where on the table you would find this information and how you would calculate it. The specific answers are not important; but what is important is to know how to obtain the information. Possible answer: You would find the proportion of students who do not play an instrument by dividing the number in the \u201cTotal\u201d row that is in the \"Does not play instrument\" column by the total number of students (35). After a few minutes, ask a team to volunteer a response. Mark up the margins on the table to show that the cells with the initial total counts are called marginal frequencies . Note: 10 b and c are marginal relative frequencies. Now suppose that we want to know the following information (display questions): How many students own a cat and play an instrument? 7 What is the proportion of students that own a cat and play and instrument? 7/35=0.2 What is the proportion of students who do not own a cat and play an instrument? 10/35 \u2248 0.286 In teams, discuss where on the table you would find this information and how you would calculate it. The specific answers are not important; but what is important is to know how to obtain the information. Possible answer: You would find the answers in the cells that make up the body of the table. The value for each proportion is the frequency for each cell over the total number of observations. After a few minutes, ask a team to volunteer a response. Mark up the cells in the body of the table to show that the cells with the initial counts are called joint frequencies . Note: 13 b and c are joint relative frequencies. Finally, suppose that we wanted to answer the question: Do a greater proportion of students in Rosa\u2019s art class who do not own cats prefer to play an instrument than those who do own cats? In teams, discuss where on the table you would find this information and how you would calculate it. The specific answers are not important; but what is important is to know how to obtain the information. After a few minutes, ask a team to volunteer a response. Encourage students to agree or disagree with the explanations provided. Lead students to see that the total for the \u201cNo cats\u201d row is important because we are only concerned with that subset of the group. Mark up \u201cNo cats\u201d row on the table to show that we have conditioned, or are bound, by this variable. Compare the values that show the conditional relative frequency for the row. More non-cat owners slightly prefer to play an instrument (display table below). Cat Ownership and Instruments Conditional Relative Frequencies by Row Plays an instrument Does not play instrument Total Owns a cat 7/16 \u2248 0.44 9/16 \u2248 0.56 16/16 \u2248 1.00 No cats 10/19 \u2248 0.53 9/19 \u2248 0.47 19/19 \u2248 1.00 Total 17/35 \u2248 0.49 18/35 \u2248 0.51 35/35 = 1.00 Note: This is a conditional relative frequency by row. We can also calculate conditional relative frequencies by column if we were interested in knowing the difference in cat preference for those who play instruments versus those who don\u2019t. Distribute one full set of cards from the Analyzing Categorical Variables file ( LMR_1.18 ) to each student team. Advanced preparation required: Print the Analyzing Categorical Variables file ( LMR_1.18 ). The handouts can then be cut into a total of 20 cards (12 visuals, 8 numerical summaries). You will need enough sets of the cards for each student team to share one full set. For example, if there are 5 student teams in a class, then 5 copies of the file will need to be printed so that each team gets all 20 cards. LMR_1.18 Distribute LMR_1.19_ Interpreting Categorical Variables to each student team. LMR_1.19 Each student team will work together and decide which visualization(s) and numerical summaries can be used to answer each statistical question. They will then answer each statistical question, citing a numerical summary as evidence. Note: Student teams may tape or glue visuals and numerical summaries onto LMR_1.19 , or they can simply write the plot letter and table number in the appropriate box. The blank column is for student teams to write a statistical question than can be answered with a visual and a numerical summary that was not used. After student teams have been allotted ample time to complete LMR_1.19 , lead a class discussion to go over the answers. It is extremely important to have students justify their answers by referring to their visuals and tables. For example, the statistical question \u201cHow many students neither own a cat or play an instrument?\u201d can be answered with Plot E, Plot G, Plot K, Plot I, and with Tables 1 and 7. Ask students to refer back to the two-way frequency tables they created earlier. Have each team create one poster that shows their two-way frequency table. Then, ask each team to ask 4 questions about the data in their table that must be answered by a: marginal frequency marginal relative frequency joint relative frequency conditional relative frequency (either by row or column) If time permits, pair teams up and ask them to present their findings to each other. Class Scribes: One team of students will give a brief talk to discuss what they think the 3 most important topics of the day were. Homework & Next Day Using the data below, generate 2 questions: one must be answered with a marginal relative frequency and the other must be answered by a conditional relative frequency. Gender and the Color Red Which emotion do you most relate with the color red? Love Anger Fear Total Male 7 11 5 23 Female 12 15 10 37 Total 19 26 15 60 Lab 1G: What\u2019s the FREQ? Complete Lab 1G prior to the Practicum .","title":"Lesson 17: Interpreting Two-Way Tables"},{"location":"unit1/lesson17/#lesson-17-interpreting-two-way-tables","text":"","title":"Lesson 17: Interpreting Two-Way Tables"},{"location":"unit1/lesson17/#objective","text":"Students will calculate conditional, marginal, and joint frequencies and explain what they mean in the context of the data.","title":"Objective:"},{"location":"unit1/lesson17/#materials","text":"Poster paper Markers Analyzing Categorical Variables ( LMR_1.18 ) Advanced preparation required (see step 19 below) Interpreting Categorical Variables ( LMR_1.19 )","title":"Materials:"},{"location":"unit1/lesson17/#vocabulary","text":"marginal frequency joint frequency conditional relative frequency","title":"Vocabulary:"},{"location":"unit1/lesson17/#essential-concepts","text":"Essential Concepts: Marginal (relative) frequencies tell us about the distribution of a single variable. Conditional relative frequencies tell us about the distribution of one variable when \"subsetting\" the other.","title":"Essential Concepts:"},{"location":"unit1/lesson17/#lesson","text":"Time Use Campaign Data Collection Monitoring: Display the IDS Campaign Monitoring Tool, found at https://portal.idsucla.org Click on Campaign Monitor and sign in. Inform students that you will be monitoring their data collection again today. See User List and sort by Total . Ask: Who has collected the most data so far? Click on the pie chart. Ask: How many active users are there? How many inactive users are there? See Total Responses . How many responses have been submitted? Using TPS, ask students to think about what can they do to increase their data collection. Remind students that this is the last day to collect data. Ask student teams to take out the 2 questions and the two-way table that they created in the previous day\u2019s lesson. Before teams ask the class their questions, ask them to strategize about how they will collect and record their data, because they can only ask the 2 questions. Students in the class will respond to each question by raising their hands. In a Whip Around , have each team ask their 2 questions. Pause briefly between teams so that the asking team has time to collect and record their data. Students will use their frequency tables before the end of the lesson. Recall that in the previous lesson, students learned to calculate relative frequencies. Now it\u2019s time to look at other ways of understanding a two-way frequency table. Display the Cat Ownership and Instruments table: Cat Ownership and Instruments Plays an instrument Does not play instrument Total Owns a cat 7 9 16 No cats 10 9 19 Total 17 18 35 Suppose that we want to know the following information (display questions): How many students own a cat? 16 What is the proportion of students who own a cat? 16/35 \u2248 0.46 What is the proportion of students who do not play an instrument? 18/35 \u2248 0.51 In teams, discuss where on the table you would find this information and how you would calculate it. The specific answers are not important; but what is important is to know how to obtain the information. Possible answer: You would find the proportion of students who do not play an instrument by dividing the number in the \u201cTotal\u201d row that is in the \"Does not play instrument\" column by the total number of students (35). After a few minutes, ask a team to volunteer a response. Mark up the margins on the table to show that the cells with the initial total counts are called marginal frequencies . Note: 10 b and c are marginal relative frequencies. Now suppose that we want to know the following information (display questions): How many students own a cat and play an instrument? 7 What is the proportion of students that own a cat and play and instrument? 7/35=0.2 What is the proportion of students who do not own a cat and play an instrument? 10/35 \u2248 0.286 In teams, discuss where on the table you would find this information and how you would calculate it. The specific answers are not important; but what is important is to know how to obtain the information. Possible answer: You would find the answers in the cells that make up the body of the table. The value for each proportion is the frequency for each cell over the total number of observations. After a few minutes, ask a team to volunteer a response. Mark up the cells in the body of the table to show that the cells with the initial counts are called joint frequencies . Note: 13 b and c are joint relative frequencies. Finally, suppose that we wanted to answer the question: Do a greater proportion of students in Rosa\u2019s art class who do not own cats prefer to play an instrument than those who do own cats? In teams, discuss where on the table you would find this information and how you would calculate it. The specific answers are not important; but what is important is to know how to obtain the information. After a few minutes, ask a team to volunteer a response. Encourage students to agree or disagree with the explanations provided. Lead students to see that the total for the \u201cNo cats\u201d row is important because we are only concerned with that subset of the group. Mark up \u201cNo cats\u201d row on the table to show that we have conditioned, or are bound, by this variable. Compare the values that show the conditional relative frequency for the row. More non-cat owners slightly prefer to play an instrument (display table below). Cat Ownership and Instruments Conditional Relative Frequencies by Row Plays an instrument Does not play instrument Total Owns a cat 7/16 \u2248 0.44 9/16 \u2248 0.56 16/16 \u2248 1.00 No cats 10/19 \u2248 0.53 9/19 \u2248 0.47 19/19 \u2248 1.00 Total 17/35 \u2248 0.49 18/35 \u2248 0.51 35/35 = 1.00 Note: This is a conditional relative frequency by row. We can also calculate conditional relative frequencies by column if we were interested in knowing the difference in cat preference for those who play instruments versus those who don\u2019t. Distribute one full set of cards from the Analyzing Categorical Variables file ( LMR_1.18 ) to each student team. Advanced preparation required: Print the Analyzing Categorical Variables file ( LMR_1.18 ). The handouts can then be cut into a total of 20 cards (12 visuals, 8 numerical summaries). You will need enough sets of the cards for each student team to share one full set. For example, if there are 5 student teams in a class, then 5 copies of the file will need to be printed so that each team gets all 20 cards. LMR_1.18 Distribute LMR_1.19_ Interpreting Categorical Variables to each student team. LMR_1.19 Each student team will work together and decide which visualization(s) and numerical summaries can be used to answer each statistical question. They will then answer each statistical question, citing a numerical summary as evidence. Note: Student teams may tape or glue visuals and numerical summaries onto LMR_1.19 , or they can simply write the plot letter and table number in the appropriate box. The blank column is for student teams to write a statistical question than can be answered with a visual and a numerical summary that was not used. After student teams have been allotted ample time to complete LMR_1.19 , lead a class discussion to go over the answers. It is extremely important to have students justify their answers by referring to their visuals and tables. For example, the statistical question \u201cHow many students neither own a cat or play an instrument?\u201d can be answered with Plot E, Plot G, Plot K, Plot I, and with Tables 1 and 7. Ask students to refer back to the two-way frequency tables they created earlier. Have each team create one poster that shows their two-way frequency table. Then, ask each team to ask 4 questions about the data in their table that must be answered by a: marginal frequency marginal relative frequency joint relative frequency conditional relative frequency (either by row or column) If time permits, pair teams up and ask them to present their findings to each other.","title":"Lesson:"},{"location":"unit1/lesson17/#class-scribes","text":"One team of students will give a brief talk to discuss what they think the 3 most important topics of the day were.","title":"Class Scribes:"},{"location":"unit1/lesson17/#homework-next-day","text":"Using the data below, generate 2 questions: one must be answered with a marginal relative frequency and the other must be answered by a conditional relative frequency. Gender and the Color Red Which emotion do you most relate with the color red? Love Anger Fear Total Male 7 11 5 23 Female 12 15 10 37 Total 19 26 15 60 Lab 1G: What\u2019s the FREQ? Complete Lab 1G prior to the Practicum .","title":"Homework &amp; Next Day"},{"location":"unit1/lesson2/","text":"Lesson 2: Stick Figures Objective: Students will learn how to observe, record, and organize data. Materials: Stick Figures cutouts ( LMR_1.2_Stick Figures ) Advanced preparation required (see step 3 below) Poster paper Markers Sticky notes Vocabulary: collect record organize representations variables Essential Concepts: Essential Concepts: Data consist of records of particular characteristics of people or objects. Data can be organized in many different ways, and some ways make it easier than others for achieving particular purposes. Lesson: Engage students in a Think-Pair-Share (see Instructional Strategies ) of the Data Diary handout that the students completed for homework. Ask them to think about the following questions as they reflect on their data collection homework: How many observations did you make? Where do you leave the most data trails? What could someone learn about you if that person had all of this data? Explain to students that they are going to act as researchers and collect data on a strange group of people who appear to be completely two-dimensional. Their goal is to record as much information as possible about these people, and to then organize the information in any way they choose. Distribute one full set of 8 cards from the Stick Figures file ( LMR_1.2 ) to each student team. Advanced preparation required: Print the Stick Figures file ( LMR_1.2 ). The handout can then be cut into the 8 cards. You will need enough sets of the cards for each student team to share one full set. For example, if there are 5 student teams in a class, then 5 copies of the file will need to be printed so that each team gets all 8 cards. LMR_1.2 Every student from the team will select one of the cards from the team\u2019s pile of 8, and should record all possible information in their DS journal. Once each student has completed this, the team should come together to share individual findings. Distribute one piece of poster paper and a set of markers to each team. The students will then begin to organize the data from all 8 cards into a visual that they think represents the data. It is important that no guidance is given during this portion of the lesson. Students should be free to come up with their own schema for organizing the data. Display all the posters around the room and allow students to participate in a Gallery Walk (see Instructional Strategies in Teacher Resources ) to view other teams\u2019 representations of the Stick Figure data. For each poster, the teams should write either a comment or a question on a sticky note and add it to the poster to provide feedback for the original team. Afterwards, engage the students in a discussion with the following questions: Describe some similarities among the team posters. Were the data organized in similar ways? Answers will vary by class. Describe some differences among the team posters. How were the data organized differently across teams? Answers will vary by class. What information was available about the stick figures on each card? The person\u2019s name, height, GPA, shoe style, sport, and number of friends on social media. Which representations made it easy to see what (or who) the objects were that were observed? Which representations made it easy to see whether different stick figures had different characteristics? Answers will vary by class. Which representation makes it easiest to see which stick figure is tallest? Answers will vary by class. If you were handed a blank stick figure and knew only the person's name, could you fill in the rest of the information? No. You would not know a person\u2019s height, GPA, shoe preference, etc. just by knowing their name. Explain to the students that the general categories of information, such as a person\u2019s height, are called variables . Variables are simply characteristics of an object or person. As statisticians, we use variable names to organize data into a simplified form so that a computer can read them. This will be discussed further in lesson 3 . Class Scribes: One team of students will give a brief talk to discuss what they think the 3 most important topics of the day were.","title":"Lesson 2: Stick Figures"},{"location":"unit1/lesson2/#lesson-2-stick-figures","text":"","title":"Lesson 2: Stick Figures"},{"location":"unit1/lesson2/#objective","text":"Students will learn how to observe, record, and organize data.","title":"Objective:"},{"location":"unit1/lesson2/#materials","text":"Stick Figures cutouts ( LMR_1.2_Stick Figures ) Advanced preparation required (see step 3 below) Poster paper Markers Sticky notes","title":"Materials:"},{"location":"unit1/lesson2/#vocabulary","text":"collect record organize representations variables","title":"Vocabulary:"},{"location":"unit1/lesson2/#essential-concepts","text":"Essential Concepts: Data consist of records of particular characteristics of people or objects. Data can be organized in many different ways, and some ways make it easier than others for achieving particular purposes.","title":"Essential Concepts:"},{"location":"unit1/lesson2/#lesson","text":"Engage students in a Think-Pair-Share (see Instructional Strategies ) of the Data Diary handout that the students completed for homework. Ask them to think about the following questions as they reflect on their data collection homework: How many observations did you make? Where do you leave the most data trails? What could someone learn about you if that person had all of this data? Explain to students that they are going to act as researchers and collect data on a strange group of people who appear to be completely two-dimensional. Their goal is to record as much information as possible about these people, and to then organize the information in any way they choose. Distribute one full set of 8 cards from the Stick Figures file ( LMR_1.2 ) to each student team. Advanced preparation required: Print the Stick Figures file ( LMR_1.2 ). The handout can then be cut into the 8 cards. You will need enough sets of the cards for each student team to share one full set. For example, if there are 5 student teams in a class, then 5 copies of the file will need to be printed so that each team gets all 8 cards. LMR_1.2 Every student from the team will select one of the cards from the team\u2019s pile of 8, and should record all possible information in their DS journal. Once each student has completed this, the team should come together to share individual findings. Distribute one piece of poster paper and a set of markers to each team. The students will then begin to organize the data from all 8 cards into a visual that they think represents the data. It is important that no guidance is given during this portion of the lesson. Students should be free to come up with their own schema for organizing the data. Display all the posters around the room and allow students to participate in a Gallery Walk (see Instructional Strategies in Teacher Resources ) to view other teams\u2019 representations of the Stick Figure data. For each poster, the teams should write either a comment or a question on a sticky note and add it to the poster to provide feedback for the original team. Afterwards, engage the students in a discussion with the following questions: Describe some similarities among the team posters. Were the data organized in similar ways? Answers will vary by class. Describe some differences among the team posters. How were the data organized differently across teams? Answers will vary by class. What information was available about the stick figures on each card? The person\u2019s name, height, GPA, shoe style, sport, and number of friends on social media. Which representations made it easy to see what (or who) the objects were that were observed? Which representations made it easy to see whether different stick figures had different characteristics? Answers will vary by class. Which representation makes it easiest to see which stick figure is tallest? Answers will vary by class. If you were handed a blank stick figure and knew only the person's name, could you fill in the rest of the information? No. You would not know a person\u2019s height, GPA, shoe preference, etc. just by knowing their name. Explain to the students that the general categories of information, such as a person\u2019s height, are called variables . Variables are simply characteristics of an object or person. As statisticians, we use variable names to organize data into a simplified form so that a computer can read them. This will be discussed further in lesson 3 .","title":"Lesson:"},{"location":"unit1/lesson2/#class-scribes","text":"One team of students will give a brief talk to discuss what they think the 3 most important topics of the day were.","title":"Class Scribes:"},{"location":"unit1/lesson3/","text":"Lesson 3: Data Structures Objective: Students will learn that data can be represented in rectangular format. Materials: DS journals (must be available during every lesson) Stick Figures cutouts (see lesson 2 ) Vocabulary: variables numerical variables categorical variables rows columns rectangular or spreadsheet format variability Essential Concepts: Essential Concepts: Variables record values that vary. By organizing data into rectangular format, we can easily see the characteristics of observations by reading across a row, or we can see the variability in a variable by reading down the column. Computers can easily process data when it is rectangular format. Lesson: Remind students that they briefly learned what variables are during the previous lesson. Have students create their own definitions of the term \u201cvariables\u201d and share their responses with their teams. Select a few students in the class to share out their definitions and discuss what could be modified (if anything) to create a more complete definition. Using the Stick Figure information from Lesson 2 , allow the class to come up with a set of variable names that describe the different categories of information. Note that it is best when variable names are short (one to three words). The variable names for the Stick Figures data could possibly be: Name Height GPA Shoe or Shoe Type Sport Friends or Number of Friends Next, have a class discussion about how the values from \u201cShoe\u201d are different than the values from \u201cHeight.\u201d The values from \u201cShoe\u201d are either \u201csneakers\u201d or \u201csandals\u201d. Note: Other terms for these shoes are acceptable \u2013 e.g., tennis shoes, flip flops, closed-toe, open-toe, etc. The values from \u201cHeight\u201d are 72, 68, 61, 66, 65, 61, 67, and 64. Students should notice that the \u201cShoe\u201d variable consists of categories or groupings, and the \u201cHeight\u201d variable consists of numbers. Therefore, we can classify variables into two types: categorical variables and numerical variables . Typically, categorical variables represent values that have words, while numerical variables represent values that have numbers. Note: Categorical variables can sometimes be coded as numbers (e.g., \u201cGender\u201d could have values 0 and 1, where 0=Male and 1=Female). As a class, determine which variables from the Stick Figures data are numerical, and which variables are categorical. The students should create two lists in their DS journals similar to the ones below (the correct classifications are in grey): Numerical Categorical 1. Height 1. Name 2. GPA 2. Shoe 3. Friends 3. Sport Explain that although we can understand many different representations of data (as evidenced by the posters from Lesson 2 ), computers are not as capable. Instead, we need to organize data in a structured way so that a computer can read and interpret them. One way to organize the data is to create a data table that consists of rows and columns . We can define this type of organization as rectangular format , or spreadsheet format . Display a generic table on the board (see example below) and explain that the columns are the vertical portions of the table, while the rows are the horizontal portions. Another way to think of it is that columns go from top to bottom, and rows go from left to right. Ask students: What should each row represent? Each row should represent one observation, or one stick figure person in this case. What should each column represent? Each column should represent one variable. As you go down a column, all the values represent the same characteristic (e.g., Height). On the board, draw the following table and have the students copy it into their DS journals (be sure to use variable names agreed upon by the class): In teams, students should complete the data table using all 8 of the Stick Figures cards. Each row of the table should represent one person on a card. Engage the class in a discussion with the following questions: Do any of the people in the data have the same value for a given variable? In other words, does a value appear more than once in a column? Give two examples. Answers will vary. One example could be that Dakota, Kamryn, Emerson, and London all wear sneakers. Another example could be that Charlie and Jessie are both 61 inches tall. Do any of the people in the data have different values for a given variable? Absolutely. There are many instances of this in the data table. Discuss the term variability . As in question (b) above, the values for each variable vary depending on which person we are observing. This shows that the data has variability, and the first step in any investigation is to notice variability. We can see the relationship between the terms variable and variability . The word \u201cvariable\u201d indicates that values vary. Class Scribes: One team of students will give a brief talk to discuss what they think the 3 most important topics of the day were.","title":"Lesson 3: Data Structures"},{"location":"unit1/lesson3/#lesson-3-data-structures","text":"","title":"Lesson 3: Data Structures"},{"location":"unit1/lesson3/#objective","text":"Students will learn that data can be represented in rectangular format.","title":"Objective:"},{"location":"unit1/lesson3/#materials","text":"DS journals (must be available during every lesson) Stick Figures cutouts (see lesson 2 )","title":"Materials:"},{"location":"unit1/lesson3/#vocabulary","text":"variables numerical variables categorical variables rows columns rectangular or spreadsheet format variability","title":"Vocabulary:"},{"location":"unit1/lesson3/#essential-concepts","text":"Essential Concepts: Variables record values that vary. By organizing data into rectangular format, we can easily see the characteristics of observations by reading across a row, or we can see the variability in a variable by reading down the column. Computers can easily process data when it is rectangular format.","title":"Essential Concepts:"},{"location":"unit1/lesson3/#lesson","text":"Remind students that they briefly learned what variables are during the previous lesson. Have students create their own definitions of the term \u201cvariables\u201d and share their responses with their teams. Select a few students in the class to share out their definitions and discuss what could be modified (if anything) to create a more complete definition. Using the Stick Figure information from Lesson 2 , allow the class to come up with a set of variable names that describe the different categories of information. Note that it is best when variable names are short (one to three words). The variable names for the Stick Figures data could possibly be: Name Height GPA Shoe or Shoe Type Sport Friends or Number of Friends Next, have a class discussion about how the values from \u201cShoe\u201d are different than the values from \u201cHeight.\u201d The values from \u201cShoe\u201d are either \u201csneakers\u201d or \u201csandals\u201d. Note: Other terms for these shoes are acceptable \u2013 e.g., tennis shoes, flip flops, closed-toe, open-toe, etc. The values from \u201cHeight\u201d are 72, 68, 61, 66, 65, 61, 67, and 64. Students should notice that the \u201cShoe\u201d variable consists of categories or groupings, and the \u201cHeight\u201d variable consists of numbers. Therefore, we can classify variables into two types: categorical variables and numerical variables . Typically, categorical variables represent values that have words, while numerical variables represent values that have numbers. Note: Categorical variables can sometimes be coded as numbers (e.g., \u201cGender\u201d could have values 0 and 1, where 0=Male and 1=Female). As a class, determine which variables from the Stick Figures data are numerical, and which variables are categorical. The students should create two lists in their DS journals similar to the ones below (the correct classifications are in grey): Numerical Categorical 1. Height 1. Name 2. GPA 2. Shoe 3. Friends 3. Sport Explain that although we can understand many different representations of data (as evidenced by the posters from Lesson 2 ), computers are not as capable. Instead, we need to organize data in a structured way so that a computer can read and interpret them. One way to organize the data is to create a data table that consists of rows and columns . We can define this type of organization as rectangular format , or spreadsheet format . Display a generic table on the board (see example below) and explain that the columns are the vertical portions of the table, while the rows are the horizontal portions. Another way to think of it is that columns go from top to bottom, and rows go from left to right. Ask students: What should each row represent? Each row should represent one observation, or one stick figure person in this case. What should each column represent? Each column should represent one variable. As you go down a column, all the values represent the same characteristic (e.g., Height). On the board, draw the following table and have the students copy it into their DS journals (be sure to use variable names agreed upon by the class): In teams, students should complete the data table using all 8 of the Stick Figures cards. Each row of the table should represent one person on a card. Engage the class in a discussion with the following questions: Do any of the people in the data have the same value for a given variable? In other words, does a value appear more than once in a column? Give two examples. Answers will vary. One example could be that Dakota, Kamryn, Emerson, and London all wear sneakers. Another example could be that Charlie and Jessie are both 61 inches tall. Do any of the people in the data have different values for a given variable? Absolutely. There are many instances of this in the data table. Discuss the term variability . As in question (b) above, the values for each variable vary depending on which person we are observing. This shows that the data has variability, and the first step in any investigation is to notice variability. We can see the relationship between the terms variable and variability . The word \u201cvariable\u201d indicates that values vary.","title":"Lesson:"},{"location":"unit1/lesson3/#class-scribes","text":"One team of students will give a brief talk to discuss what they think the 3 most important topics of the day were.","title":"Class Scribes:"},{"location":"unit1/lesson4/","text":"Lesson 4: The Data Cycle Objective: Students will learn about the stages of the Data Cycle. Materials: The Data Cycle file ( LMR_1.3_Data Cycle ) Computer, projector, or board and markers/chalk Printed description of each stage of the Data Cycle (refer to step 3 in the lesson) The Data Cycle Spinners handout ( LMR_1.4_Data Cycle Spinners ) RStudio: https://portal.idsucla.org Article headline: People Who Order Coffee Black Are More Likely To Be Psychopaths Dude Map found at (open in new tab): https://qz.com/316906/the-dude-map-how-american-men-refer-to-their-bros/ Bros & Dudes Graphics handout ( LMR_1.5_Bros & Dudes Graphics ) Sticky notes Poster paper Vocabulary: data cycle statistical investigative questions data collection data analysis data interpretation Essential Concepts: Essential Concepts: A statistical investigation consists of cycling through the four stages of the Data Cycle. The term statistical investigative questions encompasses the variety of questions asked during the statistical problem-solving process which support statistical thinking and reasoning. Statistical investigative questions are perhaps the most important because they are challenging to learn and are the types of questions that determine whether an analysis is productive or not. Statistical investigative questions are questions that address variability and are productive in that they motivate data collection, analysis, and interpretation. The Data Collection phase might consist of collecting data through Participatory Sensing or some other means, or it might consist of examining previously collected data to determine the quality of the data for answering the statistical investigative questions. Data Analysis is almost always done on the computer and consists of creating relevant graphics and numerical summaries of the data. Data Interpretation is involved with using the analysis to answer the statistical investigative questions. If you haven't done so already, create your IDS class and import users. a) Watch this video to learn how to set up and manage a class. Students will need their usernames and passwords in lesson 6. b) If students' first and last names are merged in one column separated by commas, watch this video to learn how to split them into two columns. Lesson: During the past few lessons, we have discussed what data are, how to collect and organize them, and how their values can vary. But what do we do with all this data? How can we navigate it and turn it into something useful to us? Inform students that they will be learning about the Data Cycle today. The Data Cycle is a guide we can use when learning to think about data. We usually start with posing statistical investigative questions. Display the graphic from The Data Cycle file ( LMR_1.3 ): LMR_1.3 Display the Data Cycle on the board or on a projector, and give a brief explanation of the 4 components (listed below). Note: we will explore each component of the Data Cycle more explicitly throughout the course. Pose Statistical Investigative Questions: Statistical investigative questions are questions that address variability and can be answered with data. Consider Data: This is the process of observing and recording data, or of examining previously collected data to make sure it meets the needs of the investigation. Analyze Data: During analysis, tables, graphs, and summaries of the data are produced to help us find patterns and relationships. Interpret Data: The statistical investigative questions are answered by referring to the tables, graphs, and summaries made in the Data Analysis phase. Almost all statistical investigations begin with statistical investigative questions. There are times when the questions may be given to us, so we might start at the data collection step, but this should ideally be our starting point. As an example, explain that you might ask a person \u201cHow old are you?\u201d Although this is a question, it is NOT a statistical investigative question because we are only asking one person so there is no variability in the data. The question \u201cHow old are you?\u201d is a survey question that you might ask if you were trying to answer the investigative question: \u201cHow old are the students in my school?\u201d We would need to collect data to answer the question and we would expect student\u2019s ages to vary. To help students get a firm understanding of the Data Cycle and how each component is connected, they will participate in a Four Corners strategy (see Instructional Strategies in Teacher Resources ). Write down the name of each stage of the Data Cycle on a sheet of paper and include the description of that particular stage (see step 3 for descriptions). Then tape each sheet on a different corner of your room. Explain to the students that you are going to display different artifacts from statistical investigations on the projector. For each artifact, they will move to the corner of the room they feel that artifact represents (posing a statistical investigative question, consider data, analyze data, interpret data). If you have limited space in your classroom or for students that cannot physically participate, you might consider printing LMR_1.4_Data Cycle Spinners . Students can participate by pointing to the spinner. Once students have chosen a corner of the room (stage of the Data Cycle) they will discuss the following with their classmates in that same corner: What part of the data cycle does the artifact represent (posing a statistical investigative question, consider data, analyze data, interpret data)? Why do we think that? What questions or wonderings do we have about the artifact? Allow each group time to discuss the questions and have one member from each team (corner) share the answers to the questions. This activity is not about having a correct answer. It is about having students begin to think critically about statistical artifacts that they are constantly consuming. Data are encountered through visualizations, reports from scientific studies, journalists\u2019 articles and websites. This activity is meant to begin to develop students' statistical habits of mind. Artifact 1: Spreadsheet of the CDC data Note: You can display this spreadsheet using RStudio by running the following commands: data(cdc) View(cdc) What part of the data cycle does the artifact represent (posing a statistical investigative question, consider data, analyze data, interpret data)? Why do we think that? Answers will vary. What questions or wonderings do we have about the artifact? Students should begin developing statistical habits of mind. They should be interrogating the data by asking questions such as: Who is the data about? What was the purpose of collecting the data? What was the survey question asked to collect the data? Artifact 2: Article headline People Who Order Coffee Black Are More Likely To Be Psychopaths What part of the data cycle does the artifact represent (posing a statistical investigative question, consider data, analyze data, interpret data)? Why do we think that? Answers will vary. What questions or wonderings do we have about the artifact? Students should be interrogating this headline with questions like: What type of study was this? Who funded the study? What was the purpose of the study? How was the variable measured? Artifact 3: Dude Map found at (open in new tab): https://qz.com/316906/the-dude-map-how-american-men-refer-to-their-bros/ What part of the data cycle does the artifact represent (posing a statistical investigative question, consider data, analyze data, interpret data)? Why do we think that? Answers will vary. What questions or wonderings do we have about the artifact? Students should be asking questions like: What was the purpose of this study? What variables were measured and how were they measured? Inform students that the Dude Map was created for the Quartz website by Nikhil Sonnad as a data visualization. He collected the data via Twitter. The graphic shows how common the terms: bro, buddy, dude, fella and pal are when referring to friends throughout the United States. Ask students to return to their seats, take out their DS journal and make a sketch of the Data Cycle making sure to include the names of the four stages (Pose statstical investigative question, consider data, analyze data, interpret data). Ask students to write Dude Map under the analyze data of their data cycle and the information about where the data came from (see #13) in the consider data part of their Data Cycle sketch. Have each team discuss a possible investigative question that could be answered using the Dude Map graphic. Have the Recorder/Reporter write the question on a sticky note and the resource manager bring it up to the board. Lead a class discussion around the investigative questions the student teams created, and as a class, choose one to write down as an example. Example: Where in the United States is the term dude more common to use when referring to a friend? Allow the teams to work together to answer the investigative question. Ask the Recorder/Reporter to share their team's interpretation. Have students write down the answer that resonated the most with them under the interpret part of the data cycle. Assign ONE of the pages from the Bros & Dudes Graphics handout ( LMR_1.5 ) to each team. There are 10 different versions of word pairings (10 combinations of 2 words chosen from the 5 options), so multiple teams will have the same graphic if there are more than 10 teams in a class. LMR_1.5 The goal of this activity is for each team to complete a full statistical investigation with the Bros & Dudes Graphics assigned to them. Tell the teams that they need to create a Data Cycle poster using their assigned graphic for the analyze stage. The cycle should be clearly labeled and have appropriate responses for each of the 4 components. For example, a team given the \u201cBro\u201d and \u201cBuddy\u201d graphics might come up with the following questions: Which region of the US is most likely to use the term \u201cBro\u201d when referring to a friend? Do the coastal areas prefer different terms than the Midwest? Is there a difference between northern states versus southern states? Class Scribes: One team of students will give a brief talk to discuss what they think the 3 most important topics of the day were. Homework Students reformulate any investigative questions generated by their team about the Bros & Dudes Graphics that could not be answered so that they can be answered.","title":"Lesson 4: The Data Cycle"},{"location":"unit1/lesson4/#lesson-4-the-data-cycle","text":"","title":"Lesson 4: The Data Cycle"},{"location":"unit1/lesson4/#objective","text":"Students will learn about the stages of the Data Cycle.","title":"Objective:"},{"location":"unit1/lesson4/#materials","text":"The Data Cycle file ( LMR_1.3_Data Cycle ) Computer, projector, or board and markers/chalk Printed description of each stage of the Data Cycle (refer to step 3 in the lesson) The Data Cycle Spinners handout ( LMR_1.4_Data Cycle Spinners ) RStudio: https://portal.idsucla.org Article headline: People Who Order Coffee Black Are More Likely To Be Psychopaths Dude Map found at (open in new tab): https://qz.com/316906/the-dude-map-how-american-men-refer-to-their-bros/ Bros & Dudes Graphics handout ( LMR_1.5_Bros & Dudes Graphics ) Sticky notes Poster paper","title":"Materials:"},{"location":"unit1/lesson4/#vocabulary","text":"data cycle statistical investigative questions data collection data analysis data interpretation","title":"Vocabulary:"},{"location":"unit1/lesson4/#essential-concepts","text":"Essential Concepts: A statistical investigation consists of cycling through the four stages of the Data Cycle. The term statistical investigative questions encompasses the variety of questions asked during the statistical problem-solving process which support statistical thinking and reasoning. Statistical investigative questions are perhaps the most important because they are challenging to learn and are the types of questions that determine whether an analysis is productive or not. Statistical investigative questions are questions that address variability and are productive in that they motivate data collection, analysis, and interpretation. The Data Collection phase might consist of collecting data through Participatory Sensing or some other means, or it might consist of examining previously collected data to determine the quality of the data for answering the statistical investigative questions. Data Analysis is almost always done on the computer and consists of creating relevant graphics and numerical summaries of the data. Data Interpretation is involved with using the analysis to answer the statistical investigative questions. If you haven't done so already, create your IDS class and import users. a) Watch this video to learn how to set up and manage a class. Students will need their usernames and passwords in lesson 6. b) If students' first and last names are merged in one column separated by commas, watch this video to learn how to split them into two columns.","title":"Essential Concepts:"},{"location":"unit1/lesson4/#lesson","text":"During the past few lessons, we have discussed what data are, how to collect and organize them, and how their values can vary. But what do we do with all this data? How can we navigate it and turn it into something useful to us? Inform students that they will be learning about the Data Cycle today. The Data Cycle is a guide we can use when learning to think about data. We usually start with posing statistical investigative questions. Display the graphic from The Data Cycle file ( LMR_1.3 ): LMR_1.3 Display the Data Cycle on the board or on a projector, and give a brief explanation of the 4 components (listed below). Note: we will explore each component of the Data Cycle more explicitly throughout the course. Pose Statistical Investigative Questions: Statistical investigative questions are questions that address variability and can be answered with data. Consider Data: This is the process of observing and recording data, or of examining previously collected data to make sure it meets the needs of the investigation. Analyze Data: During analysis, tables, graphs, and summaries of the data are produced to help us find patterns and relationships. Interpret Data: The statistical investigative questions are answered by referring to the tables, graphs, and summaries made in the Data Analysis phase. Almost all statistical investigations begin with statistical investigative questions. There are times when the questions may be given to us, so we might start at the data collection step, but this should ideally be our starting point. As an example, explain that you might ask a person \u201cHow old are you?\u201d Although this is a question, it is NOT a statistical investigative question because we are only asking one person so there is no variability in the data. The question \u201cHow old are you?\u201d is a survey question that you might ask if you were trying to answer the investigative question: \u201cHow old are the students in my school?\u201d We would need to collect data to answer the question and we would expect student\u2019s ages to vary. To help students get a firm understanding of the Data Cycle and how each component is connected, they will participate in a Four Corners strategy (see Instructional Strategies in Teacher Resources ). Write down the name of each stage of the Data Cycle on a sheet of paper and include the description of that particular stage (see step 3 for descriptions). Then tape each sheet on a different corner of your room. Explain to the students that you are going to display different artifacts from statistical investigations on the projector. For each artifact, they will move to the corner of the room they feel that artifact represents (posing a statistical investigative question, consider data, analyze data, interpret data). If you have limited space in your classroom or for students that cannot physically participate, you might consider printing LMR_1.4_Data Cycle Spinners . Students can participate by pointing to the spinner. Once students have chosen a corner of the room (stage of the Data Cycle) they will discuss the following with their classmates in that same corner: What part of the data cycle does the artifact represent (posing a statistical investigative question, consider data, analyze data, interpret data)? Why do we think that? What questions or wonderings do we have about the artifact? Allow each group time to discuss the questions and have one member from each team (corner) share the answers to the questions. This activity is not about having a correct answer. It is about having students begin to think critically about statistical artifacts that they are constantly consuming. Data are encountered through visualizations, reports from scientific studies, journalists\u2019 articles and websites. This activity is meant to begin to develop students' statistical habits of mind. Artifact 1: Spreadsheet of the CDC data Note: You can display this spreadsheet using RStudio by running the following commands: data(cdc) View(cdc) What part of the data cycle does the artifact represent (posing a statistical investigative question, consider data, analyze data, interpret data)? Why do we think that? Answers will vary. What questions or wonderings do we have about the artifact? Students should begin developing statistical habits of mind. They should be interrogating the data by asking questions such as: Who is the data about? What was the purpose of collecting the data? What was the survey question asked to collect the data? Artifact 2: Article headline People Who Order Coffee Black Are More Likely To Be Psychopaths What part of the data cycle does the artifact represent (posing a statistical investigative question, consider data, analyze data, interpret data)? Why do we think that? Answers will vary. What questions or wonderings do we have about the artifact? Students should be interrogating this headline with questions like: What type of study was this? Who funded the study? What was the purpose of the study? How was the variable measured? Artifact 3: Dude Map found at (open in new tab): https://qz.com/316906/the-dude-map-how-american-men-refer-to-their-bros/ What part of the data cycle does the artifact represent (posing a statistical investigative question, consider data, analyze data, interpret data)? Why do we think that? Answers will vary. What questions or wonderings do we have about the artifact? Students should be asking questions like: What was the purpose of this study? What variables were measured and how were they measured? Inform students that the Dude Map was created for the Quartz website by Nikhil Sonnad as a data visualization. He collected the data via Twitter. The graphic shows how common the terms: bro, buddy, dude, fella and pal are when referring to friends throughout the United States. Ask students to return to their seats, take out their DS journal and make a sketch of the Data Cycle making sure to include the names of the four stages (Pose statstical investigative question, consider data, analyze data, interpret data). Ask students to write Dude Map under the analyze data of their data cycle and the information about where the data came from (see #13) in the consider data part of their Data Cycle sketch. Have each team discuss a possible investigative question that could be answered using the Dude Map graphic. Have the Recorder/Reporter write the question on a sticky note and the resource manager bring it up to the board. Lead a class discussion around the investigative questions the student teams created, and as a class, choose one to write down as an example. Example: Where in the United States is the term dude more common to use when referring to a friend? Allow the teams to work together to answer the investigative question. Ask the Recorder/Reporter to share their team's interpretation. Have students write down the answer that resonated the most with them under the interpret part of the data cycle. Assign ONE of the pages from the Bros & Dudes Graphics handout ( LMR_1.5 ) to each team. There are 10 different versions of word pairings (10 combinations of 2 words chosen from the 5 options), so multiple teams will have the same graphic if there are more than 10 teams in a class. LMR_1.5 The goal of this activity is for each team to complete a full statistical investigation with the Bros & Dudes Graphics assigned to them. Tell the teams that they need to create a Data Cycle poster using their assigned graphic for the analyze stage. The cycle should be clearly labeled and have appropriate responses for each of the 4 components. For example, a team given the \u201cBro\u201d and \u201cBuddy\u201d graphics might come up with the following questions: Which region of the US is most likely to use the term \u201cBro\u201d when referring to a friend? Do the coastal areas prefer different terms than the Midwest? Is there a difference between northern states versus southern states?","title":"Lesson:"},{"location":"unit1/lesson4/#class-scribes","text":"One team of students will give a brief talk to discuss what they think the 3 most important topics of the day were.","title":"Class Scribes:"},{"location":"unit1/lesson4/#homework","text":"Students reformulate any investigative questions generated by their team about the Bros & Dudes Graphics that could not be answered so that they can be answered.","title":"Homework"},{"location":"unit1/lesson5/","text":"Lesson 5: So Many Questions Objective: Students will learn the features of a good statistical investigative question. Materials: Sticky notes Vocabulary: (statistical) investigative questions Essential Concepts: Essential Concepts: Statistical investigative questions typically begin with a vague general question, then develop into a precise question. The process of developing or creating a good investigative question is iterative and requires time and effort to get right. In her 2021 paper, What Makes a Good Statistical Question, Dr. Pip Arnold identified the following as features of a good investigative question: (1) The variable(s) of interest is/are clear (2) The group or population we are interested in is clear (3) The question can be answered with the data (4) The question asks about the whole group, not an individual (5) The intention is clear (e.g., summary, comparison, association, time series) (6) The question is one that is worth investigating, is interesting, and has a purpose Lesson: Entrance Slip (see Instructional Strategies in Teacher Resources ): Each student should submit a ticket that displays the 4 components of the Data Cycle. Inform students that they will learn about what makes a good investigative question. Ask them to recall the definition of an investigative question: Investigative questions are questions that address variability and can be answered with data. They are questions we ask of the data. A good way to determine this is to ask: Do we need to see the data to answer the question? Remind students of the two questions from the previous lesson, noting that one of the questions was an investigative question, and the other was not: How old are you? How old are the students in my school? In pairs, ask students to analyze each question using the definition of an investigative question and come to an agreement about which one is an investigative question. Using Agree/Disagree (see Instructional Strategies in Teacher Resources ), ask a pair of students for their results. Discuss why the first question IS NOT an investigative question ( there is only one possible value so there is no variability in the data ) and why the second question IS an investigative question ( not all students are the same age. The ages vary, so there is variability in the data ). Ask students to think of the data they collected about the stick figures (name, GPA, friends, sport, height, shoe). Inform them that the researchers used the following survey questions to collect the data: What is your name? What is your GPA? How many friends do you have? What sport do you play? How tall are you in inches? What type of shoe do you mostly wear? Survey questions are another example of a type of statistical question, but with a different purpose to investigative questions. Survey questions are questions we ask to get the data. Tell students that it is important to know exactly what survey questions were asked to collect the data before asking investigative questions. For example, we saw an image of a ball next to each stick figure but we don\u2019t know if that represents a sport they like to watch, their favorite sport, or a team they are on. In teams, ask students to create investigative questions that could be answered using the data collected about the stick figures. Introduce the sentence stem \u201cI wonder\u2026\u201d to help students get started. Have the Recorder/Reporter record the questions on post-its. Ask the teams to identify which variable(s) each question is investigating by having them circle the variable name(s) within their investigative questions. Have the Task Manager organize their group\u2019s investigative questions on the board, placing investigative questions that incorporate only one variable on the left-hand side of the board and investigative questions that incorporate two or more variables on the right-hand side. Note: This sorting activity will help students begin to distinguish between different types of investigative questions. Summary investigative questions are questions about a single variable. Comparison investigative questions compare a numerical variable across groups. Association investigative questions look for a relationship between paired numerical or paired categorical variables. As a class, begin the process of transforming some of the summary investigative questions so that they have all of the features of a good investigative question. Here is an example to get you started: Initial investigative question : I wonder who has the most friends? Feature Explanation The variable(s) of interest is/are clear Yes. The variable of interest is the number of friends. The group or population we are interested in is clear No. Teacher should ask: \u201cWho did the researchers want to learn about?\u201d These stick figures . The question can be answered with the data Yes. The researchers collected data on the number of friends. The question asks about the whole group, not an individual No. This question is about an individual stick figure. Teacher should ask: \u201cHow can we reword the question to include the whole group?\u201d How many friends do ... have? The intention is clear (e.g., summary, comparison, association) It is clear that this is a summary investigative question (single variable), specifically the number of friends. The question is one that is worth investigating, is interesting, and has a purpose For students, this might be something interesting. Reworded investigative question after going through the criteria: I wonder how many friends this group of stick figures have? As a class, apply the same process to a few of the comparison and association questions. Initial investigative question: I wonder if someone who plays a specific sport has more friends? Feature Explanation The variable(s) of interest is/are clear Yes. This seems to be a comparison investigative question comparing the number of friends within the sport played. The group or population we are interested in is clear No. Teacher should ask: \u201cWho did the researchers want to learn about?\u201d These stick figures . The question can be answered with the data Yes. The researchers collected data on the number of friends and the sport the stick figures played. The question asks about the whole group, not an individual No. The word someone gives the impression that we are interested in one observation. The intention is clear (e.g., summary, comparison, association) The intent is somewhat clear. This seems to be a comparison investigative question between the sport the stick figures played and the number of friends each stick figure had. Teacher should ask: \u201cWhat is the variable that is being compared? Which groups within the sport variable are you comparing (all groups, specific groups)?\u201d. The question is one that is worth investigating, is interesting, and has a purpose For students, this might be something interesting. Reworded investigative question after going through the criteria: I wonder if there is a difference in the typical number of friends the stick figures have based on the sport they play? I wonder if these stick figures who play soccer tend to have more friends that these stick figures who play tennis? Using the criteria of a good statistical investigative question, student teams will go back and modify their statistical investigative questions. Facilitators will ensure the team goes through the criteria for each investigative question. Task Managers will encourage everyone to contribute. Resource Managers will ensure all materials are easily accessible for recording and reporting. Recorders in each team will capture team members\u2019 responses while the teacher circulates the room to check for understanding. Ask the Reporters of selected teams to share their revised statistical investigative question(s). Students in the audience will listen to the presentations and provide feedback about each team\u2019s statistical investigative question(s). Be sure to discuss disagreements before moving on to different questions. Inform students that, in the next lesson, they will begin using the Data Cycle to learn about their food habits. To prepare for this, students should begin collecting the \u201cNutrition Facts\u201d labels from foods/snacks they typically eat. Class Scribes: One team of students will give a brief talk to discuss what they think the 3 most important topics of the day were. Homework Ask students to bring at least 2 cutouts of the \u201cNutrition Facts\u201d labels of the snacks they typically eat (e.g., chips, yogurt, blended drinks, etc.). Note: An alternative to collecting \u201cNutrition Facts\u201d labels is to print them from an online source and bring the printouts to class.","title":"Lesson 5: So Many Questions"},{"location":"unit1/lesson5/#lesson-5-so-many-questions","text":"","title":"Lesson 5: So Many Questions"},{"location":"unit1/lesson5/#objective","text":"Students will learn the features of a good statistical investigative question.","title":"Objective:"},{"location":"unit1/lesson5/#materials","text":"Sticky notes","title":"Materials:"},{"location":"unit1/lesson5/#vocabulary","text":"(statistical) investigative questions","title":"Vocabulary:"},{"location":"unit1/lesson5/#essential-concepts","text":"Essential Concepts: Statistical investigative questions typically begin with a vague general question, then develop into a precise question. The process of developing or creating a good investigative question is iterative and requires time and effort to get right. In her 2021 paper, What Makes a Good Statistical Question, Dr. Pip Arnold identified the following as features of a good investigative question: (1) The variable(s) of interest is/are clear (2) The group or population we are interested in is clear (3) The question can be answered with the data (4) The question asks about the whole group, not an individual (5) The intention is clear (e.g., summary, comparison, association, time series) (6) The question is one that is worth investigating, is interesting, and has a purpose","title":"Essential Concepts:"},{"location":"unit1/lesson5/#lesson","text":"Entrance Slip (see Instructional Strategies in Teacher Resources ): Each student should submit a ticket that displays the 4 components of the Data Cycle. Inform students that they will learn about what makes a good investigative question. Ask them to recall the definition of an investigative question: Investigative questions are questions that address variability and can be answered with data. They are questions we ask of the data. A good way to determine this is to ask: Do we need to see the data to answer the question? Remind students of the two questions from the previous lesson, noting that one of the questions was an investigative question, and the other was not: How old are you? How old are the students in my school? In pairs, ask students to analyze each question using the definition of an investigative question and come to an agreement about which one is an investigative question. Using Agree/Disagree (see Instructional Strategies in Teacher Resources ), ask a pair of students for their results. Discuss why the first question IS NOT an investigative question ( there is only one possible value so there is no variability in the data ) and why the second question IS an investigative question ( not all students are the same age. The ages vary, so there is variability in the data ). Ask students to think of the data they collected about the stick figures (name, GPA, friends, sport, height, shoe). Inform them that the researchers used the following survey questions to collect the data: What is your name? What is your GPA? How many friends do you have? What sport do you play? How tall are you in inches? What type of shoe do you mostly wear? Survey questions are another example of a type of statistical question, but with a different purpose to investigative questions. Survey questions are questions we ask to get the data. Tell students that it is important to know exactly what survey questions were asked to collect the data before asking investigative questions. For example, we saw an image of a ball next to each stick figure but we don\u2019t know if that represents a sport they like to watch, their favorite sport, or a team they are on. In teams, ask students to create investigative questions that could be answered using the data collected about the stick figures. Introduce the sentence stem \u201cI wonder\u2026\u201d to help students get started. Have the Recorder/Reporter record the questions on post-its. Ask the teams to identify which variable(s) each question is investigating by having them circle the variable name(s) within their investigative questions. Have the Task Manager organize their group\u2019s investigative questions on the board, placing investigative questions that incorporate only one variable on the left-hand side of the board and investigative questions that incorporate two or more variables on the right-hand side. Note: This sorting activity will help students begin to distinguish between different types of investigative questions. Summary investigative questions are questions about a single variable. Comparison investigative questions compare a numerical variable across groups. Association investigative questions look for a relationship between paired numerical or paired categorical variables. As a class, begin the process of transforming some of the summary investigative questions so that they have all of the features of a good investigative question. Here is an example to get you started: Initial investigative question : I wonder who has the most friends? Feature Explanation The variable(s) of interest is/are clear Yes. The variable of interest is the number of friends. The group or population we are interested in is clear No. Teacher should ask: \u201cWho did the researchers want to learn about?\u201d These stick figures . The question can be answered with the data Yes. The researchers collected data on the number of friends. The question asks about the whole group, not an individual No. This question is about an individual stick figure. Teacher should ask: \u201cHow can we reword the question to include the whole group?\u201d How many friends do ... have? The intention is clear (e.g., summary, comparison, association) It is clear that this is a summary investigative question (single variable), specifically the number of friends. The question is one that is worth investigating, is interesting, and has a purpose For students, this might be something interesting. Reworded investigative question after going through the criteria: I wonder how many friends this group of stick figures have? As a class, apply the same process to a few of the comparison and association questions. Initial investigative question: I wonder if someone who plays a specific sport has more friends? Feature Explanation The variable(s) of interest is/are clear Yes. This seems to be a comparison investigative question comparing the number of friends within the sport played. The group or population we are interested in is clear No. Teacher should ask: \u201cWho did the researchers want to learn about?\u201d These stick figures . The question can be answered with the data Yes. The researchers collected data on the number of friends and the sport the stick figures played. The question asks about the whole group, not an individual No. The word someone gives the impression that we are interested in one observation. The intention is clear (e.g., summary, comparison, association) The intent is somewhat clear. This seems to be a comparison investigative question between the sport the stick figures played and the number of friends each stick figure had. Teacher should ask: \u201cWhat is the variable that is being compared? Which groups within the sport variable are you comparing (all groups, specific groups)?\u201d. The question is one that is worth investigating, is interesting, and has a purpose For students, this might be something interesting. Reworded investigative question after going through the criteria: I wonder if there is a difference in the typical number of friends the stick figures have based on the sport they play? I wonder if these stick figures who play soccer tend to have more friends that these stick figures who play tennis? Using the criteria of a good statistical investigative question, student teams will go back and modify their statistical investigative questions. Facilitators will ensure the team goes through the criteria for each investigative question. Task Managers will encourage everyone to contribute. Resource Managers will ensure all materials are easily accessible for recording and reporting. Recorders in each team will capture team members\u2019 responses while the teacher circulates the room to check for understanding. Ask the Reporters of selected teams to share their revised statistical investigative question(s). Students in the audience will listen to the presentations and provide feedback about each team\u2019s statistical investigative question(s). Be sure to discuss disagreements before moving on to different questions. Inform students that, in the next lesson, they will begin using the Data Cycle to learn about their food habits. To prepare for this, students should begin collecting the \u201cNutrition Facts\u201d labels from foods/snacks they typically eat.","title":"Lesson:"},{"location":"unit1/lesson5/#class-scribes","text":"One team of students will give a brief talk to discuss what they think the 3 most important topics of the day were.","title":"Class Scribes:"},{"location":"unit1/lesson5/#homework","text":"Ask students to bring at least 2 cutouts of the \u201cNutrition Facts\u201d labels of the snacks they typically eat (e.g., chips, yogurt, blended drinks, etc.). Note: An alternative to collecting \u201cNutrition Facts\u201d labels is to print them from an online source and bring the printouts to class.","title":"Homework"},{"location":"unit1/lesson6/","text":"Lesson 6: What Do I Eat? Objective: Students will collect data using paper and pencil to understand the challenges of organizing, storing, and sharing data. They will learn that there must be an agreement about the variables that need to be recorded in order to attain consistency. Materials: Video: Jamie Oliver\u2019s Food Revolution found at: https://youtu.be/I0vYwqkoktM https://www.youtube.com/embed/I0vYwqkoktM Note: If the video is unavailable, search for \"Jamie Oliver's Food Revolution What's In a Sundae\". The video should be 5-6 minutes in length. Nutrition Facts labels or pictures (collected previously by students) Note: If needed, use Nutrition Facts Cutouts handout ( LMR_1.7_Nutrition Facts Cutouts ) Food Habits Data Collection handout ( LMR_1.8_Food Habits Data Collection ) Vocabulary: dataset(s) Essential Concepts: Essential Concepts: After raising statistical investigative questions, we examine and record data to see if the questions are appropriate. Lesson: Inform students that today\u2019s lesson will focus on the Data Collection component of the Data Cycle. To motivate this, the students will watch a short video of an episode of Jamie Oliver\u2019s show titled Food Revolution found at: https://youtu.be/I0vYwqkoktM . This particular video was recorded at a Los Angeles high school. As the students watch the video, they should use their DS journals to write down their comments and/or reactions to what they see and hear and be ready to share out. After sharing out some of their responses, have the students respond to the following question in their DS journals: Why should I care about what I eat? Student teams will share their reactions and responses by engaging in a Silent Discussion (see Instructional Strategies in Teacher Resources ). Have students recall the Stick Figures activity from Lesson 2 . During that activity, they collected data about other people. But today, they are going to be collecting data about themselves and the foods they eat. Students should have Nutrition Facts labels available from food/snacks they consumed at home between the previous lesson and today. Note: If some students forgot to bring any, then you can pass out some of the Nutrition Facts Cutouts ( LMR_1.7 ) for them to use instead. LMR_1.7 For 3-5 minutes, allow students to collect any data they can from the label and record it in their DS journals. This should be done individually. Once they have collected their facts, ask students to compare and contrast their data with their team members. They need to respond to: How are their datasets similar? How are their datasets different? Gather the students as a whole group and ask them to share out the similarities and differences they discussed. Be sure to draw responses that show that while some facts collected were the same, there were others that were collected by some students and not by others. Also point to differences in the variables collected and the data structure used. Ask students to engage in the following individual Quickwrite (see Instructional Strategies in Teacher Resources ): How can the data you just gathered be quickly displayed and easily read? Distribute the Food Habits Data Collection handout ( LMR_1.8 ). Ask students to record 8 observations. They can use their own 2 labels for the first observations, and then use some of their team members\u2019 labels to complete the table. LMR_1.8 Once they are finished, in pairs, ask students to give a one-word identifier to each variable. For example: \u201cWhat\u2019s the name of your snack?\u201d = Name Share the one-word variable identifiers with the class by conducting a quick team Whip Around (see Instructional Strategies in Teacher Resources ). For homework, students will begin to formulate statistical investigative questions based on their Food Habits data. Inform students that they are permitted to bring mobile devices to the next class. Class Scribes: One team of students will give a brief talk to discuss what they think the 3 most important topics of the day were. Homework Ask students to examine the data in their Food Habits Data Collection handout ( LMR_1.8 ) and to generate two simple and two complex statistical investigativequestions that they think can be answered by the data they collected. A simple statistical question involves one variable, whereas a complex statistical question involves two or more variables. Students may bring their mobile devices to the next class for data collection purposes.","title":"Lesson 6: What Do I Eat?"},{"location":"unit1/lesson6/#lesson-6-what-do-i-eat","text":"","title":"Lesson 6: What Do I Eat?"},{"location":"unit1/lesson6/#objective","text":"Students will collect data using paper and pencil to understand the challenges of organizing, storing, and sharing data. They will learn that there must be an agreement about the variables that need to be recorded in order to attain consistency.","title":"Objective:"},{"location":"unit1/lesson6/#materials","text":"Video: Jamie Oliver\u2019s Food Revolution found at: https://youtu.be/I0vYwqkoktM https://www.youtube.com/embed/I0vYwqkoktM Note: If the video is unavailable, search for \"Jamie Oliver's Food Revolution What's In a Sundae\". The video should be 5-6 minutes in length. Nutrition Facts labels or pictures (collected previously by students) Note: If needed, use Nutrition Facts Cutouts handout ( LMR_1.7_Nutrition Facts Cutouts ) Food Habits Data Collection handout ( LMR_1.8_Food Habits Data Collection )","title":"Materials:"},{"location":"unit1/lesson6/#vocabulary","text":"dataset(s)","title":"Vocabulary:"},{"location":"unit1/lesson6/#essential-concepts","text":"Essential Concepts: After raising statistical investigative questions, we examine and record data to see if the questions are appropriate.","title":"Essential Concepts:"},{"location":"unit1/lesson6/#lesson","text":"Inform students that today\u2019s lesson will focus on the Data Collection component of the Data Cycle. To motivate this, the students will watch a short video of an episode of Jamie Oliver\u2019s show titled Food Revolution found at: https://youtu.be/I0vYwqkoktM . This particular video was recorded at a Los Angeles high school. As the students watch the video, they should use their DS journals to write down their comments and/or reactions to what they see and hear and be ready to share out. After sharing out some of their responses, have the students respond to the following question in their DS journals: Why should I care about what I eat? Student teams will share their reactions and responses by engaging in a Silent Discussion (see Instructional Strategies in Teacher Resources ). Have students recall the Stick Figures activity from Lesson 2 . During that activity, they collected data about other people. But today, they are going to be collecting data about themselves and the foods they eat. Students should have Nutrition Facts labels available from food/snacks they consumed at home between the previous lesson and today. Note: If some students forgot to bring any, then you can pass out some of the Nutrition Facts Cutouts ( LMR_1.7 ) for them to use instead. LMR_1.7 For 3-5 minutes, allow students to collect any data they can from the label and record it in their DS journals. This should be done individually. Once they have collected their facts, ask students to compare and contrast their data with their team members. They need to respond to: How are their datasets similar? How are their datasets different? Gather the students as a whole group and ask them to share out the similarities and differences they discussed. Be sure to draw responses that show that while some facts collected were the same, there were others that were collected by some students and not by others. Also point to differences in the variables collected and the data structure used. Ask students to engage in the following individual Quickwrite (see Instructional Strategies in Teacher Resources ): How can the data you just gathered be quickly displayed and easily read? Distribute the Food Habits Data Collection handout ( LMR_1.8 ). Ask students to record 8 observations. They can use their own 2 labels for the first observations, and then use some of their team members\u2019 labels to complete the table. LMR_1.8 Once they are finished, in pairs, ask students to give a one-word identifier to each variable. For example: \u201cWhat\u2019s the name of your snack?\u201d = Name Share the one-word variable identifiers with the class by conducting a quick team Whip Around (see Instructional Strategies in Teacher Resources ). For homework, students will begin to formulate statistical investigative questions based on their Food Habits data. Inform students that they are permitted to bring mobile devices to the next class.","title":"Lesson:"},{"location":"unit1/lesson6/#class-scribes","text":"One team of students will give a brief talk to discuss what they think the 3 most important topics of the day were.","title":"Class Scribes:"},{"location":"unit1/lesson6/#homework","text":"Ask students to examine the data in their Food Habits Data Collection handout ( LMR_1.8 ) and to generate two simple and two complex statistical investigativequestions that they think can be answered by the data they collected. A simple statistical question involves one variable, whereas a complex statistical question involves two or more variables. Students may bring their mobile devices to the next class for data collection purposes.","title":"Homework"},{"location":"unit1/lesson7/","text":"Lesson 7: Setting the Stage Objective: Students will begin to collect and record data to learn more about their own eating habits, as well as those of their classmates.. They will learn about data that is collected by a Participatory Sensing campaign, and also about privacy issues and photo ethics when collecting and sharing data. Materials: Students\u2019 own mobile devices (smartphone or tablet compatible with iOS or Android) Access to App Store or Google Play Store in student devices to download IDS UCLA app Login information (username and password) for each student\u2014 generated and ready for distribution prior to the lesson Food Habits Campaign guidelines ( LMR_U1_Campaign_Food Habits ) Optional: You can show this video to show students how to take a survey using the browser or this video for the app.. Vocabulary: Participatory Sensing campaign surveys images GPS ethics photo ethics Essential Concepts: Essential Concepts: In Participatory Sensing, we humans behave as if we are robot sensors, collecting data whenever a \"trigger\" event occurs. Our ability to learn about the patterns in our life through these data depends on our being reliable data collectors. Before students begin collecting data, ensure that: a) The campaign settings are set to your preference. b) Watch this video to learn about campaign settings. Lesson: Become familiar with the Food Habits Campaign guidelines (shown at the end of this lesson), especially the big questions found under \u201c1. The Issues,\u201d to help guide students during the campaign (see Campaign Guidelines in Teacher Resources ). Distribute the usernames and passwords to student team leaders (make sure safeguards are in place so that only the owner of the username and password is able to see this information). Ask team leaders to distribute their team\u2019s information when you are ready to download the IDS UCLA app . Ask students to think about the Nutritional Facts labels from which they collected data in the previous lesson, and answer the following in their DS journals: What questions would you want answered about eating habits? What can you do to find out about your own eating habits? Ask students to refer back to their reactions and comments from Jamie Oliver\u2019s video and have them ponder the question: What are we really eating? Over the next 9 days, they will engage in a Participatory Sensing campaign in which they will act as human sensors to collect data about themselves. The data collected will be used to analyze their classmates\u2019 and their own snacking habits. For this unit, they will collect data about every snack they eat. Note: Students should NOT collect data for full meals like breakfast, lunch, or dinner. Data should only be collected for anything eaten in between meals, like fruit, chips, cookies, nuts, sodas, etc. Ask students why it makes sense to study snacks specifically. Brainstorm some questions that could be answered using the snack-only data that would be hard to answer if the data included meals as well. Inform students that they will be taking part in a specific data collection method know as Participatory Sensing via a mobile application. This application can gather data via surveys , images , and GPS tracking. For students who choose to share their location, their precise location will not appear on the shared IDS map. In the interest of protecting student privacy, our GPS is configured to show a generalized location tag, not a specific address. Make it clear to students that the reason they are collecting the data is to learn more about themselves and their classmates, NOT to provide data for an external data collection team. Students occasionally have the misconception that when they use the Participatory Sensing app, they are providing data to external researchers, such as UCLA. Inform students that they are now going to engage in their own first Participatory Sensing data collection experience, in which they will collect their own data using a smart device. Depending on the device, there are 3 different options available: Android. A native Android application called \u201c IDS UCLA \u201d is available from the Google Play Store. iOS (Apple devices) The mobile application called \u201c IDS UCLA \u201d is available from the iOS App Store. No mobile device - browser-based version. For students that do not have a mobile device (or an unsupported device, such as a Windows phone or Blackberry), a browser-based version to perform data collection is available at https://portal.idsucla.org Click on the Survey Taking icon on the page. Once students have downloaded the app or have found the website, ask team leaders to distribute the login information. Students will need to keep this information in a safe place for the entire duration of this course. Emphasize the importance of keeping their username and password confidential. When students receive their login information, they can log in to the app. If students have trouble with their logins, the teacher has the ability to reset a student\u2019s password. Once logged into the app or the browser-based version, students will see the Campaigns in which they will participate. They will then select the campaign by tapping the name of the campaign. If no campaigns are visible, ask them to tap the refresh option, located on the top right hand side of the screen. Using one of their nutrition facts cutouts or pictures, ask students to complete their first survey by going through the questions in the app. Optional: This is where you can show this video to show students how to take a survey using the browser or this video for the app. After every student has had the opportunity to complete at least one survey, ask students the meaning of the word ethics . For this course, they will need to understand photo ethics . They may NOT take pictures of any person\u2019s identifying features such as faces, hair, hands, tattoos, etc. For this campaign, they may only take pictures of their snacks and/or the nutrition facts labels. Note to teacher: Inspect students\u2019 data collection photos throughout the data collection period and before each data collection, monitoring to ensure that no inappropriate images are shared. If you believe a photo is inappropriate, please delete the data entry immediately. Setting reminders: The IDS UCLA app has a reminder feature to help students in their data collection journey. Show students that they can set reminders directly on the app by tapping the menu button on the top left hand side of the screen and selecting Reminders from the menu. Data collection norms: Ask students how many snacks they think they eat a day. From this, come up with an approximate number of surveys they think each student should complete during the data collection period (days 7 though 15). Inform students that you will be monitoring their data collection to make sure that everyone is submitting surveys regularly. Go over the previous day\u2019s homework. Ask the facilitator from each table group conduct a round robin during which each team member shares one simple statistical question and one complex statistical question. The Recorder/Reporter will select and share out one of the team\u2019s simple statistical questions and one complex statistical question with the class. Class Scribes: One team of students will give a brief talk to discuss what they think the 3 most important topics of the day were. Homework For the next 9 days, students will collect nutritional facts data using the Food Habits Participatory Sensing campaign on their smart devices or via web browser.","title":"Lesson 7: Setting the Stage"},{"location":"unit1/lesson7/#lesson-7-setting-the-stage","text":"","title":"Lesson 7: Setting the Stage"},{"location":"unit1/lesson7/#objective","text":"Students will begin to collect and record data to learn more about their own eating habits, as well as those of their classmates.. They will learn about data that is collected by a Participatory Sensing campaign, and also about privacy issues and photo ethics when collecting and sharing data.","title":"Objective:"},{"location":"unit1/lesson7/#materials","text":"Students\u2019 own mobile devices (smartphone or tablet compatible with iOS or Android) Access to App Store or Google Play Store in student devices to download IDS UCLA app Login information (username and password) for each student\u2014 generated and ready for distribution prior to the lesson Food Habits Campaign guidelines ( LMR_U1_Campaign_Food Habits ) Optional: You can show this video to show students how to take a survey using the browser or this video for the app..","title":"Materials:"},{"location":"unit1/lesson7/#vocabulary","text":"Participatory Sensing campaign surveys images GPS ethics photo ethics","title":"Vocabulary:"},{"location":"unit1/lesson7/#essential-concepts","text":"Essential Concepts: In Participatory Sensing, we humans behave as if we are robot sensors, collecting data whenever a \"trigger\" event occurs. Our ability to learn about the patterns in our life through these data depends on our being reliable data collectors. Before students begin collecting data, ensure that: a) The campaign settings are set to your preference. b) Watch this video to learn about campaign settings.","title":"Essential Concepts:"},{"location":"unit1/lesson7/#lesson","text":"Become familiar with the Food Habits Campaign guidelines (shown at the end of this lesson), especially the big questions found under \u201c1. The Issues,\u201d to help guide students during the campaign (see Campaign Guidelines in Teacher Resources ). Distribute the usernames and passwords to student team leaders (make sure safeguards are in place so that only the owner of the username and password is able to see this information). Ask team leaders to distribute their team\u2019s information when you are ready to download the IDS UCLA app . Ask students to think about the Nutritional Facts labels from which they collected data in the previous lesson, and answer the following in their DS journals: What questions would you want answered about eating habits? What can you do to find out about your own eating habits? Ask students to refer back to their reactions and comments from Jamie Oliver\u2019s video and have them ponder the question: What are we really eating? Over the next 9 days, they will engage in a Participatory Sensing campaign in which they will act as human sensors to collect data about themselves. The data collected will be used to analyze their classmates\u2019 and their own snacking habits. For this unit, they will collect data about every snack they eat. Note: Students should NOT collect data for full meals like breakfast, lunch, or dinner. Data should only be collected for anything eaten in between meals, like fruit, chips, cookies, nuts, sodas, etc. Ask students why it makes sense to study snacks specifically. Brainstorm some questions that could be answered using the snack-only data that would be hard to answer if the data included meals as well. Inform students that they will be taking part in a specific data collection method know as Participatory Sensing via a mobile application. This application can gather data via surveys , images , and GPS tracking. For students who choose to share their location, their precise location will not appear on the shared IDS map. In the interest of protecting student privacy, our GPS is configured to show a generalized location tag, not a specific address. Make it clear to students that the reason they are collecting the data is to learn more about themselves and their classmates, NOT to provide data for an external data collection team. Students occasionally have the misconception that when they use the Participatory Sensing app, they are providing data to external researchers, such as UCLA. Inform students that they are now going to engage in their own first Participatory Sensing data collection experience, in which they will collect their own data using a smart device. Depending on the device, there are 3 different options available: Android. A native Android application called \u201c IDS UCLA \u201d is available from the Google Play Store. iOS (Apple devices) The mobile application called \u201c IDS UCLA \u201d is available from the iOS App Store. No mobile device - browser-based version. For students that do not have a mobile device (or an unsupported device, such as a Windows phone or Blackberry), a browser-based version to perform data collection is available at https://portal.idsucla.org Click on the Survey Taking icon on the page. Once students have downloaded the app or have found the website, ask team leaders to distribute the login information. Students will need to keep this information in a safe place for the entire duration of this course. Emphasize the importance of keeping their username and password confidential. When students receive their login information, they can log in to the app. If students have trouble with their logins, the teacher has the ability to reset a student\u2019s password. Once logged into the app or the browser-based version, students will see the Campaigns in which they will participate. They will then select the campaign by tapping the name of the campaign. If no campaigns are visible, ask them to tap the refresh option, located on the top right hand side of the screen. Using one of their nutrition facts cutouts or pictures, ask students to complete their first survey by going through the questions in the app. Optional: This is where you can show this video to show students how to take a survey using the browser or this video for the app. After every student has had the opportunity to complete at least one survey, ask students the meaning of the word ethics . For this course, they will need to understand photo ethics . They may NOT take pictures of any person\u2019s identifying features such as faces, hair, hands, tattoos, etc. For this campaign, they may only take pictures of their snacks and/or the nutrition facts labels. Note to teacher: Inspect students\u2019 data collection photos throughout the data collection period and before each data collection, monitoring to ensure that no inappropriate images are shared. If you believe a photo is inappropriate, please delete the data entry immediately. Setting reminders: The IDS UCLA app has a reminder feature to help students in their data collection journey. Show students that they can set reminders directly on the app by tapping the menu button on the top left hand side of the screen and selecting Reminders from the menu. Data collection norms: Ask students how many snacks they think they eat a day. From this, come up with an approximate number of surveys they think each student should complete during the data collection period (days 7 though 15). Inform students that you will be monitoring their data collection to make sure that everyone is submitting surveys regularly. Go over the previous day\u2019s homework. Ask the facilitator from each table group conduct a round robin during which each team member shares one simple statistical question and one complex statistical question. The Recorder/Reporter will select and share out one of the team\u2019s simple statistical questions and one complex statistical question with the class.","title":"Lesson:"},{"location":"unit1/lesson7/#class-scribes","text":"One team of students will give a brief talk to discuss what they think the 3 most important topics of the day were.","title":"Class Scribes:"},{"location":"unit1/lesson7/#homework","text":"For the next 9 days, students will collect nutritional facts data using the Food Habits Participatory Sensing campaign on their smart devices or via web browser.","title":"Homework"},{"location":"unit1/lesson8/","text":"Lesson 8: Tangible Plots Objective: Students will learn how distributions help us organize and visualize data values, and that the shapes of the distributions give us information about the variability of the data. Materials: Computer and projector for Campaign Monitoring Video: Value of Data Visualization found at: https://www.youtube.com/watch?v=xekEXM0Vonc https://www.youtube.com/embed/xekEXM0Vonc Nutrition facts labels or pictures (collected previously by students) Food Habits Data Collection handout (from lesson 6 , LMR_1.8_Food Habits Data Collection ) 3 pieces of tape per student Poster paper Dot stickers or sticky notes Tangible Plot handout ( LMR_1.9_Tangible Plot ) Vocabulary: x-axis y-axis visualization minimum maximum frequency distribution range typical symmetric data points dotplot Essential Concepts: Essential Concepts: Distributions organize data for us by telling us (a) which values of a variable were observed, and (b) how many times the values were observed (their frequency). In preparation for this lesson, ensure that: a) You watch this video on the Campaign Monitoring Tool. b) Each day, preview student responses to ensure appropriate inputs. Watch this video to learn how to manage student responses. Lesson: Food Habits Campaign Data Collection Monitoring: Display the IDS Campaign Monitoring Tool, found at https://portal.idsucla.org Click on Campaign Monitor and sign in. Inform students that you will be monitoring their data collection. This is a good opportunity for teachers to remind students that if their data are not shared, they cannot be used in analysis. See User List and sort by Total . Ask: Who has collected the most data so far? Click on the pie chart. Ask: How many active users are there? How many inactive users are there? See Total Responses . How many responses have been submitted? Using TPS, ask students to think about what can they do to increase their data collection. Inform students that today they will be learning how to visualize their data. Show the Value of Data Visualization video at https://www.youtube.com/watch?v=xekEXM0Vonc , which describes the importance of graphical representations of data. As they watch the video, students should respond to the following in their DS journals: What is data visualization? List one example of how visualization can be used to increase data comprehension. Have a whole class discussion regarding the video\u2019s last statement: \u201cYour message is only as good as your ability to share it.\u201d Ask students: What does this statement mean? What makes a good message in terms of data and visualizations? Have students take out their nutrition facts labels or pictures, and also their Food Habits Data Collection handout (from lesson 6 ). On poster paper, make the first quadrant of a coordinate plane. Leave the labels for the x-axis and y-axis blank for now (see step 10). Distribute 3 pieces of tape to each student. Make sure they fold each piece of tape to make two sticky sides. Have each student tape one sticky side to the back of each label and ask them to have the labels ready to tape onto the poster paper. As a class, ask students to select 2 numerical variables and 1 categorical variable from the Food Habits Data Collection handout whose data they would like to see in a visualization , which is a picture of the data. For example, students may vote to see a visualization of the following numerical variables: calories per serving, total fat per serving; categorical variable: salty_sweet Once students select the variables, inform them that they will be creating a plot with the nutrition facts labels for each of the variables they selected. Create a bargraph of the categorical variable chosen by the students. Begin by showing students how to clearly label the x-axis with the categories. For instance, if salty_sweet is the variable, ask students to identify the categories for that variable. Then mark the y-axis with the label frequency , which simply means the number of times an outcome occurs. Do not put tick-marks on the y-axis. The frequency will be measured by the number of labels plotted. Have students come up and place their nutrition fact label above the category that describes their snack. Have students stack their nutrition label so that is easy to calculate the frequency. Once all the labels have been placed, create bars with the appropriate height (frequency) for each category. Make sure to leave spaces between the bars, and that bars are the same width. Ask students to respond to the following questions in their DS journals: How many data points does this distribution have? Why? What information is this visualization telling us about [insert categorical variable name] in the snacks we consume? Use another piece of poster paper to create a distribution for the first numerical variable chosen by the students. Create a dotplot of the first numerical variable chosen by the students. Begin by showing students how to clearly label the x-axis. For instance, if calories per serving is the variable, ask students for the range of values for calories per serving and determine the minimum and maximum values for the dataset. Clearly label the x-axis with adequate intervals and the variable\u2019s name. Then mark the y-axis with the label frequency , which simply means the number of times a value occurs. Do not put tick-marks on the y-axis. The frequency will be measured by the number of labels plotted. For each value in the dataset, put a nutrition facts label above that value on the x-axis. When a value occurs more than once, stack the nutrition facts labels. For example, if there are three nutrition facts labels with 120 calories per serving in the dataset, there will be three nutrition facts labels above the 120 mark on the x-axis. Once all the labels have been placed, ask students to observe the distribution of the data in the dotplot. Ask students to respond to the following questions in their DS journals: What are the minimum and the maximum values of the dataset? Answers will vary by class. The range is the largest value minus the smallest value. It is one way of measuring the variability of a variable. What is the range, and why do you think this measures the variability? Answers will vary by class. The range measures variability because if the values of the variable are really different, the range will be a big number (because the max and min will be far apart); but if there is little variability, the range will be small. For example, if all of the values were the same, we would have no variability and the range would be 0 because the max and min would be the same number. How many data points does this distribution have? Why? Answers will vary by class. What is the amount of [insert variable name] that appears most often in a snack? Why? Answers will vary by class. What do you think the phrase distribution of the data set means? It shows us how values are distributed. We learn where there are many values, where there are only a few values, and where there are no values at all. What information is this distribution telling us about the [insert variable name] in the snacks we consume? Answers will vary. We see how the value of [variable name] varies. For example, we can see whether all foods have the same number of calories, or if they differ. A distribution tells us two things: the values of the variable and the frequency of the values. \"Frequency\" is just another way of saying \"the count.\" Why is this dotplot a picture of the distribution of [variable name]? Because the location of the labels on the x-axis tells us the values we saw, and the number of labels at that value tells us the frequency for that value. Review the students\u2019 responses in a class discussion. Ask students to put a check mark next to the answers that were validated, and to revise the answers that need to be corrected. Use another piece of poster paper to create a distribution for the second numerical variable chosen by the students. Repeat steps 14-16 with this variable. On the first visualization for the numerical variable, show students how to convert the nutrition facts labels into something more readable. Draw another horizontal line on the plot above the nutrition facts labels. Explain that we can represent each label with an item such as a dot sticker or a sticky note. Then, start with the minimum x-value on the plot and place the new sticker above the second horizontal axis. Do this for each nutrition facts label in the plot. Once all values have been represented, ask the students how this new plot IS or IS NOT better than the original. Explain that we can call this type of plot a dotplot since we\u2019re using dots to represent each observation. Distribute the Tangible Plot handout ( LMR_1.9 ). Each student should pick one of the 2 numerical variables plotted on the poster paper. Then, they should complete part 1 of the Tangible Plot handout before leaving class. They will complete part 2 of the handout for homework. LMR_1.9 Ask students to think about the statistical investigative questions they came up with. Can the visualizations they created in class help answer their statistical investigative question? If yes, answer the question; if not, what visualization would be appropriate? Class Scribes: One team of students will give a brief talk to discuss what they think the 3 most important topics of the day were. Homework Students will complete part 2 of the Tangible Plot handout ( LMR_1.9 ) and bring it to the next class for assessment. Students should continue to collect nutritional facts data using the Food Habits Participatory Sensing campaign on their smart devices or via web browser.","title":"Lesson 8: Tangible Plots"},{"location":"unit1/lesson8/#lesson-8-tangible-plots","text":"","title":"Lesson 8: Tangible Plots"},{"location":"unit1/lesson8/#objective","text":"Students will learn how distributions help us organize and visualize data values, and that the shapes of the distributions give us information about the variability of the data.","title":"Objective:"},{"location":"unit1/lesson8/#materials","text":"Computer and projector for Campaign Monitoring Video: Value of Data Visualization found at: https://www.youtube.com/watch?v=xekEXM0Vonc https://www.youtube.com/embed/xekEXM0Vonc Nutrition facts labels or pictures (collected previously by students) Food Habits Data Collection handout (from lesson 6 , LMR_1.8_Food Habits Data Collection ) 3 pieces of tape per student Poster paper Dot stickers or sticky notes Tangible Plot handout ( LMR_1.9_Tangible Plot )","title":"Materials:"},{"location":"unit1/lesson8/#vocabulary","text":"x-axis y-axis visualization minimum maximum frequency distribution range typical symmetric data points dotplot","title":"Vocabulary:"},{"location":"unit1/lesson8/#essential-concepts","text":"Essential Concepts: Distributions organize data for us by telling us (a) which values of a variable were observed, and (b) how many times the values were observed (their frequency). In preparation for this lesson, ensure that: a) You watch this video on the Campaign Monitoring Tool. b) Each day, preview student responses to ensure appropriate inputs. Watch this video to learn how to manage student responses.","title":"Essential Concepts:"},{"location":"unit1/lesson8/#lesson","text":"Food Habits Campaign Data Collection Monitoring: Display the IDS Campaign Monitoring Tool, found at https://portal.idsucla.org Click on Campaign Monitor and sign in. Inform students that you will be monitoring their data collection. This is a good opportunity for teachers to remind students that if their data are not shared, they cannot be used in analysis. See User List and sort by Total . Ask: Who has collected the most data so far? Click on the pie chart. Ask: How many active users are there? How many inactive users are there? See Total Responses . How many responses have been submitted? Using TPS, ask students to think about what can they do to increase their data collection. Inform students that today they will be learning how to visualize their data. Show the Value of Data Visualization video at https://www.youtube.com/watch?v=xekEXM0Vonc , which describes the importance of graphical representations of data. As they watch the video, students should respond to the following in their DS journals: What is data visualization? List one example of how visualization can be used to increase data comprehension. Have a whole class discussion regarding the video\u2019s last statement: \u201cYour message is only as good as your ability to share it.\u201d Ask students: What does this statement mean? What makes a good message in terms of data and visualizations? Have students take out their nutrition facts labels or pictures, and also their Food Habits Data Collection handout (from lesson 6 ). On poster paper, make the first quadrant of a coordinate plane. Leave the labels for the x-axis and y-axis blank for now (see step 10). Distribute 3 pieces of tape to each student. Make sure they fold each piece of tape to make two sticky sides. Have each student tape one sticky side to the back of each label and ask them to have the labels ready to tape onto the poster paper. As a class, ask students to select 2 numerical variables and 1 categorical variable from the Food Habits Data Collection handout whose data they would like to see in a visualization , which is a picture of the data. For example, students may vote to see a visualization of the following numerical variables: calories per serving, total fat per serving; categorical variable: salty_sweet Once students select the variables, inform them that they will be creating a plot with the nutrition facts labels for each of the variables they selected. Create a bargraph of the categorical variable chosen by the students. Begin by showing students how to clearly label the x-axis with the categories. For instance, if salty_sweet is the variable, ask students to identify the categories for that variable. Then mark the y-axis with the label frequency , which simply means the number of times an outcome occurs. Do not put tick-marks on the y-axis. The frequency will be measured by the number of labels plotted. Have students come up and place their nutrition fact label above the category that describes their snack. Have students stack their nutrition label so that is easy to calculate the frequency. Once all the labels have been placed, create bars with the appropriate height (frequency) for each category. Make sure to leave spaces between the bars, and that bars are the same width. Ask students to respond to the following questions in their DS journals: How many data points does this distribution have? Why? What information is this visualization telling us about [insert categorical variable name] in the snacks we consume? Use another piece of poster paper to create a distribution for the first numerical variable chosen by the students. Create a dotplot of the first numerical variable chosen by the students. Begin by showing students how to clearly label the x-axis. For instance, if calories per serving is the variable, ask students for the range of values for calories per serving and determine the minimum and maximum values for the dataset. Clearly label the x-axis with adequate intervals and the variable\u2019s name. Then mark the y-axis with the label frequency , which simply means the number of times a value occurs. Do not put tick-marks on the y-axis. The frequency will be measured by the number of labels plotted. For each value in the dataset, put a nutrition facts label above that value on the x-axis. When a value occurs more than once, stack the nutrition facts labels. For example, if there are three nutrition facts labels with 120 calories per serving in the dataset, there will be three nutrition facts labels above the 120 mark on the x-axis. Once all the labels have been placed, ask students to observe the distribution of the data in the dotplot. Ask students to respond to the following questions in their DS journals: What are the minimum and the maximum values of the dataset? Answers will vary by class. The range is the largest value minus the smallest value. It is one way of measuring the variability of a variable. What is the range, and why do you think this measures the variability? Answers will vary by class. The range measures variability because if the values of the variable are really different, the range will be a big number (because the max and min will be far apart); but if there is little variability, the range will be small. For example, if all of the values were the same, we would have no variability and the range would be 0 because the max and min would be the same number. How many data points does this distribution have? Why? Answers will vary by class. What is the amount of [insert variable name] that appears most often in a snack? Why? Answers will vary by class. What do you think the phrase distribution of the data set means? It shows us how values are distributed. We learn where there are many values, where there are only a few values, and where there are no values at all. What information is this distribution telling us about the [insert variable name] in the snacks we consume? Answers will vary. We see how the value of [variable name] varies. For example, we can see whether all foods have the same number of calories, or if they differ. A distribution tells us two things: the values of the variable and the frequency of the values. \"Frequency\" is just another way of saying \"the count.\" Why is this dotplot a picture of the distribution of [variable name]? Because the location of the labels on the x-axis tells us the values we saw, and the number of labels at that value tells us the frequency for that value. Review the students\u2019 responses in a class discussion. Ask students to put a check mark next to the answers that were validated, and to revise the answers that need to be corrected. Use another piece of poster paper to create a distribution for the second numerical variable chosen by the students. Repeat steps 14-16 with this variable. On the first visualization for the numerical variable, show students how to convert the nutrition facts labels into something more readable. Draw another horizontal line on the plot above the nutrition facts labels. Explain that we can represent each label with an item such as a dot sticker or a sticky note. Then, start with the minimum x-value on the plot and place the new sticker above the second horizontal axis. Do this for each nutrition facts label in the plot. Once all values have been represented, ask the students how this new plot IS or IS NOT better than the original. Explain that we can call this type of plot a dotplot since we\u2019re using dots to represent each observation. Distribute the Tangible Plot handout ( LMR_1.9 ). Each student should pick one of the 2 numerical variables plotted on the poster paper. Then, they should complete part 1 of the Tangible Plot handout before leaving class. They will complete part 2 of the handout for homework. LMR_1.9 Ask students to think about the statistical investigative questions they came up with. Can the visualizations they created in class help answer their statistical investigative question? If yes, answer the question; if not, what visualization would be appropriate?","title":"Lesson:"},{"location":"unit1/lesson8/#class-scribes","text":"One team of students will give a brief talk to discuss what they think the 3 most important topics of the day were.","title":"Class Scribes:"},{"location":"unit1/lesson8/#homework","text":"Students will complete part 2 of the Tangible Plot handout ( LMR_1.9 ) and bring it to the next class for assessment. Students should continue to collect nutritional facts data using the Food Habits Participatory Sensing campaign on their smart devices or via web browser.","title":"Homework"},{"location":"unit1/lesson9/","text":"Lesson 9: What is Typical? Objective: Students will learn about the typical value when looking at a distribution by finding the \u201ccenter\u201d and determining any point clusters. Materials: Nutrition facts dotplot (from lesson 8 ) Poster paper Markers, dot stickers, or sticky notes Student Monitoring Tool Video Optional: Real-time Data Collection App Video Vocabulary: typical center shape spread Essential Concepts: Essential Concepts: The \u201ccenter\u201d of a distribution is a deliberately vague term, but it is one way to answer the subjective question \"what is a typical value?\" The center could be the perceived balancing point or the value that approximately cuts the area of the distribution in half. Optional: Advanced Preparation Needed Watch this video to learn how to collect data in real time! You may consider using this tool for steps 12-13 in this lesson. Lesson: Food Habits Campaign Data Collection Monitoring: Display the IDS Campaign Monitoring Tool, found at https://portal.idsucla.org Click on Campaign Monitor and sign in. Inform students that you will be monitoring their data collection again today. See User List and sort by Total . Ask: Who has collected the most data so far? Click on the pie chart. Ask: How many active users are there? How many inactive users are there? See Total Responses . How many responses have been submitted? Using TPS, ask students to think about what they can do to increase their data collection. Encourage students to monitor their own data collection. Show this video . Inform students that today they will be learning about a distribution\u2019s typical value. Ask the class to brainstorm characteristics of the \"typical\" student. Does the typical 12th grader differ from the typical 9th grader? How so? They may say that everyone is different, and that there's no typical student. Keep pressing them to identify characteristics that are typical. The idea is to get them to recognize that there is variability, and yet we might still have an opinion about what is typical. For instance, not all students walk to school, but this might still be the typical experience. Give students 3 minutes to write down synonyms to the word \u201ctypical\u201d in their DS journals. After time is up, have the students share their responses and keep a record on the board. Some possible synonyms might be: normal, average, usual, standard, representative, regular, ordinary, natural, etc. Once students share their synonyms, ask students to think about which terms apply best to categorical variables and which terms apply best to numerical variables. Ask volunteers to share out their thoughts and give a brief explanation of why they categorized the term as either applying best to categorical variables or numerical variables. Create a T-chart on the board to keep track of their categories. Next, display the dotplot created by the class with their nutrition facts labels during the previous class (from lesson 8 ). Ask: what value might we consider to be the typical value of this distribution? Answers will vary by class. Common answers will be to identify the mode (the value with the most labels) or the value in the center. A common wrong answer will be to confuse the frequency with the value. For example, they will say the most typical number of calories was \"3\" because, perhaps, 100 calories occurred 3 times, and that was more often than any other value. Students may also identify \"clumps\u201d of data: \"it's somewhere between 110 and 120.\" That's ok, but probe them as to why they chose that chunk and not another. The point is to get them to see that chunk as being in the middle or center of the distribution. Hopefully, at least one student will choose a value close to the center of the distribution. If not, point to a value near the extreme and ask them if they think this is typical. Then move closer to the center until they agree on which values are typical. It is ok to be vague in the definition of typical for today\u2019s lesson. The discussion needs to be very teacher-driven. Some possible points of discussion might be: Clustering/clumps of data. Most of the observations are between and . Overall range of the data. Ask students to reconsider the typical number of sugar grams. What is the typical amount of sugar (in grams) in our snacks? For example, students may come up with the same answer for different reasons: \u201cThe typical amount of sugar grams is 10.\u201d The reasons may include the data points are half below and half above; it\u2019s the mode; it has plurality. Then, tie it back to the synonyms for \"typical\" they provided earlier. Ask: Which synonym are you using? In pairs, ask students to discuss the question: Which synonyms are associated with \u201ccenter\u201d? Is this concept of center useful for numerical or categorical variables? Center is useful for numerical variables. The center of the distribution often corresponds to our notion of \u2018typical value.\u2019 For example, the typical height of the students in our class might be centered around 5\u20195. Inform students that the value at the center of the distribution often matches up with our everyday notion of the typical value of a distribution. The middle observation is not always the typical value. Similarly, the middle person would not always be the center value. Defining the center of a distribution depends on many things, such as the placement of points in the distribution (known as the shape ) and how dense the distribution is at certain values (known as the spread ). Ask the students to write down the number of hours of sleep they got last night. They will be creating a dotplot of this data, so ask them: What do you think the typical value will be? What do you think the lowest value will be? What do you think the highest value will be? What do you think the shape of the distribution will look like? One-by-one, have them come up to the board (or poster paper) and put a dot above the correct value on the dotplot. After each student has placed a dot on the board, have a discussion about the distribution. Is the typical value similar to what they originally thought? The shape? The variability? Why or why not? Next, have the students write down the number of hours of sleep they hope to get this Saturday. How do they think this plot will differ from the first plot? Focus discussion on the shape, center, and spread of the distributions. Repeat steps 8-9 and discuss how this plot is similar and/or different than the first plot. Class Scribes: One team of students will give a brief talk to discuss what they think the 3 most important topics of the day were. Homework Students should continue to collect nutritional facts data using the Food Habits Participatory Sensing campaign on their smart devices or via web browser.","title":"Lesson 9: What is Typical?"},{"location":"unit1/lesson9/#lesson-9-what-is-typical","text":"","title":"Lesson 9: What is Typical?"},{"location":"unit1/lesson9/#objective","text":"Students will learn about the typical value when looking at a distribution by finding the \u201ccenter\u201d and determining any point clusters.","title":"Objective:"},{"location":"unit1/lesson9/#materials","text":"Nutrition facts dotplot (from lesson 8 ) Poster paper Markers, dot stickers, or sticky notes Student Monitoring Tool Video Optional: Real-time Data Collection App Video","title":"Materials:"},{"location":"unit1/lesson9/#vocabulary","text":"typical center shape spread","title":"Vocabulary:"},{"location":"unit1/lesson9/#essential-concepts","text":"Essential Concepts: The \u201ccenter\u201d of a distribution is a deliberately vague term, but it is one way to answer the subjective question \"what is a typical value?\" The center could be the perceived balancing point or the value that approximately cuts the area of the distribution in half. Optional: Advanced Preparation Needed Watch this video to learn how to collect data in real time! You may consider using this tool for steps 12-13 in this lesson.","title":"Essential Concepts:"},{"location":"unit1/lesson9/#lesson","text":"Food Habits Campaign Data Collection Monitoring: Display the IDS Campaign Monitoring Tool, found at https://portal.idsucla.org Click on Campaign Monitor and sign in. Inform students that you will be monitoring their data collection again today. See User List and sort by Total . Ask: Who has collected the most data so far? Click on the pie chart. Ask: How many active users are there? How many inactive users are there? See Total Responses . How many responses have been submitted? Using TPS, ask students to think about what they can do to increase their data collection. Encourage students to monitor their own data collection. Show this video . Inform students that today they will be learning about a distribution\u2019s typical value. Ask the class to brainstorm characteristics of the \"typical\" student. Does the typical 12th grader differ from the typical 9th grader? How so? They may say that everyone is different, and that there's no typical student. Keep pressing them to identify characteristics that are typical. The idea is to get them to recognize that there is variability, and yet we might still have an opinion about what is typical. For instance, not all students walk to school, but this might still be the typical experience. Give students 3 minutes to write down synonyms to the word \u201ctypical\u201d in their DS journals. After time is up, have the students share their responses and keep a record on the board. Some possible synonyms might be: normal, average, usual, standard, representative, regular, ordinary, natural, etc. Once students share their synonyms, ask students to think about which terms apply best to categorical variables and which terms apply best to numerical variables. Ask volunteers to share out their thoughts and give a brief explanation of why they categorized the term as either applying best to categorical variables or numerical variables. Create a T-chart on the board to keep track of their categories. Next, display the dotplot created by the class with their nutrition facts labels during the previous class (from lesson 8 ). Ask: what value might we consider to be the typical value of this distribution? Answers will vary by class. Common answers will be to identify the mode (the value with the most labels) or the value in the center. A common wrong answer will be to confuse the frequency with the value. For example, they will say the most typical number of calories was \"3\" because, perhaps, 100 calories occurred 3 times, and that was more often than any other value. Students may also identify \"clumps\u201d of data: \"it's somewhere between 110 and 120.\" That's ok, but probe them as to why they chose that chunk and not another. The point is to get them to see that chunk as being in the middle or center of the distribution. Hopefully, at least one student will choose a value close to the center of the distribution. If not, point to a value near the extreme and ask them if they think this is typical. Then move closer to the center until they agree on which values are typical. It is ok to be vague in the definition of typical for today\u2019s lesson. The discussion needs to be very teacher-driven. Some possible points of discussion might be: Clustering/clumps of data. Most of the observations are between and . Overall range of the data. Ask students to reconsider the typical number of sugar grams. What is the typical amount of sugar (in grams) in our snacks? For example, students may come up with the same answer for different reasons: \u201cThe typical amount of sugar grams is 10.\u201d The reasons may include the data points are half below and half above; it\u2019s the mode; it has plurality. Then, tie it back to the synonyms for \"typical\" they provided earlier. Ask: Which synonym are you using? In pairs, ask students to discuss the question: Which synonyms are associated with \u201ccenter\u201d? Is this concept of center useful for numerical or categorical variables? Center is useful for numerical variables. The center of the distribution often corresponds to our notion of \u2018typical value.\u2019 For example, the typical height of the students in our class might be centered around 5\u20195. Inform students that the value at the center of the distribution often matches up with our everyday notion of the typical value of a distribution. The middle observation is not always the typical value. Similarly, the middle person would not always be the center value. Defining the center of a distribution depends on many things, such as the placement of points in the distribution (known as the shape ) and how dense the distribution is at certain values (known as the spread ). Ask the students to write down the number of hours of sleep they got last night. They will be creating a dotplot of this data, so ask them: What do you think the typical value will be? What do you think the lowest value will be? What do you think the highest value will be? What do you think the shape of the distribution will look like? One-by-one, have them come up to the board (or poster paper) and put a dot above the correct value on the dotplot. After each student has placed a dot on the board, have a discussion about the distribution. Is the typical value similar to what they originally thought? The shape? The variability? Why or why not? Next, have the students write down the number of hours of sleep they hope to get this Saturday. How do they think this plot will differ from the first plot? Focus discussion on the shape, center, and spread of the distributions. Repeat steps 8-9 and discuss how this plot is similar and/or different than the first plot.","title":"Lesson:"},{"location":"unit1/lesson9/#class-scribes","text":"One team of students will give a brief talk to discuss what they think the 3 most important topics of the day were.","title":"Class Scribes:"},{"location":"unit1/lesson9/#homework","text":"Students should continue to collect nutritional facts data using the Food Habits Participatory Sensing campaign on their smart devices or via web browser.","title":"Homework"},{"location":"unit1/overview/","text":"Introduction to Data Science Daily Overview: Unit 1 Unit 1 Daily Overview: Unit 1 .tg {border-collapse:collapse;border-spacing:0;} .tg td{font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;} .tg th{font-family:Arial, sans-serif;font-size:14px;font-weight:normal;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;} .tg .tg-88nc{font-weight:bold;border-color:inherit;text-align:center} .tg .tg-yj5y{background-color:#efefef;border-color:inherit;text-align:center;vertical-align:top} .tg .tg-c3ow{border-color:inherit;text-align:center;vertical-align:top} .tg .tg-7btt{font-weight:bold;border-color:inherit;text-align:center;vertical-align:top} .tg .tg-y698{background-color:#efefef;border-color:inherit;text-align:left;vertical-align:top} .tg .tg-0pky{border-color:inherit;text-align:left;vertical-align:top} Theme Day Lessons and Labs Campaign Topics Page Data Are All Around (7 days) 1 Lesson 1: Data Trails Defining data, consumer privacy 27 2 Lesson 2: Stick Figures Organizing & collecting data 29 3 Lesson 3: Data Structures Organizing data, rows & columns, variables 31 4 Lesson 4: The Data Cycle Data cycle, statistical questions 34 5 Lesson 5: So Many Questions Statistical questions, variability 38 6^ Lesson 6: What Do I Eat? Food Habits Collecting data, statistical questions 40 7 Lesson 7: Setting the Stage Food Habits \u2013 data Participatory sensing 43 Visualizing Data (14 days) 8 Lesson 8: Tangible Plots Food Habits \u2013 data Dotplots, minimum/maximum, frequency 49 9 Lesson 9: What Is Typical? Food Habits \u2013 data Typical value, center 53 10 Lesson 10: Making Histograms Food Habits \u2013 data Histograms, bin widths 55 11 Lesson 11: What Shape Are You In? Food Habits \u2013 data Shape, center, spread 58 12 Lesson 12: Exploring Food Habits Food Habits \u2013 data Single & multi-variable plots 60 13 Lesson 13: RStudio Basics Food Habits \u2013 data Intro to RStudio 62 14 Lab 1A: Data, Code & RStudio Food Habits \u2013 data RStudio basics 65 15+ Lab 1B: Get the Picture? Food Habits \u2013 data Variable types, bar graphs, histograms 68 16 Lab 1C: Export, Upload, Import Importing data 71 17 Lesson 14: Variables, Variables, Variables Multi-variable plots 75 18 Lab 1D: Zooming Through Data Subsetting 80 19 Lab 1E: What\u2019s the Relationship? Multi-variable plots 83 20 Practicum: The Data Cycle & My Food Habits Food Habits Data cycle, variability 86 21 Practicum Presentations Food Habits Data cycle, variability - Would You Look at the Time? (9 Days) 22^ Lesson 15: Americans\u2019 Time on Task Time Use \u2013 data Evaluating claims 90 23 Lab 1F: A Diamond In the Rough Time Use \u2013 data Cleaning names, categories, and strings 94 24 Lesson 16: Categorical Associations Time Use \u2013 data Joint relative frequencies in 2- way tables 98 25 Lesson 17: Interpreting Two-Way Tables Time Use \u2013 data Marginal & conditional relative frequencies 100 26+ Lab 1G: What\u2019s the FREQ? Time Use \u2013 data 2-way tables, tally 105 27 Practicum: Teen Depression Time Use Statistical questions, interpreting plots 107 28 Practicum Presentations Statistical questions, interpreting plots - 29-30 Lab 1H: Our Time Data cycle, synthesis 109 Unit 1 Project (5 Days) 31-35 End of Unit Project and Oral Presentation: Analyzing Data to Evaluate Claims Data cycle 110 ^=Data collection window begins. +=Data collection window ends.","title":"Daily Overview"},{"location":"unit1/overview/#introduction-to-data-science-daily-overview-unit-1","text":"","title":"Introduction to Data Science Daily Overview: Unit 1"},{"location":"unit1/overview/#daily-overview-unit-1","text":".tg {border-collapse:collapse;border-spacing:0;} .tg td{font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;} .tg th{font-family:Arial, sans-serif;font-size:14px;font-weight:normal;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;} .tg .tg-88nc{font-weight:bold;border-color:inherit;text-align:center} .tg .tg-yj5y{background-color:#efefef;border-color:inherit;text-align:center;vertical-align:top} .tg .tg-c3ow{border-color:inherit;text-align:center;vertical-align:top} .tg .tg-7btt{font-weight:bold;border-color:inherit;text-align:center;vertical-align:top} .tg .tg-y698{background-color:#efefef;border-color:inherit;text-align:left;vertical-align:top} .tg .tg-0pky{border-color:inherit;text-align:left;vertical-align:top} Theme Day Lessons and Labs Campaign Topics Page Data Are All Around (7 days) 1 Lesson 1: Data Trails Defining data, consumer privacy 27 2 Lesson 2: Stick Figures Organizing & collecting data 29 3 Lesson 3: Data Structures Organizing data, rows & columns, variables 31 4 Lesson 4: The Data Cycle Data cycle, statistical questions 34 5 Lesson 5: So Many Questions Statistical questions, variability 38 6^ Lesson 6: What Do I Eat? Food Habits Collecting data, statistical questions 40 7 Lesson 7: Setting the Stage Food Habits \u2013 data Participatory sensing 43 Visualizing Data (14 days) 8 Lesson 8: Tangible Plots Food Habits \u2013 data Dotplots, minimum/maximum, frequency 49 9 Lesson 9: What Is Typical? Food Habits \u2013 data Typical value, center 53 10 Lesson 10: Making Histograms Food Habits \u2013 data Histograms, bin widths 55 11 Lesson 11: What Shape Are You In? Food Habits \u2013 data Shape, center, spread 58 12 Lesson 12: Exploring Food Habits Food Habits \u2013 data Single & multi-variable plots 60 13 Lesson 13: RStudio Basics Food Habits \u2013 data Intro to RStudio 62 14 Lab 1A: Data, Code & RStudio Food Habits \u2013 data RStudio basics 65 15+ Lab 1B: Get the Picture? Food Habits \u2013 data Variable types, bar graphs, histograms 68 16 Lab 1C: Export, Upload, Import Importing data 71 17 Lesson 14: Variables, Variables, Variables Multi-variable plots 75 18 Lab 1D: Zooming Through Data Subsetting 80 19 Lab 1E: What\u2019s the Relationship? Multi-variable plots 83 20 Practicum: The Data Cycle & My Food Habits Food Habits Data cycle, variability 86 21 Practicum Presentations Food Habits Data cycle, variability - Would You Look at the Time? (9 Days) 22^ Lesson 15: Americans\u2019 Time on Task Time Use \u2013 data Evaluating claims 90 23 Lab 1F: A Diamond In the Rough Time Use \u2013 data Cleaning names, categories, and strings 94 24 Lesson 16: Categorical Associations Time Use \u2013 data Joint relative frequencies in 2- way tables 98 25 Lesson 17: Interpreting Two-Way Tables Time Use \u2013 data Marginal & conditional relative frequencies 100 26+ Lab 1G: What\u2019s the FREQ? Time Use \u2013 data 2-way tables, tally 105 27 Practicum: Teen Depression Time Use Statistical questions, interpreting plots 107 28 Practicum Presentations Statistical questions, interpreting plots - 29-30 Lab 1H: Our Time Data cycle, synthesis 109 Unit 1 Project (5 Days) 31-35 End of Unit Project and Oral Presentation: Analyzing Data to Evaluate Claims Data cycle 110 ^=Data collection window begins. +=Data collection window ends.","title":"Daily Overview: Unit 1"},{"location":"unit1/practicum1/","text":"Practicum: The Data Cycle & My Food Habits Objective: Students will apply what they have learned by engaging in the Data Cycle using the data they collected from the Food Habits campaign. Students will present their findings to the class. Materials: The Data Cycle Practicum ( LMR_U1_Practicum_Data Cycle ) Poster paper Markers Practicum The Data Cycle & My Food Habits Instructions: With a partner, you will engage in the Data Cycle to address the Research Topic: What do our snacking habits reveal about us? Task: Create a Data Cycle poster. The poster should illustrate how the Data Cycle is used to address the Research Topic. Use RStudio to create at least one statistical graphic. The graphic MUST be included on the poster. You and your partner will present your findings with appropriate evidence from the data. Awards: Your teacher will select the top posters in the following categories: Best Statistical Question Most Interesting Statistical Graphic Best Illustration of the Data Cycle Scoring Guide: Below you will find some parameters to assist you in scoring. They are meant only as a guide. 4-point response: The poster correctly illustrates how the Data Cycle is used to address the big question. A histogram, bar chart, scatterplot, or other graphical representation was correctly created. An answer and a justification for the answer to the statistical question are presented. A justification includes mention of statistics concepts learned thus far. For example, \u201cThe variables are\u2026\u201d AND it includes acknowledgment of variability. For example, \u201cThere are between and .\u201d 3-point response: The poster correctly illustrates how the Data Cycle is used to address the big question. A histogram, bar chart, scatterplot, or other graphical representation was correctly created. An answer and a justification for the answer to the statistical question are presented. A justification includes mention of statistics concepts learned thus far. For example, \u201cThe variables are\u2026\u201d OR it includes acknowledgment of variability. For example, \u201cThere are between and .\u201d 2-point response: The poster partially illustrates how the Data Cycle is used to address the big question. A histogram, bar chart, scatterplot, or other graphical representation was created. An answer and a justification for the answer to the statistical question are presented. 1-point response: The poster incorrectly illustrates how the Data Cycle addresses the big question. An answer to the statistical question is presented but a justification is missing. A histogram, bar chart, scatterplot, or other graphical representation was correctly created. 0-point response: The Data Cycle is missing OR does not show how it addresses the big question. A histogram, a dot plot, or other graphical representation was incorrectly created OR is missing. No answer AND/OR no justification for the answer to the statistical question is presented.","title":"Practicum: The Data Cycle & My Food Habits"},{"location":"unit1/practicum1/#practicum-the-data-cycle-my-food-habits","text":"","title":"Practicum: The Data Cycle &amp; My Food Habits"},{"location":"unit1/practicum1/#objective","text":"Students will apply what they have learned by engaging in the Data Cycle using the data they collected from the Food Habits campaign. Students will present their findings to the class.","title":"Objective:"},{"location":"unit1/practicum1/#materials","text":"The Data Cycle Practicum ( LMR_U1_Practicum_Data Cycle ) Poster paper Markers Practicum The Data Cycle & My Food Habits","title":"Materials:"},{"location":"unit1/practicum1/#instructions","text":"With a partner, you will engage in the Data Cycle to address the Research Topic: What do our snacking habits reveal about us?","title":"Instructions:"},{"location":"unit1/practicum1/#task","text":"Create a Data Cycle poster. The poster should illustrate how the Data Cycle is used to address the Research Topic. Use RStudio to create at least one statistical graphic. The graphic MUST be included on the poster. You and your partner will present your findings with appropriate evidence from the data.","title":"Task:"},{"location":"unit1/practicum1/#awards","text":"Your teacher will select the top posters in the following categories: Best Statistical Question Most Interesting Statistical Graphic Best Illustration of the Data Cycle","title":"Awards:"},{"location":"unit1/practicum1/#scoring-guide","text":"Below you will find some parameters to assist you in scoring. They are meant only as a guide. 4-point response: The poster correctly illustrates how the Data Cycle is used to address the big question. A histogram, bar chart, scatterplot, or other graphical representation was correctly created. An answer and a justification for the answer to the statistical question are presented. A justification includes mention of statistics concepts learned thus far. For example, \u201cThe variables are\u2026\u201d AND it includes acknowledgment of variability. For example, \u201cThere are between and .\u201d 3-point response: The poster correctly illustrates how the Data Cycle is used to address the big question. A histogram, bar chart, scatterplot, or other graphical representation was correctly created. An answer and a justification for the answer to the statistical question are presented. A justification includes mention of statistics concepts learned thus far. For example, \u201cThe variables are\u2026\u201d OR it includes acknowledgment of variability. For example, \u201cThere are between and .\u201d 2-point response: The poster partially illustrates how the Data Cycle is used to address the big question. A histogram, bar chart, scatterplot, or other graphical representation was created. An answer and a justification for the answer to the statistical question are presented. 1-point response: The poster incorrectly illustrates how the Data Cycle addresses the big question. An answer to the statistical question is presented but a justification is missing. A histogram, bar chart, scatterplot, or other graphical representation was correctly created. 0-point response: The Data Cycle is missing OR does not show how it addresses the big question. A histogram, a dot plot, or other graphical representation was incorrectly created OR is missing. No answer AND/OR no justification for the answer to the statistical question is presented.","title":"Scoring Guide:"},{"location":"unit1/practicum2/","text":"Practicum: Teen Depression Objective: Using the CDC data set, students will apply their learning of statistical concepts to determine possible factors that might be associated with depression in teens. They will create graphical representations to analyze and interpret the data. Students will present their findings to their teams the following day. Materials: Teen Depression Practicum ( LMR_U1_Practicum_Depression ) Depression Fact Sheet ( LMR_U1_Practicum_Depression_Fact Sheet ) Poster paper Markers Practicum Teen Depression Background: The Centers for Disease Control and Prevention (CDC) collect data about teenagers on a variety of topics. One of these topics is depression. According to the fact sheet published by the National Institute for Mental Health, depression is a real problem among teens. Instructions: With a partner, you will read the depression fact sheet and then use the CDC data to address the Research Topic. Research Topic: What factors are associated with depression in teens? Task: Create a poster that addresses the Research Topic. Generate a statistical question that might address the Research Topic. Use RStudio to create at least one statistical graphic. The graphic MUST be included on the poster. You and your partner will present your findings with appropriate evidence from the data. Awards: Your teacher will select the top posters in the following categories: Best Statistical Question Most Interesting Statistical Graphic Scoring Guide: 4-point response: The poster identifies possible factors that are in the data set that might be associated with depression in teens. A graphical representation that shows an association was created. An answer and a justification for the answer to the statistical question are presented. A justification includes mention of statistics concepts learned thus far. For example, \u201cThe variables are\u2026\u201d ; AND it includes acknowledgment of variability. For example, \u201cThere are between and .\u201d 3-point response: The poster identifies possible factors in the data set that might be associated with depression in teens. A graphical representation that shows an association was created. An answer and a justification for the answer to the statistical question are presented. A justification includes mention of statistics concepts learned thus far. For example, \u201cThe variables are\u2026\u201d ; OR it includes acknowledgment of variability. For example, \u201cThere are between and .\u201d 2-point response: The poster identifies possible factors that might be associated with depression in teens. A graphical representation that shows an association was created. An answer and a justification for the answer to the statistical question are presented. 1-point response: The poster identifies possible factors that might be associated with depression in teens. An answer to the statistical question is presented but a justification is missing. A histogram, bar chart, scatterplot, or other graphical representation was correctly created. 0-point response: The poster does not identify possible factors that might be associated with depression in teens A histogram, a dot plot or other graphical representation was incorrectly created OR is missing. No answer AND/OR no justification for the answer to the statistical question is presented. Next Day Lab 1H: Our time. Complete Lab 1H prior to the End of Unit Project .","title":"Practicum: Teen Depression"},{"location":"unit1/practicum2/#practicum-teen-depression","text":"","title":"Practicum: Teen Depression"},{"location":"unit1/practicum2/#objective","text":"Using the CDC data set, students will apply their learning of statistical concepts to determine possible factors that might be associated with depression in teens. They will create graphical representations to analyze and interpret the data. Students will present their findings to their teams the following day.","title":"Objective:"},{"location":"unit1/practicum2/#materials","text":"Teen Depression Practicum ( LMR_U1_Practicum_Depression ) Depression Fact Sheet ( LMR_U1_Practicum_Depression_Fact Sheet ) Poster paper Markers Practicum Teen Depression","title":"Materials:"},{"location":"unit1/practicum2/#background","text":"The Centers for Disease Control and Prevention (CDC) collect data about teenagers on a variety of topics. One of these topics is depression. According to the fact sheet published by the National Institute for Mental Health, depression is a real problem among teens.","title":"Background:"},{"location":"unit1/practicum2/#instructions","text":"With a partner, you will read the depression fact sheet and then use the CDC data to address the Research Topic.","title":"Instructions:"},{"location":"unit1/practicum2/#research-topic","text":"What factors are associated with depression in teens?","title":"Research Topic:"},{"location":"unit1/practicum2/#task","text":"Create a poster that addresses the Research Topic. Generate a statistical question that might address the Research Topic. Use RStudio to create at least one statistical graphic. The graphic MUST be included on the poster. You and your partner will present your findings with appropriate evidence from the data.","title":"Task:"},{"location":"unit1/practicum2/#awards","text":"Your teacher will select the top posters in the following categories: Best Statistical Question Most Interesting Statistical Graphic","title":"Awards:"},{"location":"unit1/practicum2/#scoring-guide","text":"4-point response: The poster identifies possible factors that are in the data set that might be associated with depression in teens. A graphical representation that shows an association was created. An answer and a justification for the answer to the statistical question are presented. A justification includes mention of statistics concepts learned thus far. For example, \u201cThe variables are\u2026\u201d ; AND it includes acknowledgment of variability. For example, \u201cThere are between and .\u201d 3-point response: The poster identifies possible factors in the data set that might be associated with depression in teens. A graphical representation that shows an association was created. An answer and a justification for the answer to the statistical question are presented. A justification includes mention of statistics concepts learned thus far. For example, \u201cThe variables are\u2026\u201d ; OR it includes acknowledgment of variability. For example, \u201cThere are between and .\u201d 2-point response: The poster identifies possible factors that might be associated with depression in teens. A graphical representation that shows an association was created. An answer and a justification for the answer to the statistical question are presented. 1-point response: The poster identifies possible factors that might be associated with depression in teens. An answer to the statistical question is presented but a justification is missing. A histogram, bar chart, scatterplot, or other graphical representation was correctly created. 0-point response: The poster does not identify possible factors that might be associated with depression in teens A histogram, a dot plot or other graphical representation was incorrectly created OR is missing. No answer AND/OR no justification for the answer to the statistical question is presented.","title":"Scoring Guide:"},{"location":"unit1/practicum2/#next-day","text":"Lab 1H: Our time. Complete Lab 1H prior to the End of Unit Project .","title":"Next Day"},{"location":"unit1/section1/","text":"Unit 1, Section 1: Data Are All Around Instructional Days: 7 Enduring Understandings Data play an important role in our everyday lives. Organizing it can provide evidence about real-life events and people. The data collected by answering survey questions produce variability. Distributions, graphs, and plots are useful tools for organizing data to understand variability. Statistical questions address people, processes, and/or events that contain variability. Situations with variability can sometimes be simplified with some basic statistics. Engagement The Target Story will introduce students to the idea that data are ubiquitous. The advent of computers has transformed the way data are collected, used, and analyzed. Video can be found at: https://www.youtube.com/watch?v=XvSA-6BJkx4 https://www.youtube.com/embed/XvSA-6BJkx4 Note Pre-loading the video on your computer prior to the beginning of class is highly recommended to avoid any technical difficulties. Learning Objectives Statistical/Mathematical: S-ID: Summarize, represent, and interpret data on a single count or measurement variable. S-ID 1: Represent data with plots on the real number line (dotplots, histograms, bar plots, and boxplots. S-ID 2: Use statistics appropriate to the shape of the data distribution to compare center (median, mean) of two or more different data sets. (Measures of spread will be studied in unit 2.) S-ID 6: Represent data on two quantitative variables on a scatterplot, and describe how the variables are related. Focus Standards for Mathematical Practice for All of Unit 1: SMP-3: Construct viable arguments and critique the reasoning of others. SMP-5: Use appropriate tools strategically. Data Science: Experience data handling using ubiquitous data, and organize data using rectangular or spreadsheet format as data storage structures. Everyday activities can be observed and recorded as data. Become aware of the difference between plots used for categorical and numerical variables. Interpret and understand graphs of distributions for numerical and categorical variables. Applied Computational Thinking using RStudio: \u2022 Work effectively in teams. \u2022 Explain how data, information, and knowledge are represented for computational use. \u2022 Collect, upload, and share personal data via a Participatory Sensing campaign. \u2022 Learn about different representations of distributions using software. \u2022 Utilize software to begin to analyze plots of data collected via Participatory Sensing. Real-World Connections: Students begin to develop an awareness that data are all around us. Information can be collected and organized. Computers are powerful tools that make organizing, storing, retrieving, and analyzing data accessible to use in problem solving and decision making. Students will begin to see the relevance of data collection to their own lives. They will begin to understand that data on its own is just collected; but once interpreted, it can lead to discoveries or understandings. Language Objectives Students will use complex sentences to construct summary statements about their understanding of data, how it is collected, how it is used, and how to work with it. Students will engage in partner and whole group discussions and presentations to express their understanding of data science concepts. Students will use complex sentences to write informative short reports that use data science concepts and skills. Data File or Data Collection Method Data Collection Method: Students will keep a Data Diary for 24 hours to track their daily data output. Students will gather data from the cards in the Stick Figures file. As a class, students will determine how to organize the Stick Figures data. Students will collect data using paper and pencil on the Food Habits Data Collection activity sheet. Food Habits Participatory Sensing Campaign: Students will collect data about their snacking habits. Legend for Activity Icons","title":"Data Are All Around"},{"location":"unit1/section1/#unit-1-section-1-data-are-all-around","text":"Instructional Days: 7","title":"Unit 1, Section 1: Data Are All Around"},{"location":"unit1/section1/#enduring-understandings","text":"Data play an important role in our everyday lives. Organizing it can provide evidence about real-life events and people. The data collected by answering survey questions produce variability. Distributions, graphs, and plots are useful tools for organizing data to understand variability. Statistical questions address people, processes, and/or events that contain variability. Situations with variability can sometimes be simplified with some basic statistics.","title":"Enduring Understandings"},{"location":"unit1/section1/#engagement","text":"The Target Story will introduce students to the idea that data are ubiquitous. The advent of computers has transformed the way data are collected, used, and analyzed. Video can be found at: https://www.youtube.com/watch?v=XvSA-6BJkx4 https://www.youtube.com/embed/XvSA-6BJkx4 Note Pre-loading the video on your computer prior to the beginning of class is highly recommended to avoid any technical difficulties.","title":"Engagement"},{"location":"unit1/section1/#learning-objectives","text":"Statistical/Mathematical: S-ID: Summarize, represent, and interpret data on a single count or measurement variable. S-ID 1: Represent data with plots on the real number line (dotplots, histograms, bar plots, and boxplots. S-ID 2: Use statistics appropriate to the shape of the data distribution to compare center (median, mean) of two or more different data sets. (Measures of spread will be studied in unit 2.) S-ID 6: Represent data on two quantitative variables on a scatterplot, and describe how the variables are related. Focus Standards for Mathematical Practice for All of Unit 1: SMP-3: Construct viable arguments and critique the reasoning of others. SMP-5: Use appropriate tools strategically. Data Science: Experience data handling using ubiquitous data, and organize data using rectangular or spreadsheet format as data storage structures. Everyday activities can be observed and recorded as data. Become aware of the difference between plots used for categorical and numerical variables. Interpret and understand graphs of distributions for numerical and categorical variables. Applied Computational Thinking using RStudio: \u2022 Work effectively in teams. \u2022 Explain how data, information, and knowledge are represented for computational use. \u2022 Collect, upload, and share personal data via a Participatory Sensing campaign. \u2022 Learn about different representations of distributions using software. \u2022 Utilize software to begin to analyze plots of data collected via Participatory Sensing. Real-World Connections: Students begin to develop an awareness that data are all around us. Information can be collected and organized. Computers are powerful tools that make organizing, storing, retrieving, and analyzing data accessible to use in problem solving and decision making. Students will begin to see the relevance of data collection to their own lives. They will begin to understand that data on its own is just collected; but once interpreted, it can lead to discoveries or understandings.","title":"Learning Objectives"},{"location":"unit1/section1/#language-objectives","text":"Students will use complex sentences to construct summary statements about their understanding of data, how it is collected, how it is used, and how to work with it. Students will engage in partner and whole group discussions and presentations to express their understanding of data science concepts. Students will use complex sentences to write informative short reports that use data science concepts and skills.","title":"Language Objectives"},{"location":"unit1/section1/#data-file-or-data-collection-method","text":"Data Collection Method: Students will keep a Data Diary for 24 hours to track their daily data output. Students will gather data from the cards in the Stick Figures file. As a class, students will determine how to organize the Stick Figures data. Students will collect data using paper and pencil on the Food Habits Data Collection activity sheet. Food Habits Participatory Sensing Campaign: Students will collect data about their snacking habits.","title":"Data File or Data Collection Method"},{"location":"unit1/section1/#legend-for-activity-icons","text":"","title":"Legend for Activity Icons"},{"location":"unit1/section2/","text":"Unit 1, Section 2: Visualizing Data Instructional Days: 14 Enduring Understandings Data collection methods affect what we can know about the real world. Visual representations help tell stories with data. Distributions of numerical and categorical variables help describe variability in the data. Technology and computers allow us to visualize complex relationships in data. Engagement Students will view the video called The Value of Data Visualization to help them understand the importance of graphical representations of data. Discussion questions will allow students to begin to think about how they would want to see a data set visualized. The video can be found at: https://www.youtube.com/watch?v=xekEXM0Vonc https://www.youtube.com/embed/xekEXM0Vonc Learning Objectives Statistical/Mathematical: S-ID 1: Represent data with plots on the real number line (dotplots, histograms, bar plots, and boxplots). S-ID 3: Interpret differences in shape, center, and spread in the context of the data sets, accounting for possible effects of extreme data points (outliers). S-ID 6: Represent data on two quantitative variables on a scatterplot and describe how the variables are related. Data Science: Create visualizations with data. Learn the difference between plots used for categorical and numerical variables. Interpret and understand graphs of distributions for numerical and categorical variables. Applied Computational Thinking Using RStudio: \u2022 Learn to download, load, upload, and work with data using RStudio syntax and structure. \u2022 Create appropriate graphical displays of data. \u2022 Differentiate between observations and variables. \u2022 Learn to use objects, functions, and assignments. Real-World Connections: Students will continue to understand that data on its own is just collected; but once interpreted, it can lead to discoveries or understandings. Language Objectives Students will use complex sentences to construct summary statements about their understanding of data, how it is collected, how it is used and how to work with it. Students will engage in partner and whole group discussions and presentations to express their understanding of data science concepts. Students will use complex sentences to write informative short reports that use data science concepts and skills. Data File or Data Collection Method Data Files: Students\u2019 Food Habits Campaign Data CDC Data File Legend for Activity Icons","title":"Visualizing Data"},{"location":"unit1/section2/#unit-1-section-2-visualizing-data","text":"Instructional Days: 14","title":"Unit 1, Section 2: Visualizing Data"},{"location":"unit1/section2/#enduring-understandings","text":"Data collection methods affect what we can know about the real world. Visual representations help tell stories with data. Distributions of numerical and categorical variables help describe variability in the data. Technology and computers allow us to visualize complex relationships in data.","title":"Enduring Understandings"},{"location":"unit1/section2/#engagement","text":"Students will view the video called The Value of Data Visualization to help them understand the importance of graphical representations of data. Discussion questions will allow students to begin to think about how they would want to see a data set visualized. The video can be found at: https://www.youtube.com/watch?v=xekEXM0Vonc https://www.youtube.com/embed/xekEXM0Vonc","title":"Engagement"},{"location":"unit1/section2/#learning-objectives","text":"Statistical/Mathematical: S-ID 1: Represent data with plots on the real number line (dotplots, histograms, bar plots, and boxplots). S-ID 3: Interpret differences in shape, center, and spread in the context of the data sets, accounting for possible effects of extreme data points (outliers). S-ID 6: Represent data on two quantitative variables on a scatterplot and describe how the variables are related. Data Science: Create visualizations with data. Learn the difference between plots used for categorical and numerical variables. Interpret and understand graphs of distributions for numerical and categorical variables. Applied Computational Thinking Using RStudio: \u2022 Learn to download, load, upload, and work with data using RStudio syntax and structure. \u2022 Create appropriate graphical displays of data. \u2022 Differentiate between observations and variables. \u2022 Learn to use objects, functions, and assignments. Real-World Connections: Students will continue to understand that data on its own is just collected; but once interpreted, it can lead to discoveries or understandings.","title":"Learning Objectives"},{"location":"unit1/section2/#language-objectives","text":"Students will use complex sentences to construct summary statements about their understanding of data, how it is collected, how it is used and how to work with it. Students will engage in partner and whole group discussions and presentations to express their understanding of data science concepts. Students will use complex sentences to write informative short reports that use data science concepts and skills.","title":"Language Objectives"},{"location":"unit1/section2/#data-file-or-data-collection-method","text":"Data Files: Students\u2019 Food Habits Campaign Data CDC Data File","title":"Data File or Data Collection Method"},{"location":"unit1/section2/#legend-for-activity-icons","text":"","title":"Legend for Activity Icons"},{"location":"unit1/section3/","text":"Unit 1, Section 3: Would You Look at the Time! Instructional Days: 9 Enduring Understandings Data are useful for evaluating claims and reports. Summaries of categorical and numerical data show important features and patterns in the data. Data summaries provide evidence to make claims. Engagement The Bureau of Labor Statistics (BLS) collects data about daily time-use of Americans. Students will explore an interactive graphic titled How Men and Women Spend Their Days created by Nathan Yau, that uses data from the American Time Use Survey, to spark their curiosity about how they spend their own time. This graphic can be found at: https://flowingdata.com/2021/09/21/how-men-and-women-spend-their-days/ Learning Objectives Statistical/Mathematical: S-ID 5: Summarize categorical data for two categories in two-way frequency tables. Interpret relative frequencies in the context of data (including joint, marginal, and conditional relative frequencies). Recognize possible associations and trends in the data. S-IC 6: Evaluate reports based on data. Data Science: Understand that data are collected and stored in particular formats. Before data can be analyzed, it must be cleaned so it can be read. Applied Computational Thinking Using RStudio: \u2022 Create tabular displays of categorical data and summaries of numerical data. \u2022 Create two-way frequency (and relative frequency) tables. \u2022 Use RStudio to calculate joint, marginal, and conditional relative frequencies. \u2022 Subset data frames and create new categorical variables from numerical variables. \u2022 Clean and polish data to make it readable. Real-World Connections: Make claims that are based on data and begin to evaluate reports that make claims based on data. Language Objectives Students will use complex sentences to construct summary statements about their understanding of data, how it is collected, how it is used, and how to work with it. Students will engage in partner and whole group discussions and presentations to express their understanding of data science concepts. Students will use complex sentences to write informative short reports that use data science concepts and skills. Students will read informative texts to evaluate claims based on data. Data File or Data Collection Method Data Collection Method: Time-Use Participatory Campaign: Students will monitor the amount of time they devote to activities such as sleeping, studying, eating, and partaking in media. Data Files: Students\u2019 Time-Use campaign data American Time-Use Survey (ATUS) data Legend for Activity Icons","title":"Would You Look at the Time!"},{"location":"unit1/section3/#unit-1-section-3-would-you-look-at-the-time","text":"Instructional Days: 9","title":"Unit 1, Section 3: Would You Look at the Time!"},{"location":"unit1/section3/#enduring-understandings","text":"Data are useful for evaluating claims and reports. Summaries of categorical and numerical data show important features and patterns in the data. Data summaries provide evidence to make claims.","title":"Enduring Understandings"},{"location":"unit1/section3/#engagement","text":"The Bureau of Labor Statistics (BLS) collects data about daily time-use of Americans. Students will explore an interactive graphic titled How Men and Women Spend Their Days created by Nathan Yau, that uses data from the American Time Use Survey, to spark their curiosity about how they spend their own time. This graphic can be found at: https://flowingdata.com/2021/09/21/how-men-and-women-spend-their-days/","title":"Engagement"},{"location":"unit1/section3/#learning-objectives","text":"Statistical/Mathematical: S-ID 5: Summarize categorical data for two categories in two-way frequency tables. Interpret relative frequencies in the context of data (including joint, marginal, and conditional relative frequencies). Recognize possible associations and trends in the data. S-IC 6: Evaluate reports based on data. Data Science: Understand that data are collected and stored in particular formats. Before data can be analyzed, it must be cleaned so it can be read. Applied Computational Thinking Using RStudio: \u2022 Create tabular displays of categorical data and summaries of numerical data. \u2022 Create two-way frequency (and relative frequency) tables. \u2022 Use RStudio to calculate joint, marginal, and conditional relative frequencies. \u2022 Subset data frames and create new categorical variables from numerical variables. \u2022 Clean and polish data to make it readable. Real-World Connections: Make claims that are based on data and begin to evaluate reports that make claims based on data.","title":"Learning Objectives"},{"location":"unit1/section3/#language-objectives","text":"Students will use complex sentences to construct summary statements about their understanding of data, how it is collected, how it is used, and how to work with it. Students will engage in partner and whole group discussions and presentations to express their understanding of data science concepts. Students will use complex sentences to write informative short reports that use data science concepts and skills. Students will read informative texts to evaluate claims based on data.","title":"Language Objectives"},{"location":"unit1/section3/#data-file-or-data-collection-method","text":"Data Collection Method: Time-Use Participatory Campaign: Students will monitor the amount of time they devote to activities such as sleeping, studying, eating, and partaking in media. Data Files: Students\u2019 Time-Use campaign data American Time-Use Survey (ATUS) data","title":"Data File or Data Collection Method"},{"location":"unit1/section3/#legend-for-activity-icons","text":"","title":"Legend for Activity Icons"},{"location":"unit2/campaign1/","text":"Campaign Guidelines \u2013 Stress/Chill 1. The Issue: People report being more and more stressed every day. This trend is extending beyond adults, it is also reported by children and teenagers. The amount of work for which people are responsible has been increasing. To understand what makes us feel stressed, some important questions to ask are: a) What factors affect my stress/chill level? b) Do different personality types have different things that make them happy/sad? c) Do you like to be alone or with people? d) Is your stress/chill level a function of the environment in which you are in? 2. Objectives: Upon completing this campaign, students will have compared groups and gained an understanding of variability within and between groups. They will have learned how to conduct and use permutations to model variability, perform informal inference, and how to do simulations to make predictions. 3. Survey Questions: Use a random number generator to generate two random times a day for the next 6 days, including a weekend if possible. If a time falls within the school day, it is up to the discretion of the teacher to use this time or not. Prompt Variable Data Type Where are you? -school -work -home -public place -others\u2019 house -commuting where categorical Why are you here (in one word)? why text How many people are you with (not counting yourself)? howmanypeople integer Who are you with? -alone -friends and/or family -teachers and classmates -strangers -coworkers who categorical How stressed are you feeling right now (3 is very stressed, 0 is not stressed at all)? stress integer AUTOMATIC location lat, long AUTOMATIC time time AUTOMATIC date date When? Surveys are taken two to three times per day at pre-determined randomly selected times. How Long? About two weeks. Ideally, two of these days include a weekend. 4. Motivation: Students must understand that they need to keep collecting data. Use the Plot App to look at the data after the first day and have a discussion. Ask: Why were most people stressed? Guide students along the way. Ask students to predict the following: What is your stress/chill level in the evening versus morning? Does it change everyday? How about during the weekend? What is the difference between groups? Data collection: After the first day, use the Campaign Monitoring tool to see who has collected the most data. 5. Technical Analysis: Students will use RStudio. 6. Guiding Questions: a) Have students generate predictions and check up on their predictions. b) What\u2019s the typical stress/chill level of the class across the campaign? c) What\u2019s my typical stress/chill level and how does it compare to the whole class? d) Do the stress/chill levels vary by weekday or weekend or the type of people you are with? e) Under which conditions is my stress/chill level affected? f) Encourage students to generate their own questions. 7. Report: Students will complete the Stress/Chill Practicum. They will analyze their stress/chill data using data analysis skills and RStudio skills learned in the unit.","title":"Campaign Guidelines \u2013 Stress/Chill"},{"location":"unit2/campaign1/#campaign-guidelines-stresschill","text":"","title":"Campaign Guidelines \u2013 Stress/Chill"},{"location":"unit2/campaign1/#1-the-issue","text":"People report being more and more stressed every day. This trend is extending beyond adults, it is also reported by children and teenagers. The amount of work for which people are responsible has been increasing. To understand what makes us feel stressed, some important questions to ask are: a) What factors affect my stress/chill level? b) Do different personality types have different things that make them happy/sad? c) Do you like to be alone or with people? d) Is your stress/chill level a function of the environment in which you are in?","title":"1. The Issue:"},{"location":"unit2/campaign1/#2-objectives","text":"Upon completing this campaign, students will have compared groups and gained an understanding of variability within and between groups. They will have learned how to conduct and use permutations to model variability, perform informal inference, and how to do simulations to make predictions.","title":"2. Objectives:"},{"location":"unit2/campaign1/#3-survey-questions","text":"Use a random number generator to generate two random times a day for the next 6 days, including a weekend if possible. If a time falls within the school day, it is up to the discretion of the teacher to use this time or not. Prompt Variable Data Type Where are you? -school -work -home -public place -others\u2019 house -commuting where categorical Why are you here (in one word)? why text How many people are you with (not counting yourself)? howmanypeople integer Who are you with? -alone -friends and/or family -teachers and classmates -strangers -coworkers who categorical How stressed are you feeling right now (3 is very stressed, 0 is not stressed at all)? stress integer AUTOMATIC location lat, long AUTOMATIC time time AUTOMATIC date date When? Surveys are taken two to three times per day at pre-determined randomly selected times. How Long? About two weeks. Ideally, two of these days include a weekend.","title":"3. Survey Questions:"},{"location":"unit2/campaign1/#4-motivation","text":"Students must understand that they need to keep collecting data. Use the Plot App to look at the data after the first day and have a discussion. Ask: Why were most people stressed? Guide students along the way. Ask students to predict the following: What is your stress/chill level in the evening versus morning? Does it change everyday? How about during the weekend? What is the difference between groups? Data collection: After the first day, use the Campaign Monitoring tool to see who has collected the most data.","title":"4. Motivation:"},{"location":"unit2/campaign1/#5-technical-analysis","text":"Students will use RStudio.","title":"5. Technical Analysis:"},{"location":"unit2/campaign1/#6-guiding-questions","text":"a) Have students generate predictions and check up on their predictions. b) What\u2019s the typical stress/chill level of the class across the campaign? c) What\u2019s my typical stress/chill level and how does it compare to the whole class? d) Do the stress/chill levels vary by weekday or weekend or the type of people you are with? e) Under which conditions is my stress/chill level affected? f) Encourage students to generate their own questions.","title":"6. Guiding Questions:"},{"location":"unit2/campaign1/#7-report","text":"Students will complete the Stress/Chill Practicum. They will analyze their stress/chill data using data analysis skills and RStudio skills learned in the unit.","title":"7. Report:"},{"location":"unit2/end/","text":"End of Unit Design Project and Oral Presentation: Asking and Answering Statistical Questions of Our Own Data Objective: Students will apply their learning of the first and second units of the curriculum by completing an end of unit design project. Materials: IDS Unit 2 \u2013 Design Project and Oral Presentation ( LMR_U2_Design Project ) End of Unit 2 Design Project and Oral Presentation: Asking and Answering Statistical Questions of Our Own Data Available data sets: Food Habits Time Use Stress/Chill Personality Color Your mission is to ask and answer a statistical question using at least one data set above. Your question must include a comparison of two distinct groups. Your analysis should address whether any observed differences are real or could be simply due to chance. You should use at least two of the following methods to answer your question with appropriate explanations: \u274f Merge data \u274f Create simulations \u274f Calculate probabilities based on simulations \u274f Use a Normal model \u274f Shuffle/permute data You will have 5 days to complete this project with your assigned partner. You need to: \u274f Prepare an oral presentation (both partners need to participate) that includes: o A 4-slide, 5-minute presentation o An explanation of why you think your statistical question is interesting. o An interpretation of supporting plots and summaries that answer your question. o A reasoning of whether you think the outcome might be due to chance. \u274f Submit a 2 -4 page typed, double-spaced summary of your analysis. Project Assignment Sequence: \u274f Day 1: Decide on a statistical question with assigned partner; get approval from teacher \u274f Day 2: Working day for analysis \u2013 create plots and numerical summaries \u274f Day 3: Working day for analysis \u2013 create presentation (4 slides maximum) \u274f Day 4: Presentations \u274f Day 5: Presentations","title":"End of Unit Project: Asking and Answering Statistical Questions of Our Own Data"},{"location":"unit2/end/#end-of-unit-design-project-and-oral-presentation-asking-and-answering-statistical-questions-of-our-own-data","text":"","title":"End of Unit Design Project and Oral Presentation: Asking and Answering Statistical Questions of Our Own Data"},{"location":"unit2/end/#objective","text":"Students will apply their learning of the first and second units of the curriculum by completing an end of unit design project.","title":"Objective:"},{"location":"unit2/end/#materials","text":"IDS Unit 2 \u2013 Design Project and Oral Presentation ( LMR_U2_Design Project ) End of Unit 2 Design Project and Oral Presentation: Asking and Answering Statistical Questions of Our Own Data Available data sets: Food Habits Time Use Stress/Chill Personality Color Your mission is to ask and answer a statistical question using at least one data set above. Your question must include a comparison of two distinct groups. Your analysis should address whether any observed differences are real or could be simply due to chance. You should use at least two of the following methods to answer your question with appropriate explanations: \u274f Merge data \u274f Create simulations \u274f Calculate probabilities based on simulations \u274f Use a Normal model \u274f Shuffle/permute data You will have 5 days to complete this project with your assigned partner. You need to: \u274f Prepare an oral presentation (both partners need to participate) that includes: o A 4-slide, 5-minute presentation o An explanation of why you think your statistical question is interesting. o An interpretation of supporting plots and summaries that answer your question. o A reasoning of whether you think the outcome might be due to chance. \u274f Submit a 2 -4 page typed, double-spaced summary of your analysis. Project Assignment Sequence: \u274f Day 1: Decide on a statistical question with assigned partner; get approval from teacher \u274f Day 2: Working day for analysis \u2013 create plots and numerical summaries \u274f Day 3: Working day for analysis \u2013 create presentation (4 slides maximum) \u274f Day 4: Presentations \u274f Day 5: Presentations","title":"Materials:"},{"location":"unit2/essential/","text":"IDS Unit 2: Essential Concepts Lesson 1: What Is Your True Color? Students will understand that the 'typical' value is a value that can represent the entire group, even though we know that not all members of the group share the same value. Lesson 2: What Does Mean Mean? The center of a distribution is the 'typical' value. One way of measuring the center is with the mean, which finds the balancing point of the distribution. The mean gives us the typical value, but does not tell the whole story. We need a way to measure the variability to understand how observations might differ from the typical value. Lesson 3: Median In the Middle Another measure of center is the median, which can also be used to represent the typical value of a distribution. The median is preferred for skewed distributions or when there are outliers, because it better matches what we think of as 'typical.' Lesson 4: How Far Is It from Typical? MAD measures the variability in a sample of data - the larger the value, the greater the variability. More precisely, the MAD is the typical distance of observations from the mean. There are other measures of spread as well, notably the standard deviation and the interquartile range (IQR). Lesson 5: Human Boxplots A common statistical question is \u201cHow does this group compare to that group?\u201d This is a hard question to answer when the groups have lots of variability. One approach is to compare the centers, spreads, and shapes of the distributions. Boxplots are a useful way of comparing distributions from different groups when all of the distributions are unimodal (one hump). Lesson 6: Face Off Writing (and saying) precise comparisons between groups in which variability is present based on the (a) center, (b) spread, (c) shape, and (d) unusual outcomes help to make statements in context of the data. Actual comparison statements should use terms such as \"less than,\" \"about the same as,\" etc. Lesson 7: Plot Match Boxplots are an alternative visualization of histograms or dot plots. They capture most, but not all, of the features we can see in a dot plot or histogram. Lesson 8: How Likely Is It? Probability is an area about which we humans have poor intuition. Probability measures a long-run proportion: 50% chance means the event happens 50% of the time if you repeated it forever. When we don't repeat forever, we see variability. Lesson 9: Bias Detective In the short-term, actual outcomes of chance experiments vary from what is 'ideal.' An ideal die has equally likely outcomes. But that does not mean we will see exactly the same number of one dots, two dots, etc. Lesson 10: Marbles, Marbles\u2026 There are two ways of sampling data that model real-life sampling situations: with and without replacement. Larger samples tend to be closer to the \"true\" probability. Lesson 11: This AND/OR That What does \"A or B\" mean versus \"A and B\" mean? These are compound events and two-way tables can be used to calculate probabilities for them. Lesson 12: Don\u2019t Take My Stress Away! Generating statistical questions is the first step in a Participatory Sensing campaign. Research and observations help create applicable campaign questions. Lesson 13: The Horror Movie Shuffle We can \"shuffle\" data based on categorical variables. The statistic we use is the difference in proportions. The distribution we form by shuffling represents what happens if chance were the only factor at play. If the actual observed difference in proportions is near the center of this shuffling distribution, then we would conclude that chance is a good explanation for the difference. But if it is extreme (in the tails or off the charts), then we should conclude that chance is NOT to blame. Sometimes, the apparent difference between groups is caused by chance. Lesson 14: The Titanic Shuffle We can also \"shuffle\" data based on numerical variables. The statistic we use is the difference in means. The distribution we form by this form of shuffling still represents what happens if chance were the only factor at play. When differences are small, we suspect that they might be due to chance. When differences are big, we suspect they might be 'real.' Lesson 15: Tangible Data Merging We can enhance the context of a statistical problem by merging related data sets together. To merge data, each data set must have a \"unique identifier\" that tells us how to match up the lines of the data. Lesson 16: What Is Normal? The Normal curve, also called the Gaussian distribution and the \"bell curve,\" is a model that describes many real-life distributions and is usually called the Normal Model. Lesson 17: A Normal Measure of Spread The standard deviation is another measure of spread. This is commonly used by statisticians because of its role in common models and distributions, such as the Normal Model. Lesson 18: Shuffling with Normal Z-scores allow us a way to measure how extreme a value is, regardless of the units of measurement. Usually, z-scores will range between -3 and +3, and so values that are at or more extreme than -3 or +3 standard deviations are considered large.","title":"Essential Concepts"},{"location":"unit2/essential/#ids-unit-2-essential-concepts","text":"","title":"IDS Unit 2: Essential Concepts"},{"location":"unit2/essential/#lesson-1-what-is-your-true-color","text":"Students will understand that the 'typical' value is a value that can represent the entire group, even though we know that not all members of the group share the same value.","title":"Lesson 1: What Is Your True Color?"},{"location":"unit2/essential/#lesson-2-what-does-mean-mean","text":"The center of a distribution is the 'typical' value. One way of measuring the center is with the mean, which finds the balancing point of the distribution. The mean gives us the typical value, but does not tell the whole story. We need a way to measure the variability to understand how observations might differ from the typical value.","title":"Lesson 2: What Does Mean Mean?"},{"location":"unit2/essential/#lesson-3-median-in-the-middle","text":"Another measure of center is the median, which can also be used to represent the typical value of a distribution. The median is preferred for skewed distributions or when there are outliers, because it better matches what we think of as 'typical.'","title":"Lesson 3: Median In the Middle"},{"location":"unit2/essential/#lesson-4-how-far-is-it-from-typical","text":"MAD measures the variability in a sample of data - the larger the value, the greater the variability. More precisely, the MAD is the typical distance of observations from the mean. There are other measures of spread as well, notably the standard deviation and the interquartile range (IQR).","title":"Lesson 4: How Far Is It from Typical?"},{"location":"unit2/essential/#lesson-5-human-boxplots","text":"A common statistical question is \u201cHow does this group compare to that group?\u201d This is a hard question to answer when the groups have lots of variability. One approach is to compare the centers, spreads, and shapes of the distributions. Boxplots are a useful way of comparing distributions from different groups when all of the distributions are unimodal (one hump).","title":"Lesson 5: Human Boxplots"},{"location":"unit2/essential/#lesson-6-face-off","text":"Writing (and saying) precise comparisons between groups in which variability is present based on the (a) center, (b) spread, (c) shape, and (d) unusual outcomes help to make statements in context of the data. Actual comparison statements should use terms such as \"less than,\" \"about the same as,\" etc.","title":"Lesson 6: Face Off"},{"location":"unit2/essential/#lesson-7-plot-match","text":"Boxplots are an alternative visualization of histograms or dot plots. They capture most, but not all, of the features we can see in a dot plot or histogram.","title":"Lesson 7: Plot Match"},{"location":"unit2/essential/#lesson-8-how-likely-is-it","text":"Probability is an area about which we humans have poor intuition. Probability measures a long-run proportion: 50% chance means the event happens 50% of the time if you repeated it forever. When we don't repeat forever, we see variability.","title":"Lesson 8: How Likely Is It?"},{"location":"unit2/essential/#lesson-9-bias-detective","text":"In the short-term, actual outcomes of chance experiments vary from what is 'ideal.' An ideal die has equally likely outcomes. But that does not mean we will see exactly the same number of one dots, two dots, etc.","title":"Lesson 9: Bias Detective"},{"location":"unit2/essential/#lesson-10-marbles-marbles","text":"There are two ways of sampling data that model real-life sampling situations: with and without replacement. Larger samples tend to be closer to the \"true\" probability.","title":"Lesson 10: Marbles, Marbles\u2026"},{"location":"unit2/essential/#lesson-11-this-andor-that","text":"What does \"A or B\" mean versus \"A and B\" mean? These are compound events and two-way tables can be used to calculate probabilities for them.","title":"Lesson 11: This AND/OR That"},{"location":"unit2/essential/#lesson-12-dont-take-my-stress-away","text":"Generating statistical questions is the first step in a Participatory Sensing campaign. Research and observations help create applicable campaign questions.","title":"Lesson 12: Don\u2019t Take My Stress Away!"},{"location":"unit2/essential/#lesson-13-the-horror-movie-shuffle","text":"We can \"shuffle\" data based on categorical variables. The statistic we use is the difference in proportions. The distribution we form by shuffling represents what happens if chance were the only factor at play. If the actual observed difference in proportions is near the center of this shuffling distribution, then we would conclude that chance is a good explanation for the difference. But if it is extreme (in the tails or off the charts), then we should conclude that chance is NOT to blame. Sometimes, the apparent difference between groups is caused by chance.","title":"Lesson 13: The Horror Movie Shuffle"},{"location":"unit2/essential/#lesson-14-the-titanic-shuffle","text":"We can also \"shuffle\" data based on numerical variables. The statistic we use is the difference in means. The distribution we form by this form of shuffling still represents what happens if chance were the only factor at play. When differences are small, we suspect that they might be due to chance. When differences are big, we suspect they might be 'real.'","title":"Lesson 14: The Titanic Shuffle"},{"location":"unit2/essential/#lesson-15-tangible-data-merging","text":"We can enhance the context of a statistical problem by merging related data sets together. To merge data, each data set must have a \"unique identifier\" that tells us how to match up the lines of the data.","title":"Lesson 15: Tangible Data Merging"},{"location":"unit2/essential/#lesson-16-what-is-normal","text":"The Normal curve, also called the Gaussian distribution and the \"bell curve,\" is a model that describes many real-life distributions and is usually called the Normal Model.","title":"Lesson 16: What Is Normal?"},{"location":"unit2/essential/#lesson-17-a-normal-measure-of-spread","text":"The standard deviation is another measure of spread. This is commonly used by statisticians because of its role in common models and distributions, such as the Normal Model.","title":"Lesson 17: A Normal Measure of Spread"},{"location":"unit2/essential/#lesson-18-shuffling-with-normal","text":"Z-scores allow us a way to measure how extreme a value is, regardless of the units of measurement. Usually, z-scores will range between -3 and +3, and so values that are at or more extreme than -3 or +3 standard deviations are considered large.","title":"Lesson 18: Shuffling with Normal"},{"location":"unit2/lab2a/","text":"Lab 2A - All About Distributions Directions: Follow along with the slides, completing the questions in blue on your computer, and answering the questions in red in your journal. In the beginning... Most of the labs thus far have covered how to visualize, summarize, and manipulate data. \u2013 We used visualizations to explore how your class spends their time. \u2013 We also learned how to clean data to prepare it for analyzing. Starting with this lab, we'll learn to use R to answer statistical questions that can be answered by calculating the mean, median and MAD. How to talk about data When we make plots of our data, we usually want to know: Where is the bulk of the data? Where is the data more sparse , or thin ? What values are typical ? How much does the data vary ? To answer these questions, we want to look at the distribution of our data. \u2013 We describe distributions by talking about where the center of the data are, how spread out the data are, and what sort of shape the data has. Let's begin! Export , upload and import your class' Personality Color data. \u2013 Name your data colors when you load it. Before analyzing a new data set, it's often helpful to get familiar with it. So: \u2013 Write down the names of the 4 variables that contain the point-totals, or scores , for each personality color. \u2013 Write down the names of the variables that tell us an observation's introvert/extrovert designation and whether they are involved in sports . \u2013 How many variables are in the data set? \u2013 How many observations are in the data set? Estimating centers Create a dotPlot of the scores for your predominant color . \u2013 Pro-tip: If the dotPlot comes out looking wonky, include the nint and cex options. Based on your dotPlot : \u2013 Which values came up the most frequently? About how many people in your class had a score similar to yours? \u2013 What, would you say, was a typical score for a person in your class for your predominant color? How does your own score for this color compare? Means and medians Means and medians are usually good ways to describe the typical value of our data. Fill in the blank to calculate the mean value of your predominant color score: mean(~____, data = colors) Use a similar line of code to calculate the median value of your predominant color. \u2013 Are the mean and median roughly the same? If not, use the dotPlot you made in the last slide to describe why. Estimating Spread Now that we know how to describe our data's typical value we might also like to describe how closely the rest of the data are to this typical value. \u2013 We often refer to this as the variability of the data. \u2013 Variability is seen in a histogram or dotPlot as the horizontal spread . Re-create a dotPlot of the scores for your predominant color and then run the code below filling in the blank with the name of your predominant color: add_line(vline = mean(~____, data = colors)) Look at the spread of the scores from the mean score then complete the sentence below: Data points in my plot will usually fall within units of the center. Mean Absolute Deviation The mean absolute deviation finds how far away, on average, the data are from the mean. \u2013 We often write mean absolute deviation as MAD. Calculate the MAD of your predominant color by filling in the blanks: MAD(~_____, data = colors) How close was your estimate of the spread for your predominant color (from the previous slide) to the actual value? Comparing introverts/extroverts Do introverts and extroverts differ in their typical scores for your predominant color? \u2013 Answer this investigative question using a dotPlot and numerical summaries. Make a dotPlot of your predominant color again; but this time, facet the plot by the introvert/extrovert variable. Include the layout option to stack the plots as well as the nint and cex options. Describe the shape of the distribution of scores for the extroverts. Do the same for the introverts. Using similar syntax to how you facet plots, calculate either the mean or median to describe the center of your predominant color for introverts and extroverts. Do introverts and extroverts differ in their typical scores for your predominant color? Based on the MAD, which group (introverts or extroverts) has more variability for your predominant color\u2019s scores? On your own Do introverts and extroverts in your class differ in their color scores? \u2013 Perform an analysis that produces numerical summaries and graphs . \u2013 Then, write a few sentence interpretations that addresses this statistical question and considers the shape , center and spread of the distributions of the graphs you create.","title":"Lab 2A - All About Distributions"},{"location":"unit2/lab2a/#lab-2a-all-about-distributions","text":"Directions: Follow along with the slides, completing the questions in blue on your computer, and answering the questions in red in your journal.","title":"Lab 2A - All About Distributions"},{"location":"unit2/lab2a/#in-the-beginning","text":"Most of the labs thus far have covered how to visualize, summarize, and manipulate data. \u2013 We used visualizations to explore how your class spends their time. \u2013 We also learned how to clean data to prepare it for analyzing. Starting with this lab, we'll learn to use R to answer statistical questions that can be answered by calculating the mean, median and MAD.","title":"In the beginning..."},{"location":"unit2/lab2a/#how-to-talk-about-data","text":"When we make plots of our data, we usually want to know: Where is the bulk of the data? Where is the data more sparse , or thin ? What values are typical ? How much does the data vary ? To answer these questions, we want to look at the distribution of our data. \u2013 We describe distributions by talking about where the center of the data are, how spread out the data are, and what sort of shape the data has.","title":"How to talk about data"},{"location":"unit2/lab2a/#lets-begin","text":"Export , upload and import your class' Personality Color data. \u2013 Name your data colors when you load it. Before analyzing a new data set, it's often helpful to get familiar with it. So: \u2013 Write down the names of the 4 variables that contain the point-totals, or scores , for each personality color. \u2013 Write down the names of the variables that tell us an observation's introvert/extrovert designation and whether they are involved in sports . \u2013 How many variables are in the data set? \u2013 How many observations are in the data set?","title":"Let's begin!"},{"location":"unit2/lab2a/#estimating-centers","text":"Create a dotPlot of the scores for your predominant color . \u2013 Pro-tip: If the dotPlot comes out looking wonky, include the nint and cex options. Based on your dotPlot : \u2013 Which values came up the most frequently? About how many people in your class had a score similar to yours? \u2013 What, would you say, was a typical score for a person in your class for your predominant color? How does your own score for this color compare?","title":"Estimating centers"},{"location":"unit2/lab2a/#means-and-medians","text":"Means and medians are usually good ways to describe the typical value of our data. Fill in the blank to calculate the mean value of your predominant color score: mean(~____, data = colors) Use a similar line of code to calculate the median value of your predominant color. \u2013 Are the mean and median roughly the same? If not, use the dotPlot you made in the last slide to describe why.","title":"Means and medians"},{"location":"unit2/lab2a/#estimating-spread","text":"Now that we know how to describe our data's typical value we might also like to describe how closely the rest of the data are to this typical value. \u2013 We often refer to this as the variability of the data. \u2013 Variability is seen in a histogram or dotPlot as the horizontal spread . Re-create a dotPlot of the scores for your predominant color and then run the code below filling in the blank with the name of your predominant color: add_line(vline = mean(~____, data = colors)) Look at the spread of the scores from the mean score then complete the sentence below: Data points in my plot will usually fall within units of the center.","title":"Estimating Spread"},{"location":"unit2/lab2a/#mean-absolute-deviation","text":"The mean absolute deviation finds how far away, on average, the data are from the mean. \u2013 We often write mean absolute deviation as MAD. Calculate the MAD of your predominant color by filling in the blanks: MAD(~_____, data = colors) How close was your estimate of the spread for your predominant color (from the previous slide) to the actual value?","title":"Mean Absolute Deviation"},{"location":"unit2/lab2a/#comparing-introvertsextroverts","text":"Do introverts and extroverts differ in their typical scores for your predominant color? \u2013 Answer this investigative question using a dotPlot and numerical summaries. Make a dotPlot of your predominant color again; but this time, facet the plot by the introvert/extrovert variable. Include the layout option to stack the plots as well as the nint and cex options. Describe the shape of the distribution of scores for the extroverts. Do the same for the introverts. Using similar syntax to how you facet plots, calculate either the mean or median to describe the center of your predominant color for introverts and extroverts. Do introverts and extroverts differ in their typical scores for your predominant color? Based on the MAD, which group (introverts or extroverts) has more variability for your predominant color\u2019s scores?","title":"Comparing introverts/extroverts"},{"location":"unit2/lab2a/#on-your-own","text":"Do introverts and extroverts in your class differ in their color scores? \u2013 Perform an analysis that produces numerical summaries and graphs . \u2013 Then, write a few sentence interpretations that addresses this statistical question and considers the shape , center and spread of the distributions of the graphs you create.","title":"On your own"},{"location":"unit2/lab2b/","text":"Lab 2B - Oh the Summaries... Directions: Follow along with the slides, completing the questions in blue on your computer, and answering the questions in red in your journal. Just the beginning Means, medians,and MAD are just a few examples of numerical summaries . In this lab, we will learn how to calculate and interpret additional summaries of distributions such as: minimums, maximums, ranges, quartiles and IQRs. \u2013 We'll also learn how to write our first custom function! Start by loading your Personality Color data again and name it colors . Extreme values Besides looking at typical values, sometimes we want to see extreme values, like the smallest and largest values. \u2013 To find these values, we can use the min , max or range functions. These functions use a similar syntax as the mean function. Find and write down the min value and max value for your predominant color. Apply the range function to your predominant color and describe the output. \u2013 The range of a variable is the difference between a variable\u2019s smallest and largest value. \u2013 Notice, however, that our range function calculates the maximum and minimum values for a variable, but not the difference between them. \u2013 Later in this lab you will create a custom Range function that will calculate the difference. Quartiles (Q1 & Q3) The median of our data is the value that splits our data in half. \u2013 Half of our data is smaller than the median , half is larger. Q1 and Q3 are similar. \u2013 25% of our data are smaller than Q1 , 75% are larger. - 75% of our data are smaller than Q3 , 25% are larger. Fill in the blanks to compute the value of Q1 for your predominant color. quantile(~____, data = ____, p = 0.25) Use a similar line of code to calculate Q3 , which is the value that's larger than 75% of our data. The Inter-Quartile-Range (IQR) Make a dotPlot of your predominant color's scores. Make sure to include the nint option. Visually (Don't worry about being super-precise): \u2013 Cut the distribution into quarters so the number of data points is equal for each piece. (Each piece should contain 25% of the data.) Hint: You might consider using the add_line(vline = ) to add vertical lines at the quarter marks. \u2013 Write down the numbers that split the data up into these 4 pieces. \u2013 How long is the interval of the middle two pieces? \u2013 This length is the IQR . Calculating the IQR The IQR is another way to describe spread . \u2013 It describes how wide or narrow the middle 50% of our data are. Just like we used the min and max to compute the range , we can also use the 1st and 3rd quartiles to compute the IQR . Use the values of Q1 and Q3 you calculated previously and find the IQR by hand. \u2013 Then use the iqr() function to calculate it for you. Which personality color score has the widest spread according to the IQR ? Which is narrowest? Boxplots By using the medians, quartiles, and min/max, we can construct a new single variable plot called the box and whisker plot, often shortened to just boxplot . By showing someone a dotPlot , how would you teach them to make a boxplot ? Write out your explanation in a series of steps for the person to use. \u2013 Use the steps you write to create a sketch of a boxplot for your predominant color's scores in your journal. \u2013 Then use the bwplot function to create a boxplot using R . Our favorite summaries In the past two labs, we've learned how to calculate numerous numerical summaries . \u2013 Computing lots of different summaries can be tedious. Fill in the blanks below to compute some of our favorite summaries for your predominant color all at once. favstats(~____, data=colors) Calculating a range value We saw in the previous slide that the range function calculates the maximum and minimum values for a variable, but not the difference between them. We could calculate this difference in two steps: \u2013 Step 1: Use the range function to assign the max and min values of a variable the name values . This will store the output from the range function in the environment pane. values <- range(~____, data=colors) \u2013 Step 2: Use the diff function to calculate the difference of values . The input for the diff function needs to be a vector containig two numeric values. diff(values) Use these two steps to calculate the range of your predominant color. Introducing custom functions Calculating the range of many variables can be tedious if we have to keep performing the same two steps over and over. \u2013 We can combine these two steps into one by writing our own custom function . Custom functions can be used to combine a task that would normally take many steps to compute and simplify them into one. The next slide shows an example of how we can create a custom function called mm_diff to calculate the absolute difference between the mean and median value of a variable in our data . Example function mm_diff <- function(variable, data) { mean_val <- mean(variable, data = data) med_val <- median(variable, data = data) abs(mean_val - med_val) } The function takes two generic arguments: variable and data It then follows the steps between the curly braces { } \u2013 Each of the generic arguments is used inside the mean and median functions. Copy and paste the code above into an R script and run it. The mm_diff function will appear in your Environment pane. Using mm_diff() After running the code used to create the function, we can use it just like we would any other numerical summary. \u2013 In the console , fill in the blanks below to calculate the absolute difference between the mean and median values of your predominant color: ____(~____, data = ____) Which of the four colors has the largest absolute difference between the mean and median values? \u2013 By examining a dotPlot for this personality color, make an argument why either the mean or median would be the better description of the center of the data. Our first function Using the previous example as a guide, create a function called Range ( Note the capial 'R' ) that calculates the range of a variable by filling in the blanks below: ____ <- function (____, ____) { values <- range(____, data = ____) diff(___) } Use the Range function to find the personality color with the largest difference between the max and min values. On your own Create a function called myIQR that uses the quantile function to compute the middle 30% of the data.","title":"Lab 2B - Oh the Summaries ..."},{"location":"unit2/lab2b/#lab-2b-oh-the-summaries","text":"Directions: Follow along with the slides, completing the questions in blue on your computer, and answering the questions in red in your journal.","title":"Lab 2B - Oh the Summaries..."},{"location":"unit2/lab2b/#just-the-beginning","text":"Means, medians,and MAD are just a few examples of numerical summaries . In this lab, we will learn how to calculate and interpret additional summaries of distributions such as: minimums, maximums, ranges, quartiles and IQRs. \u2013 We'll also learn how to write our first custom function! Start by loading your Personality Color data again and name it colors .","title":"Just the beginning"},{"location":"unit2/lab2b/#extreme-values","text":"Besides looking at typical values, sometimes we want to see extreme values, like the smallest and largest values. \u2013 To find these values, we can use the min , max or range functions. These functions use a similar syntax as the mean function. Find and write down the min value and max value for your predominant color. Apply the range function to your predominant color and describe the output. \u2013 The range of a variable is the difference between a variable\u2019s smallest and largest value. \u2013 Notice, however, that our range function calculates the maximum and minimum values for a variable, but not the difference between them. \u2013 Later in this lab you will create a custom Range function that will calculate the difference.","title":"Extreme values"},{"location":"unit2/lab2b/#quartiles-q1-q3","text":"The median of our data is the value that splits our data in half. \u2013 Half of our data is smaller than the median , half is larger. Q1 and Q3 are similar. \u2013 25% of our data are smaller than Q1 , 75% are larger. - 75% of our data are smaller than Q3 , 25% are larger. Fill in the blanks to compute the value of Q1 for your predominant color. quantile(~____, data = ____, p = 0.25) Use a similar line of code to calculate Q3 , which is the value that's larger than 75% of our data.","title":"Quartiles (Q1 &amp; Q3)"},{"location":"unit2/lab2b/#the-inter-quartile-range-iqr","text":"Make a dotPlot of your predominant color's scores. Make sure to include the nint option. Visually (Don't worry about being super-precise): \u2013 Cut the distribution into quarters so the number of data points is equal for each piece. (Each piece should contain 25% of the data.) Hint: You might consider using the add_line(vline = ) to add vertical lines at the quarter marks. \u2013 Write down the numbers that split the data up into these 4 pieces. \u2013 How long is the interval of the middle two pieces? \u2013 This length is the IQR .","title":"The Inter-Quartile-Range (IQR)"},{"location":"unit2/lab2b/#calculating-the-iqr","text":"The IQR is another way to describe spread . \u2013 It describes how wide or narrow the middle 50% of our data are. Just like we used the min and max to compute the range , we can also use the 1st and 3rd quartiles to compute the IQR . Use the values of Q1 and Q3 you calculated previously and find the IQR by hand. \u2013 Then use the iqr() function to calculate it for you. Which personality color score has the widest spread according to the IQR ? Which is narrowest?","title":"Calculating the IQR"},{"location":"unit2/lab2b/#boxplots","text":"By using the medians, quartiles, and min/max, we can construct a new single variable plot called the box and whisker plot, often shortened to just boxplot . By showing someone a dotPlot , how would you teach them to make a boxplot ? Write out your explanation in a series of steps for the person to use. \u2013 Use the steps you write to create a sketch of a boxplot for your predominant color's scores in your journal. \u2013 Then use the bwplot function to create a boxplot using R .","title":"Boxplots"},{"location":"unit2/lab2b/#our-favorite-summaries","text":"In the past two labs, we've learned how to calculate numerous numerical summaries . \u2013 Computing lots of different summaries can be tedious. Fill in the blanks below to compute some of our favorite summaries for your predominant color all at once. favstats(~____, data=colors)","title":"Our favorite summaries"},{"location":"unit2/lab2b/#calculating-a-range-value","text":"We saw in the previous slide that the range function calculates the maximum and minimum values for a variable, but not the difference between them. We could calculate this difference in two steps: \u2013 Step 1: Use the range function to assign the max and min values of a variable the name values . This will store the output from the range function in the environment pane. values <- range(~____, data=colors) \u2013 Step 2: Use the diff function to calculate the difference of values . The input for the diff function needs to be a vector containig two numeric values. diff(values) Use these two steps to calculate the range of your predominant color.","title":"Calculating a range value"},{"location":"unit2/lab2b/#introducing-custom-functions","text":"Calculating the range of many variables can be tedious if we have to keep performing the same two steps over and over. \u2013 We can combine these two steps into one by writing our own custom function . Custom functions can be used to combine a task that would normally take many steps to compute and simplify them into one. The next slide shows an example of how we can create a custom function called mm_diff to calculate the absolute difference between the mean and median value of a variable in our data .","title":"Introducing custom functions"},{"location":"unit2/lab2b/#example-function","text":"mm_diff <- function(variable, data) { mean_val <- mean(variable, data = data) med_val <- median(variable, data = data) abs(mean_val - med_val) } The function takes two generic arguments: variable and data It then follows the steps between the curly braces { } \u2013 Each of the generic arguments is used inside the mean and median functions. Copy and paste the code above into an R script and run it. The mm_diff function will appear in your Environment pane.","title":"Example function"},{"location":"unit2/lab2b/#using-mm_diff","text":"After running the code used to create the function, we can use it just like we would any other numerical summary. \u2013 In the console , fill in the blanks below to calculate the absolute difference between the mean and median values of your predominant color: ____(~____, data = ____) Which of the four colors has the largest absolute difference between the mean and median values? \u2013 By examining a dotPlot for this personality color, make an argument why either the mean or median would be the better description of the center of the data.","title":"Using mm_diff()"},{"location":"unit2/lab2b/#our-first-function","text":"Using the previous example as a guide, create a function called Range ( Note the capial 'R' ) that calculates the range of a variable by filling in the blanks below: ____ <- function (____, ____) { values <- range(____, data = ____) diff(___) } Use the Range function to find the personality color with the largest difference between the max and min values.","title":"Our first function"},{"location":"unit2/lab2b/#on-your-own","text":"Create a function called myIQR that uses the quantile function to compute the middle 30% of the data.","title":"On your own"},{"location":"unit2/lab2c/","text":"Lab 2C - Which Song Plays Next? Directions: Follow along with the slides, completing the questions in blue on your computer, and answering the questions in red in your journal. A new direction For the past two labs, we've looked at ways that we can summarize data with numbers. \u2013 Specifically, you learned how to describe the center , shape and spread of variables in our data. In this lab, we're going to estimate the probability that a rap song will be chosen from a playlist with both rap and rock songs, if the choice is made at random. \u2013 The playlist we'll work with has 100 songs: 39 are rap and 61 are rock. Estimate what ... ? To estimate the probability , we're going to imagine that we select a song at random, write down its genre ( rock or rap ), put the song back in the playlist, and repeat 499 more times for a total of 500 times. The statistical question we want to address is: On average, what proportion of our selections will be rap? Why do we put a song back each time we make a selection? What would happen in our little experiment if we did not do this? Calculating probabilities Remember that a probability is the long-run proportion of time an event occurs. \u2013 Many probabilities can be answered exactly with just a little math. \u2013 The probability we draw a single rap song from our playlist of 39 rap and 61 rock songs is 39/100 , 0.39 or 39% . Probabilities can also be answered exactly if we were willing to randomly select a song from the playlist, write down its genre , place the song back in the list, and repeatedly do this forever . \u2013 Literally, forever ... \u2013 But we don't have that much time. So we're only going to do it 500 times which will give us an estimate of the probability . Estimating probabilities You might ask, Why are we estimating the probability if we know the answer is 39%? \u2013 Sometimes, probabilities are too hard to calculate with simple division as we did above. In which case, we can often program a computer to run an experiment to estimate the probability. \u2013 We refer to these programs as simulations . The techniques you learn in this lab could be applied to very simple probability calculations or very hard and complex calculations. \u2013 In both cases, your estimated probability would be very close to the actual probability. Getting ready Simulations are meant to mimic what happens in real-life using randomness and computers. \u2013 Before we can start simulating picking songs from a playlist, we need to simulate that playlist in R . Simulate our 39 rap songs using the repeat rep() function. rap <- rep(\"rap\", times = 39) Look in the Environment pane for the vector containing your rap songs. Use a similar line of code to simulate the rock songs in our playlist of 100. Put the songs in the playlist Now that we've got some different songs, we need to combine them together. \u2013 To do this, we can use the combine function c() in R . Fill in the blanks to combine your different songs: songs <- __(rap, ____) And with that, our playlist of songs should be ready to go. \u2013 Type songs into the console and hit enter to see your individual songs . Pick a song, any song Data scientists call the act of choosing things randomly from a set, sampling . \u2013 We can randomly choose a song from our playlist by using: sample(songs, size = 1, replace = TRUE) Run this code 10 times and compute the proportion of \"rap\" songs you drew from the 10. Vocabulary Check: A proportion is a fraction of the whole. For example, if 2 rap songs were drawn from the 10, the proportion would be 2/10 It is more common to express a proportion as a decimal, in this case, 0.20 It is even more common to express a proportion as a percentage, 20% Once everyone in your class has computed their proportions , calculate the range of proportions (the largest proportion minus the smallest proportion ) for your class and write it down. Now do() it some more Instead of running the same line of code multiple times ourselves we can use R to do() multiple repetitions for us. \u2013 Fill in the blanks below to do the sample code from the previous slide 50 times: do(___) * sample(___, ___ = ___, ___ = ___) Recall that we need to store our results to be able to perform analysis. Assign the 50 selected songs the name draws and then View your file. What is the variable name? R defaulted to naming the variable based on the function used. You may use the data cleaning skills you learned in lab 6 to rename the variable if you wish. Fill in the blank below to tally how often each genre was selected: tally(~___, data = draws) Compute the proportion of \"rap\" songs for your 50 draws and find out if the range for your class' proportions is bigger or smaller than when we drew 10 songs. Proportions vs. Probability To review, so far in this lab we've: \u2013 Simulated a \"playlist\" of songs. \u2013 Repeatedly simulated drawing a song from the playlist, noting its genre and placing it back in the playlist. \u2013 Computed the proportion of the draws that were \"rap\" . These proportions are all estimates of the theoretical probability of choosing a rap song from a playlist. \u2013 As we increase the number of draws, the range of proportions should shrink. When using simulations to estimate probabilities, using a large number of repeats is better because the estimates have less variability and so we can be confident we're closer to the actual value. Non-random Randomness We've seen that random simulations can produce many different outcomes. \u2013 Some estimated probabilities in your class were smaller/larger relative to others. There are instances where you might like the same random events to occur for everyone. \u2013 We can do this by using set.seed() . For example, the output of this code will always be the same: set.seed(123) sample(songs, size = 1, replace = TRUE) ## [1] \"rap\" Playing with seeds With a partner, choose a number to include in set.seed then redo the simulation of 50 songs. \u2013 Both partners should run set.seed(___) just before simulating the 50 draws. \u2013 The blank in set.seed(___) should be the same number for both partners. \u2013 Verify that both partners compute the same proportion of \"rap\" songs. Redo the 50 simulations one last time but have each partner choose a different number for set.seed(___) . \u2013 Are the proportions still the same? If so, can you find two different values for set.seed that give different answers? On your own Suppose there are 1,200 students at your school. 400 of them went to the movies last Friday, 600 went to the park and the rest read at home. If we select a student at random, what is the probability that this student is one of those who went to the movies last Friday? Answer this by estimating the probability that a randomly chosen student went to the movies using 500 simulations. \u2013 Write down both the estimated probability and the code you used to compute your estimate. You might find it helpful to write your answer in an R Script (File -> New File -> R Script) \u2013 Include set.seed(123) in your code before you do 500 repeated samples.","title":"Lab 2C - Which Song Plays Next?"},{"location":"unit2/lab2c/#lab-2c-which-song-plays-next","text":"Directions: Follow along with the slides, completing the questions in blue on your computer, and answering the questions in red in your journal.","title":"Lab 2C - Which Song Plays Next?"},{"location":"unit2/lab2c/#a-new-direction","text":"For the past two labs, we've looked at ways that we can summarize data with numbers. \u2013 Specifically, you learned how to describe the center , shape and spread of variables in our data. In this lab, we're going to estimate the probability that a rap song will be chosen from a playlist with both rap and rock songs, if the choice is made at random. \u2013 The playlist we'll work with has 100 songs: 39 are rap and 61 are rock.","title":"A new direction"},{"location":"unit2/lab2c/#estimate-what","text":"To estimate the probability , we're going to imagine that we select a song at random, write down its genre ( rock or rap ), put the song back in the playlist, and repeat 499 more times for a total of 500 times. The statistical question we want to address is: On average, what proportion of our selections will be rap? Why do we put a song back each time we make a selection? What would happen in our little experiment if we did not do this?","title":"Estimate what ... ?"},{"location":"unit2/lab2c/#calculating-probabilities","text":"Remember that a probability is the long-run proportion of time an event occurs. \u2013 Many probabilities can be answered exactly with just a little math. \u2013 The probability we draw a single rap song from our playlist of 39 rap and 61 rock songs is 39/100 , 0.39 or 39% . Probabilities can also be answered exactly if we were willing to randomly select a song from the playlist, write down its genre , place the song back in the list, and repeatedly do this forever . \u2013 Literally, forever ... \u2013 But we don't have that much time. So we're only going to do it 500 times which will give us an estimate of the probability .","title":"Calculating probabilities"},{"location":"unit2/lab2c/#estimating-probabilities","text":"You might ask, Why are we estimating the probability if we know the answer is 39%? \u2013 Sometimes, probabilities are too hard to calculate with simple division as we did above. In which case, we can often program a computer to run an experiment to estimate the probability. \u2013 We refer to these programs as simulations . The techniques you learn in this lab could be applied to very simple probability calculations or very hard and complex calculations. \u2013 In both cases, your estimated probability would be very close to the actual probability.","title":"Estimating probabilities"},{"location":"unit2/lab2c/#getting-ready","text":"Simulations are meant to mimic what happens in real-life using randomness and computers. \u2013 Before we can start simulating picking songs from a playlist, we need to simulate that playlist in R . Simulate our 39 rap songs using the repeat rep() function. rap <- rep(\"rap\", times = 39) Look in the Environment pane for the vector containing your rap songs. Use a similar line of code to simulate the rock songs in our playlist of 100.","title":"Getting ready"},{"location":"unit2/lab2c/#put-the-songs-in-the-playlist","text":"Now that we've got some different songs, we need to combine them together. \u2013 To do this, we can use the combine function c() in R . Fill in the blanks to combine your different songs: songs <- __(rap, ____) And with that, our playlist of songs should be ready to go. \u2013 Type songs into the console and hit enter to see your individual songs .","title":"Put the songs in the playlist"},{"location":"unit2/lab2c/#pick-a-song-any-song","text":"Data scientists call the act of choosing things randomly from a set, sampling . \u2013 We can randomly choose a song from our playlist by using: sample(songs, size = 1, replace = TRUE) Run this code 10 times and compute the proportion of \"rap\" songs you drew from the 10. Vocabulary Check: A proportion is a fraction of the whole. For example, if 2 rap songs were drawn from the 10, the proportion would be 2/10 It is more common to express a proportion as a decimal, in this case, 0.20 It is even more common to express a proportion as a percentage, 20% Once everyone in your class has computed their proportions , calculate the range of proportions (the largest proportion minus the smallest proportion ) for your class and write it down.","title":"Pick a song, any song"},{"location":"unit2/lab2c/#now-do-it-some-more","text":"Instead of running the same line of code multiple times ourselves we can use R to do() multiple repetitions for us. \u2013 Fill in the blanks below to do the sample code from the previous slide 50 times: do(___) * sample(___, ___ = ___, ___ = ___) Recall that we need to store our results to be able to perform analysis. Assign the 50 selected songs the name draws and then View your file. What is the variable name? R defaulted to naming the variable based on the function used. You may use the data cleaning skills you learned in lab 6 to rename the variable if you wish. Fill in the blank below to tally how often each genre was selected: tally(~___, data = draws) Compute the proportion of \"rap\" songs for your 50 draws and find out if the range for your class' proportions is bigger or smaller than when we drew 10 songs.","title":"Now do() it some more"},{"location":"unit2/lab2c/#proportions-vs-probability","text":"To review, so far in this lab we've: \u2013 Simulated a \"playlist\" of songs. \u2013 Repeatedly simulated drawing a song from the playlist, noting its genre and placing it back in the playlist. \u2013 Computed the proportion of the draws that were \"rap\" . These proportions are all estimates of the theoretical probability of choosing a rap song from a playlist. \u2013 As we increase the number of draws, the range of proportions should shrink. When using simulations to estimate probabilities, using a large number of repeats is better because the estimates have less variability and so we can be confident we're closer to the actual value.","title":"Proportions vs. Probability"},{"location":"unit2/lab2c/#non-random-randomness","text":"We've seen that random simulations can produce many different outcomes. \u2013 Some estimated probabilities in your class were smaller/larger relative to others. There are instances where you might like the same random events to occur for everyone. \u2013 We can do this by using set.seed() . For example, the output of this code will always be the same: set.seed(123) sample(songs, size = 1, replace = TRUE) ## [1] \"rap\"","title":"Non-random Randomness"},{"location":"unit2/lab2c/#playing-with-seeds","text":"With a partner, choose a number to include in set.seed then redo the simulation of 50 songs. \u2013 Both partners should run set.seed(___) just before simulating the 50 draws. \u2013 The blank in set.seed(___) should be the same number for both partners. \u2013 Verify that both partners compute the same proportion of \"rap\" songs. Redo the 50 simulations one last time but have each partner choose a different number for set.seed(___) . \u2013 Are the proportions still the same? If so, can you find two different values for set.seed that give different answers?","title":"Playing with seeds"},{"location":"unit2/lab2c/#on-your-own","text":"Suppose there are 1,200 students at your school. 400 of them went to the movies last Friday, 600 went to the park and the rest read at home. If we select a student at random, what is the probability that this student is one of those who went to the movies last Friday? Answer this by estimating the probability that a randomly chosen student went to the movies using 500 simulations. \u2013 Write down both the estimated probability and the code you used to compute your estimate. You might find it helpful to write your answer in an R Script (File -> New File -> R Script) \u2013 Include set.seed(123) in your code before you do 500 repeated samples.","title":"On your own"},{"location":"unit2/lab2d/","text":"Lab 2D - Queue It Up! Directions: Follow along with the slides, completing the questions in blue on your computer, and answering the questions in red in your journal. Where we left off In the last lab, we looked at how we can use computer simulations to compute estimates of simple probabilities. \u2013 Like the probability of drawing a song genre from a playlist. We also saw that performing more simulations: \u2013 Took longer to finish. \u2013 Had estimates that varied less . In this lab, we'll extend our simulation methods to cover situations that are more complex. \u2013 We'll learn how to estimate their probabilities. \u2013 We also look at the roll of sampling with or without replacement . Back to songs In R , simulate a playlist of songs containing 30 \"rap\" songs, 23 \"country\" songs and 47 \"rock\" songs. \u2013 Assign the combined playlist the name songs . Simulate choosing a single song 50 times. Then use your simulated draws to estimate the probability of choosing a rap song. \u2013 The actual (theoretical) probability of choosing a rap song in this case is 0.30 . \u2013 Write a sentence comparing your estimated probability to the actual probability. With or Without? So far, you've selected songs with replacement . \u2013 We called it that, because each time you made a selection, you started with the same playlist. That is, you chose a song, wrote down its data, and then placed it back on the list. It's also possible to select without replacement by setting the replace option in the sample function to FALSE . Take a sample of size 100 from our playlist of songs without replacement . Assign this sample the name without . \u2013 Run tally(without) and describe the output. Does something similar happen if you sample with replacement ? Notice that the tilde ~ was not needed with the tally function. This is because without was not a variable within a data frame but rather a vector which acts like a lone variable. \u2013 What happens if size = 101 and replace = FALSE ? Sample with? Or without? Imagine the following two scenarios. `1. You have a coin with two sides: Heads and Tails . You're not sure if the coin is fair and so you want to estimate the probability of getting a Head . `2. A child reaches into a candy jar with 10 strawberry , 50 chocolate and 25 watermelon candies. The child is able to grab three candies with their hand and you're interested in probability that all three candies will be chocolate. Which of these scenarios would you sample with replacement and which would you sample without replacement ? Why? \u2013 Write down the line of code you would run to sample from the candy jar. Assume the simulated jar is named candies . Simulations at work In reality, songs from a playlist are chosen without replacement. \u2013 This way, you won't hear the same song several times in a row. Let's write a more realistic simulation and estimate the probability that if we select two songs at random, without replacement, that both are rap songs. \u2013 Use the do function to perform 10 simulated sample s of size 2, without replacement and assign the simulations the name draws and then View your file. Use set.seed(1) . What are the variable names? What happened in the first simulation? Did any of your 10 simulations contain two rap songs? Simulations and probability To estimate the probability from our simulations, we need to find the proportion of times that the event we're interested in occurs in the simulations. In other words, we need to count the number of times the desired events occurred, divided by the number of attempts we made (the number of simulations). The next slides will show you two ways to do this. Counting similar outcomes One way we can estimate the probability of drawing two songs of the same genre is to use the following trick to count the number of rap songs in each of the 10 simulations: mutate(draws, nrap = rowSums(draws==\"rap\")) Let\u2019s break down the code above by running each part of the code one piece at a time. As you run each line of code below describe the output. draws == \"rap\" rowSums(draws == \"rap\") mutate(draws, nrap = rowSums(draws==\"rap\")) Remember to assign a name to your mutated data set. Counting other outcomes Another method we can use to estimate the probability of complex events is to use the following 2-step procedure: `1. Subset or filter the rows of the simulations that match our desired outcomes. `2. Count the number of rows in the subset and divide by the number of simulations. The result that you obtain is an estimate of the probability that a specific combination of events occurred. We'll see an example of this method on the next slide. Step 1: Creating a subset Fill in the blanks below to: `1. Create a subset of our simulations when both draws were \"rap\" songs. `2. Count the number of rows in this subset `3. And divide by the total number of repeated simulations. draws_sub <- filter(draws, ___ == \"rap\", ___ == \"rap\") nrow(___) / ___ Estimating probabilities Answer the following questions by performing 500 simulations of sampling 2 songs from a playlist of 30 rap, 23 country and 47 rock songs. You might consider running set.seed so that your results can be reproduced: Calculate estimated probabilities for the following situations: `1. You draw two \"rap\" songs. `2. You draw a \"rap\" song in the first draw and a \"country\" song in the 2nd. Create a histogram that displays the number of times a \"rap\" song occurred in each simulation. That is, how often were zero rap songs drawn? A single rap song? Two rap songs? On your own Using what you've learned in the previous two labs, answer the following question by performing two computer simulations with 500 repetitions a piece: If we draw 5 songs from a playlist of 30 rap, 23 country and 47 rock songs, how does the estimated probability of all 5 songs being rap songs change if we draw the songs with or without replacement? For each simulation: \u2013 Create a histogram for the number of rap songs that occurred for each of the 500 repetitions. Describe how the distribution of the number of rap songs changes depending on if we use replacement or not.","title":"Lab 2D - Queue It Up!"},{"location":"unit2/lab2d/#lab-2d-queue-it-up","text":"Directions: Follow along with the slides, completing the questions in blue on your computer, and answering the questions in red in your journal.","title":"Lab 2D - Queue It Up!"},{"location":"unit2/lab2d/#where-we-left-off","text":"In the last lab, we looked at how we can use computer simulations to compute estimates of simple probabilities. \u2013 Like the probability of drawing a song genre from a playlist. We also saw that performing more simulations: \u2013 Took longer to finish. \u2013 Had estimates that varied less . In this lab, we'll extend our simulation methods to cover situations that are more complex. \u2013 We'll learn how to estimate their probabilities. \u2013 We also look at the roll of sampling with or without replacement .","title":"Where we left off"},{"location":"unit2/lab2d/#back-to-songs","text":"In R , simulate a playlist of songs containing 30 \"rap\" songs, 23 \"country\" songs and 47 \"rock\" songs. \u2013 Assign the combined playlist the name songs . Simulate choosing a single song 50 times. Then use your simulated draws to estimate the probability of choosing a rap song. \u2013 The actual (theoretical) probability of choosing a rap song in this case is 0.30 . \u2013 Write a sentence comparing your estimated probability to the actual probability.","title":"Back to songs"},{"location":"unit2/lab2d/#with-or-without","text":"So far, you've selected songs with replacement . \u2013 We called it that, because each time you made a selection, you started with the same playlist. That is, you chose a song, wrote down its data, and then placed it back on the list. It's also possible to select without replacement by setting the replace option in the sample function to FALSE . Take a sample of size 100 from our playlist of songs without replacement . Assign this sample the name without . \u2013 Run tally(without) and describe the output. Does something similar happen if you sample with replacement ? Notice that the tilde ~ was not needed with the tally function. This is because without was not a variable within a data frame but rather a vector which acts like a lone variable. \u2013 What happens if size = 101 and replace = FALSE ?","title":"With or Without?"},{"location":"unit2/lab2d/#sample-with-or-without","text":"Imagine the following two scenarios. `1. You have a coin with two sides: Heads and Tails . You're not sure if the coin is fair and so you want to estimate the probability of getting a Head . `2. A child reaches into a candy jar with 10 strawberry , 50 chocolate and 25 watermelon candies. The child is able to grab three candies with their hand and you're interested in probability that all three candies will be chocolate. Which of these scenarios would you sample with replacement and which would you sample without replacement ? Why? \u2013 Write down the line of code you would run to sample from the candy jar. Assume the simulated jar is named candies .","title":"Sample with? Or without?"},{"location":"unit2/lab2d/#simulations-at-work","text":"In reality, songs from a playlist are chosen without replacement. \u2013 This way, you won't hear the same song several times in a row. Let's write a more realistic simulation and estimate the probability that if we select two songs at random, without replacement, that both are rap songs. \u2013 Use the do function to perform 10 simulated sample s of size 2, without replacement and assign the simulations the name draws and then View your file. Use set.seed(1) . What are the variable names? What happened in the first simulation? Did any of your 10 simulations contain two rap songs?","title":"Simulations at work"},{"location":"unit2/lab2d/#simulations-and-probability","text":"To estimate the probability from our simulations, we need to find the proportion of times that the event we're interested in occurs in the simulations. In other words, we need to count the number of times the desired events occurred, divided by the number of attempts we made (the number of simulations). The next slides will show you two ways to do this.","title":"Simulations and probability"},{"location":"unit2/lab2d/#counting-similar-outcomes","text":"One way we can estimate the probability of drawing two songs of the same genre is to use the following trick to count the number of rap songs in each of the 10 simulations: mutate(draws, nrap = rowSums(draws==\"rap\")) Let\u2019s break down the code above by running each part of the code one piece at a time. As you run each line of code below describe the output. draws == \"rap\" rowSums(draws == \"rap\") mutate(draws, nrap = rowSums(draws==\"rap\")) Remember to assign a name to your mutated data set.","title":"Counting similar outcomes"},{"location":"unit2/lab2d/#counting-other-outcomes","text":"Another method we can use to estimate the probability of complex events is to use the following 2-step procedure: `1. Subset or filter the rows of the simulations that match our desired outcomes. `2. Count the number of rows in the subset and divide by the number of simulations. The result that you obtain is an estimate of the probability that a specific combination of events occurred. We'll see an example of this method on the next slide.","title":"Counting other outcomes"},{"location":"unit2/lab2d/#step-1-creating-a-subset","text":"Fill in the blanks below to: `1. Create a subset of our simulations when both draws were \"rap\" songs. `2. Count the number of rows in this subset `3. And divide by the total number of repeated simulations. draws_sub <- filter(draws, ___ == \"rap\", ___ == \"rap\") nrow(___) / ___","title":"Step 1: Creating a subset"},{"location":"unit2/lab2d/#estimating-probabilities","text":"Answer the following questions by performing 500 simulations of sampling 2 songs from a playlist of 30 rap, 23 country and 47 rock songs. You might consider running set.seed so that your results can be reproduced: Calculate estimated probabilities for the following situations: `1. You draw two \"rap\" songs. `2. You draw a \"rap\" song in the first draw and a \"country\" song in the 2nd. Create a histogram that displays the number of times a \"rap\" song occurred in each simulation. That is, how often were zero rap songs drawn? A single rap song? Two rap songs?","title":"Estimating probabilities"},{"location":"unit2/lab2d/#on-your-own","text":"Using what you've learned in the previous two labs, answer the following question by performing two computer simulations with 500 repetitions a piece: If we draw 5 songs from a playlist of 30 rap, 23 country and 47 rock songs, how does the estimated probability of all 5 songs being rap songs change if we draw the songs with or without replacement? For each simulation: \u2013 Create a histogram for the number of rap songs that occurred for each of the 500 repetitions. Describe how the distribution of the number of rap songs changes depending on if we use replacement or not.","title":"On your own"},{"location":"unit2/lab2e/","text":"Lab 2E - The Horror Movie Shuffle Directions: Follow along with the slides, completing the questions in blue on your computer, and answering the questions in red in your journal. Playing with permutations Slasher films are notoriously gory and are said to contain recurring biases. \u2013 One such bias, is that women in slasher films are more likely to survive than men. This lab will focus on the statistical question: Are women in slasher films more likely to survive until the end of the film than men? To answer this question, we'll learn how to use permuted data to gauge how likely an event occurs by chance. To begin, use the data function to load the slasher data file. \u2013 The data contains information about 485 characters from a random sample of 50 slasher horror films. Initial thoughts... To familiarize yourself with the data, answer the following: \u2013 How many variables and observations are contained in the data and what are the possible values of the variables? \u2013 Which gender had more survivors? Write down a few sentences as to how you came to your conclusion. Be sure to look at both the counts and percentages of survivors in each group before deciding. \u2013 Calculate the difference between the percentage of females who survived and the percentage of males who survived. Is the difference large enough to conclude that women tend to survive more often than men? Tally whoa ... ! Something you might have noticed is that these two lines of code aren't equivalent: tally(~gender | survival, data = slasher, margin = TRUE) tally(~survival | gender, data = slasher, margin = TRUE) The first line of code takes the group of survivors and tells us how many of them were Male or Female . The other takes the group of females / males and tells us how many of them Dies or Survives . The last question on the previous slide can be answered using the line of code below. Why? Pro-tip: Include the option format = \"percent\" to obtain a two-way table with percentages. tally(~survival | gender, format = \"percent\", data = slasher, margin = TRUE) ## gender ## survival Female Male ## Dies 77.47748 86.69202 ## Survives 22.52252 13.30798 ## Total 100.00000 100.00000 Examining differences When we're comparing the difference between two quantities, such as survival rates of slasher films, it can be difficult to decide how different two values need to be before we can conclude that the difference didn't just happen by chance. \u2013 To help us decide when a difference is not due to chance, we'll use repeated random shuffling. By using repeated random shuffling, we'll estimate how often our actual difference occurs by chance . Do the shuffle! When we shuffle data, we use our original data set as a starting point. \u2013 Run the following and write down the resulting table on a piece of paper. tally(~survival | gender, data = slasher) Now run the following to randomly reassign each survival status to each observation. Compare the resulting table to the one you wrote down. tally(~shuffle(survival) | gender, data = slasher) Let's compare ... How many people, in total, survived the slasher film before shuffling? How many people survived after shuffling? How has shuffling our data changed the percentage of women who survived compared to men who survived? \u2013 Is the difference in percentages from your shuffled data larger or smaller than the difference from the original data? Interpret what this means. Explain why shuffling our data one time is not enough to decide if the difference seen in our actual data occurs by chance or not. Detecting differences To help us decide if the difference in percentages in our actual data occurs by chance or not, we can use the do() function to shuffle our data many times and see how often our actual difference occurred by chance. **Run the following lines of code: set.seed(7) shuffled_outcomes <- do(10) * tally(~shuffle(survival) | gender, format = \"percent\", data = slasher) View(shuffled_outcomes) In how many simulations did a higher percentage of males survive than females? What is the largest difference in percentages of survival between males and females? What patterns are emerging from these simulations? Ten simulations is not enough. Use the code above and perform 500 shuffles. Assign your 500 shuffles the same name shuffled_outcomes . Use set.seed(1) . Now what? The next step to find out how often our actual difference occurs by chance is to compare it to the differences in our shuffled data. To compute the differences for each shuffle we can use the mutate function. \u2013 Fill in the blanks to add a new column that contains the difference between Survives.Female and Survives.Male to our shuffled_outcomes data. shuffled_outcomes <- mutate(shuffled_outcomes, diff = ____ - ____) Time to decide Create a histogram of the difference s in our shuffled_outcomes data. Based on your plot, answer the following: \u2013 What was the typical difference in percentages between men and women survivors? Include a vertical line in your histogram of the actual difference by running the code below: add_line(vline = 22.52252 - 13.30798) Does the actual difference occur very often by chance alone? Does gender play a role in whether or not a character will survive in a horror film? Explain your reasoning. If you wanted to survive in a horror film, would you want to play a female character or a male character? Summary By shuffling the survival label, we made it so that the proportion of males and females who survived the slasher film was random. \u2013 The males and females survived by chance alone. If surviving the film occurred purely by chance, then most of the time the difference in survival proportions was close to zero. \u2013 Notice how most values in the histogram occur close to zero. When we look to see how often our actual difference occurs in our shuffled data, if the actual difference doesn't occur very often then perhaps there is something more going on than just chance alone ... On your own Carry out another 500 simulations but this time shuffle the gender variable instead of the survival variable. \u2013 Include the code set.seed(1) before your 500 simulations to make your answer reproducible. Does shuffling the gender variable instead of the survival variable change your answer to the question? Does gender play a role in whether or not a character will survive in a horror film?* \u2013 Why or why not?","title":"Lab 2E - The Horror Movie Shuffle"},{"location":"unit2/lab2e/#lab-2e-the-horror-movie-shuffle","text":"Directions: Follow along with the slides, completing the questions in blue on your computer, and answering the questions in red in your journal.","title":"Lab 2E - The Horror Movie Shuffle"},{"location":"unit2/lab2e/#playing-with-permutations","text":"Slasher films are notoriously gory and are said to contain recurring biases. \u2013 One such bias, is that women in slasher films are more likely to survive than men. This lab will focus on the statistical question: Are women in slasher films more likely to survive until the end of the film than men? To answer this question, we'll learn how to use permuted data to gauge how likely an event occurs by chance. To begin, use the data function to load the slasher data file. \u2013 The data contains information about 485 characters from a random sample of 50 slasher horror films.","title":"Playing with permutations"},{"location":"unit2/lab2e/#initial-thoughts","text":"To familiarize yourself with the data, answer the following: \u2013 How many variables and observations are contained in the data and what are the possible values of the variables? \u2013 Which gender had more survivors? Write down a few sentences as to how you came to your conclusion. Be sure to look at both the counts and percentages of survivors in each group before deciding. \u2013 Calculate the difference between the percentage of females who survived and the percentage of males who survived. Is the difference large enough to conclude that women tend to survive more often than men?","title":"Initial thoughts..."},{"location":"unit2/lab2e/#tally-whoa","text":"Something you might have noticed is that these two lines of code aren't equivalent: tally(~gender | survival, data = slasher, margin = TRUE) tally(~survival | gender, data = slasher, margin = TRUE) The first line of code takes the group of survivors and tells us how many of them were Male or Female . The other takes the group of females / males and tells us how many of them Dies or Survives . The last question on the previous slide can be answered using the line of code below. Why? Pro-tip: Include the option format = \"percent\" to obtain a two-way table with percentages. tally(~survival | gender, format = \"percent\", data = slasher, margin = TRUE) ## gender ## survival Female Male ## Dies 77.47748 86.69202 ## Survives 22.52252 13.30798 ## Total 100.00000 100.00000","title":"Tally whoa ... !"},{"location":"unit2/lab2e/#examining-differences","text":"When we're comparing the difference between two quantities, such as survival rates of slasher films, it can be difficult to decide how different two values need to be before we can conclude that the difference didn't just happen by chance. \u2013 To help us decide when a difference is not due to chance, we'll use repeated random shuffling. By using repeated random shuffling, we'll estimate how often our actual difference occurs by chance .","title":"Examining differences"},{"location":"unit2/lab2e/#do-the-shuffle","text":"When we shuffle data, we use our original data set as a starting point. \u2013 Run the following and write down the resulting table on a piece of paper. tally(~survival | gender, data = slasher) Now run the following to randomly reassign each survival status to each observation. Compare the resulting table to the one you wrote down. tally(~shuffle(survival) | gender, data = slasher)","title":"Do the shuffle!"},{"location":"unit2/lab2e/#lets-compare","text":"How many people, in total, survived the slasher film before shuffling? How many people survived after shuffling? How has shuffling our data changed the percentage of women who survived compared to men who survived? \u2013 Is the difference in percentages from your shuffled data larger or smaller than the difference from the original data? Interpret what this means. Explain why shuffling our data one time is not enough to decide if the difference seen in our actual data occurs by chance or not.","title":"Let's compare ..."},{"location":"unit2/lab2e/#detecting-differences","text":"To help us decide if the difference in percentages in our actual data occurs by chance or not, we can use the do() function to shuffle our data many times and see how often our actual difference occurred by chance. **Run the following lines of code: set.seed(7) shuffled_outcomes <- do(10) * tally(~shuffle(survival) | gender, format = \"percent\", data = slasher) View(shuffled_outcomes) In how many simulations did a higher percentage of males survive than females? What is the largest difference in percentages of survival between males and females? What patterns are emerging from these simulations? Ten simulations is not enough. Use the code above and perform 500 shuffles. Assign your 500 shuffles the same name shuffled_outcomes . Use set.seed(1) .","title":"Detecting differences"},{"location":"unit2/lab2e/#now-what","text":"The next step to find out how often our actual difference occurs by chance is to compare it to the differences in our shuffled data. To compute the differences for each shuffle we can use the mutate function. \u2013 Fill in the blanks to add a new column that contains the difference between Survives.Female and Survives.Male to our shuffled_outcomes data. shuffled_outcomes <- mutate(shuffled_outcomes, diff = ____ - ____)","title":"Now what?"},{"location":"unit2/lab2e/#time-to-decide","text":"Create a histogram of the difference s in our shuffled_outcomes data. Based on your plot, answer the following: \u2013 What was the typical difference in percentages between men and women survivors? Include a vertical line in your histogram of the actual difference by running the code below: add_line(vline = 22.52252 - 13.30798) Does the actual difference occur very often by chance alone? Does gender play a role in whether or not a character will survive in a horror film? Explain your reasoning. If you wanted to survive in a horror film, would you want to play a female character or a male character?","title":"Time to decide"},{"location":"unit2/lab2e/#summary","text":"By shuffling the survival label, we made it so that the proportion of males and females who survived the slasher film was random. \u2013 The males and females survived by chance alone. If surviving the film occurred purely by chance, then most of the time the difference in survival proportions was close to zero. \u2013 Notice how most values in the histogram occur close to zero. When we look to see how often our actual difference occurs in our shuffled data, if the actual difference doesn't occur very often then perhaps there is something more going on than just chance alone ...","title":"Summary"},{"location":"unit2/lab2e/#on-your-own","text":"Carry out another 500 simulations but this time shuffle the gender variable instead of the survival variable. \u2013 Include the code set.seed(1) before your 500 simulations to make your answer reproducible. Does shuffling the gender variable instead of the survival variable change your answer to the question? Does gender play a role in whether or not a character will survive in a horror film?* \u2013 Why or why not?","title":"On your own"},{"location":"unit2/lab2f/","text":"Lab 2F - The Titanic Shuffle Directions: Follow along with the slides, completing the questions in blue on your computer, and answering the questions in red in your journal. Previously ... In the previous lab, we learned that by using a do -loop and the shuffle function, we could simulate randomly shuffling our data many times. This helps us determine how likely it is that a difference between groups is due to chance. For this lab, we will extend these ideas to numerical variables by using random shuffling and numerical summaries. The question we will investigate in this lab is: Is there any evidence to suggest that those who survived paid a higher fare than those who died? We will consider wealthier passengers to be those that paid a higher fare for their ticket. The Titanic The Titanic was a ship that sank en route to the U.S.A. from England after hitting an Iceberg in 1912. \u2013 At the time, it was claimed that the Titanic was unsinkable ... it wasn't ... because it did. Use the data function to load the titanic passenger and survival data. Create a boxplot of the fare s paid by passengers and facet the plot based on whether the passenger survived or not. \u2013 Based on the plot, do you believe that passengers who paid a higher fare on the Titanic were more likely to survive? Explain why and describe how certain you are of being correct. The search begins! Start your analysis by calculating how much more the typical survivor paid than the typical non-survivor in our data. Based on the distributions of fares paid, which numerical summary that describes the typical value might be preferred? What was the typical fare paid by survivors? Non-survivors? How much more did the typical survivor pay? Do the shuffle! Use the do and the shuffle functions to shuffle the passenger's survival status 500 times. \u2013 Use the previous lab if you need some help on how to do this. \u2013 For each shuffle, compute each group's median fare paid. \u2013 Assign your shuffled data the name shuffled_survival . After shuffling your data, use the mutate function to create a variable called diff which is the median fare of survivors minus the median fare of non-survivors. Assign your mutated data the name shuffled_survival again. Put your simulations to use By using your shuffled data, answer the research question we posed at the beginning of the lab. Is there any evidence to suggest that those who survived paid a higher fare than those who died? Write up your answer as a statistical analysis. Create a plot and explain how the plot supports your conclusion. Be sure to also explain why shuffling your data is important. Comparing Mean Fares What about if instead of calculating the median fare price for each group after a shuffle, we calculated the mean fare price and took the difference (mean_survivor \u2013 mean_victim). If we did this 500 times, what do you predict the distribution of differences will look like? Use the do and the shuffle functions to shuffle the passenger\u2019s survival status 500 times. For each shuffle, compute each group\u2019s mean fare paid. After shuffling your data, use the mutate function to create a variable called diff which is the mean fare of survivors minus the mean fare of non-survivors. What does the shuffled data reveal? Does the answer to the research question below change when using the mean fares instead of the median fares? Is there any evidence to suggest that those who survived paid a higher fare than those who died?","title":"Lab 2F - The Titanic Shuffle"},{"location":"unit2/lab2f/#lab-2f-the-titanic-shuffle","text":"Directions: Follow along with the slides, completing the questions in blue on your computer, and answering the questions in red in your journal.","title":"Lab 2F - The Titanic Shuffle"},{"location":"unit2/lab2f/#previously","text":"In the previous lab, we learned that by using a do -loop and the shuffle function, we could simulate randomly shuffling our data many times. This helps us determine how likely it is that a difference between groups is due to chance. For this lab, we will extend these ideas to numerical variables by using random shuffling and numerical summaries. The question we will investigate in this lab is: Is there any evidence to suggest that those who survived paid a higher fare than those who died? We will consider wealthier passengers to be those that paid a higher fare for their ticket.","title":"Previously ..."},{"location":"unit2/lab2f/#the-titanic","text":"The Titanic was a ship that sank en route to the U.S.A. from England after hitting an Iceberg in 1912. \u2013 At the time, it was claimed that the Titanic was unsinkable ... it wasn't ... because it did. Use the data function to load the titanic passenger and survival data. Create a boxplot of the fare s paid by passengers and facet the plot based on whether the passenger survived or not. \u2013 Based on the plot, do you believe that passengers who paid a higher fare on the Titanic were more likely to survive? Explain why and describe how certain you are of being correct.","title":"The Titanic"},{"location":"unit2/lab2f/#the-search-begins","text":"Start your analysis by calculating how much more the typical survivor paid than the typical non-survivor in our data. Based on the distributions of fares paid, which numerical summary that describes the typical value might be preferred? What was the typical fare paid by survivors? Non-survivors? How much more did the typical survivor pay?","title":"The search begins!"},{"location":"unit2/lab2f/#do-the-shuffle","text":"Use the do and the shuffle functions to shuffle the passenger's survival status 500 times. \u2013 Use the previous lab if you need some help on how to do this. \u2013 For each shuffle, compute each group's median fare paid. \u2013 Assign your shuffled data the name shuffled_survival . After shuffling your data, use the mutate function to create a variable called diff which is the median fare of survivors minus the median fare of non-survivors. Assign your mutated data the name shuffled_survival again.","title":"Do the shuffle!"},{"location":"unit2/lab2f/#put-your-simulations-to-use","text":"By using your shuffled data, answer the research question we posed at the beginning of the lab. Is there any evidence to suggest that those who survived paid a higher fare than those who died? Write up your answer as a statistical analysis. Create a plot and explain how the plot supports your conclusion. Be sure to also explain why shuffling your data is important.","title":"Put your simulations to use"},{"location":"unit2/lab2f/#comparing-mean-fares","text":"What about if instead of calculating the median fare price for each group after a shuffle, we calculated the mean fare price and took the difference (mean_survivor \u2013 mean_victim). If we did this 500 times, what do you predict the distribution of differences will look like? Use the do and the shuffle functions to shuffle the passenger\u2019s survival status 500 times. For each shuffle, compute each group\u2019s mean fare paid. After shuffling your data, use the mutate function to create a variable called diff which is the mean fare of survivors minus the mean fare of non-survivors. What does the shuffled data reveal? Does the answer to the research question below change when using the mean fares instead of the median fares? Is there any evidence to suggest that those who survived paid a higher fare than those who died?","title":"Comparing Mean Fares"},{"location":"unit2/lab2g/","text":"Lab 2G - Getting It Together Directions: Follow along with the slides, completing the questions in blue on your computer, and answering the questions in red in your journal. Putting data together In the labs so far, we've only ever looked at individual data files. But often times, we gain additional insights by including additional information from a separate data set. In this lab, we will learn how to merge information from our personality color data with our stress/chill data. Export , upload , import your Personality Color dataset and name it colors . Then, export , upload , import your Stress/Chill dataset and name it stress . Looking at Stress/Chill We would like to analyze the research question: How do people's personality colors and/or sports participation affect their stress levels? We already have data about personality color and a separate data set about stress . \u2013 What we don't have is a single data set with information from both ... yet. We'll start then by strategizing how to merge our data together. Deciding how to merge Before we merge data, we need to decide how we plan to merge it: We can stack our datasets, that is, take one dataset's rows and add them to the bottom of the other dataset. We can also join our data sets horizontally. This is where we take one dataset's columns and add them to the end of the other dataset's columns based on matching an ID variable. \u2013 The ID variable will have entries that we use to match observations in both datasets. To answer the statistical question of interest, would it make more sense to stack or join our colors and stress data? Finding variables in common: Look at the names of the variables in each dataset. \u2013 To merge different datasets together, we need to find variables they have in common. Which variables do the datasets have in common? Which variable would make sense to merge the datasets together with? Why not the others? Caution required Whether stacking or joining , we need to be careful when we merge data: When stacking data, we need to be absolutely certain that the variables we're stacking represent the exact same measurements. \u2013 We wouldn't want to stack height in meters and height in inches, for instance (without converting one to the other). When joining data, we need to make sure that the id variable in our primary dataset matches to one and only one observation in the joining data. \u2013 Otherwise, R won't know which observation to match to. Getting ready Our goal is to add the variables from the colors data onto the stress data. Start by ensuring that every user.id in the colors data is unique. \u2013 If there's a duplicate, have your teacher remove the duplicate from your class' Web Response Manager and then re- export , upload , import your colors data. After we add the data from colors to stress , how many rows should our merged data have? Write this number down. Putting them together We can use the merge function to join our datasets together using the variables that appear in both sets. Fill in the blanks below to join the information from the colors data onto the stress data. merge(____, ____, by = \"____\") Assign this merged data set the name stress_colors . \u2013 Make sure your data has the same number of observations that you wrote down on the previous slide. Saving your data: View your merged data and make sure nothing appears to be blatantly wrong with it. Why didn't we stack the rows of data instead? What happens if you swap the order of the data sets in the merge function? Fill in the blank below to save our stress_colors data for later use. save(stress_colors, file = \"stress_colors.rda\") Be sure to look in the Files tab to make sure your data was saved. Moving on In the next lab, we'll begin analyzing our merged data. In the meantime: Make a few plots using variables from the stress data and facet or group the plots based on variables from the colors data. \u2013 Write down the most interesting discovery you make by just exploring your data. Write out how you found your discovery and interpret what it means for the people in your class. With our colors data, we could answer questions about the typical color scores in your class. Why can we no longer answer this question in our stress_colors data?","title":"Lab 2G - Getting It Together"},{"location":"unit2/lab2g/#lab-2g-getting-it-together","text":"Directions: Follow along with the slides, completing the questions in blue on your computer, and answering the questions in red in your journal.","title":"Lab 2G - Getting It Together"},{"location":"unit2/lab2g/#putting-data-together","text":"In the labs so far, we've only ever looked at individual data files. But often times, we gain additional insights by including additional information from a separate data set. In this lab, we will learn how to merge information from our personality color data with our stress/chill data. Export , upload , import your Personality Color dataset and name it colors . Then, export , upload , import your Stress/Chill dataset and name it stress .","title":"Putting data together"},{"location":"unit2/lab2g/#looking-at-stresschill","text":"We would like to analyze the research question: How do people's personality colors and/or sports participation affect their stress levels? We already have data about personality color and a separate data set about stress . \u2013 What we don't have is a single data set with information from both ... yet. We'll start then by strategizing how to merge our data together.","title":"Looking at Stress/Chill"},{"location":"unit2/lab2g/#deciding-how-to-merge","text":"Before we merge data, we need to decide how we plan to merge it: We can stack our datasets, that is, take one dataset's rows and add them to the bottom of the other dataset. We can also join our data sets horizontally. This is where we take one dataset's columns and add them to the end of the other dataset's columns based on matching an ID variable. \u2013 The ID variable will have entries that we use to match observations in both datasets. To answer the statistical question of interest, would it make more sense to stack or join our colors and stress data?","title":"Deciding how to merge"},{"location":"unit2/lab2g/#finding-variables-in-common","text":"Look at the names of the variables in each dataset. \u2013 To merge different datasets together, we need to find variables they have in common. Which variables do the datasets have in common? Which variable would make sense to merge the datasets together with? Why not the others?","title":"Finding variables in common:"},{"location":"unit2/lab2g/#caution-required","text":"Whether stacking or joining , we need to be careful when we merge data: When stacking data, we need to be absolutely certain that the variables we're stacking represent the exact same measurements. \u2013 We wouldn't want to stack height in meters and height in inches, for instance (without converting one to the other). When joining data, we need to make sure that the id variable in our primary dataset matches to one and only one observation in the joining data. \u2013 Otherwise, R won't know which observation to match to.","title":"Caution required"},{"location":"unit2/lab2g/#getting-ready","text":"Our goal is to add the variables from the colors data onto the stress data. Start by ensuring that every user.id in the colors data is unique. \u2013 If there's a duplicate, have your teacher remove the duplicate from your class' Web Response Manager and then re- export , upload , import your colors data. After we add the data from colors to stress , how many rows should our merged data have? Write this number down.","title":"Getting ready"},{"location":"unit2/lab2g/#putting-them-together","text":"We can use the merge function to join our datasets together using the variables that appear in both sets. Fill in the blanks below to join the information from the colors data onto the stress data. merge(____, ____, by = \"____\") Assign this merged data set the name stress_colors . \u2013 Make sure your data has the same number of observations that you wrote down on the previous slide.","title":"Putting them together"},{"location":"unit2/lab2g/#saving-your-data","text":"View your merged data and make sure nothing appears to be blatantly wrong with it. Why didn't we stack the rows of data instead? What happens if you swap the order of the data sets in the merge function? Fill in the blank below to save our stress_colors data for later use. save(stress_colors, file = \"stress_colors.rda\") Be sure to look in the Files tab to make sure your data was saved.","title":"Saving your data:"},{"location":"unit2/lab2g/#moving-on","text":"In the next lab, we'll begin analyzing our merged data. In the meantime: Make a few plots using variables from the stress data and facet or group the plots based on variables from the colors data. \u2013 Write down the most interesting discovery you make by just exploring your data. Write out how you found your discovery and interpret what it means for the people in your class. With our colors data, we could answer questions about the typical color scores in your class. Why can we no longer answer this question in our stress_colors data?","title":"Moving on"},{"location":"unit2/lab2h/","text":"Lab 2H - Eyeballing Normal Directions: Follow along with the slides, completing the questions in blue on your computer, and answering the questions in red in your journal. What's normal? The normal distribution is a curve we often see in real data. \u2013 We see it in people's blood pressures and in measurement errors. When data appears to be normally distributed , we can use the normal model to: \u2013 Simulate normally distributed data. \u2013 Easily compute probabilities. In this lab, we'll look at some previous data sets to see if we can find data that are roughly normally distributed. The normal distribution The normal distribution is symmetric about the mean : \u2013 The mean is found in the very center of the distribution. \u2013 And the curve looks the same to the left of the mean as it does on the right. Use the following to draw a normal distribution: plotDist('norm', mean = 0, sd = 1) The mean and sd of it To draw a normal curve, we need to know exactly 2 things: \u2013 The mean and sd . The sd , or standard deviation , is a measure of spread that's similar to the MAD . Which part of the normal curve changes when the value of the mean changes? Which part of the normal curve changes when the value of the sd changes? Hint: Try changing the mean and sd values in the plotDist function. Finding normal distributions Load the cdc data and use the histogram function to answer the following: Think about the height and weight variables. Based on what you know about these variables, which of the variables do you think have distributions that will look like the normal distribution? \u2013 Make histograms of these variables. Which ones look like the normal distribution? \u2013 Hint: To help answer this question, try including the option fit = \"normal\" in the histogram function. You might also try faceting by gender . Using normal models Data scientists like using normal models because it often resembles real data. \u2013 But not EVERYTHING is normally distributed. As a data scientist in training, you must decide when a normal model seems appropriate. \u2013 No model is ever perfect 100% of the time. \u2013 If you choose a model, you should be able to justify why you chose it. On your own For each of the following, determine which, if any, appear to be normally distributed. Explain your reasoning: \u2013 The difference in percentages between male and female survivors in a slasher film for 500 random shuffles. \u2013 The difference in median fares between survivors and non-survivors on the Titanic for 500 random shuffles. \u2013 The difference in mean fares between survivors and non-survivors on the Titanic for 500 random shuffles. Hint: Refer to Lab 2E and 2F.","title":"Lab 2H - Eyeballing Normal"},{"location":"unit2/lab2h/#lab-2h-eyeballing-normal","text":"Directions: Follow along with the slides, completing the questions in blue on your computer, and answering the questions in red in your journal.","title":"Lab 2H - Eyeballing Normal"},{"location":"unit2/lab2h/#whats-normal","text":"The normal distribution is a curve we often see in real data. \u2013 We see it in people's blood pressures and in measurement errors. When data appears to be normally distributed , we can use the normal model to: \u2013 Simulate normally distributed data. \u2013 Easily compute probabilities. In this lab, we'll look at some previous data sets to see if we can find data that are roughly normally distributed.","title":"What's normal?"},{"location":"unit2/lab2h/#the-normal-distribution","text":"The normal distribution is symmetric about the mean : \u2013 The mean is found in the very center of the distribution. \u2013 And the curve looks the same to the left of the mean as it does on the right. Use the following to draw a normal distribution: plotDist('norm', mean = 0, sd = 1)","title":"The normal distribution"},{"location":"unit2/lab2h/#the-mean-and-sd-of-it","text":"To draw a normal curve, we need to know exactly 2 things: \u2013 The mean and sd . The sd , or standard deviation , is a measure of spread that's similar to the MAD . Which part of the normal curve changes when the value of the mean changes? Which part of the normal curve changes when the value of the sd changes? Hint: Try changing the mean and sd values in the plotDist function.","title":"The mean and sd of it"},{"location":"unit2/lab2h/#finding-normal-distributions","text":"Load the cdc data and use the histogram function to answer the following: Think about the height and weight variables. Based on what you know about these variables, which of the variables do you think have distributions that will look like the normal distribution? \u2013 Make histograms of these variables. Which ones look like the normal distribution? \u2013 Hint: To help answer this question, try including the option fit = \"normal\" in the histogram function. You might also try faceting by gender .","title":"Finding normal distributions"},{"location":"unit2/lab2h/#using-normal-models","text":"Data scientists like using normal models because it often resembles real data. \u2013 But not EVERYTHING is normally distributed. As a data scientist in training, you must decide when a normal model seems appropriate. \u2013 No model is ever perfect 100% of the time. \u2013 If you choose a model, you should be able to justify why you chose it.","title":"Using normal models"},{"location":"unit2/lab2h/#on-your-own","text":"For each of the following, determine which, if any, appear to be normally distributed. Explain your reasoning: \u2013 The difference in percentages between male and female survivors in a slasher film for 500 random shuffles. \u2013 The difference in median fares between survivors and non-survivors on the Titanic for 500 random shuffles. \u2013 The difference in mean fares between survivors and non-survivors on the Titanic for 500 random shuffles. Hint: Refer to Lab 2E and 2F.","title":"On your own"},{"location":"unit2/lab2i/","text":"Lab 2I - R's Normal Distribution Alphabet Directions: Follow along with the slides, completing the questions in blue on your computer, and answering the questions in red in your journal. Where we're headed In the last lab, you were able to overlay a normal curve on histograms of data to help you decide if the data's distribution is close to a normal distribution. \u2013 We also saw that calculating the mean of random shuffles also produces differences that are normally distributed. In this lab, we'll learn how to use some other R functions to: \u2013 Simulate random draws from a normal distribution. \u2013 Calculate probabilities with normal distributions. Get set up Start by loading the titanic data and calculate the mean age of people in the data but shuffle their survival status 500 times. \u2013 Assign this data the name shfls . After creating shfls , use mutate to add a new variable to the dataset. This new variable should have the name diff and should be the mean age of those who survived minus those who died. Finally, calculate the mean and sd of the diff variable. \u2013 Assign these values the name diff_mean and diff_sd . Is it normal? Before we proceed, we need to verify that our diff variable looks approximately normally distributed. \u2013 Is the distribution close to normal? Explain how you determined this. Describe the center and spread of the distribution. \u2013 Compute and write down the mean difference in the age of the actual survivors and the actual non-survivors. Using the normal model Since the distribution of our diff variable appears normally distributed, we can use a normal model to estimate the probability of seeing differences that are more extreme than our actual data. Draw a sketch of a normal curve. Label the mean age difference, based on your shuffles, and the actual age difference of survivors minus non-survivors from the actual data. Then shade in the areas, under normal the curve, that are smaller than the actual difference. **Fill in the blanks to calculate the probability of an even smaller difference occurring than our actual difference using a normal model. pnorm(____, mean = diff_mean, sd = ____) Extreme probabilities The probability you calculated in the previous slide is an estimate for how often we expect to see a difference smaller than the actual one we observed, by chance alone. If you wanted to instead calculate the probability that the difference would be larger than the one observed, we could run (fill in the blanks): 1 - pnorm(____, mean = diff_mean, sd = ____) Simulating normal draws We can simulate random draws from a normal distribution with the rnorm function. \u2013 Fill in the blanks in the following two lines of code to simulate 100 heights of randomly chosen men. Assume the mean height is 67 inches and the standard deviation is 3 inches. draws <- rnorm(____, mean = ____, sd = ____) \u2013 Plot your simulated heights with a histogram . histogram(draws, fit = ____) P's and Q's We've seen that we can use pnorm to calculate probabilities based on a specified quantity . \u2013 Hence, why we call it \"P\" norm. Now we'll see how to do the opposite. That is, calculate the quantity for a specific probability . \u2013 Hence why we'll call this a \"Q\" norm. How tall can a man be and still be in the shortest 25% of heights if the mean height is 67 inches with a standard deviation of 3 inches? qnorm(____, mean = ____, sd = ____) On your own Conduct one of the statistical investigations below: Using the titanic data: \u2013 Were women on the Titanic typically younger than men? \u2013 Use a histogram, 500 random shuffles and a normal model to answer the question in the bullet above. Using the cdc data: \u2013 Using 500 random shuffles and a normal model, how much taller would the typical male have to be than the typical female in order for the difference to be in the upper 1% by chance alone? \u2013 How can we use this value to justify the claim that the average Male in our data is taller than the average Female ?","title":"Lab 2I - R\u2019s Normal Distribution Alphabet"},{"location":"unit2/lab2i/#lab-2i-rs-normal-distribution-alphabet","text":"Directions: Follow along with the slides, completing the questions in blue on your computer, and answering the questions in red in your journal.","title":"Lab 2I - R's Normal Distribution Alphabet"},{"location":"unit2/lab2i/#where-were-headed","text":"In the last lab, you were able to overlay a normal curve on histograms of data to help you decide if the data's distribution is close to a normal distribution. \u2013 We also saw that calculating the mean of random shuffles also produces differences that are normally distributed. In this lab, we'll learn how to use some other R functions to: \u2013 Simulate random draws from a normal distribution. \u2013 Calculate probabilities with normal distributions.","title":"Where we're headed"},{"location":"unit2/lab2i/#get-set-up","text":"Start by loading the titanic data and calculate the mean age of people in the data but shuffle their survival status 500 times. \u2013 Assign this data the name shfls . After creating shfls , use mutate to add a new variable to the dataset. This new variable should have the name diff and should be the mean age of those who survived minus those who died. Finally, calculate the mean and sd of the diff variable. \u2013 Assign these values the name diff_mean and diff_sd .","title":"Get set up"},{"location":"unit2/lab2i/#is-it-normal","text":"Before we proceed, we need to verify that our diff variable looks approximately normally distributed. \u2013 Is the distribution close to normal? Explain how you determined this. Describe the center and spread of the distribution. \u2013 Compute and write down the mean difference in the age of the actual survivors and the actual non-survivors.","title":"Is it normal?"},{"location":"unit2/lab2i/#using-the-normal-model","text":"Since the distribution of our diff variable appears normally distributed, we can use a normal model to estimate the probability of seeing differences that are more extreme than our actual data. Draw a sketch of a normal curve. Label the mean age difference, based on your shuffles, and the actual age difference of survivors minus non-survivors from the actual data. Then shade in the areas, under normal the curve, that are smaller than the actual difference. **Fill in the blanks to calculate the probability of an even smaller difference occurring than our actual difference using a normal model. pnorm(____, mean = diff_mean, sd = ____)","title":"Using the normal model"},{"location":"unit2/lab2i/#extreme-probabilities","text":"The probability you calculated in the previous slide is an estimate for how often we expect to see a difference smaller than the actual one we observed, by chance alone. If you wanted to instead calculate the probability that the difference would be larger than the one observed, we could run (fill in the blanks): 1 - pnorm(____, mean = diff_mean, sd = ____)","title":"Extreme probabilities"},{"location":"unit2/lab2i/#simulating-normal-draws","text":"We can simulate random draws from a normal distribution with the rnorm function. \u2013 Fill in the blanks in the following two lines of code to simulate 100 heights of randomly chosen men. Assume the mean height is 67 inches and the standard deviation is 3 inches. draws <- rnorm(____, mean = ____, sd = ____) \u2013 Plot your simulated heights with a histogram . histogram(draws, fit = ____)","title":"Simulating normal draws"},{"location":"unit2/lab2i/#ps-and-qs","text":"We've seen that we can use pnorm to calculate probabilities based on a specified quantity . \u2013 Hence, why we call it \"P\" norm. Now we'll see how to do the opposite. That is, calculate the quantity for a specific probability . \u2013 Hence why we'll call this a \"Q\" norm. How tall can a man be and still be in the shortest 25% of heights if the mean height is 67 inches with a standard deviation of 3 inches? qnorm(____, mean = ____, sd = ____)","title":"P's and Q's"},{"location":"unit2/lab2i/#on-your-own","text":"Conduct one of the statistical investigations below: Using the titanic data: \u2013 Were women on the Titanic typically younger than men? \u2013 Use a histogram, 500 random shuffles and a normal model to answer the question in the bullet above. Using the cdc data: \u2013 Using 500 random shuffles and a normal model, how much taller would the typical male have to be than the typical female in order for the difference to be in the upper 1% by chance alone? \u2013 How can we use this value to justify the claim that the average Male in our data is taller than the average Female ?","title":"On your own"},{"location":"unit2/lesson1/","text":"Lesson 1: What is Your True Color? Objective: Students will collect data that might tell them about their personality type, and will understand how to subset their data. Materials: True Colors Personality Test ( LMR_2.1_True Colors Personality Test ) Posted signs for each Personality Color: Blue, Gold, Green, and Orange Advanced preparation required (see step 5 below) Poster paper Markers Data collection devices Vocabulary: subsets Essential Concepts: Essential Concepts: Students will understand that the 'typical' value is a value that can represent the entire group, even though we know that not all members of the group share the same value. Lesson: Ask students to consider the following questions (they do not need to record any responses): How well do you know yourself? How well do you know your classmates? There are things students know and don\u2019t know about themselves. The True Colors Personality Test ( LMR_2.1 ) claims to identify personality types (Later, students can gather more evidence to test these claims). Students will use these data to explore fundamental statistical concepts. LMR_2.1 Distribute the first 2 pages of the True Colors Personality Test ( LMR_2.1 ). DO NOT include the final page, which contains the descriptions of each color. Instruct students on how to complete the test, and allow time for them to complete it (see page 2 in handout). Note: When adding scores for each color at the bottom of the test, make sure that students have NOT added straight down each column. Students should have a score for each of the 4 colors. Ask students to record each color and its respective score in their DS journal. Inform them that the color with the highest score describes their personality. We can refer to this as their predominant color. They should record their predominant personality color in their DS journal as well. Tell students that you will show them what each color means at the end of the lesson. Post a sign for each personality color on different walls of the classroom. For example, Blue on the north wall, Gold on the east wall, etc. Ask students to gather by the wall corresponding to their predominant personality color. The students should record answers to the following questions in their DS journals. How many students are in your color group? How many students are in each of the other color groups? What is the predominant personality color in your class? Then ask students to determine some common characteristics of the people in their group. Questions to help steer the discussions are included below. Each team should come up with a consensus to describe their team and will share their descriptions with the whole class. The goal is to get the students to think about \u201cwhat is typical?\u201d within their groups. What are your likes and dislikes? What things do you have in common? What are your favorite activities? What\u2019s your favorite color? Do you prefer mornings or nights? What\u2019s your favorite type of music? As the groups are presenting, record some dominant characteristics on the board for each color. The students will be able to compare their perceived traits with the actual descriptions from the activity at the end of the class. Next ask students in each color group to gather into two subsets : introvert and extrovert. Inform them that subsetting is another way to organize collected data. Create a two-way frequency table like the one below on the board to record the results. Distribute poster paper and markers to each team. Inform the students that they will be creating visuals for this data by comparing subsets. The Orange and Gold groups should create visualizations that subset the color variable by introverts and extroverts, and the Blue and Green groups should create visualizations that subset introverts and extroverts by color. If students are confused or stuck, have them recall the topic of two-way tables and relative frequencies from Unit 1 ( Lessons 16 & 17 ). The Orange and Gold groups will be looking at the columns and comparing introverts/extroverts, while the Blue and Green groups will be looking at the rows and comparing colors. Once all groups have completed their visuals, the Orange and Gold teams should choose one of their 2 posters to display to the class. The Blue and Green groups should do the same and select one of their visuals. Display both visuals on the board and discuss their similarities and differences. Ask students to analyze and interpret the visualizations by discussing the following questions for each of the visualizations: What type of plot is this and how many variables are present? Answers will vary by class. What information about this subset can I gather from this visualization? Answers will vary by class. What do I see the most/least of? Answers will vary by class. What is the typical personality color for this subset? Or, what is the typical group(introverts/extroverts) for this subset? Answers will vary by class. Ask students to summarize their impressions of the class\u2019s personality color data by writing this summary in their DS journals. Distribute the description of each personality color to students (page 3 of LMR_2.1 ). Remind them that the highest score is considered their predominant color and the second highest score is considered their secondary color. If there is a tie for their predominant or secondary colors, ask students to choose the color that describes their personality better. Compare the given descriptions on the handout to the characteristics listed on the board for each group during step 7. Do the descriptions match what the students originally thought? How accurate are the descriptions? If time allows, ask a couple of students to share their comparisons. Students will now record their data by completing the Personality Color campaign on the UCLA IDS UCLA App or via web browser at https://portal.idsucla.org Class Scribes: One team of students will give a brief talk to discuss what they think the 3 most important topics of the day were. Homework If not finished in class, students should complete the Personality Color survey either through the UCLA IDS UCLA App or via web browser at https://portal.idsucla.org","title":"Lesson 1: What is Your True Color?"},{"location":"unit2/lesson1/#lesson-1-what-is-your-true-color","text":"","title":"Lesson 1: What is Your True Color?"},{"location":"unit2/lesson1/#objective","text":"Students will collect data that might tell them about their personality type, and will understand how to subset their data.","title":"Objective:"},{"location":"unit2/lesson1/#materials","text":"True Colors Personality Test ( LMR_2.1_True Colors Personality Test ) Posted signs for each Personality Color: Blue, Gold, Green, and Orange Advanced preparation required (see step 5 below) Poster paper Markers Data collection devices","title":"Materials:"},{"location":"unit2/lesson1/#vocabulary","text":"subsets","title":"Vocabulary:"},{"location":"unit2/lesson1/#essential-concepts","text":"Essential Concepts: Students will understand that the 'typical' value is a value that can represent the entire group, even though we know that not all members of the group share the same value.","title":"Essential Concepts:"},{"location":"unit2/lesson1/#lesson","text":"Ask students to consider the following questions (they do not need to record any responses): How well do you know yourself? How well do you know your classmates? There are things students know and don\u2019t know about themselves. The True Colors Personality Test ( LMR_2.1 ) claims to identify personality types (Later, students can gather more evidence to test these claims). Students will use these data to explore fundamental statistical concepts. LMR_2.1 Distribute the first 2 pages of the True Colors Personality Test ( LMR_2.1 ). DO NOT include the final page, which contains the descriptions of each color. Instruct students on how to complete the test, and allow time for them to complete it (see page 2 in handout). Note: When adding scores for each color at the bottom of the test, make sure that students have NOT added straight down each column. Students should have a score for each of the 4 colors. Ask students to record each color and its respective score in their DS journal. Inform them that the color with the highest score describes their personality. We can refer to this as their predominant color. They should record their predominant personality color in their DS journal as well. Tell students that you will show them what each color means at the end of the lesson. Post a sign for each personality color on different walls of the classroom. For example, Blue on the north wall, Gold on the east wall, etc. Ask students to gather by the wall corresponding to their predominant personality color. The students should record answers to the following questions in their DS journals. How many students are in your color group? How many students are in each of the other color groups? What is the predominant personality color in your class? Then ask students to determine some common characteristics of the people in their group. Questions to help steer the discussions are included below. Each team should come up with a consensus to describe their team and will share their descriptions with the whole class. The goal is to get the students to think about \u201cwhat is typical?\u201d within their groups. What are your likes and dislikes? What things do you have in common? What are your favorite activities? What\u2019s your favorite color? Do you prefer mornings or nights? What\u2019s your favorite type of music? As the groups are presenting, record some dominant characteristics on the board for each color. The students will be able to compare their perceived traits with the actual descriptions from the activity at the end of the class. Next ask students in each color group to gather into two subsets : introvert and extrovert. Inform them that subsetting is another way to organize collected data. Create a two-way frequency table like the one below on the board to record the results. Distribute poster paper and markers to each team. Inform the students that they will be creating visuals for this data by comparing subsets. The Orange and Gold groups should create visualizations that subset the color variable by introverts and extroverts, and the Blue and Green groups should create visualizations that subset introverts and extroverts by color. If students are confused or stuck, have them recall the topic of two-way tables and relative frequencies from Unit 1 ( Lessons 16 & 17 ). The Orange and Gold groups will be looking at the columns and comparing introverts/extroverts, while the Blue and Green groups will be looking at the rows and comparing colors. Once all groups have completed their visuals, the Orange and Gold teams should choose one of their 2 posters to display to the class. The Blue and Green groups should do the same and select one of their visuals. Display both visuals on the board and discuss their similarities and differences. Ask students to analyze and interpret the visualizations by discussing the following questions for each of the visualizations: What type of plot is this and how many variables are present? Answers will vary by class. What information about this subset can I gather from this visualization? Answers will vary by class. What do I see the most/least of? Answers will vary by class. What is the typical personality color for this subset? Or, what is the typical group(introverts/extroverts) for this subset? Answers will vary by class. Ask students to summarize their impressions of the class\u2019s personality color data by writing this summary in their DS journals. Distribute the description of each personality color to students (page 3 of LMR_2.1 ). Remind them that the highest score is considered their predominant color and the second highest score is considered their secondary color. If there is a tie for their predominant or secondary colors, ask students to choose the color that describes their personality better. Compare the given descriptions on the handout to the characteristics listed on the board for each group during step 7. Do the descriptions match what the students originally thought? How accurate are the descriptions? If time allows, ask a couple of students to share their comparisons. Students will now record their data by completing the Personality Color campaign on the UCLA IDS UCLA App or via web browser at https://portal.idsucla.org","title":"Lesson:"},{"location":"unit2/lesson1/#class-scribes","text":"One team of students will give a brief talk to discuss what they think the 3 most important topics of the day were.","title":"Class Scribes:"},{"location":"unit2/lesson1/#homework","text":"If not finished in class, students should complete the Personality Color survey either through the UCLA IDS UCLA App or via web browser at https://portal.idsucla.org","title":"Homework"},{"location":"unit2/lesson10/","text":"Lesson 10: Marbles, Marbles... Objective: Students will understand that random events vary, so that the percentage predicted by a probability isn't exact, but varies. Students practice converting percentages to proportions. Materials: For each student team: 50 marbles \u2013 20 of one color, and 30 of another color Note: Marbles can be substituted for other materials, such as small blocks, as long as they are the same size. Vocabulary: proportion percentage event with replacement without replacement Essential Concepts: Essential Concepts: There are two ways of sampling data that model real-life sampling situations: with and without replacement. Larger samples tend to be closer to the \"true\" probability. Lesson: Remind students that, during the previous two lessons, they learned how to estimate probabilities for a population with the help of simulations to create sample data. Both lessons had nice, prepackaged functions already available in RStudio, which made the simulations fairly quick and easy \u2013 in Lesson 8 , the rflip() function was used to simulate flipping a coin; and in Lesson 9 , the roll_die() function was used to simulate rolling one of two dice. But what if we don\u2019t have a nice function to perform a simulation for us? Can we create our own method? Yes! We will actually learn to create a simulation from scratch during Lab 2C . Ask students: If you have a bag of 50 marbles, where 20 of them are blue and 30 of them are red, what is the probability of drawing a red marble? 30/50 or 60%. Select a student to answer the question. Ask the class if they agree or disagree. If they agree, ask them to raise their hand. If there are students who disagree, lead a class discussion until a consensus is reached. Ask students to share their strategies on how to convert the proportion into a percentage . As strategies are being shared, students should take notes in their IDS journals. Review how to turn fractions into percentages, if necessary. Ask students: What if we actually drew out one marble, recorded its color, then replaced it back in the bag, and did this 10 times? Answers will vary by class. Would the percentage of red marbles in this sample necessarily be exactly the same as the probability? Identify that each time a marble is drawn, we are creating an event . Answers will vary by class. Distribute the bags of marbles to each team. Ask each team to: Shake the bag of marbles. Draw one marble out of the bag. Record the marble\u2019s color in their IDS journal. Replace the marble back into the bag. Inform them that this is called sampling with replacement . Ask them to consider what \u201cwith replacement\u201d means and discuss with the class. \u201cWith replacement\u201d means that after you select a marble from the bag, you have to put it back into the bag (replace it) before you select another marble. They should draw 10 marbles from the bag and record the observed colors. Do a Whip Around to find out how many times each team drew a red marble out of their 10 draws. Have them calculate the corresponding sample proportions. For example, if one team drew 7 red marbles out of their 10 draws, their sample proportion is 7/10 = 0.70 (which is the sample as a sample percentage of 70%). Ask students why the proportions are perhaps different from each other and from the \u201ctrue\u201d probability of drawing a red marble? Have the student teams continue drawing marbles, one at a time and with replacement, until they have 50 events recorded. Discuss the following questions: How many times did they draw a red marble out of these 50? Answers will vary by class. What\u2019s the corresponding sample proportion? Is it closer to the true probability than when you only drew 10 marbles? Answers will vary by class. But, they should notice that, as the sample size increases, the sample proportion gets closer to the true population proportion. Engage students in a discussion about how the sample size affects the sample proportion. They should see that as they draw more marbles, their sample probability gets closer and closer to the true probability. If we were to continue drawing marbles forever, in the long run, our sample proportion should equal our true probability. Have students consider what it might mean to sample without replacement . How would they do that with their bag of marbles? \u201cWithout replacement\u201d means that after you select a marble from the bag, you never put it back into the bag (do not replace it). Instead, you simply select another marble from the bag immediately. Students should recognize that, by not replacing the marble each time, the probabilities will change. This means each draw from the marble bag is NOT independent from another draw because removing one marble impacts the next event. Exit Slip : Based on this lesson, ask students to describe a sample, an event, and replacement. Class Scribes: One team of students will give a brief talk to discuss what they think the 3 most important topics of the day were. Next Day LAB 2C: Which Song Plays Next? Complete Lab 2C prior to Lesson 11 .","title":"Lesson 10: Marbles, Marbles\u2026"},{"location":"unit2/lesson10/#lesson-10-marbles-marbles","text":"","title":"Lesson 10: Marbles, Marbles..."},{"location":"unit2/lesson10/#objective","text":"Students will understand that random events vary, so that the percentage predicted by a probability isn't exact, but varies. Students practice converting percentages to proportions.","title":"Objective:"},{"location":"unit2/lesson10/#materials","text":"For each student team: 50 marbles \u2013 20 of one color, and 30 of another color Note: Marbles can be substituted for other materials, such as small blocks, as long as they are the same size.","title":"Materials:"},{"location":"unit2/lesson10/#vocabulary","text":"proportion percentage event with replacement without replacement","title":"Vocabulary:"},{"location":"unit2/lesson10/#essential-concepts","text":"Essential Concepts: There are two ways of sampling data that model real-life sampling situations: with and without replacement. Larger samples tend to be closer to the \"true\" probability.","title":"Essential Concepts:"},{"location":"unit2/lesson10/#lesson","text":"Remind students that, during the previous two lessons, they learned how to estimate probabilities for a population with the help of simulations to create sample data. Both lessons had nice, prepackaged functions already available in RStudio, which made the simulations fairly quick and easy \u2013 in Lesson 8 , the rflip() function was used to simulate flipping a coin; and in Lesson 9 , the roll_die() function was used to simulate rolling one of two dice. But what if we don\u2019t have a nice function to perform a simulation for us? Can we create our own method? Yes! We will actually learn to create a simulation from scratch during Lab 2C . Ask students: If you have a bag of 50 marbles, where 20 of them are blue and 30 of them are red, what is the probability of drawing a red marble? 30/50 or 60%. Select a student to answer the question. Ask the class if they agree or disagree. If they agree, ask them to raise their hand. If there are students who disagree, lead a class discussion until a consensus is reached. Ask students to share their strategies on how to convert the proportion into a percentage . As strategies are being shared, students should take notes in their IDS journals. Review how to turn fractions into percentages, if necessary. Ask students: What if we actually drew out one marble, recorded its color, then replaced it back in the bag, and did this 10 times? Answers will vary by class. Would the percentage of red marbles in this sample necessarily be exactly the same as the probability? Identify that each time a marble is drawn, we are creating an event . Answers will vary by class. Distribute the bags of marbles to each team. Ask each team to: Shake the bag of marbles. Draw one marble out of the bag. Record the marble\u2019s color in their IDS journal. Replace the marble back into the bag. Inform them that this is called sampling with replacement . Ask them to consider what \u201cwith replacement\u201d means and discuss with the class. \u201cWith replacement\u201d means that after you select a marble from the bag, you have to put it back into the bag (replace it) before you select another marble. They should draw 10 marbles from the bag and record the observed colors. Do a Whip Around to find out how many times each team drew a red marble out of their 10 draws. Have them calculate the corresponding sample proportions. For example, if one team drew 7 red marbles out of their 10 draws, their sample proportion is 7/10 = 0.70 (which is the sample as a sample percentage of 70%). Ask students why the proportions are perhaps different from each other and from the \u201ctrue\u201d probability of drawing a red marble? Have the student teams continue drawing marbles, one at a time and with replacement, until they have 50 events recorded. Discuss the following questions: How many times did they draw a red marble out of these 50? Answers will vary by class. What\u2019s the corresponding sample proportion? Is it closer to the true probability than when you only drew 10 marbles? Answers will vary by class. But, they should notice that, as the sample size increases, the sample proportion gets closer to the true population proportion. Engage students in a discussion about how the sample size affects the sample proportion. They should see that as they draw more marbles, their sample probability gets closer and closer to the true probability. If we were to continue drawing marbles forever, in the long run, our sample proportion should equal our true probability. Have students consider what it might mean to sample without replacement . How would they do that with their bag of marbles? \u201cWithout replacement\u201d means that after you select a marble from the bag, you never put it back into the bag (do not replace it). Instead, you simply select another marble from the bag immediately. Students should recognize that, by not replacing the marble each time, the probabilities will change. This means each draw from the marble bag is NOT independent from another draw because removing one marble impacts the next event. Exit Slip : Based on this lesson, ask students to describe a sample, an event, and replacement.","title":"Lesson:"},{"location":"unit2/lesson10/#class-scribes","text":"One team of students will give a brief talk to discuss what they think the 3 most important topics of the day were.","title":"Class Scribes:"},{"location":"unit2/lesson10/#next-day","text":"LAB 2C: Which Song Plays Next? Complete Lab 2C prior to Lesson 11 .","title":"Next Day"},{"location":"unit2/lesson11/","text":"Lesson 11: This AND/OR That Objective: Students will understand how AND/OR probabilities are defined and will be able to use frequency tables to compute these probabilities. Materials: Compound Probabilities handout ( LMR_2.13_Compound Probabilities ) Blue sticky notes Gold sticky notes Four signs on the board labeled: Pickles, Mayonnaise, Both, None (in that exact order, and equally spaced across the length of the board) Vocabulary: compound probabilities Essential Concepts: Essential Concepts: What does \"A or B\" mean versus \"A and B\" mean? These are compound events and two-way tables can be used to calculate probabilities for them. Lesson: Remind the students that they have been learning about estimating probabilities of single events based on sample proportions. Inform them that, today, they will learn how to calculate proportions when multiple events happen. Review the basic idea of computing probabilities; in other words, the number of outcomes we are interested in divided by the total number of outcomes possible. Pose the questions below to the class. Note: They do not need to come up with specific answers; this is a time for them to make suggestions. How would we compute the probability of two outcomes occurring at the same time? For example, what is the probability that a randomly chosen student likes both pickles AND mayonnaise? How would we compute the probability of either of two outcomes occurring? For example, what is the probability that a randomly chosen student likes either pickles OR mayonnaise? For both questions, steer the students towards using the definition from Step 2. That is, we want the students to realize that they can count the number of people that qualify for the given circumstance and divide by the total number of people to calculate a probability. In order to define AND/OR probabilities, students will participate in an activity where they are grouped by their food preferences. Divide the board into 4 groups and write the words \u201cPickles,\u201d \u201cBoth,\u201d \u201cMayonnaise,\u201d and \u201cNone,\u201d in that order, from left to right. Ask for 10 volunteers to stand by the word that represents their preferences. That is, if they only like pickles, they should stand by the word \u201cPickles.\u201d If they like both pickles and mayonnaise, they should stand by the word \u201cBoth.\u201d Note: If all 4 groups do not have at least one student in them, select a few more students to stand at the board. Ask the remaining students (those still seated) to count the total number of people standing by the board and have a student volunteer share the answer with the class. Answers will vary by class. Next, create a 2-way frequency table like the one below to organize the values of student preferences as follows: Counts for students who like both go in the Yes/Like Mayonnaise and Yes/Like Pickles box. Counts for students who like none go in the No/Like Mayonnaise and No/Like Pickles box Counts for students who like only mayonnaise go in the Yes/Like Mayonnaise and No/Like Pickles box. Counts for students who like only pickles go in the No/Like Mayonnaise and Yes/Like Pickles box. Note: A Venn diagram like the one below may be used as well, depending on student understanding and at teacher discretion. Next, ask the students sitting down the following questions: How many students like both pickles AND mayonnaise? Answers will vary by class. What is the probability that a randomly selected student at the board likes both pickles AND mayonnaise? Answers will vary by class. The probability should be calculated by dividing the number of people who are standing under \u201cBoth\u201d (number given in Step 9(a)) by the number of students at the board (number given in Step 8). Now, ask a student from the audience: How many students like pickles? Answers will vary by class. Note: Avoid phrasing the question with \u201cStudents that like ONLY pickles.\u201d Students need to see that students who like \u201cBoth\u201d items also belong to the groups liking the individual items. If students mistakenly report the number of students who like ONLY pickles, ask the people at the board to raise their hands if they like pickles and then ask the mistaken student to recount. What is the probability that a randomly selected student at the board likes pickles? Answers will vary by class. The probability should be calculated by dividing the number of people who are standing under \u201cPickles\u201d and \u201cBoth\u201d by the total number of students at the board. Finally, ask one more student from the audience: How many students like pickles OR mayonnaise? Answers will vary by class. Note: Avoid phrasing the question with \u201cStudents that like ONLY pickles OR ONLY mayonnaise.\u201d If students mistakenly report the number of students who like ONLY pickles plus the students who like ONLY mayonnaise, ask the people at the board to raise their hands if they like either pickles or mayonnaise (All students at the board should raise their hand except for the students who like \u201cNone\u201d) and then ask the mistaken student to recount. What is the probability that a randomly selected student at the board likes pickles OR mayonnaise? Answers will vary by class. The probability should be calculated by dividing the number of people who are standing under \u201cPickles,\u201d \u201cMayonnaise,\u201d and \u201cBoth\u201d by the total number of students at the board. Informs students that AND/OR probabilities are called compound probabilities . In teams, have students record their own definitions of AND/OR probabilities based on the activity they just completed. A compound probability is the probability of some combination of events occurring. Distribute the Compound Probabilities handout ( LMR_2.13 ). LMR_2.13 Pass out a blue sticky note to each student who plays a sport and a gold sticky note to each student who does not play a sport. Draw the table from the worksheet on the board (make it large and legible). Have each student who plays a sport hold up their sticky note. Count them and record the number of students who play a sport in the appropriate row of the TOTAL column in the table. Have each student who does not play a sport hold up their sticky note. Count them and record the number of students who do not play a sport in the appropriate row of the TOTAL column in the table. Ask each student which of the following ice cream flavors they most prefer (each student must choose exactly one option): Vanilla, Chocolate or Rocky Road. Have the students write their ice cream preference on their sticky note. Fill out the remainder of the table by asking each group of students, those who play a sport and those who do not, to hold up their preference. Make sure the totals for preferred ice cream flavors and sports involvement add up to the same number. Instruct the students to work in pairs to answer the questions on the Compound Probabilities handout ( LMR_2.13 ). Class Scribes: One team of students will give a brief talk to discuss what they think the 3 most important topics of the day were. Homework & Next Day If not completed in class, students should finish the Compound Probabilities handout ( LMR_2.13 ). LAB 2D: Queue It Up! Complete Lab 2D prior to the Practicum .","title":"Lesson 11: This AND/OR That"},{"location":"unit2/lesson11/#lesson-11-this-andor-that","text":"","title":"Lesson 11: This AND/OR That"},{"location":"unit2/lesson11/#objective","text":"Students will understand how AND/OR probabilities are defined and will be able to use frequency tables to compute these probabilities.","title":"Objective:"},{"location":"unit2/lesson11/#materials","text":"Compound Probabilities handout ( LMR_2.13_Compound Probabilities ) Blue sticky notes Gold sticky notes Four signs on the board labeled: Pickles, Mayonnaise, Both, None (in that exact order, and equally spaced across the length of the board)","title":"Materials:"},{"location":"unit2/lesson11/#vocabulary","text":"compound probabilities","title":"Vocabulary:"},{"location":"unit2/lesson11/#essential-concepts","text":"Essential Concepts: What does \"A or B\" mean versus \"A and B\" mean? These are compound events and two-way tables can be used to calculate probabilities for them.","title":"Essential Concepts:"},{"location":"unit2/lesson11/#lesson","text":"Remind the students that they have been learning about estimating probabilities of single events based on sample proportions. Inform them that, today, they will learn how to calculate proportions when multiple events happen. Review the basic idea of computing probabilities; in other words, the number of outcomes we are interested in divided by the total number of outcomes possible. Pose the questions below to the class. Note: They do not need to come up with specific answers; this is a time for them to make suggestions. How would we compute the probability of two outcomes occurring at the same time? For example, what is the probability that a randomly chosen student likes both pickles AND mayonnaise? How would we compute the probability of either of two outcomes occurring? For example, what is the probability that a randomly chosen student likes either pickles OR mayonnaise? For both questions, steer the students towards using the definition from Step 2. That is, we want the students to realize that they can count the number of people that qualify for the given circumstance and divide by the total number of people to calculate a probability. In order to define AND/OR probabilities, students will participate in an activity where they are grouped by their food preferences. Divide the board into 4 groups and write the words \u201cPickles,\u201d \u201cBoth,\u201d \u201cMayonnaise,\u201d and \u201cNone,\u201d in that order, from left to right. Ask for 10 volunteers to stand by the word that represents their preferences. That is, if they only like pickles, they should stand by the word \u201cPickles.\u201d If they like both pickles and mayonnaise, they should stand by the word \u201cBoth.\u201d Note: If all 4 groups do not have at least one student in them, select a few more students to stand at the board. Ask the remaining students (those still seated) to count the total number of people standing by the board and have a student volunteer share the answer with the class. Answers will vary by class. Next, create a 2-way frequency table like the one below to organize the values of student preferences as follows: Counts for students who like both go in the Yes/Like Mayonnaise and Yes/Like Pickles box. Counts for students who like none go in the No/Like Mayonnaise and No/Like Pickles box Counts for students who like only mayonnaise go in the Yes/Like Mayonnaise and No/Like Pickles box. Counts for students who like only pickles go in the No/Like Mayonnaise and Yes/Like Pickles box. Note: A Venn diagram like the one below may be used as well, depending on student understanding and at teacher discretion. Next, ask the students sitting down the following questions: How many students like both pickles AND mayonnaise? Answers will vary by class. What is the probability that a randomly selected student at the board likes both pickles AND mayonnaise? Answers will vary by class. The probability should be calculated by dividing the number of people who are standing under \u201cBoth\u201d (number given in Step 9(a)) by the number of students at the board (number given in Step 8). Now, ask a student from the audience: How many students like pickles? Answers will vary by class. Note: Avoid phrasing the question with \u201cStudents that like ONLY pickles.\u201d Students need to see that students who like \u201cBoth\u201d items also belong to the groups liking the individual items. If students mistakenly report the number of students who like ONLY pickles, ask the people at the board to raise their hands if they like pickles and then ask the mistaken student to recount. What is the probability that a randomly selected student at the board likes pickles? Answers will vary by class. The probability should be calculated by dividing the number of people who are standing under \u201cPickles\u201d and \u201cBoth\u201d by the total number of students at the board. Finally, ask one more student from the audience: How many students like pickles OR mayonnaise? Answers will vary by class. Note: Avoid phrasing the question with \u201cStudents that like ONLY pickles OR ONLY mayonnaise.\u201d If students mistakenly report the number of students who like ONLY pickles plus the students who like ONLY mayonnaise, ask the people at the board to raise their hands if they like either pickles or mayonnaise (All students at the board should raise their hand except for the students who like \u201cNone\u201d) and then ask the mistaken student to recount. What is the probability that a randomly selected student at the board likes pickles OR mayonnaise? Answers will vary by class. The probability should be calculated by dividing the number of people who are standing under \u201cPickles,\u201d \u201cMayonnaise,\u201d and \u201cBoth\u201d by the total number of students at the board. Informs students that AND/OR probabilities are called compound probabilities . In teams, have students record their own definitions of AND/OR probabilities based on the activity they just completed. A compound probability is the probability of some combination of events occurring. Distribute the Compound Probabilities handout ( LMR_2.13 ). LMR_2.13 Pass out a blue sticky note to each student who plays a sport and a gold sticky note to each student who does not play a sport. Draw the table from the worksheet on the board (make it large and legible). Have each student who plays a sport hold up their sticky note. Count them and record the number of students who play a sport in the appropriate row of the TOTAL column in the table. Have each student who does not play a sport hold up their sticky note. Count them and record the number of students who do not play a sport in the appropriate row of the TOTAL column in the table. Ask each student which of the following ice cream flavors they most prefer (each student must choose exactly one option): Vanilla, Chocolate or Rocky Road. Have the students write their ice cream preference on their sticky note. Fill out the remainder of the table by asking each group of students, those who play a sport and those who do not, to hold up their preference. Make sure the totals for preferred ice cream flavors and sports involvement add up to the same number. Instruct the students to work in pairs to answer the questions on the Compound Probabilities handout ( LMR_2.13 ).","title":"Lesson:"},{"location":"unit2/lesson11/#class-scribes","text":"One team of students will give a brief talk to discuss what they think the 3 most important topics of the day were.","title":"Class Scribes:"},{"location":"unit2/lesson11/#homework-next-day","text":"If not completed in class, students should finish the Compound Probabilities handout ( LMR_2.13 ). LAB 2D: Queue It Up! Complete Lab 2D prior to the Practicum .","title":"Homework &amp; Next Day"},{"location":"unit2/lesson12/","text":"Lesson 12: Don't Take My Stress Away! Objective: Students will read the Huffington Post article titled Don\u2019t Take My Stress Away to spark their interest about how they spend their time, and will continue to read reports critically to look for claims that may or may not be based on data. Materials: Article: Huffington Post\u2019s Don\u2019t Take My Stress Away found at: http://www.huffingtonpost.com/jack-cahn/dont-take-my-stress-away_b_2090203.html Data collection devices Essential Concepts: Essential Concepts: Generating statistical investigative questions is the first step in a Participatory Sensing campaign. Research and observations help create applicable campaign questions. Lesson: Become familiar with the Stress/Chill Campaign Guidelines (shown at the end of this lesson), particularly the questions, to help guide students during the campaign (see Campaign Guidelines in Teacher Resources ). Ask students the following questions and conduct a brief share out of their responses. Do you know anyone who seems to be always stressed or anyone who seems to be always chilled? Answers will vary by class. What are some observations you have made that make that person extremely stressed or chilled? Answers will vary by class. Inform students that they will be learning about some high school students who view stress as a part of life in the Huffington Post article titled Don\u2019t Take My Stress Away . Provide students the link to the article and allow time for them to read it: http://www.huffingtonpost.com/jack-cahn/dont-take-my-stress-away_b_2090203.html As they read the article, students should note whether they agree or disagree with the authors and should write down their comments and/or reactions to the article in their DS journals. Ask student pairs to share if they agree or disagree with the authors of the article and why. Conduct a Share Out of student responses. Inform students that for this unit, we will be investigating how stressed or chilled they are at certain times of the day. Students will collect data using the Stress/Chill Participatory Sensing campaign. They will add the Stress/Chill campaign to their list of available campaigns either through the UCLA IDS UCLA App or via web browser at https://portal.idsucla.org Ask students to complete their first survey. After students have completed their first survey, use a random number generator to generate two random times a day for the next 6 days (RStudio example given below). It is recommended that you create 6 sets of random numbers so that students are polled at different times each day. Example for RStudio (assuming students are awake between the hours of 7:00 am and 11:00 pm): > sample(7:23, size = 2, replace = FALSE) Note: If a time falls within the school day, it is up to the discretion of the teacher to use this time or not. Based on the times generated, ask students to set reminders on the IDS UCLA App for the next 6 days. Students without a mobile device may set reminders using a method available to them. Focus students\u2019 attention on the Stress/Chill survey questions (you may display the questions on the Campaign Guidelines document). Ask students to generate three statistical investigative questions that could be answered using the Stress/Chill data. Then, ask them to write down in their DS journals some predictions about what they think they will see after they collect some data. Class Scribes: One team of students will give a brief talk to discuss what they think the 3 most important topics of the day were. Homework For the next 6 days, students will collect data for the Stress/Chill campaign either through the IDS UCLA App or via web browser at https://portal.idsucla.org","title":"Lesson 12: Don\u2019t Take My Stress Away!"},{"location":"unit2/lesson12/#lesson-12-dont-take-my-stress-away","text":"","title":"Lesson 12: Don't Take My Stress Away!"},{"location":"unit2/lesson12/#objective","text":"Students will read the Huffington Post article titled Don\u2019t Take My Stress Away to spark their interest about how they spend their time, and will continue to read reports critically to look for claims that may or may not be based on data.","title":"Objective:"},{"location":"unit2/lesson12/#materials","text":"Article: Huffington Post\u2019s Don\u2019t Take My Stress Away found at: http://www.huffingtonpost.com/jack-cahn/dont-take-my-stress-away_b_2090203.html Data collection devices","title":"Materials:"},{"location":"unit2/lesson12/#essential-concepts","text":"Essential Concepts: Generating statistical investigative questions is the first step in a Participatory Sensing campaign. Research and observations help create applicable campaign questions.","title":"Essential Concepts:"},{"location":"unit2/lesson12/#lesson","text":"Become familiar with the Stress/Chill Campaign Guidelines (shown at the end of this lesson), particularly the questions, to help guide students during the campaign (see Campaign Guidelines in Teacher Resources ). Ask students the following questions and conduct a brief share out of their responses. Do you know anyone who seems to be always stressed or anyone who seems to be always chilled? Answers will vary by class. What are some observations you have made that make that person extremely stressed or chilled? Answers will vary by class. Inform students that they will be learning about some high school students who view stress as a part of life in the Huffington Post article titled Don\u2019t Take My Stress Away . Provide students the link to the article and allow time for them to read it: http://www.huffingtonpost.com/jack-cahn/dont-take-my-stress-away_b_2090203.html As they read the article, students should note whether they agree or disagree with the authors and should write down their comments and/or reactions to the article in their DS journals. Ask student pairs to share if they agree or disagree with the authors of the article and why. Conduct a Share Out of student responses. Inform students that for this unit, we will be investigating how stressed or chilled they are at certain times of the day. Students will collect data using the Stress/Chill Participatory Sensing campaign. They will add the Stress/Chill campaign to their list of available campaigns either through the UCLA IDS UCLA App or via web browser at https://portal.idsucla.org Ask students to complete their first survey. After students have completed their first survey, use a random number generator to generate two random times a day for the next 6 days (RStudio example given below). It is recommended that you create 6 sets of random numbers so that students are polled at different times each day. Example for RStudio (assuming students are awake between the hours of 7:00 am and 11:00 pm): > sample(7:23, size = 2, replace = FALSE) Note: If a time falls within the school day, it is up to the discretion of the teacher to use this time or not. Based on the times generated, ask students to set reminders on the IDS UCLA App for the next 6 days. Students without a mobile device may set reminders using a method available to them. Focus students\u2019 attention on the Stress/Chill survey questions (you may display the questions on the Campaign Guidelines document). Ask students to generate three statistical investigative questions that could be answered using the Stress/Chill data. Then, ask them to write down in their DS journals some predictions about what they think they will see after they collect some data.","title":"Lesson:"},{"location":"unit2/lesson12/#class-scribes","text":"One team of students will give a brief talk to discuss what they think the 3 most important topics of the day were.","title":"Class Scribes:"},{"location":"unit2/lesson12/#homework","text":"For the next 6 days, students will collect data for the Stress/Chill campaign either through the IDS UCLA App or via web browser at https://portal.idsucla.org","title":"Homework"},{"location":"unit2/lesson13/","text":"Lesson 13: The Horror Movie Shuffle Objective: Students will understand that, just by chance, we will see differences between two groups. They will understand that these differences are usually small. Specifically, they will learn that we can determine if outcomes are due to chance for categorical variables by calculating differences in the proportions between two groups. Materials: 3\u201d x 5\u201d cards (1 per student) Vocabulary: chance simulations randomness shuffle Essential Concepts: Essential Concepts: We can \"shuffle\" data based on categorical variables. The statistic we use is the difference in proportions. The distribution we form by shuffling represents what happens if chance were the only factor at play. If the actual observed difference in proportions is near the center of this shuffling distribution, then we would conclude that chance is a good explanation for the difference. But if it is extreme (in the tails or off the charts), then we should conclude that chance is NOT to blame. Sometimes, the apparent difference between groups is caused by chance. Lesson: Data Collection Monitoring: Display the IDS Campaign Monitoring Tool, found at https://portal.idsucla.org . Click on Campaign Monitor and sign in. Inform students that you will be monitoring their data collection. Ask: Who has collected the most data so far? See User List and sort by Total. How many active users are there? How many inactive users are there? Click on pie chart. How many responses were submitted yesterday and today? See Total Responses. How many responses have been shared? How many remain private? Click on pie chart. Using TPS, ask students to think about what they can do to increase their data collection. Conduct a discussion about the data that has been collected. Have students recall what they have learned about chance (see Lesson 8 ). Synonyms: possibility, prospect, expectation, unintentional, unplanned. The actual definition of chance is \u201ca possibility of something happening.\u201d To expand on the flow chart from Lesson 9 (chance \u2192 probability \u2192 simulations), explain that we can use simulations to show that sometimes, when we think two groups are different, the difference is really just because of chance, or randomness , and does not mean anything. This brings us back to \u201cchance\u201d in the flow chart. Remind students that a simulation is a model for creating random outcomes. Randomness means that something just happens without a specific order. In pairs, ask students to name situations where two groups could be compared, and then have the students record these situations in their DS journals. Some examples include: \u2022 Men earn more money than women for some work. \u2022 Basketball players are faster runners than baseball players. \u2022 Los Angeles students are smarter than . \u2022 UCLA football players are better athletes than USC football players. \u2022 You and a friend flipped a coin 10 times, and you got more \"heads.\" Then, ask students to write next to each situation whether they think the differences are either real or due to chance because sometimes differences between two groups are real, but sometimes they might just be due to chance, and they will be learning ways to tell the difference. Explain to the class that we are interested in finding out who will survive by the end of a horror movie. Ask the students: Do you think men and women have an equal likelihood of surviving by the end of a horror movie? Answers will vary by class. Have a few students share out their opinions along with their reasoning. Inform the students that they will be pretending to be actors from horror movies during today\u2019s lesson. Explain that data from horror movies (sometimes called slasher films) were collected of 485 characters from 50 films. For each character, 2 variables were recorded: Gender and Survival. The values for Gender were \u201cMale\u201d and \u201cFemale.\u201d The values for Survival were \u201cDies\u201d and \u201cSurvives.\u201d Gender Survival Female Male Dies 172 228 Survives 50 35 Total 222 263 Notice that there were more male characters than female characters and that most characters in slasher films do not survive. From this data, the proportion of survivors was calculated for each gender. In other words, for all female characters, the number of female survivors was divided by the total number of females. Similarly, for all male characters, the number of male survivors was divided by the total number of males. The percent of females who survived by the end of a horror movie was about 23% , and the percent of males who survived by the end of a horror movie was about 13% . Ask the students: Is this what you expected? (Refer back to the discussion from Step 9.) Answers will vary by class. If students thought males would survive more often, then these results would be unexpected. If students thought females would survive more often, then these results would be exactly what they expected. If students thought there was an equal likelihood of survival, these results would also be surprising. What is the difference in the proportions of survival rates between genders? What does this mean in the context of surviving a horror movie? The difference is 23% - 13% = 10%. This means that 10% more women characters survived than men. Is this difference \u201cbig\u201d or \u201csmall\u201d? How can they define what is \u201cbig\u201d and what is \u201csmall?\u201d Answers will vary by class. Upon first glance, it may seem like 10% is a big difference, but we do not know for sure. Explain that they will participate in an activity to determine if the 10% difference seen in the actual data set is big or small. This will help them determine if there really is a difference in survival rates for males versus females, or if the 10% difference was just due to chance. Split the class into two groups, 46% of them on one side of the room and the other 54% on the other side of the room. Tell the smaller group they have been assigned to play female characters in the horror movie (regardless of their gender) and tell the larger group that they have been assigned to play male characters in the horror movie (regardless of their gender). Once those groups have been created, have the class calculate the number of students in each group that would have survived a horror film using the actual proportions given in Step 14. For example: For a class of 30 students: \u2022 46% of 30 (0.46x30) \u2248 14 students representing female characters. \u2022 Of those 14 female characters, 23%, or 3 (0.23x14 \u2248 3), are survivors. \u2022 The remaining 16 students (30 \u2013 14 = 16) represent male characters. \u2022 Of those 16 male characters, 13%, or 2 (0.13x16 \u2248 2), are survivors. Each group should then decide which students will be survivors. Using the 3\u201d x 5\u201d cards, students should write either \u201cdies\u201d or \u201csurvives\u201d on their card. For example (continued from above): Three of the female characters are survivors; so 3 female characters from the group should write \u201csurvives\u201d on their cards. The rest of the group should write \u201cdies\u201d on their cards. Two of the male characters are survivors; so 2 male characters from the group should write \u201csurvives\u201d on their cards. The rest of the group should write \u201cdies\" on their cards. Explain to students that IF there really is no difference between genders in horror films, then the characters who survived would only have done so by chance. In other words, males and females would have an equal likelihood of surviving. Have students discuss the following questions: How many total people in our class are survivors? What is the total proportion of survivors? Answers will vary by class. Using the example above, there would be a total of 5 survivors from the class of 30 students. The proportion of survivors would be 5/30 = 0.17 = 17% How many of the survivors would we expect to be male? How many would we expect to be female? Answers will vary by class. Using the example above, we would expect to see 17% of males and 17% of females survive since that was the overall proportion of survivors. So, we would expect 0.17x16 \u2248 3 male survivors, and 0.17x14 \u2248 2 female survivors. Collect all of the 3\u201d x 5\u201d cards from the students and explain that you are going to shuffle the cards and redistribute them so that their genders have no influence on whether or not they survive the horror movie. Visibly shuffle the survives/dies cards to create a random shuffle. Once the cards have been well-shuffled, pass them back out to the students face down. After all the cards are given out, each group should identify the number of people that are survivors and calculate the corresponding proportion of the survivors. On the board, create a table to display the proportions of survivors for each gender, and include a column for the difference (female survivors \u2013 male survivors). Fill in the table with the values the students found in Step 20. Note: The first row has been filled in with the example data from above BEFORE the shuffles have taken place. Exact numbers were not used so that the proportions would match the actual horror movie data set. # of Female Survivors # of Male Survivors Proportion of Female Survivors Proportion of Male Survivors Difference in Proportions (Female \u2013 Male) 3.22 2.08 3.22/14 = 0.23 2.08/16 = 0.13 0.23 \u2013 0.13 = 0.10 ? Note that values in the \u201cDifference in Proportions\u201d column can be positive or negative because sometimes more women will survive, and other times more men will survive. Draw a dotplot on the board labeled \u201cDifference in Proportions.\u201d Include a vertical line at 10% to represent the actual difference in gender survival rates in real horror movies (see example below). Using the information from Steps 20 and 21, place a dot at the corresponding value for the shuffled data\u2019s difference in proportions. Ask the students: How does this difference compare to the actual data set\u2019s difference of 10%? Answers will vary by class. Most likely, the difference in proportions will be much smaller than 10%. In fact, the difference in proportions will be centered around 0. Repeat Steps 19 \u2013 24 a few more times (depending on how much class time you have available). Ask the students to record their responses to the following questions: What was the biggest difference we saw from our shuffles? What was the smallest? Answers will vary by class. What do you think this dotplot would look like if we shuffled our survival cards 1000 times? The dotplot would look roughly symmetric and centered around 0, meaning that if there were no relationship between a character\u2019s gender and whether or not they survive, the difference in proportions would typically be 0. Have a discussion about how the actual difference in gender survival (10%) is rarely seen when we assign \u201csurvives\u201d or \u201cdies\u201d just by chance (aka when shuffling). What does this mean in terms of who will die in actual horror movies? Since we never (or rarely) saw a 10% difference in the proportions of female survivors versus male survivors, it seems that horror movies actually favor female survivors. Ask the students: If you were going to be cast in a horror movie, would you want to be a male character or a female character? You would want to be a female character because they are more likely to survive by the end of the film. Inform the students that they will learn how to shuffle in RStudio in order to determine if an event is real or simply due to chance. Class Scribes: One team of students will give a brief talk to discuss what they think the 3 most important topics of the day were. Homework & Next Day For the next 5 days, students will collect data for the Stress/Chill campaign either through the UCLA IDS UCLA App or via web browser at https://portal.idsucla.org LAB 2E: The Horror Movie Shuffle Complete Lab 2E prior to Lesson 14 .","title":"Lesson 13: The Horror Movie Shuffle"},{"location":"unit2/lesson13/#lesson-13-the-horror-movie-shuffle","text":"","title":"Lesson 13: The Horror Movie Shuffle"},{"location":"unit2/lesson13/#objective","text":"Students will understand that, just by chance, we will see differences between two groups. They will understand that these differences are usually small. Specifically, they will learn that we can determine if outcomes are due to chance for categorical variables by calculating differences in the proportions between two groups.","title":"Objective:"},{"location":"unit2/lesson13/#materials","text":"3\u201d x 5\u201d cards (1 per student)","title":"Materials:"},{"location":"unit2/lesson13/#vocabulary","text":"chance simulations randomness shuffle","title":"Vocabulary:"},{"location":"unit2/lesson13/#essential-concepts","text":"Essential Concepts: We can \"shuffle\" data based on categorical variables. The statistic we use is the difference in proportions. The distribution we form by shuffling represents what happens if chance were the only factor at play. If the actual observed difference in proportions is near the center of this shuffling distribution, then we would conclude that chance is a good explanation for the difference. But if it is extreme (in the tails or off the charts), then we should conclude that chance is NOT to blame. Sometimes, the apparent difference between groups is caused by chance.","title":"Essential Concepts:"},{"location":"unit2/lesson13/#lesson","text":"Data Collection Monitoring: Display the IDS Campaign Monitoring Tool, found at https://portal.idsucla.org . Click on Campaign Monitor and sign in. Inform students that you will be monitoring their data collection. Ask: Who has collected the most data so far? See User List and sort by Total. How many active users are there? How many inactive users are there? Click on pie chart. How many responses were submitted yesterday and today? See Total Responses. How many responses have been shared? How many remain private? Click on pie chart. Using TPS, ask students to think about what they can do to increase their data collection. Conduct a discussion about the data that has been collected. Have students recall what they have learned about chance (see Lesson 8 ). Synonyms: possibility, prospect, expectation, unintentional, unplanned. The actual definition of chance is \u201ca possibility of something happening.\u201d To expand on the flow chart from Lesson 9 (chance \u2192 probability \u2192 simulations), explain that we can use simulations to show that sometimes, when we think two groups are different, the difference is really just because of chance, or randomness , and does not mean anything. This brings us back to \u201cchance\u201d in the flow chart. Remind students that a simulation is a model for creating random outcomes. Randomness means that something just happens without a specific order. In pairs, ask students to name situations where two groups could be compared, and then have the students record these situations in their DS journals. Some examples include: \u2022 Men earn more money than women for some work. \u2022 Basketball players are faster runners than baseball players. \u2022 Los Angeles students are smarter than . \u2022 UCLA football players are better athletes than USC football players. \u2022 You and a friend flipped a coin 10 times, and you got more \"heads.\" Then, ask students to write next to each situation whether they think the differences are either real or due to chance because sometimes differences between two groups are real, but sometimes they might just be due to chance, and they will be learning ways to tell the difference. Explain to the class that we are interested in finding out who will survive by the end of a horror movie. Ask the students: Do you think men and women have an equal likelihood of surviving by the end of a horror movie? Answers will vary by class. Have a few students share out their opinions along with their reasoning. Inform the students that they will be pretending to be actors from horror movies during today\u2019s lesson. Explain that data from horror movies (sometimes called slasher films) were collected of 485 characters from 50 films. For each character, 2 variables were recorded: Gender and Survival. The values for Gender were \u201cMale\u201d and \u201cFemale.\u201d The values for Survival were \u201cDies\u201d and \u201cSurvives.\u201d Gender Survival Female Male Dies 172 228 Survives 50 35 Total 222 263 Notice that there were more male characters than female characters and that most characters in slasher films do not survive. From this data, the proportion of survivors was calculated for each gender. In other words, for all female characters, the number of female survivors was divided by the total number of females. Similarly, for all male characters, the number of male survivors was divided by the total number of males. The percent of females who survived by the end of a horror movie was about 23% , and the percent of males who survived by the end of a horror movie was about 13% . Ask the students: Is this what you expected? (Refer back to the discussion from Step 9.) Answers will vary by class. If students thought males would survive more often, then these results would be unexpected. If students thought females would survive more often, then these results would be exactly what they expected. If students thought there was an equal likelihood of survival, these results would also be surprising. What is the difference in the proportions of survival rates between genders? What does this mean in the context of surviving a horror movie? The difference is 23% - 13% = 10%. This means that 10% more women characters survived than men. Is this difference \u201cbig\u201d or \u201csmall\u201d? How can they define what is \u201cbig\u201d and what is \u201csmall?\u201d Answers will vary by class. Upon first glance, it may seem like 10% is a big difference, but we do not know for sure. Explain that they will participate in an activity to determine if the 10% difference seen in the actual data set is big or small. This will help them determine if there really is a difference in survival rates for males versus females, or if the 10% difference was just due to chance. Split the class into two groups, 46% of them on one side of the room and the other 54% on the other side of the room. Tell the smaller group they have been assigned to play female characters in the horror movie (regardless of their gender) and tell the larger group that they have been assigned to play male characters in the horror movie (regardless of their gender). Once those groups have been created, have the class calculate the number of students in each group that would have survived a horror film using the actual proportions given in Step 14. For example: For a class of 30 students: \u2022 46% of 30 (0.46x30) \u2248 14 students representing female characters. \u2022 Of those 14 female characters, 23%, or 3 (0.23x14 \u2248 3), are survivors. \u2022 The remaining 16 students (30 \u2013 14 = 16) represent male characters. \u2022 Of those 16 male characters, 13%, or 2 (0.13x16 \u2248 2), are survivors. Each group should then decide which students will be survivors. Using the 3\u201d x 5\u201d cards, students should write either \u201cdies\u201d or \u201csurvives\u201d on their card. For example (continued from above): Three of the female characters are survivors; so 3 female characters from the group should write \u201csurvives\u201d on their cards. The rest of the group should write \u201cdies\u201d on their cards. Two of the male characters are survivors; so 2 male characters from the group should write \u201csurvives\u201d on their cards. The rest of the group should write \u201cdies\" on their cards. Explain to students that IF there really is no difference between genders in horror films, then the characters who survived would only have done so by chance. In other words, males and females would have an equal likelihood of surviving. Have students discuss the following questions: How many total people in our class are survivors? What is the total proportion of survivors? Answers will vary by class. Using the example above, there would be a total of 5 survivors from the class of 30 students. The proportion of survivors would be 5/30 = 0.17 = 17% How many of the survivors would we expect to be male? How many would we expect to be female? Answers will vary by class. Using the example above, we would expect to see 17% of males and 17% of females survive since that was the overall proportion of survivors. So, we would expect 0.17x16 \u2248 3 male survivors, and 0.17x14 \u2248 2 female survivors. Collect all of the 3\u201d x 5\u201d cards from the students and explain that you are going to shuffle the cards and redistribute them so that their genders have no influence on whether or not they survive the horror movie. Visibly shuffle the survives/dies cards to create a random shuffle. Once the cards have been well-shuffled, pass them back out to the students face down. After all the cards are given out, each group should identify the number of people that are survivors and calculate the corresponding proportion of the survivors. On the board, create a table to display the proportions of survivors for each gender, and include a column for the difference (female survivors \u2013 male survivors). Fill in the table with the values the students found in Step 20. Note: The first row has been filled in with the example data from above BEFORE the shuffles have taken place. Exact numbers were not used so that the proportions would match the actual horror movie data set. # of Female Survivors # of Male Survivors Proportion of Female Survivors Proportion of Male Survivors Difference in Proportions (Female \u2013 Male) 3.22 2.08 3.22/14 = 0.23 2.08/16 = 0.13 0.23 \u2013 0.13 = 0.10 ? Note that values in the \u201cDifference in Proportions\u201d column can be positive or negative because sometimes more women will survive, and other times more men will survive. Draw a dotplot on the board labeled \u201cDifference in Proportions.\u201d Include a vertical line at 10% to represent the actual difference in gender survival rates in real horror movies (see example below). Using the information from Steps 20 and 21, place a dot at the corresponding value for the shuffled data\u2019s difference in proportions. Ask the students: How does this difference compare to the actual data set\u2019s difference of 10%? Answers will vary by class. Most likely, the difference in proportions will be much smaller than 10%. In fact, the difference in proportions will be centered around 0. Repeat Steps 19 \u2013 24 a few more times (depending on how much class time you have available). Ask the students to record their responses to the following questions: What was the biggest difference we saw from our shuffles? What was the smallest? Answers will vary by class. What do you think this dotplot would look like if we shuffled our survival cards 1000 times? The dotplot would look roughly symmetric and centered around 0, meaning that if there were no relationship between a character\u2019s gender and whether or not they survive, the difference in proportions would typically be 0. Have a discussion about how the actual difference in gender survival (10%) is rarely seen when we assign \u201csurvives\u201d or \u201cdies\u201d just by chance (aka when shuffling). What does this mean in terms of who will die in actual horror movies? Since we never (or rarely) saw a 10% difference in the proportions of female survivors versus male survivors, it seems that horror movies actually favor female survivors. Ask the students: If you were going to be cast in a horror movie, would you want to be a male character or a female character? You would want to be a female character because they are more likely to survive by the end of the film. Inform the students that they will learn how to shuffle in RStudio in order to determine if an event is real or simply due to chance.","title":"Lesson:"},{"location":"unit2/lesson13/#class-scribes","text":"One team of students will give a brief talk to discuss what they think the 3 most important topics of the day were.","title":"Class Scribes:"},{"location":"unit2/lesson13/#homework-next-day","text":"For the next 5 days, students will collect data for the Stress/Chill campaign either through the UCLA IDS UCLA App or via web browser at https://portal.idsucla.org LAB 2E: The Horror Movie Shuffle Complete Lab 2E prior to Lesson 14 .","title":"Homework &amp; Next Day"},{"location":"unit2/lesson14/","text":"Lesson 14: The Titanic Shuffle Objective: Students will continue to understand that, just by chance, we will see differences between two groups. They will understand that these differences are usually small. Materials: LMR_Titanic Strips Advanced preparation required (see Step 8 of lesson) Poster paper Markers Essential Concepts: Essential Concepts: We can also \"shuffle\" data based on numerical variables. The statistic we use is the difference in medians. The distribution we form by this form of shuffling still represents what happens if chance were the only factor at play. When differences are small, we suspect that they might be due to chance. When differences are big, we suspect they might be 'real'. Lesson: Remind students that they previously learned how to determine if a difference is due to chance by shuffling based on categorical variables (gender and survival). Display the dotplot created during Lesson 13 of the difference in proportions between female and survivors of horror movies. Remind the students that, \"by chance,\" the differences were typically zero . Most of the time, they were pretty small. Sometimes they were bigger, but that was rare and this tells us that if we see \"small\" differences, we might think they are due to chance. But if we see \"big\" differences, they are not. Lead a short discussion about what students think small and big differences mean. Make sure they answer in units (which are percentage points for the horror movie data). So, for example, a \"big\" difference might be 5 percentage points (but don't let them just say \"5\"). Inform students that, during today\u2019s lesson, they will learn how to determine if there is a difference between groups when a numerical variable is involved. In particular, they will assume the roles of passengers in the Titanic for today\u2019s lesson. In case some students may not know about the Titanic , ask a volunteer to share what he/she knows. Explain that, at its time, the Titanic was the largest cruise ship ever built and was declared to be unsinkable. However, on its first voyage, it sank and was one of the worst maritime disasters in history. About 40% of passengers survived; however, your chances of survival depended very much on your age, gender, and wealth. Inform the students that we are going to look at whether the amount of money a passenger paid for his/her cabin (the fare price) had anything to do with whether or not he/she survived. Each student will need a strip from the LMR_Titanic Strips file\u2014see below for instructions. Advanced preparation required: The Titanic Strips LMR contains data from 40 actual passengers on the titanic. Each strip represents the data from one passenger: the left hand side shows the fare paid and right hand side contains the survival information of that passenger after the collision. Cut the LMR into strips such that the fare price is attached to the survivor status for each of the 40 observations. LMR_Titanic Strips 40 strips were created for large classes. If your class has less than 40 students, assign the students to two groups such that roughly 40% of them are in the survivor group (15/40 = 37.5% \u2248 40%), and the rest are in the victim group. If your class is small (smaller than 10), then put the students in two equal sized groups. The split does not have to be exactly 40%, as the actual value from our data is 38.8%. Inform the smaller group that they are the survivors and distribute a survivor strip with its corresponding fare to each student. Set aside any leftover strips. Tell them that the price on the strip represents the amount of money paid for their ticket to board the Titanic . Notify them that \u00a320 in 1912 is worth about \u00a32952 today. Divulge to the larger group that they, unfortunately, are the victims and distribute a victim strip with its corresponding fare to each student. Set aside any leftover strips. Ask each group to create a dotplot of their fare prices on a poster. Lead a quick discussion comparing the two dotplots visually. Then, ask each group to calculate the median fare for their group. As a class, find the difference between the median fares for the two groups. median of \u201cSurvivor\u201d fares \u2013 median of \u201cVictim\u201d fares For example: If all 15 survivor cards and all 25 victim cards are used, the difference is medians would be: \u00a326.00 \u2013 \u00a313.00 = \u00a313.00 Explain that one of the controversies of the Titanic disaster was that some people felt that the rich people were given better access to the lifeboats than were the poor, so rich people were more likely to survive. Note that the data represented on the fare cards are only a subset of the actual Titanic data, which had over 800 passengers. However, the data were randomly selected from the real data and are considered representative of the 800 passengers. In pairs, ask students to discuss the following: Based on the data from our dotplots, do you think rich people were more likely to survive? In other words, did passengers who paid more for their tickets have a better chance of survival? Yes, there is evidence that rich passengers survived more often than poorer passengers. The median difference between the fare prices of the survivors and the victims is \u00a313.00 (see Step 13). Most survivors had higher fare prices than the victims, so the distribution of survivor fares is shifted to the right and is more right-skewed. Share out a couple of responses with the whole class. Have students tear their strip such that they separate the fare from the outcome (survivor or victim). Collect only the outcomes and randomly shuffle them. Students will keep their fare. Distribute the shuffled outcome strips face down to the students. Once everyone has a new outcome strip, ask students to turn their outcome strip over and re-group based on their new survival status. Ask the students: Why do we shuffle the survivor/victim strips and not the fare strips? We want to know if the price someone paid for his/her ticket affects whether or not he/she survived. So, when we shuffle, we assume that fare price has nothing to do with survival, so the prices should be irrelevant. What do you think the median fare difference of our shuffled groups will be? The median fare difference of the shuffled groups should be close to 0, meaning that there should be NO difference in fare price for the survivors and the victims. Everyone would have the same chances of surviving, regardless of their ticket price. Have each group calculate the median fare price for their new groups. Then, ask: Do you think this difference, of ___ dollars, is real or due to chance? Answers will vary by class. Since the data were shuffled, any difference should be due to chance. On the board, create a table to display the median fare prices for each group, and include a column for the difference (median \u201cSurvivor\u201d fare \u2013 median \u201cVictim\u201d fare). Fill in the table with the values the students found in Step 13. Note: The first row has been filled in with the example data from above BEFORE the shuffles have taken place. Median Fare Price of Survivors Median Fare Price of Victims Difference in Medians (Survivors - Victims) \u00a326.00 \u00a313.00 \u00a326.00 - \u00a313.00 = \u00a313.00 ? Note that values in the \u201cDifference in Medians\u201d column can be positive or negative because sometimes the survivors will pay more for their tickets, and other times the victims will pay more for their tickets. Draw a dotplot on the board labeled \u201cDifference in Medians.\u201d Include a vertical line at \u00a313.00 (or whatever value was calculated in Step 13 by the class) to represent the actual difference in the median fare prices between the survivors and the victims (see example below). Using the information from Steps 19 and 20, place a dot at the corresponding value for the shuffled data\u2019s difference in medians. Ask the students: How does this difference compare to the actual difference of \u00a313.00 (from Step 13)? Answers will vary by class. Most likely, the difference in medians will be much smaller than \u00a313.00. In fact, the difference in medians will be centered around 0. Remind students that small differences might be due to chance and big differences typically mean that there is a \u201creal\u201d difference between groups. In this case, a big difference might mean that the rich passengers were more likely to survive. And a small difference might mean that survival was just a matter of plain luck. Repeat Steps 17 \u2013 23 a few more times (depending on how much class time you have available). In pairs, ask students to discuss whether they think the real difference in median fare prices they calculated in Step 13 (\u00a313.00 if all cards were used) is small or large. Answers will vary by class. Guide students to look at the MAD value of the distribution of differences in median fares. Explain that one way that we can decide what is \u201clarge\u201d or \u201csmall\u201d is by creating cut-off values that we think are too far away from the center of the distributions of differences. In general, we can assign a rule that states that any difference in median fare prices that is greater than 2 MAD values above or below the median is considered unusual. This means that any value in the outer edges of the plot would indicate that a passenger\u2019s ticket price impacted his/her chances of survival. Inform students that they will use RStudio to shuffle the actual Titanic data of all 800 passengers during the next class and can decide if the difference in survival rates of rich passengers and poor passengers was real, or just due to chance. Class Scribes: One team of students will give a brief talk to discuss what they think the 3 most important topics of the day were. Homework & Next Day For the next 3 days, students will collect data for the Stress/Chill campaign either through the UCLA IDS UCLA App or via web browser at https://portal.idsucla.org LAB 2F: The Titanic Shuffle Complete Lab 2F prior to Lesson 15 .","title":"Lesson 14: The Titanic Shuffle"},{"location":"unit2/lesson14/#lesson-14-the-titanic-shuffle","text":"","title":"Lesson 14: The Titanic Shuffle"},{"location":"unit2/lesson14/#objective","text":"Students will continue to understand that, just by chance, we will see differences between two groups. They will understand that these differences are usually small.","title":"Objective:"},{"location":"unit2/lesson14/#materials","text":"LMR_Titanic Strips Advanced preparation required (see Step 8 of lesson) Poster paper Markers","title":"Materials:"},{"location":"unit2/lesson14/#essential-concepts","text":"Essential Concepts: We can also \"shuffle\" data based on numerical variables. The statistic we use is the difference in medians. The distribution we form by this form of shuffling still represents what happens if chance were the only factor at play. When differences are small, we suspect that they might be due to chance. When differences are big, we suspect they might be 'real'.","title":"Essential Concepts:"},{"location":"unit2/lesson14/#lesson","text":"Remind students that they previously learned how to determine if a difference is due to chance by shuffling based on categorical variables (gender and survival). Display the dotplot created during Lesson 13 of the difference in proportions between female and survivors of horror movies. Remind the students that, \"by chance,\" the differences were typically zero . Most of the time, they were pretty small. Sometimes they were bigger, but that was rare and this tells us that if we see \"small\" differences, we might think they are due to chance. But if we see \"big\" differences, they are not. Lead a short discussion about what students think small and big differences mean. Make sure they answer in units (which are percentage points for the horror movie data). So, for example, a \"big\" difference might be 5 percentage points (but don't let them just say \"5\"). Inform students that, during today\u2019s lesson, they will learn how to determine if there is a difference between groups when a numerical variable is involved. In particular, they will assume the roles of passengers in the Titanic for today\u2019s lesson. In case some students may not know about the Titanic , ask a volunteer to share what he/she knows. Explain that, at its time, the Titanic was the largest cruise ship ever built and was declared to be unsinkable. However, on its first voyage, it sank and was one of the worst maritime disasters in history. About 40% of passengers survived; however, your chances of survival depended very much on your age, gender, and wealth. Inform the students that we are going to look at whether the amount of money a passenger paid for his/her cabin (the fare price) had anything to do with whether or not he/she survived. Each student will need a strip from the LMR_Titanic Strips file\u2014see below for instructions. Advanced preparation required: The Titanic Strips LMR contains data from 40 actual passengers on the titanic. Each strip represents the data from one passenger: the left hand side shows the fare paid and right hand side contains the survival information of that passenger after the collision. Cut the LMR into strips such that the fare price is attached to the survivor status for each of the 40 observations. LMR_Titanic Strips 40 strips were created for large classes. If your class has less than 40 students, assign the students to two groups such that roughly 40% of them are in the survivor group (15/40 = 37.5% \u2248 40%), and the rest are in the victim group. If your class is small (smaller than 10), then put the students in two equal sized groups. The split does not have to be exactly 40%, as the actual value from our data is 38.8%. Inform the smaller group that they are the survivors and distribute a survivor strip with its corresponding fare to each student. Set aside any leftover strips. Tell them that the price on the strip represents the amount of money paid for their ticket to board the Titanic . Notify them that \u00a320 in 1912 is worth about \u00a32952 today. Divulge to the larger group that they, unfortunately, are the victims and distribute a victim strip with its corresponding fare to each student. Set aside any leftover strips. Ask each group to create a dotplot of their fare prices on a poster. Lead a quick discussion comparing the two dotplots visually. Then, ask each group to calculate the median fare for their group. As a class, find the difference between the median fares for the two groups. median of \u201cSurvivor\u201d fares \u2013 median of \u201cVictim\u201d fares For example: If all 15 survivor cards and all 25 victim cards are used, the difference is medians would be: \u00a326.00 \u2013 \u00a313.00 = \u00a313.00 Explain that one of the controversies of the Titanic disaster was that some people felt that the rich people were given better access to the lifeboats than were the poor, so rich people were more likely to survive. Note that the data represented on the fare cards are only a subset of the actual Titanic data, which had over 800 passengers. However, the data were randomly selected from the real data and are considered representative of the 800 passengers. In pairs, ask students to discuss the following: Based on the data from our dotplots, do you think rich people were more likely to survive? In other words, did passengers who paid more for their tickets have a better chance of survival? Yes, there is evidence that rich passengers survived more often than poorer passengers. The median difference between the fare prices of the survivors and the victims is \u00a313.00 (see Step 13). Most survivors had higher fare prices than the victims, so the distribution of survivor fares is shifted to the right and is more right-skewed. Share out a couple of responses with the whole class. Have students tear their strip such that they separate the fare from the outcome (survivor or victim). Collect only the outcomes and randomly shuffle them. Students will keep their fare. Distribute the shuffled outcome strips face down to the students. Once everyone has a new outcome strip, ask students to turn their outcome strip over and re-group based on their new survival status. Ask the students: Why do we shuffle the survivor/victim strips and not the fare strips? We want to know if the price someone paid for his/her ticket affects whether or not he/she survived. So, when we shuffle, we assume that fare price has nothing to do with survival, so the prices should be irrelevant. What do you think the median fare difference of our shuffled groups will be? The median fare difference of the shuffled groups should be close to 0, meaning that there should be NO difference in fare price for the survivors and the victims. Everyone would have the same chances of surviving, regardless of their ticket price. Have each group calculate the median fare price for their new groups. Then, ask: Do you think this difference, of ___ dollars, is real or due to chance? Answers will vary by class. Since the data were shuffled, any difference should be due to chance. On the board, create a table to display the median fare prices for each group, and include a column for the difference (median \u201cSurvivor\u201d fare \u2013 median \u201cVictim\u201d fare). Fill in the table with the values the students found in Step 13. Note: The first row has been filled in with the example data from above BEFORE the shuffles have taken place. Median Fare Price of Survivors Median Fare Price of Victims Difference in Medians (Survivors - Victims) \u00a326.00 \u00a313.00 \u00a326.00 - \u00a313.00 = \u00a313.00 ? Note that values in the \u201cDifference in Medians\u201d column can be positive or negative because sometimes the survivors will pay more for their tickets, and other times the victims will pay more for their tickets. Draw a dotplot on the board labeled \u201cDifference in Medians.\u201d Include a vertical line at \u00a313.00 (or whatever value was calculated in Step 13 by the class) to represent the actual difference in the median fare prices between the survivors and the victims (see example below). Using the information from Steps 19 and 20, place a dot at the corresponding value for the shuffled data\u2019s difference in medians. Ask the students: How does this difference compare to the actual difference of \u00a313.00 (from Step 13)? Answers will vary by class. Most likely, the difference in medians will be much smaller than \u00a313.00. In fact, the difference in medians will be centered around 0. Remind students that small differences might be due to chance and big differences typically mean that there is a \u201creal\u201d difference between groups. In this case, a big difference might mean that the rich passengers were more likely to survive. And a small difference might mean that survival was just a matter of plain luck. Repeat Steps 17 \u2013 23 a few more times (depending on how much class time you have available). In pairs, ask students to discuss whether they think the real difference in median fare prices they calculated in Step 13 (\u00a313.00 if all cards were used) is small or large. Answers will vary by class. Guide students to look at the MAD value of the distribution of differences in median fares. Explain that one way that we can decide what is \u201clarge\u201d or \u201csmall\u201d is by creating cut-off values that we think are too far away from the center of the distributions of differences. In general, we can assign a rule that states that any difference in median fare prices that is greater than 2 MAD values above or below the median is considered unusual. This means that any value in the outer edges of the plot would indicate that a passenger\u2019s ticket price impacted his/her chances of survival. Inform students that they will use RStudio to shuffle the actual Titanic data of all 800 passengers during the next class and can decide if the difference in survival rates of rich passengers and poor passengers was real, or just due to chance.","title":"Lesson:"},{"location":"unit2/lesson14/#class-scribes","text":"One team of students will give a brief talk to discuss what they think the 3 most important topics of the day were.","title":"Class Scribes:"},{"location":"unit2/lesson14/#homework-next-day","text":"For the next 3 days, students will collect data for the Stress/Chill campaign either through the UCLA IDS UCLA App or via web browser at https://portal.idsucla.org LAB 2F: The Titanic Shuffle Complete Lab 2F prior to Lesson 15 .","title":"Homework &amp; Next Day"},{"location":"unit2/lesson15/","text":"Lesson 15: Tangible Data Merging Objective: Students will learn how to merge two datasets and ask statistical investigative questions about the merged data. Materials: Tangible Data Merging file ( LMR_2.14_Tangible Data Merging ) Advanced preparation required (see Step 4 of lesson) Copy paper in two colors Advanced preparation required (see Step 4 of lesson) Vocabulary: merge Essential Concepts: Essential Concepts: We can enhance the context of a statistical problem by merging related datasets together. To merge data, each dataset must have a \"unique identifier\" that tells us how to match up the lines of the data. Lesson: Inform students that they are going to examine the research question \"Does the personality color test really work?\" To answer this, we're going to examine whether the different color groups actually differ on particular beliefs or attitudes, or if these differences might just be due to chance. In particular, we are going to use the Stress/Chill data to see if there is evidence that the \"colors\" actually differ. Show students the variables in each of these datasets. Give students time to brainstorm statistical investigative questions of interest with their teams and record their questions in their DS journals. Encourage them to think of two- and three-variable questions. Conduct a share out of some of the questions students came up with. Examples include: (1) Do people whose predominant color is Gold tend to stress more than people whose predominant color is Blue? (2) Is there a difference between the sorts of things that stress out the different personality colors? In order to answer the above questions, we will need to merge our class\u2019s 2 datasets together ( Personality Color and Stress/Chill ). In order to do this, we will be practicing how to merge datasets today. Print out the material from the Tangible Data Merging file ( LMR_2.14 ). Use a different color of paper for each of the two datasets. For example, Data Set 1 could be on plain white paper and Data Set 2 could be on blue paper. Cut the paper by creating horizontal strips of each observation of data. For example, from the screenshot below, of the first page of Data Set 1, you would create 12 different strips of paper, one for each observation. LMR_2.14 Hand each student in the class a strip of paper. Ask them to try to find someone with the other dataset (i.e., a person with a different colored strip of paper) that they can \u201cmatch up,\u201d or merge , with. For example, a student with the first row of data listed below from Data Set 1 might want to match up with the second row of data listed below from Data Set 2 because a person who is 21 has probably graduated high school. Birth Month Zip Code Age ID Number Favorite Movie January 90064 21 1742 The Notebook Zip Code ID Number Birth Month Siblings Education 91331 1352 August 2 High School However, they should notice that they cannot just make guesses about a person\u2019s characteristics in order to match up the data. They should realize that only 3 of the variables are the same in both datasets: Birth Month , Zip Code , and ID Number . Since multiple people have the same Birth Month , discuss why this may not be the best variable to merge with. Multiple people are born in January, so we would have no way of differentiating between those people. The same is true for the Zip Codes variable. Although there are less repeats with Zip Codes , we still see some overlap between observations. So, the only UNIQUE identifier in both data sets is ID Number . So the students should end up in pairs at the end of the exercise \u2013 a student from Data Set 1 is matched with the student from Data Set 2 that has the same ID Number . Have the students write about the experience of tangible data merging in their DS journals and ask: Why is it important to have at least one unique identifier for both datasets? It is the only way to know which information belongs to which person. We want to make sure we do not match up observations (in this case, people) incorrectly because that will compromise any analysis we do later. Inform students that they will learn to merge datasets using RStudio during the next lab. Class Scribes: One team of students will give a brief talk to discuss what they think the 3 most important topics of the day were. Homework & Next Day Students will collect data for one more day for the Stress/Chill campaign either through the UCLA IDS UCLA App or via web browser at https://portal.idsucla.org LAB 2G: Getting it Together Complete Lab 2G prior to the Practicum .","title":"Lesson 15: Tangible Data Merging"},{"location":"unit2/lesson15/#lesson-15-tangible-data-merging","text":"","title":"Lesson 15: Tangible Data Merging"},{"location":"unit2/lesson15/#objective","text":"Students will learn how to merge two datasets and ask statistical investigative questions about the merged data.","title":"Objective:"},{"location":"unit2/lesson15/#materials","text":"Tangible Data Merging file ( LMR_2.14_Tangible Data Merging ) Advanced preparation required (see Step 4 of lesson) Copy paper in two colors Advanced preparation required (see Step 4 of lesson)","title":"Materials:"},{"location":"unit2/lesson15/#vocabulary","text":"merge","title":"Vocabulary:"},{"location":"unit2/lesson15/#essential-concepts","text":"Essential Concepts: We can enhance the context of a statistical problem by merging related datasets together. To merge data, each dataset must have a \"unique identifier\" that tells us how to match up the lines of the data.","title":"Essential Concepts:"},{"location":"unit2/lesson15/#lesson","text":"Inform students that they are going to examine the research question \"Does the personality color test really work?\" To answer this, we're going to examine whether the different color groups actually differ on particular beliefs or attitudes, or if these differences might just be due to chance. In particular, we are going to use the Stress/Chill data to see if there is evidence that the \"colors\" actually differ. Show students the variables in each of these datasets. Give students time to brainstorm statistical investigative questions of interest with their teams and record their questions in their DS journals. Encourage them to think of two- and three-variable questions. Conduct a share out of some of the questions students came up with. Examples include: (1) Do people whose predominant color is Gold tend to stress more than people whose predominant color is Blue? (2) Is there a difference between the sorts of things that stress out the different personality colors? In order to answer the above questions, we will need to merge our class\u2019s 2 datasets together ( Personality Color and Stress/Chill ). In order to do this, we will be practicing how to merge datasets today. Print out the material from the Tangible Data Merging file ( LMR_2.14 ). Use a different color of paper for each of the two datasets. For example, Data Set 1 could be on plain white paper and Data Set 2 could be on blue paper. Cut the paper by creating horizontal strips of each observation of data. For example, from the screenshot below, of the first page of Data Set 1, you would create 12 different strips of paper, one for each observation. LMR_2.14 Hand each student in the class a strip of paper. Ask them to try to find someone with the other dataset (i.e., a person with a different colored strip of paper) that they can \u201cmatch up,\u201d or merge , with. For example, a student with the first row of data listed below from Data Set 1 might want to match up with the second row of data listed below from Data Set 2 because a person who is 21 has probably graduated high school. Birth Month Zip Code Age ID Number Favorite Movie January 90064 21 1742 The Notebook Zip Code ID Number Birth Month Siblings Education 91331 1352 August 2 High School However, they should notice that they cannot just make guesses about a person\u2019s characteristics in order to match up the data. They should realize that only 3 of the variables are the same in both datasets: Birth Month , Zip Code , and ID Number . Since multiple people have the same Birth Month , discuss why this may not be the best variable to merge with. Multiple people are born in January, so we would have no way of differentiating between those people. The same is true for the Zip Codes variable. Although there are less repeats with Zip Codes , we still see some overlap between observations. So, the only UNIQUE identifier in both data sets is ID Number . So the students should end up in pairs at the end of the exercise \u2013 a student from Data Set 1 is matched with the student from Data Set 2 that has the same ID Number . Have the students write about the experience of tangible data merging in their DS journals and ask: Why is it important to have at least one unique identifier for both datasets? It is the only way to know which information belongs to which person. We want to make sure we do not match up observations (in this case, people) incorrectly because that will compromise any analysis we do later. Inform students that they will learn to merge datasets using RStudio during the next lab.","title":"Lesson:"},{"location":"unit2/lesson15/#class-scribes","text":"One team of students will give a brief talk to discuss what they think the 3 most important topics of the day were.","title":"Class Scribes:"},{"location":"unit2/lesson15/#homework-next-day","text":"Students will collect data for one more day for the Stress/Chill campaign either through the UCLA IDS UCLA App or via web browser at https://portal.idsucla.org LAB 2G: Getting it Together Complete Lab 2G prior to the Practicum .","title":"Homework &amp; Next Day"},{"location":"unit2/lesson16/","text":"Lesson 16: What is Normal? Objective: Students will learn what a Normal distribution is and learn how to identify a Normal distribution. Materials: Video: New York Times\u2019 \u201cBunnies, Dragons, and the Normal World\u201d found at: http://www.nytimes.com/video/science/100000002452709/bunnies-dragons-and-the-normal-world.html Note: Show only the first 41 seconds of the video. Graphics from the Normal Plots file ( LMR_2.15_Normal Plots ) Projector to display plots 3\u201d x 5\u201d cards (1 per student) Vocabulary: bell-shaped normal curve normal distribution Essential Concepts: Essential Concepts: The Normal curve, also called the Gaussian distribution and the \"bell curve,\" is a model that describes many real-life distributions and is usually called the Normal Model. Lesson: Remind students that in Unit 1 , Lesson 11 ( What Shape Are You In? ), they sorted histograms into groups based on their shapes. The Normal Plots file ( LMR_2.15 ) contains some of the unimodal bell-shaped distributions from the original handout of that lesson ( Sorting Histograms handout ( LMR_1.10 )). Note: You do not need the original handout from Unit 1 \u2013 all relevant plots have been compiled in the Normal Plots file ( LMR_2.15 ) for accessibility. Six plots are included: SAT Math, SAT Verb, ACT Mathematics, ACT Reading, ACT English, and ACT Science Reasoning. LMR_2.15 Display the group of bell-shaped distributions from page 1 of the Normal Plots file ( LMR_2.15 ) to the class and ask the students: What characteristic does this particular group share? All of these plots are unimodal (one mode/peak) and symmetric. Inform students that these types of distributions are often referred to as bell-shaped. Why might this term be used? The histograms look very similar to the shape of a bell. To show the similarities between the shape of a bell and the shape of these distributions, a clip-art image (shown here) has been included in the Normal Plots file ( LMR_2.15 ) on page 2. Explain that this shape occurs often in real-life. It occurs so often that it\u2019s been given its own name: the normal curve , or normal distribution . Can the students think of distributions where they have seen Normal curves in previous labs? To give some more background on the normal distribution, play the New York Times video titled \u201cBunnies, Dragons, and the Normal World\u201d found at: http://www.nytimes.com/video/science/100000002452709/bunnies-dragons-and-the-normal-world.html Note: Show only the first 41 seconds of the video. Discuss that the normal curve has a very precise mathematical definition, which is pretty complex. But the result is a curve that looks like the one in the \u201cBunnies\u201d video. In general, the curve looks like the plot shown below. Note: You can either draw the diagram below on the board or display it via a projector \u2013 the image can be found on page 2 of the Normal Plots file ( LMR_2.15 ). Explain that normal distributions are good for describing some populations of people. For example, people\u2019s heights are often considered to be normally distributed. Display the famous Frank Anscombe photograph (shown below) via a projector. The graphic can be found on page 2 of the Normal Plots file ( LMR_2.15 ). Inform the students that this photo was taken of a group of randomly selected college women who stood in height order. Next, lead a discussion about why the normal curve is a good fit to the histogram in the above picture. Notice that more people are near the center of the distribution, and fewer are in the outer edges, or tails. Engage the class in a conversation using the following probing questions: Notice that there is a peak in the center of the distribution. What height do you think is at the center? The average height of American women is approximately 5\u20195\u201d (5 feet, 5 inches) tall. We might therefore expect the average height of this group to be close to 5'5\" as well. Why are more people in the center, and less people in the edges, or tails, of the distribution? The center represents the mean height. Most women will fall somewhere close to the mean and may be a few inches shorter or taller than it. However, less people are likely to be MUCH shorter or MUCH taller than the mean. For example, we would not expect to see many women who are 4\u201910\u201d tall nor would we expect to see many women who are 6\u20190\u201d tall. Explain that the normal curve is a good description of a distribution when it makes sense that there is a single 'typical' value with random deviations above and below that value. Ask students: Why does this make sense with heights but not with incomes? With heights, we expect most people to be near the average, with some deviations above and below the mean (people who are taller or shorter than the mean height). However, with incomes, the distribution will have more deviations that are above the typical value, since there is no upper bound for a maximum income (ex. Bill Gates, Warren Buffett, etc. Are there more real-life examples, other than height, that students think might follow a normal distribution? Answers will vary by class. Some examples include: (1) scores on standardized math and reading tests (like the SAT and ACT), (2) IQ scores, and (3) body temperatures. Does it matter that the curve drawn on the photograph does not match exactly to the women\u2019s heights? No. We often refer to the curve as the \u201cnormal model\u201d because the curve is a just a model of the true population distribution. So, even though the red curve is not exactly the same as the women\u2019s heights, it is a close enough approximation of the shape of their heights. Note to teacher: The main role the normal distribution has historically played has been in modeling errors (most measurements will be close to the actual value while larger errors occur less often) and sample means. Inform the students that, during the next few lessons, they will be learning more about the normal distribution. In particular, they will learn about a new measure of spread used to describe a normal distribution, how to calculate probabilities from this distribution, and how to randomly sample from this distribution. Cheat Card: Distribute an index card to students and ask them to create a cheat card that will help them remember information about the normal curve. Class Scribes: One team of students will give a brief talk to discuss what they think the 3 most important topics of the day were. Homework Students will complete their cheat cards if they were not able to finish in class.","title":"Lesson 16: What Is Normal?"},{"location":"unit2/lesson16/#lesson-16-what-is-normal","text":"","title":"Lesson 16: What is Normal?"},{"location":"unit2/lesson16/#objective","text":"Students will learn what a Normal distribution is and learn how to identify a Normal distribution.","title":"Objective:"},{"location":"unit2/lesson16/#materials","text":"Video: New York Times\u2019 \u201cBunnies, Dragons, and the Normal World\u201d found at: http://www.nytimes.com/video/science/100000002452709/bunnies-dragons-and-the-normal-world.html Note: Show only the first 41 seconds of the video. Graphics from the Normal Plots file ( LMR_2.15_Normal Plots ) Projector to display plots 3\u201d x 5\u201d cards (1 per student)","title":"Materials:"},{"location":"unit2/lesson16/#vocabulary","text":"bell-shaped normal curve normal distribution","title":"Vocabulary:"},{"location":"unit2/lesson16/#essential-concepts","text":"Essential Concepts: The Normal curve, also called the Gaussian distribution and the \"bell curve,\" is a model that describes many real-life distributions and is usually called the Normal Model.","title":"Essential Concepts:"},{"location":"unit2/lesson16/#lesson","text":"Remind students that in Unit 1 , Lesson 11 ( What Shape Are You In? ), they sorted histograms into groups based on their shapes. The Normal Plots file ( LMR_2.15 ) contains some of the unimodal bell-shaped distributions from the original handout of that lesson ( Sorting Histograms handout ( LMR_1.10 )). Note: You do not need the original handout from Unit 1 \u2013 all relevant plots have been compiled in the Normal Plots file ( LMR_2.15 ) for accessibility. Six plots are included: SAT Math, SAT Verb, ACT Mathematics, ACT Reading, ACT English, and ACT Science Reasoning. LMR_2.15 Display the group of bell-shaped distributions from page 1 of the Normal Plots file ( LMR_2.15 ) to the class and ask the students: What characteristic does this particular group share? All of these plots are unimodal (one mode/peak) and symmetric. Inform students that these types of distributions are often referred to as bell-shaped. Why might this term be used? The histograms look very similar to the shape of a bell. To show the similarities between the shape of a bell and the shape of these distributions, a clip-art image (shown here) has been included in the Normal Plots file ( LMR_2.15 ) on page 2. Explain that this shape occurs often in real-life. It occurs so often that it\u2019s been given its own name: the normal curve , or normal distribution . Can the students think of distributions where they have seen Normal curves in previous labs? To give some more background on the normal distribution, play the New York Times video titled \u201cBunnies, Dragons, and the Normal World\u201d found at: http://www.nytimes.com/video/science/100000002452709/bunnies-dragons-and-the-normal-world.html Note: Show only the first 41 seconds of the video. Discuss that the normal curve has a very precise mathematical definition, which is pretty complex. But the result is a curve that looks like the one in the \u201cBunnies\u201d video. In general, the curve looks like the plot shown below. Note: You can either draw the diagram below on the board or display it via a projector \u2013 the image can be found on page 2 of the Normal Plots file ( LMR_2.15 ). Explain that normal distributions are good for describing some populations of people. For example, people\u2019s heights are often considered to be normally distributed. Display the famous Frank Anscombe photograph (shown below) via a projector. The graphic can be found on page 2 of the Normal Plots file ( LMR_2.15 ). Inform the students that this photo was taken of a group of randomly selected college women who stood in height order. Next, lead a discussion about why the normal curve is a good fit to the histogram in the above picture. Notice that more people are near the center of the distribution, and fewer are in the outer edges, or tails. Engage the class in a conversation using the following probing questions: Notice that there is a peak in the center of the distribution. What height do you think is at the center? The average height of American women is approximately 5\u20195\u201d (5 feet, 5 inches) tall. We might therefore expect the average height of this group to be close to 5'5\" as well. Why are more people in the center, and less people in the edges, or tails, of the distribution? The center represents the mean height. Most women will fall somewhere close to the mean and may be a few inches shorter or taller than it. However, less people are likely to be MUCH shorter or MUCH taller than the mean. For example, we would not expect to see many women who are 4\u201910\u201d tall nor would we expect to see many women who are 6\u20190\u201d tall. Explain that the normal curve is a good description of a distribution when it makes sense that there is a single 'typical' value with random deviations above and below that value. Ask students: Why does this make sense with heights but not with incomes? With heights, we expect most people to be near the average, with some deviations above and below the mean (people who are taller or shorter than the mean height). However, with incomes, the distribution will have more deviations that are above the typical value, since there is no upper bound for a maximum income (ex. Bill Gates, Warren Buffett, etc. Are there more real-life examples, other than height, that students think might follow a normal distribution? Answers will vary by class. Some examples include: (1) scores on standardized math and reading tests (like the SAT and ACT), (2) IQ scores, and (3) body temperatures. Does it matter that the curve drawn on the photograph does not match exactly to the women\u2019s heights? No. We often refer to the curve as the \u201cnormal model\u201d because the curve is a just a model of the true population distribution. So, even though the red curve is not exactly the same as the women\u2019s heights, it is a close enough approximation of the shape of their heights. Note to teacher: The main role the normal distribution has historically played has been in modeling errors (most measurements will be close to the actual value while larger errors occur less often) and sample means. Inform the students that, during the next few lessons, they will be learning more about the normal distribution. In particular, they will learn about a new measure of spread used to describe a normal distribution, how to calculate probabilities from this distribution, and how to randomly sample from this distribution. Cheat Card: Distribute an index card to students and ask them to create a cheat card that will help them remember information about the normal curve.","title":"Lesson:"},{"location":"unit2/lesson16/#class-scribes","text":"One team of students will give a brief talk to discuss what they think the 3 most important topics of the day were.","title":"Class Scribes:"},{"location":"unit2/lesson16/#homework","text":"Students will complete their cheat cards if they were not able to finish in class.","title":"Homework"},{"location":"unit2/lesson17/","text":"Lesson 17: A Normal Measure of Spread Objective: Students will learn that standard deviation is another way to measure variability. Materials: How Far Apart? handout ( LMR_2.6_How Far Apart ) \u2013 completed during Lesson 4 How Far Apart? (with standard deviation \u2013 SD) handout ( LMR_2.16_How Far Apart SD ) Projector to display visuals using RStudio RScript with the functions in this lesson Vocabulary: standard deviation (SD) Essential Concepts: Essential Concepts: The standard deviation is another measure of spread. This is commonly used by statisticians because of its role in common models and distributions, such as the Normal Model. Lesson: In their DS journals, ask students to create a two-column table and label the left-column as Measures of Center (Central Tendency) and the right column as Measures of Spread (Dispersion) . In pairs, ask students to recall methods they have learned so far for measuring center and measuring spread in distributions. Measures of Center: mean (average or typical value), median Measures of Spread: mean absolute deviation (MAD), interquartile range (IQR) Share out a pair\u2019s explanation and ask the rest of the pairs to agree or disagree. If there is disagreement, hold a class discussion until the lists are correct. Point out that a measure of center or a measure of spread depicts one value for a distribution. Ask student pairs to discuss the following question: What does the value of each measure tell us about the data in the distribution? Possible answer: A measure of center tells us the value that is typical, or in the center. A measure of spread tells us how variable, or how spread apart, the data are. Next, ask students to add the term standard deviation (SD) to their Measures of Spread column. Inform students that the standard deviation of a distribution is another way to measure spread, or variability. The standard deviation is similar to the mean absolute deviation (MAD). Ask students to recall the formula for calculating the MAD: While the MAD measures the absolute distance of each data point from the mean, the standard deviation squares the distances of each data point from the mean. Both methods result in positive measurements because distance is always positive. Ask students to recall that they calculated MAD values in the How Far Apart? handout ( LMR_2.6 ) during Lesson 4 of this unit. Show and discuss the formula for calculating the standard deviation of a dataset: Note to teacher: There are different formulas for the standard deviation. We are presenting the simpler one, which divides by n. In AP Statistics (or college introductory statistics), students will learn that if they are using a sample of data to estimate the standard deviation for the population, then dividing by n\u22121 is a better estimator than dividing by n. But this technically requires a lot of scaffolding and leads to little understanding, and so we will stick with the simpler version. (In some books, this is called the \u201cpopulation value of the standard deviation\u201d and the n\u22121 version is called the \u201csample estimate of the standard deviation.\u201d) Guide the class to complete the How Far Apart? (with standard deviation -- SD) handout ( LMR_2.16 ) to calculate standard deviations of the dotplots using the formula listed above. LMR_2.16 Answers: Plot (a) \u2013 SD = 1.0847 candies; Plot (c) \u2013 SD = 1.3770 candies As a whole group, ask students to compare and contrast the standard deviations with the MAD values for the two plots in the handout. Similarities between SD and MAD: \u2022 Measure the same idea: variability, or spread \u2022 Are based on looking at the \"deviations\" from the mean: the difference between an observation and the mean \u2022 Uses the \"typical\" deviation Differences between SD and MAD: \u2022 The MAD uses the absolute value and finds the average of the absolute deviations from the mean \u2022 The SD uses the square of each deviation from the mean, and finds the average of the squares \u2022 The SD takes the square root of the average of the squares Ask students why they think the SD takes the square root of the average of the squares. Possible response: Taking the square root of the average of the squares returns the measurements to their original units instead of square units. To reinforce students\u2019 conceptual understanding of standard deviation, student teams will estimate the standard deviation for a few numerical distributions and explain the reasoning for their estimate. Load and view the atus data, then run the following functions one by one: > histogram(~sleep, data=atus, breaks=seq(0,1500,by=100), main = \u201cDistribution of sleep in minutes\u201d) > sleep_mean<-mean(~sleep, data=atus) > add_line(vline=sleep_mean) Zoom in on the visual and give student teams a few minutes to discuss what they estimate the standard deviation to be. Have the reporter from each team report their estimate using the following sentence frame: \u201cThe time spent sleeping (in minutes) typically varies from the mean by minutes.\u201d Reveal the actual standard deviation by running the function: > sd(~sleep, data=atus) Choose a reporter from a student team that had a good approximation to explain their reasoning. Repeat this process with a few more numerical variables. Functions are provided below. Household size > histogram(~household_size, data=atus, nint=13) > household_mean<-mean(~household_size, data=atus) > add_line(vline=household_mean) \u201cHousehold sizes typically vary from the mean by people.\u201d > sd(~household_size, data=atus) Socializing > histogram(~socializing, data=atus, breaks=seq(0,2000,by=100)) > social_mean<-mean(~socializing, data=atus) > add_line(vline=social_mean) \u201cThe time spent socializing (in minutes) typically varies from the mean by minutes.\u201d > sd(~socializing, data=atus) Class Scribes: One team of students will give a brief talk to discuss what they think the 3 most important topics of the day were.","title":"Lesson 17: A Normal Measure of Spread"},{"location":"unit2/lesson17/#lesson-17-a-normal-measure-of-spread","text":"","title":"Lesson 17: A Normal Measure of Spread"},{"location":"unit2/lesson17/#objective","text":"Students will learn that standard deviation is another way to measure variability.","title":"Objective:"},{"location":"unit2/lesson17/#materials","text":"How Far Apart? handout ( LMR_2.6_How Far Apart ) \u2013 completed during Lesson 4 How Far Apart? (with standard deviation \u2013 SD) handout ( LMR_2.16_How Far Apart SD ) Projector to display visuals using RStudio RScript with the functions in this lesson","title":"Materials:"},{"location":"unit2/lesson17/#vocabulary","text":"standard deviation (SD)","title":"Vocabulary:"},{"location":"unit2/lesson17/#essential-concepts","text":"Essential Concepts: The standard deviation is another measure of spread. This is commonly used by statisticians because of its role in common models and distributions, such as the Normal Model.","title":"Essential Concepts:"},{"location":"unit2/lesson17/#lesson","text":"In their DS journals, ask students to create a two-column table and label the left-column as Measures of Center (Central Tendency) and the right column as Measures of Spread (Dispersion) . In pairs, ask students to recall methods they have learned so far for measuring center and measuring spread in distributions. Measures of Center: mean (average or typical value), median Measures of Spread: mean absolute deviation (MAD), interquartile range (IQR) Share out a pair\u2019s explanation and ask the rest of the pairs to agree or disagree. If there is disagreement, hold a class discussion until the lists are correct. Point out that a measure of center or a measure of spread depicts one value for a distribution. Ask student pairs to discuss the following question: What does the value of each measure tell us about the data in the distribution? Possible answer: A measure of center tells us the value that is typical, or in the center. A measure of spread tells us how variable, or how spread apart, the data are. Next, ask students to add the term standard deviation (SD) to their Measures of Spread column. Inform students that the standard deviation of a distribution is another way to measure spread, or variability. The standard deviation is similar to the mean absolute deviation (MAD). Ask students to recall the formula for calculating the MAD: While the MAD measures the absolute distance of each data point from the mean, the standard deviation squares the distances of each data point from the mean. Both methods result in positive measurements because distance is always positive. Ask students to recall that they calculated MAD values in the How Far Apart? handout ( LMR_2.6 ) during Lesson 4 of this unit. Show and discuss the formula for calculating the standard deviation of a dataset: Note to teacher: There are different formulas for the standard deviation. We are presenting the simpler one, which divides by n. In AP Statistics (or college introductory statistics), students will learn that if they are using a sample of data to estimate the standard deviation for the population, then dividing by n\u22121 is a better estimator than dividing by n. But this technically requires a lot of scaffolding and leads to little understanding, and so we will stick with the simpler version. (In some books, this is called the \u201cpopulation value of the standard deviation\u201d and the n\u22121 version is called the \u201csample estimate of the standard deviation.\u201d) Guide the class to complete the How Far Apart? (with standard deviation -- SD) handout ( LMR_2.16 ) to calculate standard deviations of the dotplots using the formula listed above. LMR_2.16 Answers: Plot (a) \u2013 SD = 1.0847 candies; Plot (c) \u2013 SD = 1.3770 candies As a whole group, ask students to compare and contrast the standard deviations with the MAD values for the two plots in the handout. Similarities between SD and MAD: \u2022 Measure the same idea: variability, or spread \u2022 Are based on looking at the \"deviations\" from the mean: the difference between an observation and the mean \u2022 Uses the \"typical\" deviation Differences between SD and MAD: \u2022 The MAD uses the absolute value and finds the average of the absolute deviations from the mean \u2022 The SD uses the square of each deviation from the mean, and finds the average of the squares \u2022 The SD takes the square root of the average of the squares Ask students why they think the SD takes the square root of the average of the squares. Possible response: Taking the square root of the average of the squares returns the measurements to their original units instead of square units. To reinforce students\u2019 conceptual understanding of standard deviation, student teams will estimate the standard deviation for a few numerical distributions and explain the reasoning for their estimate. Load and view the atus data, then run the following functions one by one: > histogram(~sleep, data=atus, breaks=seq(0,1500,by=100), main = \u201cDistribution of sleep in minutes\u201d) > sleep_mean<-mean(~sleep, data=atus) > add_line(vline=sleep_mean) Zoom in on the visual and give student teams a few minutes to discuss what they estimate the standard deviation to be. Have the reporter from each team report their estimate using the following sentence frame: \u201cThe time spent sleeping (in minutes) typically varies from the mean by minutes.\u201d Reveal the actual standard deviation by running the function: > sd(~sleep, data=atus) Choose a reporter from a student team that had a good approximation to explain their reasoning. Repeat this process with a few more numerical variables. Functions are provided below. Household size > histogram(~household_size, data=atus, nint=13) > household_mean<-mean(~household_size, data=atus) > add_line(vline=household_mean) \u201cHousehold sizes typically vary from the mean by people.\u201d > sd(~household_size, data=atus) Socializing > histogram(~socializing, data=atus, breaks=seq(0,2000,by=100)) > social_mean<-mean(~socializing, data=atus) > add_line(vline=social_mean) \u201cThe time spent socializing (in minutes) typically varies from the mean by minutes.\u201d > sd(~socializing, data=atus)","title":"Lesson:"},{"location":"unit2/lesson17/#class-scribes","text":"One team of students will give a brief talk to discuss what they think the 3 most important topics of the day were.","title":"Class Scribes:"},{"location":"unit2/lesson18/","text":"Lesson 18: What's Your Z-Score? Objective: Students will understand that a z-score can be used to measure how far - or how many standard deviations - an observation is away from the mean. Typically z-scores will range between -3 and +3. For simulations involving shuffling, if we compute a z-score that lies far away from the mean, then we might conclude that the outcome was not due to chance. If we see a z-score that lies close to the mean, then we might conclude it was by chance. Materials: Projector to display RStudio function RScript with all of the functions in this lesson A ruler with centimeter marks on it Vocabulary: z-score standardized score Empirical Rule Essential Concepts: Essential Concepts: z-scores offer us a way to measure how extreme a value is, regardless of the units of measurement. Typically z-scores will range between -3 and +3, so values that are at or are more extreme than -3 or +3 standard deviations are considered extremely rare. Lesson: Ask students to recall what they remember about normal distributions. Answer: Normal distributions are unimodal and symmetric, and are often referred to as bell-shaped. Some real-life examples of variables that produce normal distributions are people\u2019s heights, scores on standardized tests, and body temperatures. Display the following statement to students: \u201cAll normal distributions are bell-shaped, but not all bell-shaped distributions are normal.\u201d Then inform students that normal distributions have special properties. Display the image below and introduce the Empirical Rule , which states: \u2022 Approximately 68% of the observations in a normal distribution fall within one standard deviation of the mean \u2022 Approximately 95% of the observations in a normal distribution fall within 2 standard deviations of the mean \u2022 Approximately 99.7% of the observations in a normal distribution fall within 3 standard deviations of the mean Open RStudio and project for students to see. Load the babies dataset, named Gestation by following these steps: Enter data(Gestation) in the Console You should see Gestation located in your Environment Enter View(Gestation) in the Console Display the help documentation by typing ?Gestation . Ask student teams to predict which of the variables in the \"Gestation\" dataset they think might be normally distributed. Choose a couple of teams to share out. Description of numerical variables: wt - birth weight (in ounces) gestation - length of the pregnancy (in days) parity - 0 if baby was first born, 1-13 otherwise age - mother\u2019s age (in years) ht - mother\u2019s height (to the last completed inch) wt.1 - mother\u2019s weight (in pounds) dage - father's age (in years) dht - father's height (to the last completed inch) dwt - father's weight (in pounds) Create histograms using the variables shared by student teams. There are a few variables that look normally distributed, such as the birth mother's heights. We will investigate the babies\u2019 birth weights. histogram(~wt, data = Gestation) Ask students: Does the distribution of baby birth weights look approximately normal? Explain. Answer: The distribution of baby birth weights is unimodal, roughly symmetric, and somewhat bell-shaped, so it might be approximately normal. What do you approximate the mean weight of the distribution to be? How about the standard deviation? Answers will vary. Use this as a check for understanding of standard deviation as well as estimating the mean using the balancing point concept. See next step for calculating the actual mean weight and standard deviation. Use RStudio to calculate the actual mean and standard deviation. mean_wt <- mean(~wt, data = Gestation) sd_wt <- sd(~wt, data = Gestation) Have students draw a number line with seven equally spaced intervals and label it \u201cBaby birth weights in ounces.\u201d Make sure students leave about 5 centimeters of space above the number line to draw a normal curve. Have students label the middle tick mark with the mean baby weight (round to the nearest tenth of an ounce=119.6 ounces). Then ask students: What weight is one standard deviation above the mean? Answer: A baby whose weight is 137.8 ounces is one standard deviation above the mean baby weight. What weight is one standard deviation below the mean? Answer: A baby whose weight is 101.4 ounces is one standard deviation below the mean baby weight. Have students label their number line with these values. Have students continue filling their number line with the corresponding weights that are two and three standard deviations from the mean. Answer: A baby who weighs 156 ounces is two standard deviations above the mean weight, and a baby who weighs 174.2 ounces is three standard deviations above the mean weight. A baby who weighs 83.2 ounces is two standard deviations below the mean weight, and a baby who weighs 65 ounces is three standard deviations below the mean weight. Ask students: If the distribution of baby weights is approximately normal, what percentage of babies weigh between 101.4 ounces and 137.8 ounces? Answer: If the distribution of baby weights is approximately normal, about 68% of babies should weigh between 101.4 ounces and 137.8 ounces. Use RStudio to confirm if indeed the distribution of baby weights is approximately normal. one_sd_wt <- filter(Gestation, wt > 101.4, wt < 137.8) Answer: In this sample of 1236 observations, there are 861 babies whose weights are one standard deviation from the mean, so 861/1236 = 0.697. This means that around 69.7% of the weights of the babies in this sample fall within one standard deviation from the mean baby weight. This is close to 68%, so it seems that the distribution of baby weights is approximately normally distributed. Note: If you continue this process for this sample, you will find that the distribution of baby weights is normally distributed as defined by the Empirical Rule. In this sample, 1171/1236 = 94.7% of the baby weights fall within two standard deviations of the mean, and 1229/1236 =99.4% of the baby weights fall within three standard deviations of the mean. Now that it has been verified that a normal distribution is an appropriate model for this distribution, have students draw a normal curve above the number line. Suggested method to obtain a decent normal curve: \u2022 Step 1: Draw a dot 4 centimeters above the mean weight \u2022 Step 2: Draw dots 2.4 cm above the weights that are 1 standard deviation from the mean \u2022 Step 3: Draw dots 0.36 cm above the weights that are 2 standard deviations from the mean \u2022 Step 4: Draw dots right above the number line for the weights that are 3 standard deviations from the mean \u2022 Step 5: Connect the dots with a smooth curve Tell students that we are using this normal curve as a model to represent the distribution of all baby weights. This will allow us to make comparisons, draw conclusions, and make predictions about baby weights. Let\u2019s see: What percentage of babies weigh less than 119.6 ounces? Explain. Answer: About 50% of babies weigh less than 119.6 ounces. Since normal distributions are symmetric, the mean and the median are about the same. Since the median divides a distribution into equal halves, in this case so does the mean. What percentage of babies weigh between 119.6 and 137.8 ounces? Answer: About 34% of babies weigh between 119.6 and 137.8 ounces. According to the Empirical Rule, 68% of the observations fall within one standard deviation of the mean, and since normal distributions are symmetric, the area under the curve from the mean to one standard deviation is half of 68% or 34%. What percentage of babies weigh more than 137.8 ounces? Answer: About 16% of babies weigh more than 137.8 ounces. From part a and b above, we know that 50%+34%=84% of babies weigh less than 137.8 ounces, so 100%-84%=16% weigh more than 137.8 ounces. Explain that statisticians use something called a z-score to compare values. A z-score tells us how many standard deviations away from the mean an observation is. Another name for z-score is a standardized score . Introduce the formula for calculating a z-score and discuss what each symbol in the formula means. Explain that z-scores answer the question: \u201cHow typical is x?\u201d If x is the same as the typical value (the mean), then z = 0. If x is one standard deviation away from the mean, then z = -1 or +1. Remind students from the normal curve that as you move farther from the center (from the mean), there are fewer observations. Therefore, a large z-score is considered an unusual value. Have students calculate the z-score for a baby that weighs 100 ounces: z = (100 - 119.6) / 18.2 = -1.08 Ask the class: What does a negative z-score mean? A negative z-score means the x value is below the mean. This means that the weight is below average. What does a positive z-score mean? A positive z-score means the x value is above the mean. This means that the weight is above average. What is the most negative z-score you think we will find? What is the most positive z-score? Typically, values in a normal distribution rarely fall outside two or three standard deviations from the mean. For data derived from chance, we probably won\u2019t see any values that are less than -3 or greater than +3. Ask students: \u201cWhere does a baby that weighs 100 ounces fall within the distribution of baby weights?\u201d Have students find 100 ounces on the x-axis of the normal curve and draw a vertical line from the x-axis until it intersects the normal curve. Have them shade the area under the curve to the left of the vertical line. Tell students that the shaded area represents a percentile in the distribution. A percentile is the exact value in which the desired proportion of observations lies below the specific value in a distribution. Use RStudio to calculate the percentile. pnorm(100, mean = 119.6, sd = 18.2) = 0.140 Doctors report percentiles to describe a child's development compared to other children their age. For a baby that weighs 100 ounces, a doctor would report the following: \u201cThe baby is at the 14th percentile in weight.\u201d This means that the baby weighs more than 14% of all babies. Note: A z-score can also be used to calculate a percentile, but since a z-score is a standardized score, the mean of the distribution would be zero and the standard deviation would be one. pnorm(-1.08, mean = 0, sd = 1) = 0.140 Inform the class that they will be using RStudio during the next few days to practice using normal models. Class Scribes: One team of students will give a brief talk to discuss what they think the 3 most important topics of the day were. Next 2 Days LAB 2H: Eyeballing Normal LAB 2I: R\u2019s Normal Distribution Alphabet Complete Labs 2H and 2I prior to the End of Unit Design Project .","title":"Lesson 18: What\u2019s Your Z-Score?"},{"location":"unit2/lesson18/#lesson-18-whats-your-z-score","text":"","title":"Lesson 18: What's Your Z-Score?"},{"location":"unit2/lesson18/#objective","text":"Students will understand that a z-score can be used to measure how far - or how many standard deviations - an observation is away from the mean. Typically z-scores will range between -3 and +3. For simulations involving shuffling, if we compute a z-score that lies far away from the mean, then we might conclude that the outcome was not due to chance. If we see a z-score that lies close to the mean, then we might conclude it was by chance.","title":"Objective:"},{"location":"unit2/lesson18/#materials","text":"Projector to display RStudio function RScript with all of the functions in this lesson A ruler with centimeter marks on it","title":"Materials:"},{"location":"unit2/lesson18/#vocabulary","text":"z-score standardized score Empirical Rule","title":"Vocabulary:"},{"location":"unit2/lesson18/#essential-concepts","text":"Essential Concepts: z-scores offer us a way to measure how extreme a value is, regardless of the units of measurement. Typically z-scores will range between -3 and +3, so values that are at or are more extreme than -3 or +3 standard deviations are considered extremely rare.","title":"Essential Concepts:"},{"location":"unit2/lesson18/#lesson","text":"Ask students to recall what they remember about normal distributions. Answer: Normal distributions are unimodal and symmetric, and are often referred to as bell-shaped. Some real-life examples of variables that produce normal distributions are people\u2019s heights, scores on standardized tests, and body temperatures. Display the following statement to students: \u201cAll normal distributions are bell-shaped, but not all bell-shaped distributions are normal.\u201d Then inform students that normal distributions have special properties. Display the image below and introduce the Empirical Rule , which states: \u2022 Approximately 68% of the observations in a normal distribution fall within one standard deviation of the mean \u2022 Approximately 95% of the observations in a normal distribution fall within 2 standard deviations of the mean \u2022 Approximately 99.7% of the observations in a normal distribution fall within 3 standard deviations of the mean Open RStudio and project for students to see. Load the babies dataset, named Gestation by following these steps: Enter data(Gestation) in the Console You should see Gestation located in your Environment Enter View(Gestation) in the Console Display the help documentation by typing ?Gestation . Ask student teams to predict which of the variables in the \"Gestation\" dataset they think might be normally distributed. Choose a couple of teams to share out. Description of numerical variables: wt - birth weight (in ounces) gestation - length of the pregnancy (in days) parity - 0 if baby was first born, 1-13 otherwise age - mother\u2019s age (in years) ht - mother\u2019s height (to the last completed inch) wt.1 - mother\u2019s weight (in pounds) dage - father's age (in years) dht - father's height (to the last completed inch) dwt - father's weight (in pounds) Create histograms using the variables shared by student teams. There are a few variables that look normally distributed, such as the birth mother's heights. We will investigate the babies\u2019 birth weights. histogram(~wt, data = Gestation) Ask students: Does the distribution of baby birth weights look approximately normal? Explain. Answer: The distribution of baby birth weights is unimodal, roughly symmetric, and somewhat bell-shaped, so it might be approximately normal. What do you approximate the mean weight of the distribution to be? How about the standard deviation? Answers will vary. Use this as a check for understanding of standard deviation as well as estimating the mean using the balancing point concept. See next step for calculating the actual mean weight and standard deviation. Use RStudio to calculate the actual mean and standard deviation. mean_wt <- mean(~wt, data = Gestation) sd_wt <- sd(~wt, data = Gestation) Have students draw a number line with seven equally spaced intervals and label it \u201cBaby birth weights in ounces.\u201d Make sure students leave about 5 centimeters of space above the number line to draw a normal curve. Have students label the middle tick mark with the mean baby weight (round to the nearest tenth of an ounce=119.6 ounces). Then ask students: What weight is one standard deviation above the mean? Answer: A baby whose weight is 137.8 ounces is one standard deviation above the mean baby weight. What weight is one standard deviation below the mean? Answer: A baby whose weight is 101.4 ounces is one standard deviation below the mean baby weight. Have students label their number line with these values. Have students continue filling their number line with the corresponding weights that are two and three standard deviations from the mean. Answer: A baby who weighs 156 ounces is two standard deviations above the mean weight, and a baby who weighs 174.2 ounces is three standard deviations above the mean weight. A baby who weighs 83.2 ounces is two standard deviations below the mean weight, and a baby who weighs 65 ounces is three standard deviations below the mean weight. Ask students: If the distribution of baby weights is approximately normal, what percentage of babies weigh between 101.4 ounces and 137.8 ounces? Answer: If the distribution of baby weights is approximately normal, about 68% of babies should weigh between 101.4 ounces and 137.8 ounces. Use RStudio to confirm if indeed the distribution of baby weights is approximately normal. one_sd_wt <- filter(Gestation, wt > 101.4, wt < 137.8) Answer: In this sample of 1236 observations, there are 861 babies whose weights are one standard deviation from the mean, so 861/1236 = 0.697. This means that around 69.7% of the weights of the babies in this sample fall within one standard deviation from the mean baby weight. This is close to 68%, so it seems that the distribution of baby weights is approximately normally distributed. Note: If you continue this process for this sample, you will find that the distribution of baby weights is normally distributed as defined by the Empirical Rule. In this sample, 1171/1236 = 94.7% of the baby weights fall within two standard deviations of the mean, and 1229/1236 =99.4% of the baby weights fall within three standard deviations of the mean. Now that it has been verified that a normal distribution is an appropriate model for this distribution, have students draw a normal curve above the number line. Suggested method to obtain a decent normal curve: \u2022 Step 1: Draw a dot 4 centimeters above the mean weight \u2022 Step 2: Draw dots 2.4 cm above the weights that are 1 standard deviation from the mean \u2022 Step 3: Draw dots 0.36 cm above the weights that are 2 standard deviations from the mean \u2022 Step 4: Draw dots right above the number line for the weights that are 3 standard deviations from the mean \u2022 Step 5: Connect the dots with a smooth curve Tell students that we are using this normal curve as a model to represent the distribution of all baby weights. This will allow us to make comparisons, draw conclusions, and make predictions about baby weights. Let\u2019s see: What percentage of babies weigh less than 119.6 ounces? Explain. Answer: About 50% of babies weigh less than 119.6 ounces. Since normal distributions are symmetric, the mean and the median are about the same. Since the median divides a distribution into equal halves, in this case so does the mean. What percentage of babies weigh between 119.6 and 137.8 ounces? Answer: About 34% of babies weigh between 119.6 and 137.8 ounces. According to the Empirical Rule, 68% of the observations fall within one standard deviation of the mean, and since normal distributions are symmetric, the area under the curve from the mean to one standard deviation is half of 68% or 34%. What percentage of babies weigh more than 137.8 ounces? Answer: About 16% of babies weigh more than 137.8 ounces. From part a and b above, we know that 50%+34%=84% of babies weigh less than 137.8 ounces, so 100%-84%=16% weigh more than 137.8 ounces. Explain that statisticians use something called a z-score to compare values. A z-score tells us how many standard deviations away from the mean an observation is. Another name for z-score is a standardized score . Introduce the formula for calculating a z-score and discuss what each symbol in the formula means. Explain that z-scores answer the question: \u201cHow typical is x?\u201d If x is the same as the typical value (the mean), then z = 0. If x is one standard deviation away from the mean, then z = -1 or +1. Remind students from the normal curve that as you move farther from the center (from the mean), there are fewer observations. Therefore, a large z-score is considered an unusual value. Have students calculate the z-score for a baby that weighs 100 ounces: z = (100 - 119.6) / 18.2 = -1.08 Ask the class: What does a negative z-score mean? A negative z-score means the x value is below the mean. This means that the weight is below average. What does a positive z-score mean? A positive z-score means the x value is above the mean. This means that the weight is above average. What is the most negative z-score you think we will find? What is the most positive z-score? Typically, values in a normal distribution rarely fall outside two or three standard deviations from the mean. For data derived from chance, we probably won\u2019t see any values that are less than -3 or greater than +3. Ask students: \u201cWhere does a baby that weighs 100 ounces fall within the distribution of baby weights?\u201d Have students find 100 ounces on the x-axis of the normal curve and draw a vertical line from the x-axis until it intersects the normal curve. Have them shade the area under the curve to the left of the vertical line. Tell students that the shaded area represents a percentile in the distribution. A percentile is the exact value in which the desired proportion of observations lies below the specific value in a distribution. Use RStudio to calculate the percentile. pnorm(100, mean = 119.6, sd = 18.2) = 0.140 Doctors report percentiles to describe a child's development compared to other children their age. For a baby that weighs 100 ounces, a doctor would report the following: \u201cThe baby is at the 14th percentile in weight.\u201d This means that the baby weighs more than 14% of all babies. Note: A z-score can also be used to calculate a percentile, but since a z-score is a standardized score, the mean of the distribution would be zero and the standard deviation would be one. pnorm(-1.08, mean = 0, sd = 1) = 0.140 Inform the class that they will be using RStudio during the next few days to practice using normal models.","title":"Lesson:"},{"location":"unit2/lesson18/#class-scribes","text":"One team of students will give a brief talk to discuss what they think the 3 most important topics of the day were.","title":"Class Scribes:"},{"location":"unit2/lesson18/#next-2-days","text":"LAB 2H: Eyeballing Normal LAB 2I: R\u2019s Normal Distribution Alphabet Complete Labs 2H and 2I prior to the End of Unit Design Project .","title":"Next 2 Days"},{"location":"unit2/lesson2/","text":"Lesson 2: What Does Mean Mean? Objective: Students will learn that values that gather around the center of a distribution show the typical value. This value is also referred to as the mean, or average. Materials: Pennies on a Ruler handout ( LMR_2.2_Pennies on a Ruler ) Markers (1 for each table) Rulers (1 for each table) Pennies (6 for each table group) Tape Digital Option : IDS Balancing Point app Balancing Point handout ( LMR_2.2b_Balancing_Point.pdf ) Exported, printed, and reproduced class\u2019s Personality Color survey data Advanced preparation required : The teacher must share students\u2019 data on the IDS Home page ( https://portal.idsucla.org ) before it can be exported and printed. Students will keep for use in subsequent lessons. Mr. Jones Mile Run Times handout ( LMR_2.3_Mr. Jones Run Times ) Vocabulary: measures of central tendency (or center) typical measures of variability (or spread) mean average balancing point Essential Concepts: Essential Concepts: The center of a distribution is the 'typical' value. One way of measuring the center is with the mean, which finds the balancing point of the distribution. The mean gives us the typical value, but does not tell the whole story. We need a way to measure the variability to understand how observations might differ from the typical value. Lesson: In student pairs, ask students to discuss what they think the following terms mean: Measures of central tendency. A value that shows the tendency of quantitative data to gather around a central, or typical , value. Also known as measures of center . Students will learn about two such measures: the mean and the median. Measures of variability. Values that show how much the quantitative data varies. Also known as measures of spread . Note: This is not taught during this lesson, but will be addressed as part of Lesson 4 . Ask a pair to share what they think these two terms mean. Pairs who are listening must decide whether they agree or disagree with the pair that shared. Lead a discussion based on their statements of agreement or disagreement. Communicate to the class that they will be learning more about these measures and what they tell us about data as we progress through this unit. By a show of hands, ask students how many are familiar with finding the mean , or average . Select a student to share his/her process for finding the mean. Possible answer: Add up all of the numbers. Then divide by how many numbers there are. Another way to find the mean is to find the balancing point of a distribution. They will learn about the balancing point via the activity in Steps 7 & 8. Distribute the Pennies on a Ruler handout ( LMR_2.2 ) along with a marker, ruler, tape, and 6 pennies to each table group. If you prefer to not print the document, you can project it on the board instead. LMR_2.2 Guide the students through the handout and have them share their findings throughout the activity. Be sure to emphasize the idea that the mean of a distribution can be identified by finding its balancing point. Next, distribute the class\u2019s Personality Color survey data to the students. Have student pairs find the variable Blue (whether or not that was their predominant color) in the class\u2019s printed data. As a class, make a dot plot on the board to show the distribution of Blue values. Each student should come to the board and draw a dot to indicate where their value is in the distribution. Ask the students: What do you think the typical Blue score is? Answers will vary by class. They should be driven to an answer in the center of the distribution. Are the data roughly symmetric? Where is the balancing point of this distribution? Answers will vary by class. Once a value is chosen, indicate the location on the dot plots. As a class, compute the mean Blue score for the entire class on the board and compare this value to the class\u2019s prediction of the balancing point. Students may not remember exactly how to compute the mean, so you can remind them of the general algorithm or refer them back to their responses from Step 5 above. Show the students the formula for calculating the mean: Now that they have calculated the mean for the Blue score, ask them to identify each symbol in the formula with a step in their algorithm for finding the mean, and discuss the meaning of the symbols in the formula as a class. x i represents each individual data point and n represents the total number of observations. Indicate the location of the calculated mean on the dot plots by drawing a vertical line at the value on the x-axis. Ask student pairs to engage in a conversation about how close the mean value is to their predicted balancing point and why their prediction was made that way. Select a pair to share their discussion with the whole class. Using the Personality Color survey data from Step 9, ask student pairs to compute the mean score for each of the other three personality colors. Inform the students that, during the next lesson , they will learn about another method that can be used for measuring the center of a distribution. Now, you can inform the class about an even easier method of calculating the mean \u2013 using RStudio! Explain that the command RStudio uses to calculate the mean incorporates the algorithm of summing up all the data and dividing by the total number of observations. Students will be able to use this command for quick calculations now. Note: If you have already \u201c Exported , Uploaded , Imported \u201d the class\u2019s Personality Color campaign data, you can simply use the exact command below to calculate the mean Blue score: > mean(~blue, data = colors) In general, the function can be denoted as follows: > mean(~variable, data = datafile) So, for our specific example, blue is the variable we want to find the mean value of, and colors is the datafile . Have the students Think-Pair-Share to discuss how the mean value of a group of data could be used to easily describe complicated things. For example, instead of giving someone the entire class\u2019s Blue scores, we could just tell him/her the mean score and he/she would have a general idea about the class. Class Scribes: One team of students will give a brief talk to discuss what they think the 3 most important topics of the day were. Homework Students should complete the Mr. Jones Mile Run Times handout ( LMR_2.3 ) for homework. They can practice finding the mean of distributions by determining a balancing point for the data. Answers to the handout are below. Note: The mean values in part (3) do NOT need to be exact. LMR_2.3 What kind of plots did Mr. Jones create for his classes? Histograms. Where does each distribution balance? Find and label the balancing point of each distribution. The balancing point for all of these distributions is at the mean. Based on the balancing points you found, what would you say the mean mile run time is for each class? i. Period 1: 9.91 ii. Period 2: 8.48 iii. Period 3: 8.45 iv. Period 4: 8.17","title":"Lesson 2: What Does Mean Mean?"},{"location":"unit2/lesson2/#lesson-2-what-does-mean-mean","text":"","title":"Lesson 2: What Does Mean Mean?"},{"location":"unit2/lesson2/#objective","text":"Students will learn that values that gather around the center of a distribution show the typical value. This value is also referred to as the mean, or average.","title":"Objective:"},{"location":"unit2/lesson2/#materials","text":"Pennies on a Ruler handout ( LMR_2.2_Pennies on a Ruler ) Markers (1 for each table) Rulers (1 for each table) Pennies (6 for each table group) Tape Digital Option : IDS Balancing Point app Balancing Point handout ( LMR_2.2b_Balancing_Point.pdf ) Exported, printed, and reproduced class\u2019s Personality Color survey data Advanced preparation required : The teacher must share students\u2019 data on the IDS Home page ( https://portal.idsucla.org ) before it can be exported and printed. Students will keep for use in subsequent lessons. Mr. Jones Mile Run Times handout ( LMR_2.3_Mr. Jones Run Times )","title":"Materials:"},{"location":"unit2/lesson2/#vocabulary","text":"measures of central tendency (or center) typical measures of variability (or spread) mean average balancing point","title":"Vocabulary:"},{"location":"unit2/lesson2/#essential-concepts","text":"Essential Concepts: The center of a distribution is the 'typical' value. One way of measuring the center is with the mean, which finds the balancing point of the distribution. The mean gives us the typical value, but does not tell the whole story. We need a way to measure the variability to understand how observations might differ from the typical value.","title":"Essential Concepts:"},{"location":"unit2/lesson2/#lesson","text":"In student pairs, ask students to discuss what they think the following terms mean: Measures of central tendency. A value that shows the tendency of quantitative data to gather around a central, or typical , value. Also known as measures of center . Students will learn about two such measures: the mean and the median. Measures of variability. Values that show how much the quantitative data varies. Also known as measures of spread . Note: This is not taught during this lesson, but will be addressed as part of Lesson 4 . Ask a pair to share what they think these two terms mean. Pairs who are listening must decide whether they agree or disagree with the pair that shared. Lead a discussion based on their statements of agreement or disagreement. Communicate to the class that they will be learning more about these measures and what they tell us about data as we progress through this unit. By a show of hands, ask students how many are familiar with finding the mean , or average . Select a student to share his/her process for finding the mean. Possible answer: Add up all of the numbers. Then divide by how many numbers there are. Another way to find the mean is to find the balancing point of a distribution. They will learn about the balancing point via the activity in Steps 7 & 8. Distribute the Pennies on a Ruler handout ( LMR_2.2 ) along with a marker, ruler, tape, and 6 pennies to each table group. If you prefer to not print the document, you can project it on the board instead. LMR_2.2 Guide the students through the handout and have them share their findings throughout the activity. Be sure to emphasize the idea that the mean of a distribution can be identified by finding its balancing point. Next, distribute the class\u2019s Personality Color survey data to the students. Have student pairs find the variable Blue (whether or not that was their predominant color) in the class\u2019s printed data. As a class, make a dot plot on the board to show the distribution of Blue values. Each student should come to the board and draw a dot to indicate where their value is in the distribution. Ask the students: What do you think the typical Blue score is? Answers will vary by class. They should be driven to an answer in the center of the distribution. Are the data roughly symmetric? Where is the balancing point of this distribution? Answers will vary by class. Once a value is chosen, indicate the location on the dot plots. As a class, compute the mean Blue score for the entire class on the board and compare this value to the class\u2019s prediction of the balancing point. Students may not remember exactly how to compute the mean, so you can remind them of the general algorithm or refer them back to their responses from Step 5 above. Show the students the formula for calculating the mean: Now that they have calculated the mean for the Blue score, ask them to identify each symbol in the formula with a step in their algorithm for finding the mean, and discuss the meaning of the symbols in the formula as a class. x i represents each individual data point and n represents the total number of observations. Indicate the location of the calculated mean on the dot plots by drawing a vertical line at the value on the x-axis. Ask student pairs to engage in a conversation about how close the mean value is to their predicted balancing point and why their prediction was made that way. Select a pair to share their discussion with the whole class. Using the Personality Color survey data from Step 9, ask student pairs to compute the mean score for each of the other three personality colors. Inform the students that, during the next lesson , they will learn about another method that can be used for measuring the center of a distribution. Now, you can inform the class about an even easier method of calculating the mean \u2013 using RStudio! Explain that the command RStudio uses to calculate the mean incorporates the algorithm of summing up all the data and dividing by the total number of observations. Students will be able to use this command for quick calculations now. Note: If you have already \u201c Exported , Uploaded , Imported \u201d the class\u2019s Personality Color campaign data, you can simply use the exact command below to calculate the mean Blue score: > mean(~blue, data = colors) In general, the function can be denoted as follows: > mean(~variable, data = datafile) So, for our specific example, blue is the variable we want to find the mean value of, and colors is the datafile . Have the students Think-Pair-Share to discuss how the mean value of a group of data could be used to easily describe complicated things. For example, instead of giving someone the entire class\u2019s Blue scores, we could just tell him/her the mean score and he/she would have a general idea about the class.","title":"Lesson:"},{"location":"unit2/lesson2/#class-scribes","text":"One team of students will give a brief talk to discuss what they think the 3 most important topics of the day were.","title":"Class Scribes:"},{"location":"unit2/lesson2/#homework","text":"Students should complete the Mr. Jones Mile Run Times handout ( LMR_2.3 ) for homework. They can practice finding the mean of distributions by determining a balancing point for the data. Answers to the handout are below. Note: The mean values in part (3) do NOT need to be exact. LMR_2.3 What kind of plots did Mr. Jones create for his classes? Histograms. Where does each distribution balance? Find and label the balancing point of each distribution. The balancing point for all of these distributions is at the mean. Based on the balancing points you found, what would you say the mean mile run time is for each class? i. Period 1: 9.91 ii. Period 2: 8.48 iii. Period 3: 8.45 iv. Period 4: 8.17","title":"Homework"},{"location":"unit2/lesson3/","text":"Lesson 3: Median in the Middle Objective: Students will learn that the median is another way to measure the center, or typical-ness, of a distribution, and will understand how medians compare and contrast with the mean. Materials: Sticky notes (one per student) Advanced preparation required (see step 6 below) Poster paper Graphics from Medians \u2013 Dotplots or Histograms ? ( LMR_2.4_Medians \u2013 Dotplots or Histograms ) Where is the Middle ? handout ( LMR_2.5_Where is the Middle ) Exported, printed, and reproduced class\u2019s Personality Color survey data Vocabulary: median Essential Concepts: Essential Concepts: Another measure of center is the median, which can also be used to represent the typical value of a distribution. The median is preferred for skewed distributions or when there are outliers because it better matches what we think of as 'typical.' Lesson: Remind students that, during the previous lesson, they learned about the mean as the balancing point of a distribution and as a measure of center. In statistics, there are a few values that can be considered as measures of center \u2013 the mean is one, and another is the median . The median is the middle value in a group of ordered observations. As a simple example, write or display the following group of numbers on the board: 8, 2, 6, 3, 7, 4, 9, 5, 5 Since there are 9 numbers in the list above, we should use the 5 th number as the median because it is directly in the middle and there are 4 numbers above it, and 4 numbers below it. However, students should realize that they cannot simply pick the middle number of the list as it is currently written (this would give a median value of 7). Instead, they must first arrange the numbers in numerical order (from lowest to highest). 2, 3, 4, 5, 5, 6, 7, 8, 9 Now they can identify that the true median value of this list of numbers is 5. Next, randomly distribute one sticky note to each student. Advanced preparation required: There should be one card for every student in the class. All of the cards, except one, need to have the value 0 written on them. One card should have the value 1,000,000 written on it. Place poster paper on the board and have the students create a dot plot by placing their sticky notes at the corresponding values on the axis. Then, ask and record answers to the following questions: What is the typical value of these data? 0 \u2013 all sticky notes but one have a value of 0. Using the formula we learned in class, calculate the mean, or average, value of this distribution. Answer will vary by class/class size. Example: for a class with 28 students enrolled, there would be 27 values of 0 and 1 value of 1,000,000. Therefore, the mean value would be (0*27 + 1,000,000)/28 \u2248 35,714.3. Does the mean you calculated match your understanding of \u201ctypical?\u201d Why is the mean not capturing our notion of \u201ctypical?\u201d The 1,000,000 value is heavily skewing the calculation of the mean. It is pulling the mean to a higher value than what we consider to be typical for these data. Since we introduced the idea of the median as a measure of center at the beginning of class, have the students find the median value of the data on their sticky notes. If time permits, have them place the sticky notes in a line across the board in order (from least to greatest) and have them find the middle number. The median value will be 0. Ask students why there is such a large difference between the mean and median values even though they are both measures of center? Is there a specific reason why the mean is larger than the median for this particular set of data? In this case, there was an outlier value that skewed the distribution and forced the balancing point to move to the right. Display the first 2 plots in the Medians \u2013 Dotplots or Histograms ? file ( LMR_2.4 ). They are labeled as plots for discussion for the beginning of class. Both the dotplot and histogram depict the number of candies eaten by a group of 17 high school students. LMR_2.4 For the first 2 plots, ask students: Which plot makes it easier to find the median number of candies eaten \u2013 the dot plot or the histogram? Why? The dot plot is easier because we can simply find the middle dot and record the value. It is harder on the histogram, because we would have to add up the amount in each bar to find the middle person. What is the median value? The median number of candies eaten is 1 candy. Inform the students that they will practice finding medians of distributions using the Where is the Middle? handout ( LMR_2.5 ). They will be determining medians when distributions have different shapes (e.g., symmetric, left-skewed, right-skewed). Distribute the Where is the Middle? handout ( LMR_2.5 ). Students should complete the handout individually first, then compare answers with their team members. Once each team has agreed upon their answers, discuss the handout as a class. LMR_2.5 Ask the following questions to elicit a team discussion about the relationship between means and medians: What did you notice about the relationship between the mean and median values for the symmetric distributions? The mean and median values in the symmetric distributions - plots (a) and (d) - are fairly similar. For plot (a), the mean and median are exactly equal. For plot (d), the mean is actually larger than the median, but not by much (2.29 > 2). What did you notice about the relationship between the mean and median values for the left-skewed distributions? The mean value was smaller than the median value in both of the left-skewed distributions - plots (c) and (f). Both plots had the same values for the mean (2.53) and the median (3.00) - clearly, the mean is much smaller than the median (2.53 < 3). What did you notice about the relationship between the mean and median values for the right-skewed distributions? The mean value was larger than the median value in both of the right-skewed distributions - plots (b) and (e). For plot (b), the mean was only slightly higher than the median (1.18 > 1). For plot (e), the mean was a decent amount higher than the median (0.47 > 0). Steer the discussion towards the relationship between the shape of a distribution and its corresponding mean and median values. Is there a pattern that emerges between the mean and median values for differently shaped distributions? Yes! It seems that symmetric distributions will produce similar mean and median values, left-skewed distributions will produce smaller means and higher medians, and right-skewed distributions will produce higher means and smaller medians. For each of the plots in the Where is the Middle? handout ( LMR_2.5 ), which value better matches your idea of \u201ctypical\u201d for that specific distribution? For plot (a), both the mean and median agree and appear to be the balancing point of the distribution \u2013 both match what we think is typical. For plot (b), the median seems to be more typical, but the values are very close. For plot (c), the median appears to be a more typical value. For plot (d), both the mean and median appear to be capturing our idea of typical. For plot (e), the median is a better match to typical. For plot (f), the median is also a better match. Steer the discussion so that students recognize that the better measures of center for skewed distributions are typically medians, and the better measures for center for symmetric distributions are typically means. Display the last 2 plots in the Medians \u2013 Dotplots or Histograms file ( LMR_2.4 ). They are labeled as plots for discussion for the end of class. Both the dot plot and histogram depict the number of candies eaten by a group of 330 high school students. For the last 2 plots, ask students: Which plot makes it easier to find the median number of candies eaten \u2013 the dot plot or the histogram? Why? The histogram is easier because we can estimate based on the distribution\u2019s shape. There are too many dots in the dot plot to find the exact middle person. What is the median value? The median number of candies eaten is 7 candies. Class Scribes: One team of students will give a brief talk to discuss what they think the 3 most important topics of the day were. Homework Students should calculate the median values for each of their personality color scores. They should compare the median values to the mean values (calculated in Lesson 2 ) and make a decision about the possible shape of the distribution if we were to create a dot plot of the scores.","title":"Lesson 3: Median in the Middle"},{"location":"unit2/lesson3/#lesson-3-median-in-the-middle","text":"","title":"Lesson 3: Median in the Middle"},{"location":"unit2/lesson3/#objective","text":"Students will learn that the median is another way to measure the center, or typical-ness, of a distribution, and will understand how medians compare and contrast with the mean.","title":"Objective:"},{"location":"unit2/lesson3/#materials","text":"Sticky notes (one per student) Advanced preparation required (see step 6 below) Poster paper Graphics from Medians \u2013 Dotplots or Histograms ? ( LMR_2.4_Medians \u2013 Dotplots or Histograms ) Where is the Middle ? handout ( LMR_2.5_Where is the Middle ) Exported, printed, and reproduced class\u2019s Personality Color survey data","title":"Materials:"},{"location":"unit2/lesson3/#vocabulary","text":"median","title":"Vocabulary:"},{"location":"unit2/lesson3/#essential-concepts","text":"Essential Concepts: Another measure of center is the median, which can also be used to represent the typical value of a distribution. The median is preferred for skewed distributions or when there are outliers because it better matches what we think of as 'typical.'","title":"Essential Concepts:"},{"location":"unit2/lesson3/#lesson","text":"Remind students that, during the previous lesson, they learned about the mean as the balancing point of a distribution and as a measure of center. In statistics, there are a few values that can be considered as measures of center \u2013 the mean is one, and another is the median . The median is the middle value in a group of ordered observations. As a simple example, write or display the following group of numbers on the board: 8, 2, 6, 3, 7, 4, 9, 5, 5 Since there are 9 numbers in the list above, we should use the 5 th number as the median because it is directly in the middle and there are 4 numbers above it, and 4 numbers below it. However, students should realize that they cannot simply pick the middle number of the list as it is currently written (this would give a median value of 7). Instead, they must first arrange the numbers in numerical order (from lowest to highest). 2, 3, 4, 5, 5, 6, 7, 8, 9 Now they can identify that the true median value of this list of numbers is 5. Next, randomly distribute one sticky note to each student. Advanced preparation required: There should be one card for every student in the class. All of the cards, except one, need to have the value 0 written on them. One card should have the value 1,000,000 written on it. Place poster paper on the board and have the students create a dot plot by placing their sticky notes at the corresponding values on the axis. Then, ask and record answers to the following questions: What is the typical value of these data? 0 \u2013 all sticky notes but one have a value of 0. Using the formula we learned in class, calculate the mean, or average, value of this distribution. Answer will vary by class/class size. Example: for a class with 28 students enrolled, there would be 27 values of 0 and 1 value of 1,000,000. Therefore, the mean value would be (0*27 + 1,000,000)/28 \u2248 35,714.3. Does the mean you calculated match your understanding of \u201ctypical?\u201d Why is the mean not capturing our notion of \u201ctypical?\u201d The 1,000,000 value is heavily skewing the calculation of the mean. It is pulling the mean to a higher value than what we consider to be typical for these data. Since we introduced the idea of the median as a measure of center at the beginning of class, have the students find the median value of the data on their sticky notes. If time permits, have them place the sticky notes in a line across the board in order (from least to greatest) and have them find the middle number. The median value will be 0. Ask students why there is such a large difference between the mean and median values even though they are both measures of center? Is there a specific reason why the mean is larger than the median for this particular set of data? In this case, there was an outlier value that skewed the distribution and forced the balancing point to move to the right. Display the first 2 plots in the Medians \u2013 Dotplots or Histograms ? file ( LMR_2.4 ). They are labeled as plots for discussion for the beginning of class. Both the dotplot and histogram depict the number of candies eaten by a group of 17 high school students. LMR_2.4 For the first 2 plots, ask students: Which plot makes it easier to find the median number of candies eaten \u2013 the dot plot or the histogram? Why? The dot plot is easier because we can simply find the middle dot and record the value. It is harder on the histogram, because we would have to add up the amount in each bar to find the middle person. What is the median value? The median number of candies eaten is 1 candy. Inform the students that they will practice finding medians of distributions using the Where is the Middle? handout ( LMR_2.5 ). They will be determining medians when distributions have different shapes (e.g., symmetric, left-skewed, right-skewed). Distribute the Where is the Middle? handout ( LMR_2.5 ). Students should complete the handout individually first, then compare answers with their team members. Once each team has agreed upon their answers, discuss the handout as a class. LMR_2.5 Ask the following questions to elicit a team discussion about the relationship between means and medians: What did you notice about the relationship between the mean and median values for the symmetric distributions? The mean and median values in the symmetric distributions - plots (a) and (d) - are fairly similar. For plot (a), the mean and median are exactly equal. For plot (d), the mean is actually larger than the median, but not by much (2.29 > 2). What did you notice about the relationship between the mean and median values for the left-skewed distributions? The mean value was smaller than the median value in both of the left-skewed distributions - plots (c) and (f). Both plots had the same values for the mean (2.53) and the median (3.00) - clearly, the mean is much smaller than the median (2.53 < 3). What did you notice about the relationship between the mean and median values for the right-skewed distributions? The mean value was larger than the median value in both of the right-skewed distributions - plots (b) and (e). For plot (b), the mean was only slightly higher than the median (1.18 > 1). For plot (e), the mean was a decent amount higher than the median (0.47 > 0). Steer the discussion towards the relationship between the shape of a distribution and its corresponding mean and median values. Is there a pattern that emerges between the mean and median values for differently shaped distributions? Yes! It seems that symmetric distributions will produce similar mean and median values, left-skewed distributions will produce smaller means and higher medians, and right-skewed distributions will produce higher means and smaller medians. For each of the plots in the Where is the Middle? handout ( LMR_2.5 ), which value better matches your idea of \u201ctypical\u201d for that specific distribution? For plot (a), both the mean and median agree and appear to be the balancing point of the distribution \u2013 both match what we think is typical. For plot (b), the median seems to be more typical, but the values are very close. For plot (c), the median appears to be a more typical value. For plot (d), both the mean and median appear to be capturing our idea of typical. For plot (e), the median is a better match to typical. For plot (f), the median is also a better match. Steer the discussion so that students recognize that the better measures of center for skewed distributions are typically medians, and the better measures for center for symmetric distributions are typically means. Display the last 2 plots in the Medians \u2013 Dotplots or Histograms file ( LMR_2.4 ). They are labeled as plots for discussion for the end of class. Both the dot plot and histogram depict the number of candies eaten by a group of 330 high school students. For the last 2 plots, ask students: Which plot makes it easier to find the median number of candies eaten \u2013 the dot plot or the histogram? Why? The histogram is easier because we can estimate based on the distribution\u2019s shape. There are too many dots in the dot plot to find the exact middle person. What is the median value? The median number of candies eaten is 7 candies.","title":"Lesson:"},{"location":"unit2/lesson3/#class-scribes","text":"One team of students will give a brief talk to discuss what they think the 3 most important topics of the day were.","title":"Class Scribes:"},{"location":"unit2/lesson3/#homework","text":"Students should calculate the median values for each of their personality color scores. They should compare the median values to the mean values (calculated in Lesson 2 ) and make a decision about the possible shape of the distribution if we were to create a dot plot of the scores.","title":"Homework"},{"location":"unit2/lesson4/","text":"Lesson 4: How Far is it from Typical? Objective: Students will understand that the mean of the absolute deviations (MAD) is a way to assess the degree of variation in the data from the mean and adjusts for differences in the number of points in the data set ( n ). The MAD measures the total distance between all the data values from the mean and divides it by the number of observations in the data set. Materials: Masking tape (or painter\u2019s tape) \u2013 approximately 4-5 feet long \u2013 one for each student team How Far Apart? handout ( LMR_2.6_How Far Apart ) \u2013 will be used again in Lesson 17 Exported, printed, and reproduced class\u2019s Personality Color survey data Vocabulary: measures of variability (or spread) deviation mean of absolute deviations (MAD) Essential Concepts: Essential Concepts: MAD measures the variability in a sample of data - the larger the value, the greater the variability. More precisely, the MAD is the typical distance of observations from the mean. There are other measures of spread as well, notably the standard deviation and the interquartile range (IQR). Lesson: Remind students that they learned about 2 different measures of center during the previous 2 lessons: the mean and the median. Have the students recall when it is appropriate to use each value based on the shape of the distribution. Mean \u2013 use with symmetric distributions. Median \u2013 use with skewed distributions or when there are outliers. Inform the students that, during today\u2019s lesson, they will learn about measures of variability \u2013 also known as measures of spread . These values show us how much the quantitative data varies from the center of a distribution. Similar to measures of center, we will use two different measures of spread: (1) the mean of absolute deviations (MAD), and (2) the interquartile range (IQR). Note: IQR will be discussed in detail during Lesson 5 . Introduce the term deviation . Using Think , Pair , Share , ask students what they think this word means and how it could relate to variability. A deviation is the act of departing from an established course or accepted standard. Common synonyms include departure, detour, difference, digression, divergence, fluctuation, inconsistency, modification, shift, etc. On the classroom floor next to each student team, place a 4-5 foot long piece of masking tape (or painter\u2019s tape). Then, propose the following scenario: Your team has been invited to guest star at the circus! You have been asked to perform as part of the tightrope act \u2013 a routine that requires tremendous focus and balance to walk across a tightly pulled rope that is suspended high in the air. In order to practice your balancing skills, the circus has provided your team with a line of tape that will represent the tightrope. Have the students consider the piece of tape (aka the rope) to be the \u201ctypical\u201d path they must take to finish the circus act. Since they do not want to fall from the suspended tightrope while performing at the actual circus, they will need to practice walking directly on the middle of the line at all times. If they deviate from the line, they will no longer be walking the \u201ctypical\u201d path, and will likely fall. Each team should select one student to be their starting performer. In teams of 4, one student is the performer, two are measuring the distance of the deviation (one on each side of the tape), and one is the recorder. Place a ruler perpendicular to the \u201crope\u201d and measure the distance, in centimeters, from the path to the center of the back of their heel as the student walks and attempts to balance across the \u201crope.\u201d The performer will walk the tightrope by looking straight up to the sky \u2013 first they look to place a foot on the line, then walk naturally while looking up to the sky, and repeating one step at a time for 4 steps, measuring after each step. Any time the performer missteps, this is considered a variation from the typical value. You can have students take turns so everyone gets a chance to balance, walk, and to measure, depending on time in your class. Now that the students have an idea about what it means to deviate from something they consider \u201ctypical,\u201d they can start looking at distributions to see how data points vary from their typical value. Inform students that they were observing deviations from typical while calculating actual differences between the rope and the performer\u2019s steps. When data are quantified with numbers, we can then calculate how far away each value is from the center. One such calculation that is popular among data scientists is the mean of absolute deviations (MAD). Ask students to consider the components of the MAD in math terms, and brainstorm what the MAD value might represent. mean \u2013 an average absolute \u2013 in mathematics, we talk about absolute value, the positive difference between 2 numerical values deviation \u2013 as discussed earlier in the lesson, deviation represents how much things vary Using the 3 components in Step 12, explain that the MAD measures the absolute distance of each data point from the mean, and then finds the average of all those distances. Display the formula for the MAD distribution for the whole class to see. Discuss what each symbol in the formula means and how we use it to perform the calculation. x i represents each individual data point, x\u0304 represents the mean value, and n represents the total number of observations. The symbol \u03a3 represents the summation \u2013 this tells us to add up all the absolute distances from each point to the mean. To practice using this formula with actual data, students will calculate and compare the MAD values for 2 distributions. Distribute the How Far Apart? handout ( LMR_2.6 ), which contains 2 of the dot plots - plots (a) and (c) from the Where is the Middle? handout ( LMR_2.5 ) used in Lesson 3 . As before, the dot plots depict the number of candies eaten by a group of 17 high school students on different days of the week. The means are also given. LMR_2.6 The calculations for each plot are shown below for the teacher\u2019s reference. MAD for plot (a) MAD for plot (c) Students may work in pairs to complete the handout. After all student pairs have come to an agreement on their answers, pose the following questions to the class as a whole: Which MAD value did you think would be larger based only on the look/shape of the distributions? Why? Since plot (c) is skewed to the left, it probably has a larger MAD because more points will be further away from the mean than in plot (a). Which MAD value was actually larger when you calculated it? The MAD value for plot (c) was larger (1.1418 > 0.8253). Did your prediction match the actual calculated values, or were you surprised by the results? Yes. The distribution with the wider spread (more variability) had the larger MAD value. To continue exploring with the class\u2019s Personality Color survey data, student teams should calculate the MAD value for their Blue scores. Does the MAD value seem reasonable based on the dot plot they created during Lesson 2 ? Class Scribes: One team of students will give a brief talk to discuss what they think the 3 most important topics of the day were. Homework & Next Day Students should calculate the MAD values for each of the other 3 personality color scores and compare the values of the 4 color scores. Declutter your Environment Pane Unit 2 utilizes new datasets so it's a good idea to declutter your Environment Pane. Refer students to this video or demonstrate it on your own Environment. LAB 2A: All About Distributions Complete Lab 2A prior to Lesson 5 .","title":"Lesson 4: How Far is it from Typical?"},{"location":"unit2/lesson4/#lesson-4-how-far-is-it-from-typical","text":"","title":"Lesson 4: How Far is it from Typical?"},{"location":"unit2/lesson4/#objective","text":"Students will understand that the mean of the absolute deviations (MAD) is a way to assess the degree of variation in the data from the mean and adjusts for differences in the number of points in the data set ( n ). The MAD measures the total distance between all the data values from the mean and divides it by the number of observations in the data set.","title":"Objective:"},{"location":"unit2/lesson4/#materials","text":"Masking tape (or painter\u2019s tape) \u2013 approximately 4-5 feet long \u2013 one for each student team How Far Apart? handout ( LMR_2.6_How Far Apart ) \u2013 will be used again in Lesson 17 Exported, printed, and reproduced class\u2019s Personality Color survey data","title":"Materials:"},{"location":"unit2/lesson4/#vocabulary","text":"measures of variability (or spread) deviation mean of absolute deviations (MAD)","title":"Vocabulary:"},{"location":"unit2/lesson4/#essential-concepts","text":"Essential Concepts: MAD measures the variability in a sample of data - the larger the value, the greater the variability. More precisely, the MAD is the typical distance of observations from the mean. There are other measures of spread as well, notably the standard deviation and the interquartile range (IQR).","title":"Essential Concepts:"},{"location":"unit2/lesson4/#lesson","text":"Remind students that they learned about 2 different measures of center during the previous 2 lessons: the mean and the median. Have the students recall when it is appropriate to use each value based on the shape of the distribution. Mean \u2013 use with symmetric distributions. Median \u2013 use with skewed distributions or when there are outliers. Inform the students that, during today\u2019s lesson, they will learn about measures of variability \u2013 also known as measures of spread . These values show us how much the quantitative data varies from the center of a distribution. Similar to measures of center, we will use two different measures of spread: (1) the mean of absolute deviations (MAD), and (2) the interquartile range (IQR). Note: IQR will be discussed in detail during Lesson 5 . Introduce the term deviation . Using Think , Pair , Share , ask students what they think this word means and how it could relate to variability. A deviation is the act of departing from an established course or accepted standard. Common synonyms include departure, detour, difference, digression, divergence, fluctuation, inconsistency, modification, shift, etc. On the classroom floor next to each student team, place a 4-5 foot long piece of masking tape (or painter\u2019s tape). Then, propose the following scenario: Your team has been invited to guest star at the circus! You have been asked to perform as part of the tightrope act \u2013 a routine that requires tremendous focus and balance to walk across a tightly pulled rope that is suspended high in the air. In order to practice your balancing skills, the circus has provided your team with a line of tape that will represent the tightrope. Have the students consider the piece of tape (aka the rope) to be the \u201ctypical\u201d path they must take to finish the circus act. Since they do not want to fall from the suspended tightrope while performing at the actual circus, they will need to practice walking directly on the middle of the line at all times. If they deviate from the line, they will no longer be walking the \u201ctypical\u201d path, and will likely fall. Each team should select one student to be their starting performer. In teams of 4, one student is the performer, two are measuring the distance of the deviation (one on each side of the tape), and one is the recorder. Place a ruler perpendicular to the \u201crope\u201d and measure the distance, in centimeters, from the path to the center of the back of their heel as the student walks and attempts to balance across the \u201crope.\u201d The performer will walk the tightrope by looking straight up to the sky \u2013 first they look to place a foot on the line, then walk naturally while looking up to the sky, and repeating one step at a time for 4 steps, measuring after each step. Any time the performer missteps, this is considered a variation from the typical value. You can have students take turns so everyone gets a chance to balance, walk, and to measure, depending on time in your class. Now that the students have an idea about what it means to deviate from something they consider \u201ctypical,\u201d they can start looking at distributions to see how data points vary from their typical value. Inform students that they were observing deviations from typical while calculating actual differences between the rope and the performer\u2019s steps. When data are quantified with numbers, we can then calculate how far away each value is from the center. One such calculation that is popular among data scientists is the mean of absolute deviations (MAD). Ask students to consider the components of the MAD in math terms, and brainstorm what the MAD value might represent. mean \u2013 an average absolute \u2013 in mathematics, we talk about absolute value, the positive difference between 2 numerical values deviation \u2013 as discussed earlier in the lesson, deviation represents how much things vary Using the 3 components in Step 12, explain that the MAD measures the absolute distance of each data point from the mean, and then finds the average of all those distances. Display the formula for the MAD distribution for the whole class to see. Discuss what each symbol in the formula means and how we use it to perform the calculation. x i represents each individual data point, x\u0304 represents the mean value, and n represents the total number of observations. The symbol \u03a3 represents the summation \u2013 this tells us to add up all the absolute distances from each point to the mean. To practice using this formula with actual data, students will calculate and compare the MAD values for 2 distributions. Distribute the How Far Apart? handout ( LMR_2.6 ), which contains 2 of the dot plots - plots (a) and (c) from the Where is the Middle? handout ( LMR_2.5 ) used in Lesson 3 . As before, the dot plots depict the number of candies eaten by a group of 17 high school students on different days of the week. The means are also given. LMR_2.6 The calculations for each plot are shown below for the teacher\u2019s reference. MAD for plot (a) MAD for plot (c) Students may work in pairs to complete the handout. After all student pairs have come to an agreement on their answers, pose the following questions to the class as a whole: Which MAD value did you think would be larger based only on the look/shape of the distributions? Why? Since plot (c) is skewed to the left, it probably has a larger MAD because more points will be further away from the mean than in plot (a). Which MAD value was actually larger when you calculated it? The MAD value for plot (c) was larger (1.1418 > 0.8253). Did your prediction match the actual calculated values, or were you surprised by the results? Yes. The distribution with the wider spread (more variability) had the larger MAD value. To continue exploring with the class\u2019s Personality Color survey data, student teams should calculate the MAD value for their Blue scores. Does the MAD value seem reasonable based on the dot plot they created during Lesson 2 ?","title":"Lesson:"},{"location":"unit2/lesson4/#class-scribes","text":"One team of students will give a brief talk to discuss what they think the 3 most important topics of the day were.","title":"Class Scribes:"},{"location":"unit2/lesson4/#homework-next-day","text":"Students should calculate the MAD values for each of the other 3 personality color scores and compare the values of the 4 color scores. Declutter your Environment Pane Unit 2 utilizes new datasets so it's a good idea to declutter your Environment Pane. Refer students to this video or demonstrate it on your own Environment. LAB 2A: All About Distributions Complete Lab 2A prior to Lesson 5 .","title":"Homework &amp; Next Day"},{"location":"unit2/lesson5/","text":"Lesson 5: Human Boxplots Objective: Students will learn how and when to use boxplots to compare groups of data. They will learn how to compute and interpret another measure of spread: the IQR. Materials: Poster paper, 3-4 feet long Advanced preparation required (see Step 9 below) Tape Poster paper Markers Ages of Oscar Winners handout ( LMR_2.7_Oscar Ages ) Vocabulary: boxplot quartiles first quartile (Q 1 ) third quartile (Q 3 ) quantiles minimum maximum five-number summary range interquartile range (IQR) Essential Concepts: Essential Concepts: A common statistical investigative question is \u201cHow does this group compare to that group?\u201d This is a hard question to answer when the groups have lots of variability. One approach is to compare the centers, spreads, and shapes of the distributions. Boxplots are a useful way of comparing distributions from different groups when all of the distributions are unimodal (one hump). Lesson: Remind students that we have been using the following numerical and graphical summaries to look at data: Measures of center \u2013 mean, median Measures of spread \u2013 MAD Graphing \u2013 dotPlots, histograms Explain that all of these tools help us describe data to someone who may not actually be viewing it. Today, we will explore another way to summarize and describe data to others with the use of another type of statistical plot that involves breaking data up into distinct pieces: a boxplot . For the next activity, students will need to carry their DS journals and a pen with them. Instruct students to stand up and move their chairs away from the longest wall in the classroom. Ask them to line up against the wall (in no particular order). Note: If there isn\u2019t enough room for everyone to line up together inside the classroom, you may do this activity outside along a building wall. Say, \u201cI want to know which person represents the typical height of students in our class. Can I tell by looking at the line as it currently stands? How would I be able to tell?\u201d Students should discuss with a partner. Ask students to share their discussions. Call on students to contribute to what has been shared if needed. Guide students to see that organizing the data (in other words, themselves) can give you a visual for their heights. Then tell them to line up in height order from shortest to tallest along the wall. Once students are arranged (and this may take a little time\u2014allow students to develop their own algorithm for finding the ordering), ask them how they might be able to describe their distribution of heights. Possible answers include: mean, median, MAD. Ask them to split themselves into two groups, one half that is taller and one half that is shorter, and have them decide which student represents the class\u2019s median height. Have the median student stand next to the wall directly in front of the poster paper. Advanced preparation required: Before class begins, tape a piece of poster paper, approximately 3-4 feet long, vertically to a wall in the classroom. The students will be creating a plot using lines drawn at certain students\u2019 heights. Draw a horizontal line on the poster paper to mark the location of the median by having the actual student stand in front of the poster paper so you can mark his/her exact height. Be sure to label this point as the median and include the student\u2019s actual height, in inches. Next, ask the two halves to split again, so there are now four groups of students. The breaks between each group are called quartiles because they break the data into four groups ( quartile comes from the Latin word quartus , which is also the root of the Spanish word cuatro ). The lower break represents the first quartile (because 25% of the class is shorter than this student\u2019s height), and the upper break represents the third quartile (because 75% of the class is shorter than this height). Another term that can be used in place of percentiles is quantiles because they represent the quantity of data that is lower than that value. Using the student who represents the first quartile, draw another horizontal line on the poster paper marking his/her height. The student should stand in the same spot as the student who represented the median so that the line for this student is drawn underneath the median line. Be sure to label this point as the first quartile (or Q 1 ) and include the student\u2019s actual height, in inches. Using the student who represents the third quartile, draw another horizontal line on the poster paper marking his/her height. The student should stand in the same spot as the student who represented the median so that the line for this student is drawn above the median line. Be sure to label this point as the third quartile (or Q 3 ) and include the student\u2019s actual height, in inches. Finally, ask the tallest and shortest student to stand in front of the poster paper and draw horizontal lines at their heights. The shortest person represents the minimum height of the students in the class, and the tallest person represents the maximum height. Be sure to label the points as the minimum and maximum, and include the students\u2019 actual heights, in inches. When you finish, you should have five lines, which represent the five-number summary : minimum, first quartile, median, third quartile, and maximum. Draw a box using the first and third quartiles as the edges of the box. The median line will be contained within the box. Extend a line from the first quartile down to the minimum and extend a line from the third quartile up to the maximum. Your class\u2019s boxplot should look similar to the following: Students should now be facing the newly created boxplot. Allow students time to sketch the boxplot in their DS journals, with the appropriate labels. Ask students: What is the difference between the largest and smallest heights? Is there a large difference between the tallest and shortest person? Students should calculate maximum \u2013 minimum. Inform students that this difference is known as the range of the data set. What is the difference between the quartiles Q 1 and Q 3 ? What percent of our class falls within these two values? Students should calculate Q 3 \u2013 Q 1 . 50% of the class falls between these two height values. Inform students that this difference is known as the interquartile range (or IQR) . Remind students that they learned about one measure of spread (the MAD) during the previous lesson, and tell them that we now have another measure of spread \u2013 the IQR. Pose the following questions to the students: What does it mean when the IQR is small? The middle 50% of heights are close to each other. What does it mean when the IQR is large? The middle 50% of heights are more spread out. Finally, subset the class into introverts and extroverts. Ask each group of students (the introverts and extroverts) to create a boxplot of their group\u2019s heights on a piece of poster paper using the techniques they just learned as a class. Ask each group to share their boxplot with the class. Lead a discussion about the similarities and differences between the plots, and be sure to include how they compare to the overall combined boxplot of heights they created earlier. In the discussion, have the students calculate the IQR for both plots and make a comparison by asking: What does the IQR tell us about each group? Answers will vary by class. Class Scribes: One team of students will give a brief talk to discuss what they think the 3 most important topics of the day were. Homework Students should complete the Ages of Oscar Winners handout ( LMR_2.7 ) for homework using their newly acquired knowledge of boxplots LMR_2.7","title":"Lesson 5: Human Boxplots"},{"location":"unit2/lesson5/#lesson-5-human-boxplots","text":"","title":"Lesson 5: Human Boxplots"},{"location":"unit2/lesson5/#objective","text":"Students will learn how and when to use boxplots to compare groups of data. They will learn how to compute and interpret another measure of spread: the IQR.","title":"Objective:"},{"location":"unit2/lesson5/#materials","text":"Poster paper, 3-4 feet long Advanced preparation required (see Step 9 below) Tape Poster paper Markers Ages of Oscar Winners handout ( LMR_2.7_Oscar Ages )","title":"Materials:"},{"location":"unit2/lesson5/#vocabulary","text":"boxplot quartiles first quartile (Q 1 ) third quartile (Q 3 ) quantiles minimum maximum five-number summary range interquartile range (IQR)","title":"Vocabulary:"},{"location":"unit2/lesson5/#essential-concepts","text":"Essential Concepts: A common statistical investigative question is \u201cHow does this group compare to that group?\u201d This is a hard question to answer when the groups have lots of variability. One approach is to compare the centers, spreads, and shapes of the distributions. Boxplots are a useful way of comparing distributions from different groups when all of the distributions are unimodal (one hump).","title":"Essential Concepts:"},{"location":"unit2/lesson5/#lesson","text":"Remind students that we have been using the following numerical and graphical summaries to look at data: Measures of center \u2013 mean, median Measures of spread \u2013 MAD Graphing \u2013 dotPlots, histograms Explain that all of these tools help us describe data to someone who may not actually be viewing it. Today, we will explore another way to summarize and describe data to others with the use of another type of statistical plot that involves breaking data up into distinct pieces: a boxplot . For the next activity, students will need to carry their DS journals and a pen with them. Instruct students to stand up and move their chairs away from the longest wall in the classroom. Ask them to line up against the wall (in no particular order). Note: If there isn\u2019t enough room for everyone to line up together inside the classroom, you may do this activity outside along a building wall. Say, \u201cI want to know which person represents the typical height of students in our class. Can I tell by looking at the line as it currently stands? How would I be able to tell?\u201d Students should discuss with a partner. Ask students to share their discussions. Call on students to contribute to what has been shared if needed. Guide students to see that organizing the data (in other words, themselves) can give you a visual for their heights. Then tell them to line up in height order from shortest to tallest along the wall. Once students are arranged (and this may take a little time\u2014allow students to develop their own algorithm for finding the ordering), ask them how they might be able to describe their distribution of heights. Possible answers include: mean, median, MAD. Ask them to split themselves into two groups, one half that is taller and one half that is shorter, and have them decide which student represents the class\u2019s median height. Have the median student stand next to the wall directly in front of the poster paper. Advanced preparation required: Before class begins, tape a piece of poster paper, approximately 3-4 feet long, vertically to a wall in the classroom. The students will be creating a plot using lines drawn at certain students\u2019 heights. Draw a horizontal line on the poster paper to mark the location of the median by having the actual student stand in front of the poster paper so you can mark his/her exact height. Be sure to label this point as the median and include the student\u2019s actual height, in inches. Next, ask the two halves to split again, so there are now four groups of students. The breaks between each group are called quartiles because they break the data into four groups ( quartile comes from the Latin word quartus , which is also the root of the Spanish word cuatro ). The lower break represents the first quartile (because 25% of the class is shorter than this student\u2019s height), and the upper break represents the third quartile (because 75% of the class is shorter than this height). Another term that can be used in place of percentiles is quantiles because they represent the quantity of data that is lower than that value. Using the student who represents the first quartile, draw another horizontal line on the poster paper marking his/her height. The student should stand in the same spot as the student who represented the median so that the line for this student is drawn underneath the median line. Be sure to label this point as the first quartile (or Q 1 ) and include the student\u2019s actual height, in inches. Using the student who represents the third quartile, draw another horizontal line on the poster paper marking his/her height. The student should stand in the same spot as the student who represented the median so that the line for this student is drawn above the median line. Be sure to label this point as the third quartile (or Q 3 ) and include the student\u2019s actual height, in inches. Finally, ask the tallest and shortest student to stand in front of the poster paper and draw horizontal lines at their heights. The shortest person represents the minimum height of the students in the class, and the tallest person represents the maximum height. Be sure to label the points as the minimum and maximum, and include the students\u2019 actual heights, in inches. When you finish, you should have five lines, which represent the five-number summary : minimum, first quartile, median, third quartile, and maximum. Draw a box using the first and third quartiles as the edges of the box. The median line will be contained within the box. Extend a line from the first quartile down to the minimum and extend a line from the third quartile up to the maximum. Your class\u2019s boxplot should look similar to the following: Students should now be facing the newly created boxplot. Allow students time to sketch the boxplot in their DS journals, with the appropriate labels. Ask students: What is the difference between the largest and smallest heights? Is there a large difference between the tallest and shortest person? Students should calculate maximum \u2013 minimum. Inform students that this difference is known as the range of the data set. What is the difference between the quartiles Q 1 and Q 3 ? What percent of our class falls within these two values? Students should calculate Q 3 \u2013 Q 1 . 50% of the class falls between these two height values. Inform students that this difference is known as the interquartile range (or IQR) . Remind students that they learned about one measure of spread (the MAD) during the previous lesson, and tell them that we now have another measure of spread \u2013 the IQR. Pose the following questions to the students: What does it mean when the IQR is small? The middle 50% of heights are close to each other. What does it mean when the IQR is large? The middle 50% of heights are more spread out. Finally, subset the class into introverts and extroverts. Ask each group of students (the introverts and extroverts) to create a boxplot of their group\u2019s heights on a piece of poster paper using the techniques they just learned as a class. Ask each group to share their boxplot with the class. Lead a discussion about the similarities and differences between the plots, and be sure to include how they compare to the overall combined boxplot of heights they created earlier. In the discussion, have the students calculate the IQR for both plots and make a comparison by asking: What does the IQR tell us about each group? Answers will vary by class.","title":"Lesson:"},{"location":"unit2/lesson5/#class-scribes","text":"One team of students will give a brief talk to discuss what they think the 3 most important topics of the day were.","title":"Class Scribes:"},{"location":"unit2/lesson5/#homework","text":"Students should complete the Ages of Oscar Winners handout ( LMR_2.7 ) for homework using their newly acquired knowledge of boxplots LMR_2.7","title":"Homework"},{"location":"unit2/lesson6/","text":"Lesson 6: Face Off Objective: Students will informally compare two or more distributions using their knowledge of shape, center, and spread to answer statistical investigative questions. They will learn how to find the difference between two means and two medians using a histogram or dotplot. Materials: Comparing Commute Times with Dotplots handout ( LMR_2.8_Commute Times \u2013 Dotplots ) Comparing Exam Scores with Histograms handout ( LMR_2.9_Exam Scores \u2013 Histograms ) Timer Comparing Fuel Efficiency with Boxplots handout ( LMR_2.10_Fuel Efficiency \u2013 Boxplots ) Vocabulary: rebuttal Essential Concepts: Essential Concepts: Writing (and saying) precise comparisons between groups in which variability is present based on the (a) center, (b) spread, (c) shape, and (d) unusual outcomes help to make statements in context of the data. Actual comparison statements should use terms such as \"less than,\" \"about the same as,\" etc. Lesson: Poll students about the method of transportation they use for their daily school commute. How many of them walk, ride in a car, take the bus, ride a bike, etc.? Record their responses on the board. Ask them to estimate the typical amount of time it takes for them to get to school, in minutes. Inform students that they have learned important features of distributions that will allow them to make decisions when working with data. More specifically, they will be able to use their knowledge of measures of center and measures of spread to compare 2 distributions in order to make a decision. In teams, have students complete the Comparing Commute Times with Dotplots handout ( LMR_2.8 ). Allow students time to read the \u201cBackground\u201d portion of the handout, and then discuss what statistical investigative question(s) the student in the scenario is trying to answer. Note: Page 2 of the handout is an answer key for teacher reference only. LMR_2.8 Once teams decide on their recommendation, engage half of the class in an Active Debate . Half of the students will stand in a debate line and the other half will \u201cfishbowl\u201d the debate. Roles will reverse later in the lesson (see Step14). Of those students standing on the debate line, half will argue the reasons why they recommend street travel and the other half will argue the reasons why they recommend freeway travel. On the debate line, each student will stand face to face with a student who has the opposite recommendation. In other words, a student who recommends street travel will stand facing a student who recommends freeway travel. Using a timer, allow one minute for students who recommend freeway travel to argue their point to the person they are facing. Then, repeat for students who recommend street travel. Students should not interrupt or respond; they should only listen to the other side. Next, give debaters two minutes to prepare a rebuttal of the other person\u2019s argument. For example, if one student claimed that freeway travel is better, the other student may ask where the evidence is in the data or show that the data does not support the claim. Allow each debater two minutes to present his/her rebuttal. Finally, ask debaters if any of them changed their recommendations after engaging in the debate. In teams, have students complete the Comparing Exam Scores with Histograms handout ( LMR_2.9 ). Allow students time to read the \u201cBackground\u201d portion of the handout, and then discuss what statistical investigative question(s) the student in the scenario is trying to answer. Note: Page 2 of the handout is an answer key for teacher reference only LMR_2.9 Repeat debate process (Steps 4 - 10) with the other half of the class. Summarize the lesson by conducting a class discussion about what to look for when comparing distributions. Students should be precise when estimating values of means, medians, MAD, and IQR. They should also be able to comment on when it is most appropriate to use each measure of center and spread. If a distribution is symmetric, it is best to use the mean as a measure of center and the MAD as a measure of spread. If a distribution is skewed, or has outliers, it is best to use the median as a measure of center and the IQR as a measure of spread. Class Scribes: One team of students will give a brief talk to discuss what they think the 3 most important topics of the day were. Homework Similar to the activities they did during class today, for homework, students should complete the Comparing Fuel Efficiency with Boxplots handout ( LMR_2.10 ). Note: Page 2 of the handout is an answer key for teacher reference only LMR_2.10","title":"Lesson 6: Face Off"},{"location":"unit2/lesson6/#lesson-6-face-off","text":"","title":"Lesson 6: Face Off"},{"location":"unit2/lesson6/#objective","text":"Students will informally compare two or more distributions using their knowledge of shape, center, and spread to answer statistical investigative questions. They will learn how to find the difference between two means and two medians using a histogram or dotplot.","title":"Objective:"},{"location":"unit2/lesson6/#materials","text":"Comparing Commute Times with Dotplots handout ( LMR_2.8_Commute Times \u2013 Dotplots ) Comparing Exam Scores with Histograms handout ( LMR_2.9_Exam Scores \u2013 Histograms ) Timer Comparing Fuel Efficiency with Boxplots handout ( LMR_2.10_Fuel Efficiency \u2013 Boxplots )","title":"Materials:"},{"location":"unit2/lesson6/#vocabulary","text":"rebuttal","title":"Vocabulary:"},{"location":"unit2/lesson6/#essential-concepts","text":"Essential Concepts: Writing (and saying) precise comparisons between groups in which variability is present based on the (a) center, (b) spread, (c) shape, and (d) unusual outcomes help to make statements in context of the data. Actual comparison statements should use terms such as \"less than,\" \"about the same as,\" etc.","title":"Essential Concepts:"},{"location":"unit2/lesson6/#lesson","text":"Poll students about the method of transportation they use for their daily school commute. How many of them walk, ride in a car, take the bus, ride a bike, etc.? Record their responses on the board. Ask them to estimate the typical amount of time it takes for them to get to school, in minutes. Inform students that they have learned important features of distributions that will allow them to make decisions when working with data. More specifically, they will be able to use their knowledge of measures of center and measures of spread to compare 2 distributions in order to make a decision. In teams, have students complete the Comparing Commute Times with Dotplots handout ( LMR_2.8 ). Allow students time to read the \u201cBackground\u201d portion of the handout, and then discuss what statistical investigative question(s) the student in the scenario is trying to answer. Note: Page 2 of the handout is an answer key for teacher reference only. LMR_2.8 Once teams decide on their recommendation, engage half of the class in an Active Debate . Half of the students will stand in a debate line and the other half will \u201cfishbowl\u201d the debate. Roles will reverse later in the lesson (see Step14). Of those students standing on the debate line, half will argue the reasons why they recommend street travel and the other half will argue the reasons why they recommend freeway travel. On the debate line, each student will stand face to face with a student who has the opposite recommendation. In other words, a student who recommends street travel will stand facing a student who recommends freeway travel. Using a timer, allow one minute for students who recommend freeway travel to argue their point to the person they are facing. Then, repeat for students who recommend street travel. Students should not interrupt or respond; they should only listen to the other side. Next, give debaters two minutes to prepare a rebuttal of the other person\u2019s argument. For example, if one student claimed that freeway travel is better, the other student may ask where the evidence is in the data or show that the data does not support the claim. Allow each debater two minutes to present his/her rebuttal. Finally, ask debaters if any of them changed their recommendations after engaging in the debate. In teams, have students complete the Comparing Exam Scores with Histograms handout ( LMR_2.9 ). Allow students time to read the \u201cBackground\u201d portion of the handout, and then discuss what statistical investigative question(s) the student in the scenario is trying to answer. Note: Page 2 of the handout is an answer key for teacher reference only LMR_2.9 Repeat debate process (Steps 4 - 10) with the other half of the class. Summarize the lesson by conducting a class discussion about what to look for when comparing distributions. Students should be precise when estimating values of means, medians, MAD, and IQR. They should also be able to comment on when it is most appropriate to use each measure of center and spread. If a distribution is symmetric, it is best to use the mean as a measure of center and the MAD as a measure of spread. If a distribution is skewed, or has outliers, it is best to use the median as a measure of center and the IQR as a measure of spread.","title":"Lesson:"},{"location":"unit2/lesson6/#class-scribes","text":"One team of students will give a brief talk to discuss what they think the 3 most important topics of the day were.","title":"Class Scribes:"},{"location":"unit2/lesson6/#homework","text":"Similar to the activities they did during class today, for homework, students should complete the Comparing Fuel Efficiency with Boxplots handout ( LMR_2.10 ). Note: Page 2 of the handout is an answer key for teacher reference only LMR_2.10","title":"Homework"},{"location":"unit2/lesson7/","text":"Lesson 7: Plot Match Objective: Students will learn how to create a boxplot from an already-established dotplot. Materials: From Dotplots to Boxplots handout ( LMR_2.11_Dotplots to Boxplots ) Sets of plots from Plot Match file ( LMR_2.12_Plot Match ) \u2013 one for each team Advanced preparation required (see Step 7 below) Vocabulary: representation Essential Concepts: Essential Concepts: Boxplots are an alternative visualization of histograms or dotplots. They capture most, but not all, of the features we can see in a dotplot or histogram. Lesson: Ask students to complete an Entrance Slip by recalling the components of the five-number summary that make up a boxplot. Five-number summary: minimum, 1 st quartile (Q 1 ), median, 3 rd quartile (Q 3 ), maximum. Randomly select students to share the components and briefly discuss what each means in a boxplot. If students are missing a component, ask them to add the component to their list. Remind students that during Lesson 5 , they created a boxplot from students\u2019 heights. Explain that a boxplot is one representation of the distribution of a variable in a data set. They have worked with other representations of distributions. Ask students: What other representations of distributions have we seen? Answers may include: dotPlots, bar charts, scatterplots, histograms, and tables. Distribute the From Dotplots to Boxplots handout ( LMR_2.11 ). In teams, students will sketch boxplots from dotplots. They will need to determine the five-number summaries of each plot, and should clearly label each value on their boxplots. LMR_2.11 Students should answer the 3 questions included in the handout. They can discuss their answers in pairs, and then have a class share out of the responses. Once the discussion wraps up, inform the students that they will now attempt to find plots that represent the same data but are plotted differently. Distribute one set of plots, from the Plot Match file ( LMR_2.12 ), to each student team. Advanced preparation required : Each student team will receive a set of plots containing all 15 plots from the Plot Match file ( LMR_2.12 ). Copies will need to be cut and sorted prior to class time. To keep the plots together, you can either paper clip them or place them in zippered bags. Note: Do not distribute the handout for students to cut out the plots! LMR_2.12 Inform students that they are now going to gather in their teams and practice matching different representations of distributions. Each group will receive 15 plots (5 dotPlots, 5 histograms, and 5 boxplots). Their task is to determine which dotplots, histograms, and boxplots represent the same data. Once each group has decided upon their 5 groupings, engage the students in a class share out until all students agree. Then, have the students record their responses to the following statements and/or questions in their DS journals: What types of data are best for using a histogram? Histograms are useful for almost any type of data. They can easily show the shape of a distribution (including skewness and multiple peaks). They are usually best with larger data sets. What types of data are best for using a dotplot? Dotplots can also easily show the shape of a distribution. They are preferred over histograms when there is a relatively small amount of data. What types of data are best for using a boxplot? Boxplots are useful when the distribution has one mode (one peak). They are also useful to describe data that are heavily skewed or that contain outliers. Describe some characteristics of data that become hidden when a boxplot is used instead of a dotplot or histogram. Dotplots and histograms can show the number of modes in a distribution, but a boxplot cannot. If a distribution is bimodal, we will not be able to tell in a boxplot. In general, we lose the ability to talk about the overall shape of the distribution. Display the uncut version of the Plot Match file ( LMR_2.12 ) so that students see the letters that correspond to each set of representations. Solution key: Set 1: Plots (d), (a), (b) Set 2: Plots (m), (c), (h) Set 3: Plots (f), (j), (o) Set 4: Plots (e), (l), (i) Set 5: Plots (n), (k), (g) Have a few students share out their responses. For homework, students will record some pros and cons of using different types of graphical representations to display the same data. Class Scribes: One team of students will give a brief talk to discuss what they think the 3 most important topics of the day were. Homework Students should reflect on today\u2019s class discussion and record their ideas of some pros and cons of using different types of graphical representations to display the same data. LAB 2B: Oh the Summaries\u2026 Complete Lab 2B prior to the Practicum .","title":"Lesson 7: Plot Match"},{"location":"unit2/lesson7/#lesson-7-plot-match","text":"","title":"Lesson 7: Plot Match"},{"location":"unit2/lesson7/#objective","text":"Students will learn how to create a boxplot from an already-established dotplot.","title":"Objective:"},{"location":"unit2/lesson7/#materials","text":"From Dotplots to Boxplots handout ( LMR_2.11_Dotplots to Boxplots ) Sets of plots from Plot Match file ( LMR_2.12_Plot Match ) \u2013 one for each team Advanced preparation required (see Step 7 below)","title":"Materials:"},{"location":"unit2/lesson7/#vocabulary","text":"representation","title":"Vocabulary:"},{"location":"unit2/lesson7/#essential-concepts","text":"Essential Concepts: Boxplots are an alternative visualization of histograms or dotplots. They capture most, but not all, of the features we can see in a dotplot or histogram.","title":"Essential Concepts:"},{"location":"unit2/lesson7/#lesson","text":"Ask students to complete an Entrance Slip by recalling the components of the five-number summary that make up a boxplot. Five-number summary: minimum, 1 st quartile (Q 1 ), median, 3 rd quartile (Q 3 ), maximum. Randomly select students to share the components and briefly discuss what each means in a boxplot. If students are missing a component, ask them to add the component to their list. Remind students that during Lesson 5 , they created a boxplot from students\u2019 heights. Explain that a boxplot is one representation of the distribution of a variable in a data set. They have worked with other representations of distributions. Ask students: What other representations of distributions have we seen? Answers may include: dotPlots, bar charts, scatterplots, histograms, and tables. Distribute the From Dotplots to Boxplots handout ( LMR_2.11 ). In teams, students will sketch boxplots from dotplots. They will need to determine the five-number summaries of each plot, and should clearly label each value on their boxplots. LMR_2.11 Students should answer the 3 questions included in the handout. They can discuss their answers in pairs, and then have a class share out of the responses. Once the discussion wraps up, inform the students that they will now attempt to find plots that represent the same data but are plotted differently. Distribute one set of plots, from the Plot Match file ( LMR_2.12 ), to each student team. Advanced preparation required : Each student team will receive a set of plots containing all 15 plots from the Plot Match file ( LMR_2.12 ). Copies will need to be cut and sorted prior to class time. To keep the plots together, you can either paper clip them or place them in zippered bags. Note: Do not distribute the handout for students to cut out the plots! LMR_2.12 Inform students that they are now going to gather in their teams and practice matching different representations of distributions. Each group will receive 15 plots (5 dotPlots, 5 histograms, and 5 boxplots). Their task is to determine which dotplots, histograms, and boxplots represent the same data. Once each group has decided upon their 5 groupings, engage the students in a class share out until all students agree. Then, have the students record their responses to the following statements and/or questions in their DS journals: What types of data are best for using a histogram? Histograms are useful for almost any type of data. They can easily show the shape of a distribution (including skewness and multiple peaks). They are usually best with larger data sets. What types of data are best for using a dotplot? Dotplots can also easily show the shape of a distribution. They are preferred over histograms when there is a relatively small amount of data. What types of data are best for using a boxplot? Boxplots are useful when the distribution has one mode (one peak). They are also useful to describe data that are heavily skewed or that contain outliers. Describe some characteristics of data that become hidden when a boxplot is used instead of a dotplot or histogram. Dotplots and histograms can show the number of modes in a distribution, but a boxplot cannot. If a distribution is bimodal, we will not be able to tell in a boxplot. In general, we lose the ability to talk about the overall shape of the distribution. Display the uncut version of the Plot Match file ( LMR_2.12 ) so that students see the letters that correspond to each set of representations. Solution key: Set 1: Plots (d), (a), (b) Set 2: Plots (m), (c), (h) Set 3: Plots (f), (j), (o) Set 4: Plots (e), (l), (i) Set 5: Plots (n), (k), (g) Have a few students share out their responses. For homework, students will record some pros and cons of using different types of graphical representations to display the same data.","title":"Lesson:"},{"location":"unit2/lesson7/#class-scribes","text":"One team of students will give a brief talk to discuss what they think the 3 most important topics of the day were.","title":"Class Scribes:"},{"location":"unit2/lesson7/#homework","text":"Students should reflect on today\u2019s class discussion and record their ideas of some pros and cons of using different types of graphical representations to display the same data. LAB 2B: Oh the Summaries\u2026 Complete Lab 2B prior to the Practicum .","title":"Homework"},{"location":"unit2/lesson8/","text":"Lesson 8: How Likely Is It? Objective: Students will understand the basic rules of probability. They will learn that previous outcomes do not give information about future outcomes if the events are independent. Materials: Video: \u201cHeads\u201d from the movie Rosencrantz and Guildenstern are Dead found at: https://www.youtube.com/watch?v=NbInZ5oJ0bc https://www.youtube.com/embed/NbInZ5oJ0bc Projector for RStudio functions Vocabulary: probability simulation model sample proportion chance independence Essential Concepts: Essential Concepts: Probability is an area about which we humans have poor intuition. Probability measures a long-run proportion: 50% chance means the event happens 50% of the time if you repeated it forever . When we don't repeat forever, we see variability. Lesson: Ask students to consider possible synonyms for the word chance . If someone says, \u201cThat just happened by chance,\u201d what does that mean? Synonyms: possibility, prospect, expectation, unintentional, unplanned. The actual definition of chance is \u201ca possibility of something happening.\u201d Then, ask them which game \u2013 chess or the board game, \u201cSorry\u201d \u2013 is more based on chance. Why? Note: Any game can be chosen. \"Sorry\" is more based on chance because many outcomes are determined by dice rolls. In chess there are certain strategies and movements that can be planned, so it is more a game of skill. For \"Sorry\" the players roll a die (number cube), so the numbers they roll have an impact on how well they do in the game. Next ask students if they can think of situations where chance is the only force at play. Possible responses: card games, slot machines, the lottery, coin flipping, and rock-paper-scissors. Play the \u201cHeads\u201d video from the movie Rosencrantz and Guildenstern are Dead found at: https://www.youtube.com/watch?v=NbInZ5oJ0bc . In their IDS Journals, ask students to write down their initial reactions to the clip by responding to the following questions: Is it possible to get 78 heads in a row when tossing a coin? Yes, it is possible to get 78 heads in a row since one coin toss does not determine the next coin toss. Do you think it is likely to get 78 heads in a row? No, although it is possible to get 78 heads in a row. How many times should we get heads when tossing a coin? 1 out of 2 times, or 50% of the time. On average, how many times out of the 78 tosses should the characters have gotten heads? Roughly 39 times. Ask students to discuss their findings with their team members and come to an agreement on their responses. Afterwards, conduct a Whip Around and ask each team to share its findings. Are there any differences between the teams? Any similarities? As teams share their responses, students should add to or revise their individual findings in their IDS Journals. Explain to students that, from the concept of chance, we can start learning about probability . Chance is simply the possibility that something will happen, and probability is a measurement for how often something happens in the \u201clong run.\u201d Students may have ideas about how to calculate probabilities based on prior classes or knowledge, but inform them that IDS will be taking a different approach by using simulations (see next step). Since we don\u2019t want to actually flip a coin 78 times like the actors did in the video, we can have RStudio simulate them for us. A simulation is a way of creating random events that are close to real-life situations without actually doing them. It is a kind of model , which is a way of representing real-world situations so that predictions can be made. Explain to students that R has a function that does coin flipping for us, and that it assumes an equal probability of heads and tails. Using a projector to display your computer screen to the whole class, demonstrate how to do one simulation of a coin flip in RStudio. Use the following function: > rflip(1) Explain that the value of 1 in the argument part of the function tells R to flip the coin 1 time. If we want to flip the coin 10 times, we could simply change the function to rflip(10) . Run the function again using 10 as the number of times to flip the coin. Ask students: How many heads (\u201cH\u201ds) were there? Answers will vary for each sample. How many Tails (\u201cT\u201ds) were there? Answers will vary for each sample. In the output, what does Flipping 10 coins [Prob(Heads) = 0.5] mean? This is RStudio telling us that we are tossing the coin 10 times and that the probability of getting heads is 0.5 because it is a fair coin. In the output, what does Number of Heads: 3 [Proportion Heads: 0.3] mean? Note: This is an example of an output. Your sample may have a different value for the number of heads that appeared, and therefore a different value for the proportion of heads. This is RStudio telling us that in our sample, we got heads 3 out of the 10 times we flipped the coin. The sample proportion is automatically calculated for us by dividing the number of heads by the total number of tosses (in this case, 3/10 = 0.3). To relate back to the video at the beginning of class, repeat the simulation once more, but use 78 as the number of coin flips rflip(78) . Ask students: How many heads (\u201cH\u201ds) were there? Since we know to expect about 39 heads if the coin is fair, does the value seem reasonable? Answers will vary for each sample. Most likely, you will see values near 39. How many Tails (\u201cT\u201ds) were there? Answers will vary for each sample. What proportion of the coin flips were heads? Answers will vary for each sample. Using the rflip(78) command, run the simulation 3-5 more times and have students record the values for the number of heads and the proportion of heads. As an example, we ran the function 3 times and saw the following values: Sample 1 \u2013 amount of heads: 45 proportion of heads: 0.577 Sample 2 \u2013 amount of heads: 33 proportion of heads: 0.423 Sample 3 \u2013 amount of heads: 42 proportion of heads: 0.538 Have students answer the questions below. The important thing to note is that the values can and (almost always) WILL change each time you run the simulation to create a new sample. How do the proportions of heads in the samples compare to each other? Answers will vary. How do the proportions of heads compare to the true probability of heads (1/2 or 50%)? Answers will vary, but students should notice that most of the probabilities are close to 50%. Why is there a 50% chance of getting heads during each coin flip? Since there are two sides to a coin, both should be equally likely to come up. So there is a 1 out of 2 chance of getting heads and 1 out of 2 chance of getting tails. Ask students to engage in a discussion with their group about the statement below, then have a few group reporters share out. If a coin was flipped 78 times, I would claim that the coin is unfair if I got less than # heads or more than # heads. Inform students that you are going to perform 500 simulations. Each simulation represents a coin being flipped 78 times. For each simulation, the computer will record the number of heads in the 78 flips. A histogram will be created that represents the number of heads in each of the simulations. The histogram is a model that will display what typically happens when a fair coin is flipped 78 times. Copy and paste the code below in an RScript and run each line of code, one at a time, for the students: set.seed(11) #reproducibility flips <- do(500)*rflip(78) View(flips) # 4 variables histogram(~heads, data = flips) favstats(~heads, data = flips) Engage the students in a discussion about the histogram: What is this distribution telling us? When flipping a fair coin 78 times, what typically happened was that it landed on heads between 36 and 40 times (36/78 = 0.46 to 40/78 = 0.51). It was not uncommon for the coin to land on heads 31-35 (0.40-0.45) times or 41-45 (0.52-0.58) times. Even landing on heads between 46-50 (0.59-0.64) times was not too uncommon. What was very uncommon, however, was landing on heads less than 30 times (less than 38%) or more than 51 times (more than 65%). Were your group's cut-offs (item #16) similar to what the chance model displayed? Answers will vary. Some groups' intervals might be very wide and others very narrow. Using the chance model (histogram) which displays what typically happens when a fair coin is flipped 78 times, make a call for the scenarios below - fair or unfair? You flip a coin 78 times and get 37 heads. Fair. 37 was very common based on the histogram. You flip a coin 78 times and get 46 heads. Fair. 46 was less common but not too uncommon. You flip a coin 78 times and get 20 heads. Unfair. In the 500 simulations not once did we see a FAIR coin land on heads 20 times. Next pose the following question: If you get a heads on the first toss of a coin, will you definitely get a heads on the next toss? Will you definitely get a tails on the next toss? No. One coin toss should not affect another coin toss. Each time you flip the coin, the chances of getting heads versus tails remains the same. Introduce the concept of independence . Explain that, when tossing a fair coin, there is no relationship between each toss. The second toss does NOT depend on the first toss; therefore, the coin tosses are independent of each other. Class Scribes: One team of students will give a brief talk to discuss what they think the 3 most important topics of the day were. Homework Students will create a Tweet (they do not have to post it online). Using 280 characters or fewer, write a Tweet about the meaning of probability.","title":"Lesson 8: How Likely Is It?"},{"location":"unit2/lesson8/#lesson-8-how-likely-is-it","text":"","title":"Lesson 8: How Likely Is It?"},{"location":"unit2/lesson8/#objective","text":"Students will understand the basic rules of probability. They will learn that previous outcomes do not give information about future outcomes if the events are independent.","title":"Objective:"},{"location":"unit2/lesson8/#materials","text":"Video: \u201cHeads\u201d from the movie Rosencrantz and Guildenstern are Dead found at: https://www.youtube.com/watch?v=NbInZ5oJ0bc https://www.youtube.com/embed/NbInZ5oJ0bc Projector for RStudio functions","title":"Materials:"},{"location":"unit2/lesson8/#vocabulary","text":"probability simulation model sample proportion chance independence","title":"Vocabulary:"},{"location":"unit2/lesson8/#essential-concepts","text":"Essential Concepts: Probability is an area about which we humans have poor intuition. Probability measures a long-run proportion: 50% chance means the event happens 50% of the time if you repeated it forever . When we don't repeat forever, we see variability.","title":"Essential Concepts:"},{"location":"unit2/lesson8/#lesson","text":"Ask students to consider possible synonyms for the word chance . If someone says, \u201cThat just happened by chance,\u201d what does that mean? Synonyms: possibility, prospect, expectation, unintentional, unplanned. The actual definition of chance is \u201ca possibility of something happening.\u201d Then, ask them which game \u2013 chess or the board game, \u201cSorry\u201d \u2013 is more based on chance. Why? Note: Any game can be chosen. \"Sorry\" is more based on chance because many outcomes are determined by dice rolls. In chess there are certain strategies and movements that can be planned, so it is more a game of skill. For \"Sorry\" the players roll a die (number cube), so the numbers they roll have an impact on how well they do in the game. Next ask students if they can think of situations where chance is the only force at play. Possible responses: card games, slot machines, the lottery, coin flipping, and rock-paper-scissors. Play the \u201cHeads\u201d video from the movie Rosencrantz and Guildenstern are Dead found at: https://www.youtube.com/watch?v=NbInZ5oJ0bc . In their IDS Journals, ask students to write down their initial reactions to the clip by responding to the following questions: Is it possible to get 78 heads in a row when tossing a coin? Yes, it is possible to get 78 heads in a row since one coin toss does not determine the next coin toss. Do you think it is likely to get 78 heads in a row? No, although it is possible to get 78 heads in a row. How many times should we get heads when tossing a coin? 1 out of 2 times, or 50% of the time. On average, how many times out of the 78 tosses should the characters have gotten heads? Roughly 39 times. Ask students to discuss their findings with their team members and come to an agreement on their responses. Afterwards, conduct a Whip Around and ask each team to share its findings. Are there any differences between the teams? Any similarities? As teams share their responses, students should add to or revise their individual findings in their IDS Journals. Explain to students that, from the concept of chance, we can start learning about probability . Chance is simply the possibility that something will happen, and probability is a measurement for how often something happens in the \u201clong run.\u201d Students may have ideas about how to calculate probabilities based on prior classes or knowledge, but inform them that IDS will be taking a different approach by using simulations (see next step). Since we don\u2019t want to actually flip a coin 78 times like the actors did in the video, we can have RStudio simulate them for us. A simulation is a way of creating random events that are close to real-life situations without actually doing them. It is a kind of model , which is a way of representing real-world situations so that predictions can be made. Explain to students that R has a function that does coin flipping for us, and that it assumes an equal probability of heads and tails. Using a projector to display your computer screen to the whole class, demonstrate how to do one simulation of a coin flip in RStudio. Use the following function: > rflip(1) Explain that the value of 1 in the argument part of the function tells R to flip the coin 1 time. If we want to flip the coin 10 times, we could simply change the function to rflip(10) . Run the function again using 10 as the number of times to flip the coin. Ask students: How many heads (\u201cH\u201ds) were there? Answers will vary for each sample. How many Tails (\u201cT\u201ds) were there? Answers will vary for each sample. In the output, what does Flipping 10 coins [Prob(Heads) = 0.5] mean? This is RStudio telling us that we are tossing the coin 10 times and that the probability of getting heads is 0.5 because it is a fair coin. In the output, what does Number of Heads: 3 [Proportion Heads: 0.3] mean? Note: This is an example of an output. Your sample may have a different value for the number of heads that appeared, and therefore a different value for the proportion of heads. This is RStudio telling us that in our sample, we got heads 3 out of the 10 times we flipped the coin. The sample proportion is automatically calculated for us by dividing the number of heads by the total number of tosses (in this case, 3/10 = 0.3). To relate back to the video at the beginning of class, repeat the simulation once more, but use 78 as the number of coin flips rflip(78) . Ask students: How many heads (\u201cH\u201ds) were there? Since we know to expect about 39 heads if the coin is fair, does the value seem reasonable? Answers will vary for each sample. Most likely, you will see values near 39. How many Tails (\u201cT\u201ds) were there? Answers will vary for each sample. What proportion of the coin flips were heads? Answers will vary for each sample. Using the rflip(78) command, run the simulation 3-5 more times and have students record the values for the number of heads and the proportion of heads. As an example, we ran the function 3 times and saw the following values: Sample 1 \u2013 amount of heads: 45 proportion of heads: 0.577 Sample 2 \u2013 amount of heads: 33 proportion of heads: 0.423 Sample 3 \u2013 amount of heads: 42 proportion of heads: 0.538 Have students answer the questions below. The important thing to note is that the values can and (almost always) WILL change each time you run the simulation to create a new sample. How do the proportions of heads in the samples compare to each other? Answers will vary. How do the proportions of heads compare to the true probability of heads (1/2 or 50%)? Answers will vary, but students should notice that most of the probabilities are close to 50%. Why is there a 50% chance of getting heads during each coin flip? Since there are two sides to a coin, both should be equally likely to come up. So there is a 1 out of 2 chance of getting heads and 1 out of 2 chance of getting tails. Ask students to engage in a discussion with their group about the statement below, then have a few group reporters share out. If a coin was flipped 78 times, I would claim that the coin is unfair if I got less than # heads or more than # heads. Inform students that you are going to perform 500 simulations. Each simulation represents a coin being flipped 78 times. For each simulation, the computer will record the number of heads in the 78 flips. A histogram will be created that represents the number of heads in each of the simulations. The histogram is a model that will display what typically happens when a fair coin is flipped 78 times. Copy and paste the code below in an RScript and run each line of code, one at a time, for the students: set.seed(11) #reproducibility flips <- do(500)*rflip(78) View(flips) # 4 variables histogram(~heads, data = flips) favstats(~heads, data = flips) Engage the students in a discussion about the histogram: What is this distribution telling us? When flipping a fair coin 78 times, what typically happened was that it landed on heads between 36 and 40 times (36/78 = 0.46 to 40/78 = 0.51). It was not uncommon for the coin to land on heads 31-35 (0.40-0.45) times or 41-45 (0.52-0.58) times. Even landing on heads between 46-50 (0.59-0.64) times was not too uncommon. What was very uncommon, however, was landing on heads less than 30 times (less than 38%) or more than 51 times (more than 65%). Were your group's cut-offs (item #16) similar to what the chance model displayed? Answers will vary. Some groups' intervals might be very wide and others very narrow. Using the chance model (histogram) which displays what typically happens when a fair coin is flipped 78 times, make a call for the scenarios below - fair or unfair? You flip a coin 78 times and get 37 heads. Fair. 37 was very common based on the histogram. You flip a coin 78 times and get 46 heads. Fair. 46 was less common but not too uncommon. You flip a coin 78 times and get 20 heads. Unfair. In the 500 simulations not once did we see a FAIR coin land on heads 20 times. Next pose the following question: If you get a heads on the first toss of a coin, will you definitely get a heads on the next toss? Will you definitely get a tails on the next toss? No. One coin toss should not affect another coin toss. Each time you flip the coin, the chances of getting heads versus tails remains the same. Introduce the concept of independence . Explain that, when tossing a fair coin, there is no relationship between each toss. The second toss does NOT depend on the first toss; therefore, the coin tosses are independent of each other.","title":"Lesson:"},{"location":"unit2/lesson8/#class-scribes","text":"One team of students will give a brief talk to discuss what they think the 3 most important topics of the day were.","title":"Class Scribes:"},{"location":"unit2/lesson8/#homework","text":"Students will create a Tweet (they do not have to post it online). Using 280 characters or fewer, write a Tweet about the meaning of probability.","title":"Homework"},{"location":"unit2/lesson9/","text":"Lesson 9: Dice Detective Objective: Students will learn how to use simulations to detect an unfair die. Materials: Poster paper \u2013 with 6 columns labeled 1, 2, 3, 4, 5, 6 2 dice (number cubes) Note: You can use regular hard dice, or soft foam dice (can be found at dollar stores) Projector for RStudio functions Essential Concepts: Essential Concepts: In the short-term, actual outcomes of chance experiments vary from what is 'ideal.' A fair die has equally likely outcomes. But that does not mean we will see exactly the same number of one-dots, two-dots, etc. Lesson: In pairs, ask students to quickly share their Tweets from the previous lesson\u2019s homework. Collect the Tweets and select a few to share with the class. Of the Tweets shared, ask students which one is closest to the definition: probability measures how often something happens in the \u201clong run.\u201d Remind students that, during the previous lesson, they were introduced to simulations. The progression following this path: chance \u2192 probability \u2192 simulations. The motivation for using simulations is that we can use the calculated sample proportions to estimate probabilities of reallife events. During today\u2019s lesson, we will be continuing to learn about probability and simulations to determine if an event is not fair (one example: a coin is weighted and lands on heads more often than tails). Ask students what they know about dice (number cubes). If they have never heard of them, show one to the class and explain how it works. A die (number cube) is a 6-sided cube. Each face of the cube is labeled with dots to represent a number between 1 and 6. For example, if the face has 3 dots, then it represents the number 3. The cube itself is weighted so that there is an equal probability of rolling each of the 6 numbers. Have a discussion about what the students would expect the probability of rolling the number 1 should be if a die (number cube) were tossed into the air and allowed to fall back to the ground (or table). Since there are 6 numbers on the die, each number should be equally likely to occur, so the probability of rolling a 1 is 1/6. Display a piece of poster paper on the board with columns labeled 1, 2, 3, 4, 5, 6. Explain that each column represents the numbers on the die (number cube). We will be using poster to tally the results of actual dice (number cube) rolls. Select two students to be dice (number cube) rollers and give each student one die. As noted in the Materials section above, you can have the students use either regular hard dice, or softer foam ones (can be found at dollar stores). Tell the class that each student roller will be rolling the dice 6 times (so there will be a total of 12 rolls for our sample). Ask: If they are rolling the dice 6 times, how often do you think Student 1 will roll a 3? Would you expect it to be the same for Student 2? Out of 6 rolls, we would expect to see each of the numbers one time, so we will most likely see about one 3 for Student 1. Would you expect the Student 2 to roll a 3 just as often? Why? Yes, we should expect the same thing from Student 2 because we have independent events. There are actually two ways that independence plays a part here: (1) each student is independent from the other and has no effect on what the other will roll, (2) the 6 die rolls for Student 1 are all independent of each other because each face of the cube has an equal chance of happening on any given roll. So, if Student 1 gets a 3 during his/her first roll, that doesn\u2019t give us any information about what he/she will get on the second roll. Since we will have 12 rolls (and therefore 12 samples), how many tally marks should we expect in each column on the chart? We would expect to see 2 tally marks in each column (each number will probably be rolled twice). Have each student roller toss his/her die one time and share the outcomes with the rest of the class. As they do this, place a tally mark in the corresponding column on the chart. Repeat this process 5 more times so that each student has a total of 6 rolls. As a class, observe the results in the chart and discuss the following: Do the data from these 12 rolls match what we expected (see responses from Step 8)? Is this surprising? Answers will vary by class. Some values may have shown up more than we expected (example: the number 3 was rolled 3 times), and others may not have been rolled at all (example: the number 5 was never an outcome). We only have a small sample of data, so it\u2019s not surprising for our results to vary from the expected outcomes. If the data do not match our expectations, does this mean the dice are unfair in some way? Even if they don\u2019t match our expectations, this does not mean the dice are unfair \u2013 we simply don\u2019t have enough data yet to know. We would need to roll the dice more. If we wanted to purposely create an unfair die, what are some ways we could achieve that? Answers will vary by class. Some examples include: (1) We could add tape to one face of the die to give that side more weight. This would increase the chances of the number that is directly opposite of it appearing because the die will land on the heavier side more (and therefore the side facing up will be the number opposite). (2) We could chip the edge of one corner of the die. This would throw off the original balance and favor certain sides. Similar to the previous day\u2019s lesson with coin flipping, we can also simulate dice rolls in RStudio. The function required is called roll_die() . The arguments for this function are a bit different than the rflip() function from yesterday. We cannot simply put roll_die(1) for the computer to roll a die one time. Instead, the function was built with 2 possible dice to choose from \u2013 die A and die B. Inform the students that one of the dice in the function is fair and the other is unfair. A die is unfair if it favors one outcome over another. They will attempt to determine which dice is the unfair one by doing multiple simulations. Note to Teacher: Many simulations require multiple functions, or code, to perform. This is where RScripts are helpful. An RScript can be used to test code, write notes, and let us easily execute multiple lines of code at one time. This would be a good place to introduce students to RScripts. Using a projector to display your computer screen to the whole class, demonstrate how to open an RScript. Type the following function on your script and click Run. Run simply pastes the function onto the console. roll_die(\u201cA\u201d, times = 1) The output will show one number that represents what value on the die the computer rolled. Go back to your script and modify the function to roll die A 12 times. roll_die(\u201cA\u201d, times = 12) Compare the results of these 12 simulated rolls to the results of the 12 actual rolls completed by the two students during Step 9. If there is space available on the tally chart, you can add the computer results to it for an easy comparison. Ask students how we could record data from these simulations if we wanted to roll the die 100 times. Would they want to hand count the number of times each value occurred? Is there a function in RStudio that will count them for us? It would be difficult to count every individual value in the output on the screen. However, we can use the tally() function to find out how many times each die value appeared. To make using the tally() function easier, we should assign a name to each simulation so we don\u2019t have to type the entire function multiple times. We can also have it calculate the proportions for each value. Add the functions below to your RScript and run them one at a time. sample1 <- roll_die(\u201cA\u201d, times = 100) tally(sample1) tally(sample1, format = \u201cproportion\u201d) Remind the students that if the die is fair, then each side of the die should appear roughly the same amount of times. Therefore, the proportions should be fairly similar to each other and to the true probability of 1/6. Add the function below to your script, but before running it, ask the students what they think a histogram of the simulated data might look like and then run the command on your screen. Note: Be sure to include the argument nint = 6 so that the resulting histogram has six bars. If the die is fair, each of the bars in the histogram should be roughly the same height. histogram(sample1, nint = 6) Note to teacher: Show students how to save an RScript. Inform students that they can take notes on their RScript by including a hashtag (also known as a pound sign or #) at the beginning of the note. Data scientists refer to these types of notes as \u201ccode comments\u201d or simply \u201ccomments\u201d. See image below. The Script will be stored in the files tab. To run each function individually, place your cursor on the line and click the Run button. To run multiple lines of code at once, highlight them and click Run. Allow the students to access their school computers now to start creating their own simulations in an RScript using die B. Students can pair up, if needed. Have them begin by asking RStudio to roll the die 100 times. They should note their output from both the tally() function and the histogram. They can then compare the results to those from Step 14. Are they similar? Can they determine which die is unfair yet? Answers will vary by class. The results will be similar, but not exact. With the sample sizes of each simulation being fairly small, we cannot see a clear difference between the two dice yet. Let the students explore by changing the number of times RStudio rolls the dice. Remind them that the goal is to determine which of the two dice is unfair. The sample sizes need to be very large in order for them to see a clear difference between the 2 histograms. The pattern becomes more visible when times = 2000 . Note: The maximum value for times within the roll_die() command is 2000. Simulations can be combined using the concatenate function c() . For example, suppose s1 represents 2000 rolls of die A and s2 is a second sample of 2000 rolls for die A. To combine these two samples the following can be used more_rolls <- c(s1, s2) When students have had enough time to make a decision regarding which dice is unfair and how, engage the class in a discussion to verify that everyone agrees. Die B is unfair; Die A is fair. Die B favors the number 3. Then, steer the conversation towards why the sample size affected the results. The sample size needed to be large because the difference between the probabilities of the die rolls was very small. In order to detect small differences, we must have larger sample sizes. Class Scribes: One team of students will give a brief talk to discuss what they think the 3 most important topics of the day were. Homework Students will consider a four-sided die and imagine rolling it 20 times. They should sketch a histogram of (a) the ideal, expected outcome, (b) an outcome that they think is \u201crealistic,\u201d and (c) an outcome they might see if the die were unfair such that it produced more 4\u2019s.","title":"Lesson 9: Dice Detective"},{"location":"unit2/lesson9/#lesson-9-dice-detective","text":"","title":"Lesson 9: Dice Detective"},{"location":"unit2/lesson9/#objective","text":"Students will learn how to use simulations to detect an unfair die.","title":"Objective:"},{"location":"unit2/lesson9/#materials","text":"Poster paper \u2013 with 6 columns labeled 1, 2, 3, 4, 5, 6 2 dice (number cubes) Note: You can use regular hard dice, or soft foam dice (can be found at dollar stores) Projector for RStudio functions","title":"Materials:"},{"location":"unit2/lesson9/#essential-concepts","text":"Essential Concepts: In the short-term, actual outcomes of chance experiments vary from what is 'ideal.' A fair die has equally likely outcomes. But that does not mean we will see exactly the same number of one-dots, two-dots, etc.","title":"Essential Concepts:"},{"location":"unit2/lesson9/#lesson","text":"In pairs, ask students to quickly share their Tweets from the previous lesson\u2019s homework. Collect the Tweets and select a few to share with the class. Of the Tweets shared, ask students which one is closest to the definition: probability measures how often something happens in the \u201clong run.\u201d Remind students that, during the previous lesson, they were introduced to simulations. The progression following this path: chance \u2192 probability \u2192 simulations. The motivation for using simulations is that we can use the calculated sample proportions to estimate probabilities of reallife events. During today\u2019s lesson, we will be continuing to learn about probability and simulations to determine if an event is not fair (one example: a coin is weighted and lands on heads more often than tails). Ask students what they know about dice (number cubes). If they have never heard of them, show one to the class and explain how it works. A die (number cube) is a 6-sided cube. Each face of the cube is labeled with dots to represent a number between 1 and 6. For example, if the face has 3 dots, then it represents the number 3. The cube itself is weighted so that there is an equal probability of rolling each of the 6 numbers. Have a discussion about what the students would expect the probability of rolling the number 1 should be if a die (number cube) were tossed into the air and allowed to fall back to the ground (or table). Since there are 6 numbers on the die, each number should be equally likely to occur, so the probability of rolling a 1 is 1/6. Display a piece of poster paper on the board with columns labeled 1, 2, 3, 4, 5, 6. Explain that each column represents the numbers on the die (number cube). We will be using poster to tally the results of actual dice (number cube) rolls. Select two students to be dice (number cube) rollers and give each student one die. As noted in the Materials section above, you can have the students use either regular hard dice, or softer foam ones (can be found at dollar stores). Tell the class that each student roller will be rolling the dice 6 times (so there will be a total of 12 rolls for our sample). Ask: If they are rolling the dice 6 times, how often do you think Student 1 will roll a 3? Would you expect it to be the same for Student 2? Out of 6 rolls, we would expect to see each of the numbers one time, so we will most likely see about one 3 for Student 1. Would you expect the Student 2 to roll a 3 just as often? Why? Yes, we should expect the same thing from Student 2 because we have independent events. There are actually two ways that independence plays a part here: (1) each student is independent from the other and has no effect on what the other will roll, (2) the 6 die rolls for Student 1 are all independent of each other because each face of the cube has an equal chance of happening on any given roll. So, if Student 1 gets a 3 during his/her first roll, that doesn\u2019t give us any information about what he/she will get on the second roll. Since we will have 12 rolls (and therefore 12 samples), how many tally marks should we expect in each column on the chart? We would expect to see 2 tally marks in each column (each number will probably be rolled twice). Have each student roller toss his/her die one time and share the outcomes with the rest of the class. As they do this, place a tally mark in the corresponding column on the chart. Repeat this process 5 more times so that each student has a total of 6 rolls. As a class, observe the results in the chart and discuss the following: Do the data from these 12 rolls match what we expected (see responses from Step 8)? Is this surprising? Answers will vary by class. Some values may have shown up more than we expected (example: the number 3 was rolled 3 times), and others may not have been rolled at all (example: the number 5 was never an outcome). We only have a small sample of data, so it\u2019s not surprising for our results to vary from the expected outcomes. If the data do not match our expectations, does this mean the dice are unfair in some way? Even if they don\u2019t match our expectations, this does not mean the dice are unfair \u2013 we simply don\u2019t have enough data yet to know. We would need to roll the dice more. If we wanted to purposely create an unfair die, what are some ways we could achieve that? Answers will vary by class. Some examples include: (1) We could add tape to one face of the die to give that side more weight. This would increase the chances of the number that is directly opposite of it appearing because the die will land on the heavier side more (and therefore the side facing up will be the number opposite). (2) We could chip the edge of one corner of the die. This would throw off the original balance and favor certain sides. Similar to the previous day\u2019s lesson with coin flipping, we can also simulate dice rolls in RStudio. The function required is called roll_die() . The arguments for this function are a bit different than the rflip() function from yesterday. We cannot simply put roll_die(1) for the computer to roll a die one time. Instead, the function was built with 2 possible dice to choose from \u2013 die A and die B. Inform the students that one of the dice in the function is fair and the other is unfair. A die is unfair if it favors one outcome over another. They will attempt to determine which dice is the unfair one by doing multiple simulations. Note to Teacher: Many simulations require multiple functions, or code, to perform. This is where RScripts are helpful. An RScript can be used to test code, write notes, and let us easily execute multiple lines of code at one time. This would be a good place to introduce students to RScripts. Using a projector to display your computer screen to the whole class, demonstrate how to open an RScript. Type the following function on your script and click Run. Run simply pastes the function onto the console. roll_die(\u201cA\u201d, times = 1) The output will show one number that represents what value on the die the computer rolled. Go back to your script and modify the function to roll die A 12 times. roll_die(\u201cA\u201d, times = 12) Compare the results of these 12 simulated rolls to the results of the 12 actual rolls completed by the two students during Step 9. If there is space available on the tally chart, you can add the computer results to it for an easy comparison. Ask students how we could record data from these simulations if we wanted to roll the die 100 times. Would they want to hand count the number of times each value occurred? Is there a function in RStudio that will count them for us? It would be difficult to count every individual value in the output on the screen. However, we can use the tally() function to find out how many times each die value appeared. To make using the tally() function easier, we should assign a name to each simulation so we don\u2019t have to type the entire function multiple times. We can also have it calculate the proportions for each value. Add the functions below to your RScript and run them one at a time. sample1 <- roll_die(\u201cA\u201d, times = 100) tally(sample1) tally(sample1, format = \u201cproportion\u201d) Remind the students that if the die is fair, then each side of the die should appear roughly the same amount of times. Therefore, the proportions should be fairly similar to each other and to the true probability of 1/6. Add the function below to your script, but before running it, ask the students what they think a histogram of the simulated data might look like and then run the command on your screen. Note: Be sure to include the argument nint = 6 so that the resulting histogram has six bars. If the die is fair, each of the bars in the histogram should be roughly the same height. histogram(sample1, nint = 6) Note to teacher: Show students how to save an RScript. Inform students that they can take notes on their RScript by including a hashtag (also known as a pound sign or #) at the beginning of the note. Data scientists refer to these types of notes as \u201ccode comments\u201d or simply \u201ccomments\u201d. See image below. The Script will be stored in the files tab. To run each function individually, place your cursor on the line and click the Run button. To run multiple lines of code at once, highlight them and click Run. Allow the students to access their school computers now to start creating their own simulations in an RScript using die B. Students can pair up, if needed. Have them begin by asking RStudio to roll the die 100 times. They should note their output from both the tally() function and the histogram. They can then compare the results to those from Step 14. Are they similar? Can they determine which die is unfair yet? Answers will vary by class. The results will be similar, but not exact. With the sample sizes of each simulation being fairly small, we cannot see a clear difference between the two dice yet. Let the students explore by changing the number of times RStudio rolls the dice. Remind them that the goal is to determine which of the two dice is unfair. The sample sizes need to be very large in order for them to see a clear difference between the 2 histograms. The pattern becomes more visible when times = 2000 . Note: The maximum value for times within the roll_die() command is 2000. Simulations can be combined using the concatenate function c() . For example, suppose s1 represents 2000 rolls of die A and s2 is a second sample of 2000 rolls for die A. To combine these two samples the following can be used more_rolls <- c(s1, s2) When students have had enough time to make a decision regarding which dice is unfair and how, engage the class in a discussion to verify that everyone agrees. Die B is unfair; Die A is fair. Die B favors the number 3. Then, steer the conversation towards why the sample size affected the results. The sample size needed to be large because the difference between the probabilities of the die rolls was very small. In order to detect small differences, we must have larger sample sizes.","title":"Lesson:"},{"location":"unit2/lesson9/#class-scribes","text":"One team of students will give a brief talk to discuss what they think the 3 most important topics of the day were.","title":"Class Scribes:"},{"location":"unit2/lesson9/#homework","text":"Students will consider a four-sided die and imagine rolling it 20 times. They should sketch a histogram of (a) the ideal, expected outcome, (b) an outcome that they think is \u201crealistic,\u201d and (c) an outcome they might see if the die were unfair such that it produced more 4\u2019s.","title":"Homework"},{"location":"unit2/overview/","text":"Introduction to Data Science Daily Overview: Unit 2 Unit 2 Daily Overview: Unit 2 .tg {border-collapse:collapse;border-spacing:0;} .tg td{font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;} .tg th{font-family:Arial, sans-serif;font-size:14px;font-weight:normal;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;} .tg .tg-88nc{font-weight:bold;border-color:inherit;text-align:center} .tg .tg-yj5y{background-color:#efefef;border-color:inherit;text-align:center;vertical-align:top} .tg .tg-uys7{border-color:inherit;text-align:center} .tg .tg-pwj7{background-color:#efefef;border-color:inherit;text-align:left} .tg .tg-5e9r{background-color:#efefef;border-color:inherit;text-align:center} .tg .tg-xldj{border-color:inherit;text-align:left} .tg .tg-y698{background-color:#efefef;border-color:inherit;text-align:left;vertical-align:top} Theme Day Lessons and Labs Campaign Topics Page What Is Your True Color? (10 days) 1 Lesson 1: What Is Your True Color? Personality Color - data Subsets, relative frequency 117 2 Lesson 2: What Does Mean Mean? Personality Color Measures of center \u2013 mean 120 3 Lesson 3: Median In the Middle Personality Color Measures of center \u2013 median 124 4 Lesson 4: How Far Is It from Typical? Personality Color Measures of spread \u2013 MAD 128 5 Lab 2A: All About Distributions Personality Color Measures of center & spread \u2013 mean, median, MAD 132 6 Lesson 5: Human Boxplots Boxplots, IQR 134 7 Lesson 6: Face Off Comparing distributions 137 8 Lesson 7: Plot Match Comparing distributions 140 9 Lab 2B: Oh, the Summaries\u2026 Personality Color Boxplots, IQR, numerical summaries, custom functions 143 10 Practicum: The Summaries Food Habits or Time Use Statistical questions, comparing distributions 146 How Likely Is It? (7 days) 11 Lesson 8: How Likely is It? Probability, simulations 150 12 Lesson 9: Bias Detective Simulations to detect bias 153 13 Lesson 10: Marbles, Marbles Probability, with replacement 157 14 Lab 2C: Which Song Plays Next? Probability of simple events, do loops, set.seed() 159 15 Lesson 11: This AND/OR That Compound probabilities 162 16 Lab 2D: Queue It Up! Probability with & without replacement, sample() 166 17 Practicum: Win, Win, Win Probability estimation through repeated simulations 169 Are You Stressing or Chilling? (8 Days) 18 Lesson 12: Don\u2019t Take My Stress Away Stress/Chill \u2013 data Introduction to campaign 172 19 Lesson 13: The Horror Movie Shuffle Stress/Chill \u2013 data Chance differences \u2013 cat var 176 20 Lab 2E: The Horror Movie Shuffle Stress/Chill \u2013 data Inference for categorical variable, do loops, shuffle() 180 21 Lesson 14: The Titanic Shuffle Stress/Chill \u2013 data Chance differences \u2013 num var 183 22 Lab 2F: The Titanic Shuffle Stress/Chill \u2013 data Inference for numerical variable, do loops, shuffle() 187 23 Lesson 15: Tangible Data Merging Stress/Chill \u2013 data Merging data sets 189 24 Lab 2G: Getting It Together Stress/Chill & Personality Color Merging data sets, stacking vs. joining 192 25 Practicum: What Stresses Us? Stress/Chill & Personality Color Answering statistical questions of merged data 194 What\u2019s Normal? (5 Days) 26 Lesson 16: What Is Normal? Introduction to normal curve 197 27 Lesson 17: Normal Measure of Spread Measures of spread - SD 201 28 Lesson 18: What\u2019s Your Z-Score? z-scores, shuffling 204 29 Lab 2H: Eyeballing Normal Normal curves overlaid on distributions & simulated data 220 30 Lab 2I: R\u2019s Normal Distribution Alphabet Normal probability, rnorm(), pnorm(), quantiles, qnorm() 212 Unit 2 Project (5 Days) 31-35 End of Unit Project and OralPresentations: Asking and AnsweringStatistical Questions of Our Own Data Stress/Chill, Personality Color, Food Habits, or Time Use Synthesis of above 214 ^=Data collection window begins. +=Data collection window ends.","title":"Daily Overview"},{"location":"unit2/overview/#introduction-to-data-science-daily-overview-unit-2","text":"","title":"Introduction to Data Science Daily Overview: Unit 2"},{"location":"unit2/overview/#daily-overview-unit-2","text":".tg {border-collapse:collapse;border-spacing:0;} .tg td{font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;} .tg th{font-family:Arial, sans-serif;font-size:14px;font-weight:normal;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;} .tg .tg-88nc{font-weight:bold;border-color:inherit;text-align:center} .tg .tg-yj5y{background-color:#efefef;border-color:inherit;text-align:center;vertical-align:top} .tg .tg-uys7{border-color:inherit;text-align:center} .tg .tg-pwj7{background-color:#efefef;border-color:inherit;text-align:left} .tg .tg-5e9r{background-color:#efefef;border-color:inherit;text-align:center} .tg .tg-xldj{border-color:inherit;text-align:left} .tg .tg-y698{background-color:#efefef;border-color:inherit;text-align:left;vertical-align:top} Theme Day Lessons and Labs Campaign Topics Page What Is Your True Color? (10 days) 1 Lesson 1: What Is Your True Color? Personality Color - data Subsets, relative frequency 117 2 Lesson 2: What Does Mean Mean? Personality Color Measures of center \u2013 mean 120 3 Lesson 3: Median In the Middle Personality Color Measures of center \u2013 median 124 4 Lesson 4: How Far Is It from Typical? Personality Color Measures of spread \u2013 MAD 128 5 Lab 2A: All About Distributions Personality Color Measures of center & spread \u2013 mean, median, MAD 132 6 Lesson 5: Human Boxplots Boxplots, IQR 134 7 Lesson 6: Face Off Comparing distributions 137 8 Lesson 7: Plot Match Comparing distributions 140 9 Lab 2B: Oh, the Summaries\u2026 Personality Color Boxplots, IQR, numerical summaries, custom functions 143 10 Practicum: The Summaries Food Habits or Time Use Statistical questions, comparing distributions 146 How Likely Is It? (7 days) 11 Lesson 8: How Likely is It? Probability, simulations 150 12 Lesson 9: Bias Detective Simulations to detect bias 153 13 Lesson 10: Marbles, Marbles Probability, with replacement 157 14 Lab 2C: Which Song Plays Next? Probability of simple events, do loops, set.seed() 159 15 Lesson 11: This AND/OR That Compound probabilities 162 16 Lab 2D: Queue It Up! Probability with & without replacement, sample() 166 17 Practicum: Win, Win, Win Probability estimation through repeated simulations 169 Are You Stressing or Chilling? (8 Days) 18 Lesson 12: Don\u2019t Take My Stress Away Stress/Chill \u2013 data Introduction to campaign 172 19 Lesson 13: The Horror Movie Shuffle Stress/Chill \u2013 data Chance differences \u2013 cat var 176 20 Lab 2E: The Horror Movie Shuffle Stress/Chill \u2013 data Inference for categorical variable, do loops, shuffle() 180 21 Lesson 14: The Titanic Shuffle Stress/Chill \u2013 data Chance differences \u2013 num var 183 22 Lab 2F: The Titanic Shuffle Stress/Chill \u2013 data Inference for numerical variable, do loops, shuffle() 187 23 Lesson 15: Tangible Data Merging Stress/Chill \u2013 data Merging data sets 189 24 Lab 2G: Getting It Together Stress/Chill & Personality Color Merging data sets, stacking vs. joining 192 25 Practicum: What Stresses Us? Stress/Chill & Personality Color Answering statistical questions of merged data 194 What\u2019s Normal? (5 Days) 26 Lesson 16: What Is Normal? Introduction to normal curve 197 27 Lesson 17: Normal Measure of Spread Measures of spread - SD 201 28 Lesson 18: What\u2019s Your Z-Score? z-scores, shuffling 204 29 Lab 2H: Eyeballing Normal Normal curves overlaid on distributions & simulated data 220 30 Lab 2I: R\u2019s Normal Distribution Alphabet Normal probability, rnorm(), pnorm(), quantiles, qnorm() 212 Unit 2 Project (5 Days) 31-35 End of Unit Project and OralPresentations: Asking and AnsweringStatistical Questions of Our Own Data Stress/Chill, Personality Color, Food Habits, or Time Use Synthesis of above 214 ^=Data collection window begins. +=Data collection window ends.","title":"Daily Overview: Unit 2"},{"location":"unit2/practicum1/","text":"Practicum: The Summaries Objective: Students will engage in their first statistical investigation using the Data Cycle. They will pose a statistical investigative question based on a data set from a Participatory Sensing campaign, analyze, and interpret the data. Materials: The Summaries Practicum ( LMR_U2_Practicum_The Summaries ) Note to Teacher : Before assigning the practicum to your students, engage the class in a discussion about the sample statistical investigative questions below. Guide the discussion so that students identify not only the groups being compared in each question, but also what is being compared about the groups. Remind them of the Data Cycle from Unit 1 . Using the Food Habits campaign data or Personality Color survey data, develop a new statistical investigative question that compares two or more groups. Some sample statistical investigative questions (about other data sets) are below: Which gender shows a bigger range in age, male or female Oscar winners? Grouping variable: gender (male, female) Variable: ages Do children, teenagers, or adults spend more money on candy? Grouping variable: age group (child, teenager, adult) Variable: the amount of money spent on candy How does the median height of teenage males compare to that of females? Grouping variable: gender (male, female) Variable: height How do the average temperatures of Los Angeles, Las Vegas, and San Francisco compare? Grouping variable: city (Los Angeles, Las Vegas, San Francisco) Variable: daily maximum temperature Remember, a statistical investigative question is one that anticipates variability in the question and then addresses the variability in the answer: Based on the data you chose ( Food Habits or Personality Color ), you need to: Write down your question and think about ways you could answer it using RStudio. Describe the data you are using to answer your question and explain why it is appropriate. Analyze the data to provide evidence that supports the answer to your question. Include plot(s) and numerical summaries (mean, median, MAD, IQR, etc.) related to your plots. Interpret the data to answer your statistical investigative question. You should: a. Provide the plot(s) and numerical summaries related to your plot(s). b. Describe what the plot shows. c. Explain why you chose to make that particular plot(s) and the related numerical summaries. d. Explain how the plot and numerical summary answer your statistical investigative question. Write and submit a one-page report. Note : You may use the scoring guide in Unit 1 to give you an idea of how to score the Practicum.","title":"Practicum: The Summaries"},{"location":"unit2/practicum1/#practicum-the-summaries","text":"","title":"Practicum: The Summaries"},{"location":"unit2/practicum1/#objective","text":"Students will engage in their first statistical investigation using the Data Cycle. They will pose a statistical investigative question based on a data set from a Participatory Sensing campaign, analyze, and interpret the data.","title":"Objective:"},{"location":"unit2/practicum1/#materials","text":"The Summaries Practicum ( LMR_U2_Practicum_The Summaries ) Note to Teacher : Before assigning the practicum to your students, engage the class in a discussion about the sample statistical investigative questions below. Guide the discussion so that students identify not only the groups being compared in each question, but also what is being compared about the groups. Remind them of the Data Cycle from Unit 1 . Using the Food Habits campaign data or Personality Color survey data, develop a new statistical investigative question that compares two or more groups. Some sample statistical investigative questions (about other data sets) are below: Which gender shows a bigger range in age, male or female Oscar winners? Grouping variable: gender (male, female) Variable: ages Do children, teenagers, or adults spend more money on candy? Grouping variable: age group (child, teenager, adult) Variable: the amount of money spent on candy How does the median height of teenage males compare to that of females? Grouping variable: gender (male, female) Variable: height How do the average temperatures of Los Angeles, Las Vegas, and San Francisco compare? Grouping variable: city (Los Angeles, Las Vegas, San Francisco) Variable: daily maximum temperature Remember, a statistical investigative question is one that anticipates variability in the question and then addresses the variability in the answer: Based on the data you chose ( Food Habits or Personality Color ), you need to: Write down your question and think about ways you could answer it using RStudio. Describe the data you are using to answer your question and explain why it is appropriate. Analyze the data to provide evidence that supports the answer to your question. Include plot(s) and numerical summaries (mean, median, MAD, IQR, etc.) related to your plots. Interpret the data to answer your statistical investigative question. You should: a. Provide the plot(s) and numerical summaries related to your plot(s). b. Describe what the plot shows. c. Explain why you chose to make that particular plot(s) and the related numerical summaries. d. Explain how the plot and numerical summary answer your statistical investigative question. Write and submit a one-page report. Note : You may use the scoring guide in Unit 1 to give you an idea of how to score the Practicum.","title":"Materials:"},{"location":"unit2/practicum2/","text":"Practicum: Win Win Win Objective: Students will create and combine simulations to assess probabilities. Materials: Win Win Win Practicum ( LMR_U2_Practicum_Win Win Win ) Practicum Win Win Win The California lottery has a game called the Daily 3 . \u2022 It consists of 3 numbers between 0 - 9 that are drawn daily. \u2022 The numbers are drawn with replacement . \u2022 Winners are usually awarded a couple hundred dollars. \u2022 To win the maximum amount of money, players must correctly choose the numbers that are drawn, in order. Based on what you learned in Lab 2C and Lab 2D ( Which Song Plays Next and Queue it Up! ) and using the rules of the Daily 3 , you need to: Write down the code to correctly simulate the Daily 3 once. Use your code to simulate the Daily 3 500 times. Compute the estimated probability of getting the first 2 numbers of the Daily 3 correct. Should the estimated probability of correctly guessing the last 2 numbers of the Daily 3 be less than, the same as, or more than guessing the first 2 numbers? Why? In teams of 4: a. Each team member chooses 3 numbers for the Daily 3 . b. Each team member simulates the Daily 3 game 500 times. c. Within your group, combine the team simulation estimates to estimate the probability of winning the Daily 3 . Write and submit a one-page report. Your report should include the code.","title":"Practicum: Win, Win, Win"},{"location":"unit2/practicum2/#practicum-win-win-win","text":"","title":"Practicum: Win Win Win"},{"location":"unit2/practicum2/#objective","text":"Students will create and combine simulations to assess probabilities.","title":"Objective:"},{"location":"unit2/practicum2/#materials","text":"Win Win Win Practicum ( LMR_U2_Practicum_Win Win Win ) Practicum Win Win Win The California lottery has a game called the Daily 3 . \u2022 It consists of 3 numbers between 0 - 9 that are drawn daily. \u2022 The numbers are drawn with replacement . \u2022 Winners are usually awarded a couple hundred dollars. \u2022 To win the maximum amount of money, players must correctly choose the numbers that are drawn, in order. Based on what you learned in Lab 2C and Lab 2D ( Which Song Plays Next and Queue it Up! ) and using the rules of the Daily 3 , you need to: Write down the code to correctly simulate the Daily 3 once. Use your code to simulate the Daily 3 500 times. Compute the estimated probability of getting the first 2 numbers of the Daily 3 correct. Should the estimated probability of correctly guessing the last 2 numbers of the Daily 3 be less than, the same as, or more than guessing the first 2 numbers? Why? In teams of 4: a. Each team member chooses 3 numbers for the Daily 3 . b. Each team member simulates the Daily 3 game 500 times. c. Within your group, combine the team simulation estimates to estimate the probability of winning the Daily 3 . Write and submit a one-page report. Your report should include the code.","title":"Materials:"},{"location":"unit2/practicum3/","text":"Practicum: What Stresses Us? Objective: Students will use RStudio to make graphical representations or numerical summaries of their Stress/Chill and Personality Color data to answer research questions. Materials: What Stresses Us? Practicum ( LMR_U2_Practicum_What Stresses Us ) Practicum What Stresses Us? We made a data set that combined our Stress/Chill data with our Personality Color data. You will use this data to answer the following research questions: \u2022 Do color personalities really predict a person\u2019s personality? \u2022 Do people with different personality colors tend to have different stress levels? Based on the merged data, you need to: Write a one-page report to address these research questions. Use the Data Cycle. Your analysis should include both numerical methods (means, medians, etc.) and graphical methods (plots). The research questions are fairly broad, and you should first think of simpler statistical investigative questions you could ask that would address these research questions. In your report, be sure to: a. Provide the plot(s) and numerical summary (or summaries). b. Describe what the plot shows. c. Explain why you chose to make that particular plot. d. Explain how the plot and numerical summary answers your statistical investigative question. Present your report to another member of the class who is not in your team. a. Make sure to include any relevant plots or numerical summaries that you use to make any conclusion in your analysis. Note: You may use the scoring guide in Unit 1 to give you an idea of how to score the Practicum.","title":"Practicum: What Stresses Us?"},{"location":"unit2/practicum3/#practicum-what-stresses-us","text":"","title":"Practicum: What Stresses Us?"},{"location":"unit2/practicum3/#objective","text":"Students will use RStudio to make graphical representations or numerical summaries of their Stress/Chill and Personality Color data to answer research questions.","title":"Objective:"},{"location":"unit2/practicum3/#materials","text":"What Stresses Us? Practicum ( LMR_U2_Practicum_What Stresses Us ) Practicum What Stresses Us? We made a data set that combined our Stress/Chill data with our Personality Color data. You will use this data to answer the following research questions: \u2022 Do color personalities really predict a person\u2019s personality? \u2022 Do people with different personality colors tend to have different stress levels? Based on the merged data, you need to: Write a one-page report to address these research questions. Use the Data Cycle. Your analysis should include both numerical methods (means, medians, etc.) and graphical methods (plots). The research questions are fairly broad, and you should first think of simpler statistical investigative questions you could ask that would address these research questions. In your report, be sure to: a. Provide the plot(s) and numerical summary (or summaries). b. Describe what the plot shows. c. Explain why you chose to make that particular plot. d. Explain how the plot and numerical summary answers your statistical investigative question. Present your report to another member of the class who is not in your team. a. Make sure to include any relevant plots or numerical summaries that you use to make any conclusion in your analysis. Note: You may use the scoring guide in Unit 1 to give you an idea of how to score the Practicum.","title":"Materials:"},{"location":"unit2/section1/","text":"Unit 2, Section 1: What is Your True Color? Instructional Days: 7 Enduring Understandings Statistics enable us to make sense of large amounts of data. Numerical summaries capture important elements of a distribution. Measures of center, also known as measures of central tendency, show the tendency of quantitative data to gather around a central value. Measures of spread, also known as measures of variability, show how much the quantitative data is spread out. Measurements of the propensity for the data to cluster on a central location and the range of variability within the data can provide insightful indicators about the data. Engagement Students will complete the True Colors Personality Test to discover the qualities and characteristics of their personality styles. Students will use the results from the personality color test to learn about subsetting data and finding measures of center and spread. The data from their personality test will be collected in a survey using the IDS UCLA App or via web browser at https://portal.idsucla.org Learning Objectives Statistical/Mathematical: S-ID 2: Use statistics appropriate to the shape of the data distribution to compare center (median, mean) and spread (interquartile range, standard deviation) of two or more different data sets. S-ID 3: Interpret differences in shape, center, and spread in the context of the data sets, accounting for possible effects of extreme data points (outliers). S-IC 6: Evaluate reports based on data. Focus Standards for Mathematical Practice for All of Unit 2: SMP-4: Model with mathematics. SMP-5: Use appropriate tools strategically. Data Science: Understand the information that numerical summaries provide about the data. Understand that a boxplot is a graphical representation of a numerical summary. Applied Computational Thinking using RStudio: \u2022 Calculate numerical summaries (mean, median, Sum of Absolute Deviations (SAD), and Mean of Absolute Deviations (MAD)). \u2022 Create graphical representations to compare two or more data sets, including boxplots. Real-World Connections: We must be able to synthesize vast amounts of data into coherent, comprehensible measures. Today\u2019s media is continuously publishing articles that include statistical references. Critical consumerism requires that we understand the information provided in summaries of data. Language Objectives Students will use complex sentences to construct summary statements about their understanding of data, how it is collected, how it is used and how to work with it. Students will engage in partner and whole group discussions and presentations to express their understanding of data science concepts. Data File or Data Collection Method Data Collection Method: True Colors Personality Test: Students will complete the Personality Color survey that will collect their data about their personality styles. Data Files: Students\u2019 True Colors Personality survey data (colors) Legend for Activity Icons","title":"What is Your True Color?"},{"location":"unit2/section1/#unit-2-section-1-what-is-your-true-color","text":"Instructional Days: 7","title":"Unit 2, Section 1: What is Your True Color?"},{"location":"unit2/section1/#enduring-understandings","text":"Statistics enable us to make sense of large amounts of data. Numerical summaries capture important elements of a distribution. Measures of center, also known as measures of central tendency, show the tendency of quantitative data to gather around a central value. Measures of spread, also known as measures of variability, show how much the quantitative data is spread out. Measurements of the propensity for the data to cluster on a central location and the range of variability within the data can provide insightful indicators about the data.","title":"Enduring Understandings"},{"location":"unit2/section1/#engagement","text":"Students will complete the True Colors Personality Test to discover the qualities and characteristics of their personality styles. Students will use the results from the personality color test to learn about subsetting data and finding measures of center and spread. The data from their personality test will be collected in a survey using the IDS UCLA App or via web browser at https://portal.idsucla.org","title":"Engagement"},{"location":"unit2/section1/#learning-objectives","text":"Statistical/Mathematical: S-ID 2: Use statistics appropriate to the shape of the data distribution to compare center (median, mean) and spread (interquartile range, standard deviation) of two or more different data sets. S-ID 3: Interpret differences in shape, center, and spread in the context of the data sets, accounting for possible effects of extreme data points (outliers). S-IC 6: Evaluate reports based on data. Focus Standards for Mathematical Practice for All of Unit 2: SMP-4: Model with mathematics. SMP-5: Use appropriate tools strategically. Data Science: Understand the information that numerical summaries provide about the data. Understand that a boxplot is a graphical representation of a numerical summary. Applied Computational Thinking using RStudio: \u2022 Calculate numerical summaries (mean, median, Sum of Absolute Deviations (SAD), and Mean of Absolute Deviations (MAD)). \u2022 Create graphical representations to compare two or more data sets, including boxplots. Real-World Connections: We must be able to synthesize vast amounts of data into coherent, comprehensible measures. Today\u2019s media is continuously publishing articles that include statistical references. Critical consumerism requires that we understand the information provided in summaries of data.","title":"Learning Objectives"},{"location":"unit2/section1/#language-objectives","text":"Students will use complex sentences to construct summary statements about their understanding of data, how it is collected, how it is used and how to work with it. Students will engage in partner and whole group discussions and presentations to express their understanding of data science concepts.","title":"Language Objectives"},{"location":"unit2/section1/#data-file-or-data-collection-method","text":"Data Collection Method: True Colors Personality Test: Students will complete the Personality Color survey that will collect their data about their personality styles. Data Files: Students\u2019 True Colors Personality survey data (colors)","title":"Data File or Data Collection Method"},{"location":"unit2/section1/#legend-for-activity-icons","text":"","title":"Legend for Activity Icons"},{"location":"unit2/section2/","text":"Unit 2, Section 2: How Likely Is It? Instructional Days: 7 Enduring Understandings Probability measures the long run frequency of occurrence for chance outcomes. Probabilities can be approximated by designing and conducting simulations, and also via mathematical calculation. Engagement Students will watch a scene from the movie Rosencrantz and Guildenstern are Dead and discuss the likelihood of getting \u201cheads\u201d when tossing a coin 78 times in a row. The scene can be found at: https://www.youtube.com/watch?v=NbInZ5oJ0bc https://www.youtube.com/embed/NbInZ5oJ0bc Learning Objectives Statistical/Mathematical: S-CP 2: Understand that two events A and B are independent if the probability of A and B occurring together is the product of their probabilities, and use this characterization to determine if they are independent. S-CP 9: (+) Use permutations to perform [informal] inference. *This standard will be addressed in the context of data science. S-IC 6: Evaluate reports based on data. Data Science: Understand how algorithms are used to design simulations. Applied Computational Thinking using RStudio: \u2022 Design and conduct simulations in RStudio. \u2022 Compare actual data to simulated data using RStudio. \u2022 Re-randomization of permuted data. \u2022 Use estimated probabilities from samples to determine theoretical probabilities Real-World Connections: Learn to use simulations to determine expectations of events. Language Objectives Students will use complex sentences to construct summary statements about their understanding of data, how it is collected, how it is used and how to work with it. Students will engage in partner and whole group discussions and presentations to express their understanding of data science concepts. Data File or Data Collection Method Simulated data in RStudio. Legend for Activity Icons","title":"How Likely is it?"},{"location":"unit2/section2/#unit-2-section-2-how-likely-is-it","text":"Instructional Days: 7","title":"Unit 2, Section 2: How Likely Is It?"},{"location":"unit2/section2/#enduring-understandings","text":"Probability measures the long run frequency of occurrence for chance outcomes. Probabilities can be approximated by designing and conducting simulations, and also via mathematical calculation.","title":"Enduring Understandings"},{"location":"unit2/section2/#engagement","text":"Students will watch a scene from the movie Rosencrantz and Guildenstern are Dead and discuss the likelihood of getting \u201cheads\u201d when tossing a coin 78 times in a row. The scene can be found at: https://www.youtube.com/watch?v=NbInZ5oJ0bc https://www.youtube.com/embed/NbInZ5oJ0bc","title":"Engagement"},{"location":"unit2/section2/#learning-objectives","text":"Statistical/Mathematical: S-CP 2: Understand that two events A and B are independent if the probability of A and B occurring together is the product of their probabilities, and use this characterization to determine if they are independent. S-CP 9: (+) Use permutations to perform [informal] inference. *This standard will be addressed in the context of data science. S-IC 6: Evaluate reports based on data. Data Science: Understand how algorithms are used to design simulations. Applied Computational Thinking using RStudio: \u2022 Design and conduct simulations in RStudio. \u2022 Compare actual data to simulated data using RStudio. \u2022 Re-randomization of permuted data. \u2022 Use estimated probabilities from samples to determine theoretical probabilities Real-World Connections: Learn to use simulations to determine expectations of events.","title":"Learning Objectives"},{"location":"unit2/section2/#language-objectives","text":"Students will use complex sentences to construct summary statements about their understanding of data, how it is collected, how it is used and how to work with it. Students will engage in partner and whole group discussions and presentations to express their understanding of data science concepts.","title":"Language Objectives"},{"location":"unit2/section2/#data-file-or-data-collection-method","text":"Simulated data in RStudio.","title":"Data File or Data Collection Method"},{"location":"unit2/section2/#legend-for-activity-icons","text":"","title":"Legend for Activity Icons"},{"location":"unit2/section3/","text":"Unit 2, Section 3: What Are the Chances That You Are Stressing or Chilling? Instructional Days: 8 Enduring Understandings Permutations of data provide a model that shows us how the world behaves if chance is the only reason for differences between groups or for associations between variables. If our actual observation is a rare permutation, this suggests that chance is not a good explanation for the difference or association. On the other hand, if the actual observation is a common permutation, this suggests that chance may be a valid explanation. Differences between permuted data and actual data suggest that the chance model can be rejected and there is a dependent relationship between two variables. Engagement Students will read the Huffington Post article titled Don\u2019t Take My Stress Away to set the stage for the Stress/Chill Campaign. High school students who expected, and wanted, to feel stressed out by school wrote this article. The article is found at: http://www.huffingtonpost.com/jack-cahn/dont-take-my-stress-away_b_2090203.html Learning Objectives Statistical/Mathematical: S-IC 2: Decide if a specified model is consistent with results from a given data-generating process, e.g., using simulation. S-IC 6: Evaluate reports based on data. Data Science: Understand that a chance model serves as an indicator of whether or not associations in the actual data are due to chance (understand why a plot might appear to have a trend, but may actually be the result of randomness). Understand that simulations provide a way of comparing expected chance outcomes to real outcomes in order to determine if a model and actual data appear consistent. Learn about merging data sets by understanding the structure of both data sets and the logic of the way they will be combined. Applied Computational Thinking using RStudio: \u2022 Permutations of data, determining if actual data is similar to permuted data \u2022 Merge multiple data sets together based on a common variable \u2022 Create permutations using a merged data set Real-World Connections: In media, citizens read about results and scientific studies in which treatments are applied. In real life, one can ask the question: Does this happen by chance? Understanding chance helps us interpret media reports of scientific and medical findings. Language Objectives Students will use complex sentences to construct summary statements about their understanding of data, how it is collected, how it used, and how to work with it. Students will engage in partner and whole group discussions and presentations to express their understanding of data science concepts. Students will use complex sentences to write informative short reports that use data science concepts and skills. Students will read informative texts to evaluate claims based on data. Data File or Data Collection Method Data Collection Method: Stress/Chill Participatory Sensing Campaign: Students will monitor how they feel at different times of the day \u2013 whether they are \u201cstressing\u201d or \u201cchilling.\u201d Along with how they feel, they will make observations regarding other factors, such as being alone or with others, what they are doing at that moment, and why they are doing that activity. Data Files: Students\u2019 Personality Color survey data ( colors ) Students\u2019 Stress/Chill campaign data Titanic data set ( titanic.rda ) Horror Movie data set ( slasher.rda ) Legend for Activity Icons","title":"What Are the Chances That You Are Stressing or Chilling?"},{"location":"unit2/section3/#unit-2-section-3-what-are-the-chances-that-you-are-stressing-or-chilling","text":"Instructional Days: 8","title":"Unit 2, Section 3: What Are the Chances That You Are Stressing or Chilling?"},{"location":"unit2/section3/#enduring-understandings","text":"Permutations of data provide a model that shows us how the world behaves if chance is the only reason for differences between groups or for associations between variables. If our actual observation is a rare permutation, this suggests that chance is not a good explanation for the difference or association. On the other hand, if the actual observation is a common permutation, this suggests that chance may be a valid explanation. Differences between permuted data and actual data suggest that the chance model can be rejected and there is a dependent relationship between two variables.","title":"Enduring Understandings"},{"location":"unit2/section3/#engagement","text":"Students will read the Huffington Post article titled Don\u2019t Take My Stress Away to set the stage for the Stress/Chill Campaign. High school students who expected, and wanted, to feel stressed out by school wrote this article. The article is found at: http://www.huffingtonpost.com/jack-cahn/dont-take-my-stress-away_b_2090203.html","title":"Engagement"},{"location":"unit2/section3/#learning-objectives","text":"Statistical/Mathematical: S-IC 2: Decide if a specified model is consistent with results from a given data-generating process, e.g., using simulation. S-IC 6: Evaluate reports based on data. Data Science: Understand that a chance model serves as an indicator of whether or not associations in the actual data are due to chance (understand why a plot might appear to have a trend, but may actually be the result of randomness). Understand that simulations provide a way of comparing expected chance outcomes to real outcomes in order to determine if a model and actual data appear consistent. Learn about merging data sets by understanding the structure of both data sets and the logic of the way they will be combined. Applied Computational Thinking using RStudio: \u2022 Permutations of data, determining if actual data is similar to permuted data \u2022 Merge multiple data sets together based on a common variable \u2022 Create permutations using a merged data set Real-World Connections: In media, citizens read about results and scientific studies in which treatments are applied. In real life, one can ask the question: Does this happen by chance? Understanding chance helps us interpret media reports of scientific and medical findings.","title":"Learning Objectives"},{"location":"unit2/section3/#language-objectives","text":"Students will use complex sentences to construct summary statements about their understanding of data, how it is collected, how it used, and how to work with it. Students will engage in partner and whole group discussions and presentations to express their understanding of data science concepts. Students will use complex sentences to write informative short reports that use data science concepts and skills. Students will read informative texts to evaluate claims based on data.","title":"Language Objectives"},{"location":"unit2/section3/#data-file-or-data-collection-method","text":"Data Collection Method: Stress/Chill Participatory Sensing Campaign: Students will monitor how they feel at different times of the day \u2013 whether they are \u201cstressing\u201d or \u201cchilling.\u201d Along with how they feel, they will make observations regarding other factors, such as being alone or with others, what they are doing at that moment, and why they are doing that activity. Data Files: Students\u2019 Personality Color survey data ( colors ) Students\u2019 Stress/Chill campaign data Titanic data set ( titanic.rda ) Horror Movie data set ( slasher.rda )","title":"Data File or Data Collection Method"},{"location":"unit2/section3/#legend-for-activity-icons","text":"","title":"Legend for Activity Icons"},{"location":"unit2/section4/","text":"Unit 2, Section 4: What\u2019s Normal? Instructional Days: 5 Enduring Understandings Students learn that the Normal curve can be used as a model that describes many real phenomena. Drawing plots of the Normal curve over histograms helps data scientists determine if the distribution represented by the histogram is close to Normal. The Normal curve suggests that one is more likely to obtain values that are close to typical (average), which are found in the center of the curve, and less likely to obtain values that are extreme and farther away from typical. Engagement Students will learn about the Normal curve by watching the first 35 seconds the New York Times Video \u201cBunnies, Dragons, and the Normal World\u201d found at: http://www.nytimes.com/video/science/100000002452709/bunnies-dragons-and-the-normal-world.html Learning Objectives Statistical/Mathematical: S-ID 4: Use the mean and standard deviation of a data set to fit it to a normal distribution and to estimate population percentages. Understand that there are data sets for which such a procedure is not appropriate. Use calculators and RStudio to estimate areas under the normal curve. S-IC 6: Evaluate reports based on data. Data Science: Learn to eyeball Normal distributions and overlay a Normal curve on a histogram; learn to simulate draws from a Normal distribution, and the impact of sample size; learn that estimating probabilities with a model leads to stable estimates; and estimate probabilities by finding the area under the Normal curve using RStudio. Applied Computational Thinking Using RStudio: \u2022 Use software to find the area under a Normal curve \u2022 Use software to compare sample distributions (with histograms, for example) with the Normal distribution and make a decision as to whether the distribution appears Normally distributed. \u2022 Draw random samples from a Normal distribution using software. Real-World Connections: The Normal curve is used to make inferences about a population. The model makes it possible to estimate the probability of occurrence of any value of a Normally distributed variable. For example, heights are Normally distributed. Using a Normal curve, we can find the probability of that a person would be a height of 6\u2019 2\u201d. Language Objectives Students will use complex sentences to construct summary statements about their understanding of data, how it is collected, how it used and how to work with it. Students will engage in partner and whole group discussions and presentations to express their understanding of data science concepts. Students will use complex sentences to write informative short reports that use data science concepts and skills. Data File or Data Collection Method Data Files: CDC data ( cdc ) Titanic data ( titanic ) Legend for Activity Icons","title":"What\u2019s Normal?"},{"location":"unit2/section4/#unit-2-section-4-whats-normal","text":"Instructional Days: 5","title":"Unit 2, Section 4: What\u2019s Normal?"},{"location":"unit2/section4/#enduring-understandings","text":"Students learn that the Normal curve can be used as a model that describes many real phenomena. Drawing plots of the Normal curve over histograms helps data scientists determine if the distribution represented by the histogram is close to Normal. The Normal curve suggests that one is more likely to obtain values that are close to typical (average), which are found in the center of the curve, and less likely to obtain values that are extreme and farther away from typical.","title":"Enduring Understandings"},{"location":"unit2/section4/#engagement","text":"Students will learn about the Normal curve by watching the first 35 seconds the New York Times Video \u201cBunnies, Dragons, and the Normal World\u201d found at: http://www.nytimes.com/video/science/100000002452709/bunnies-dragons-and-the-normal-world.html","title":"Engagement"},{"location":"unit2/section4/#learning-objectives","text":"Statistical/Mathematical: S-ID 4: Use the mean and standard deviation of a data set to fit it to a normal distribution and to estimate population percentages. Understand that there are data sets for which such a procedure is not appropriate. Use calculators and RStudio to estimate areas under the normal curve. S-IC 6: Evaluate reports based on data. Data Science: Learn to eyeball Normal distributions and overlay a Normal curve on a histogram; learn to simulate draws from a Normal distribution, and the impact of sample size; learn that estimating probabilities with a model leads to stable estimates; and estimate probabilities by finding the area under the Normal curve using RStudio. Applied Computational Thinking Using RStudio: \u2022 Use software to find the area under a Normal curve \u2022 Use software to compare sample distributions (with histograms, for example) with the Normal distribution and make a decision as to whether the distribution appears Normally distributed. \u2022 Draw random samples from a Normal distribution using software. Real-World Connections: The Normal curve is used to make inferences about a population. The model makes it possible to estimate the probability of occurrence of any value of a Normally distributed variable. For example, heights are Normally distributed. Using a Normal curve, we can find the probability of that a person would be a height of 6\u2019 2\u201d.","title":"Learning Objectives"},{"location":"unit2/section4/#language-objectives","text":"Students will use complex sentences to construct summary statements about their understanding of data, how it is collected, how it used and how to work with it. Students will engage in partner and whole group discussions and presentations to express their understanding of data science concepts. Students will use complex sentences to write informative short reports that use data science concepts and skills.","title":"Language Objectives"},{"location":"unit2/section4/#data-file-or-data-collection-method","text":"Data Files: CDC data ( cdc ) Titanic data ( titanic )","title":"Data File or Data Collection Method"},{"location":"unit2/section4/#legend-for-activity-icons","text":"","title":"Legend for Activity Icons"},{"location":"unit3/end/","text":"End of Unit Project and Oral Presentation: TB or Not TB? Objective: Students will apply what they have learned in the unit. Materials: Computers IDS Unit 3 \u2013 Project and Oral Presentation ( LMR_U3_End of Unit Project ) IDS Unit 3 \u2013 End of Unit Project TB or Not TB Experiments in the medical field that involve new treatments (new medications) are called clinical trials. You have received a data set that shows the results from Sir Austin Bradford Hill\u2019s first randomized study in 1948 examining the effects of the antibiotic Streptomycin on 107 tuberculosis patients. You and a partner will use this data set to find out if Streptomycin is an effective treatment for tuberculosis. A short article about tuberculosis facts can be found at: http://www.cdc.gov/tb/publications/factsheets/general/tb.htm Since this is an experiment, answer the following questions below. You may need to research the answer to some of the questions. a. What is the research question? b. Who are the subjects that participated in the experiment? c. What is the treatment? d. Who is in the treatment group? e. Who is in the control group? f. How were the subjects assigned to each group? g. What population is this experiment representative of? h. What is the variable that we will be measuring? i. What is the outcome of this experiment? To answer your research question, you and a partner will compare the outcome of the data with the outcomes given by a chance model (in which Streptomycin has no effect on TB). First, scrape the data. Refer to the web scraping lab if you need to recall how to scrape data. To access Sir Hill\u2019s data, go to: https://labs.idsucla.org/extras/webdata/tb.html Second, determine the percentages of subjects in the study that died and the percentages of the subjects that recovered for each group. Third, assuming that the treatment had no effect, use the data to: a. Calculate the percentage of people with tuberculosis we would expect to die. b. Use the expected percentage for (a), above, to calculate the number of people we expect to die from the treatment group. c. Compare the percentage from (b) to the percentage from the treatment group actually died. Then, if we assume that the outcome does not depend on the treatment, design and complete an appropriate simulation in RStudio using a chance model to replicate Sir Hill\u2019s study: a. Shuffle the treatment and control labels 300 times; each time, calculate the percentage of treatment patients who \u201cdied\u201d. Plot the distribution of the 300 percentages. Refer to the simulation labs if you need to recall how to create a simulation. b. Use the results from the chance model (shuffling) to determine whether (i.) or (ii.) below is the most reasonable explanation for the actual data in Sir Hill\u2019s study and state why: i. Streptomycin is a much better treatment for tuberculosis than bed rest. So, the outcome depends on the treatment. ii. The actual difference between treatments is due to chance; Streptomycin may not be effective on tuberculosis. So, it is possible that treatment and outcome are independent. Can we say that Streptomycin causes the recovery of tuberculosis patients? Explain your answer. Create a 4-5 slide, 5-minute presentation that shows your results. Be sure to include a detailed explanation of how you and your partner decided to conduct your simulation. Each person must participate in the presentation. In addition to the presentation, submit a 2-4 page, double-spaced summary of your analysis.","title":"End of Unit Project: TB or Not TB"},{"location":"unit3/end/#end-of-unit-project-and-oral-presentation-tb-or-not-tb","text":"","title":"End of Unit Project and Oral Presentation: TB or Not TB?"},{"location":"unit3/end/#objective","text":"Students will apply what they have learned in the unit.","title":"Objective:"},{"location":"unit3/end/#materials","text":"Computers IDS Unit 3 \u2013 Project and Oral Presentation ( LMR_U3_End of Unit Project ) IDS Unit 3 \u2013 End of Unit Project TB or Not TB Experiments in the medical field that involve new treatments (new medications) are called clinical trials. You have received a data set that shows the results from Sir Austin Bradford Hill\u2019s first randomized study in 1948 examining the effects of the antibiotic Streptomycin on 107 tuberculosis patients. You and a partner will use this data set to find out if Streptomycin is an effective treatment for tuberculosis. A short article about tuberculosis facts can be found at: http://www.cdc.gov/tb/publications/factsheets/general/tb.htm Since this is an experiment, answer the following questions below. You may need to research the answer to some of the questions. a. What is the research question? b. Who are the subjects that participated in the experiment? c. What is the treatment? d. Who is in the treatment group? e. Who is in the control group? f. How were the subjects assigned to each group? g. What population is this experiment representative of? h. What is the variable that we will be measuring? i. What is the outcome of this experiment? To answer your research question, you and a partner will compare the outcome of the data with the outcomes given by a chance model (in which Streptomycin has no effect on TB). First, scrape the data. Refer to the web scraping lab if you need to recall how to scrape data. To access Sir Hill\u2019s data, go to: https://labs.idsucla.org/extras/webdata/tb.html Second, determine the percentages of subjects in the study that died and the percentages of the subjects that recovered for each group. Third, assuming that the treatment had no effect, use the data to: a. Calculate the percentage of people with tuberculosis we would expect to die. b. Use the expected percentage for (a), above, to calculate the number of people we expect to die from the treatment group. c. Compare the percentage from (b) to the percentage from the treatment group actually died. Then, if we assume that the outcome does not depend on the treatment, design and complete an appropriate simulation in RStudio using a chance model to replicate Sir Hill\u2019s study: a. Shuffle the treatment and control labels 300 times; each time, calculate the percentage of treatment patients who \u201cdied\u201d. Plot the distribution of the 300 percentages. Refer to the simulation labs if you need to recall how to create a simulation. b. Use the results from the chance model (shuffling) to determine whether (i.) or (ii.) below is the most reasonable explanation for the actual data in Sir Hill\u2019s study and state why: i. Streptomycin is a much better treatment for tuberculosis than bed rest. So, the outcome depends on the treatment. ii. The actual difference between treatments is due to chance; Streptomycin may not be effective on tuberculosis. So, it is possible that treatment and outcome are independent. Can we say that Streptomycin causes the recovery of tuberculosis patients? Explain your answer. Create a 4-5 slide, 5-minute presentation that shows your results. Be sure to include a detailed explanation of how you and your partner decided to conduct your simulation. Each person must participate in the presentation. In addition to the presentation, submit a 2-4 page, double-spaced summary of your analysis.","title":"Materials:"},{"location":"unit3/essential/","text":"IDS Unit 3: Essential Concepts Lesson 1: Anecdotes vs. Data Data beat anecdotes. In science, we need to closely examine the quality of evidence in order to make sound conclusions. Anecdotes can contain personal bias, might be carefully selected to represent a particular point of view, and, in general, may be completely different from the general trend. Lesson 2: What is an Experiment? Science is often concerned with the question \"What causes things to happen?\". To answer this, controlled experiments are required. Controlled experiments have several key features: (1) there is a treatment variable and a response variable, and we wish to see if the treatment causes a change that we can measure with the response variable; (2) There is a comparison/control group; (3) Subjects are assigned randomly to treatment or control (randomized assignment); (4) Subjects are not aware of which group they are in (a 'blind'). This may require the use of a placebo for those in the control group; and (5) those who measure the response variable do not know which group the subjects were in (if both 4 and 5 are satisfied, this is a 'double blind' experiment). Lesson 3: Let\u2019s Try an Experiment! Randomized assignment is required to determine cause-and-effect. Lesson 4: Predictions, Predictions Designing an experiment requires making many decisions, including what to measure and how to measure it. Lesson 5: Time Perception Experiment Designing and carrying out an experiment helps us answer specific statistical questions of interest. Lesson 6: Observational Studies Observational studies are those for which there is no intervention applied by researchers. Lesson 7: Observational Studies vs. Experiments Experiments are not always possible because of various factors such as ethics, cost limitations, and feasibility. Lesson 8: Monsters that Hide in Observational Studies Confounding factors/variables make it difficult to determine a cause-and-effect relationship between two variables. Lesson 9: Survey Says\u2026 Surveys ask simple, straightforward questions in order to collect data that can be used to answer statistical investigative questions. Writing such questions can be hard (but fun)! Lesson 10: We\u2019re So Random Another popular data collection method involves collecting data from a random sample of people or objects. Percentages based on random samples tend to \"center\" on the population parameter value. Lesson 11: The Gettysburg Address Statistics vary from sample to sample. If the typical value across many samples is equal to the population parameter, the statistic is \"unbiased\". Bias means that we tend to \u201cmiss the mark.\u201d If we don't do random sampling, we can get biased estimates. Lesson 12: Bias in Survey Sampling Bias concerning survey sampling includes identifying sampling methods that may lead to biased samples, recognizing potential over- or under-representation in samples, and acquiring skills to choose more reliable sampling techniques. Lesson 13: The Confidence Game There is uncertainty when we estimate population parameters. Because of this, it is better to give a range of plausible values, rather than a single value. Lesson 14: How Confident Are You? The margin of error expresses our uncertainty in an estimate. The estimate, plus or minus the margin of error, gives us an interval in which we are very confident the true value lies. Lesson 15 Ready, Sense, Go! Sensors are another data collection method. Unlike what we have seen so far, sensors do not involve humans (much). They collect data according to an algorithm. Lesson 16: Does it have a Trigger? A key feature that distinguishes the way sensors collect data from more traditional approaches is that sensors collect data when a 'trigger' event occurs. In Participatory Sensing, this event is something we humans agree upon beforehand. Every time that trigger happens, we collect data. Lesson 17: Creating Our Own Participatory Sensing Campaign Creating a Participatory Sensing Campaign requires that survey questions must be completed whenever they are \u201ctriggered\u201d. Research questions provide an overall direction in a Participatory Sensing Campaign. Lesson 18: Evaluating Our Own Participatory Sensing Campaign Statistical investigative questions guide a Participatory Sensing Campaign so that we can learn about a community or ourselves. These campaigns should be evaluated before implementing to make sure they are reasonable and ethically sound. Lesson 19: Implementing Our Own Participatory Sensing Campaign Practicing data collection prior to implementation allows optimization of a Participatory Sensing Campaign. Lesson 20: Online Data-ing Stretching the conception of data involves seeing that many web pages present information that can be turned into data. Lesson 21: Learning to Love XML XML is a programming language that we use with our campaigns. We create basic XML \"tags\" in the code, which help us store data in a format we understand. Lesson 22: Changing Orientation Converting XML to spreadsheet format helps us better understand and view our data.","title":"Essential Concepts"},{"location":"unit3/essential/#ids-unit-3-essential-concepts","text":"","title":"IDS Unit 3: Essential Concepts"},{"location":"unit3/essential/#lesson-1-anecdotes-vs-data","text":"Data beat anecdotes. In science, we need to closely examine the quality of evidence in order to make sound conclusions. Anecdotes can contain personal bias, might be carefully selected to represent a particular point of view, and, in general, may be completely different from the general trend.","title":"Lesson 1: Anecdotes vs. Data"},{"location":"unit3/essential/#lesson-2-what-is-an-experiment","text":"Science is often concerned with the question \"What causes things to happen?\". To answer this, controlled experiments are required. Controlled experiments have several key features: (1) there is a treatment variable and a response variable, and we wish to see if the treatment causes a change that we can measure with the response variable; (2) There is a comparison/control group; (3) Subjects are assigned randomly to treatment or control (randomized assignment); (4) Subjects are not aware of which group they are in (a 'blind'). This may require the use of a placebo for those in the control group; and (5) those who measure the response variable do not know which group the subjects were in (if both 4 and 5 are satisfied, this is a 'double blind' experiment).","title":"Lesson 2: What is an Experiment?"},{"location":"unit3/essential/#lesson-3-lets-try-an-experiment","text":"Randomized assignment is required to determine cause-and-effect.","title":"Lesson 3: Let\u2019s Try an Experiment!"},{"location":"unit3/essential/#lesson-4-predictions-predictions","text":"Designing an experiment requires making many decisions, including what to measure and how to measure it.","title":"Lesson 4: Predictions, Predictions"},{"location":"unit3/essential/#lesson-5-time-perception-experiment","text":"Designing and carrying out an experiment helps us answer specific statistical questions of interest.","title":"Lesson 5: Time Perception Experiment"},{"location":"unit3/essential/#lesson-6-observational-studies","text":"Observational studies are those for which there is no intervention applied by researchers.","title":"Lesson 6: Observational Studies"},{"location":"unit3/essential/#lesson-7-observational-studies-vs-experiments","text":"Experiments are not always possible because of various factors such as ethics, cost limitations, and feasibility.","title":"Lesson 7: Observational Studies vs. Experiments"},{"location":"unit3/essential/#lesson-8-monsters-that-hide-in-observational-studies","text":"Confounding factors/variables make it difficult to determine a cause-and-effect relationship between two variables.","title":"Lesson 8: Monsters that Hide in Observational Studies"},{"location":"unit3/essential/#lesson-9-survey-says","text":"Surveys ask simple, straightforward questions in order to collect data that can be used to answer statistical investigative questions. Writing such questions can be hard (but fun)!","title":"Lesson 9: Survey Says\u2026"},{"location":"unit3/essential/#lesson-10-were-so-random","text":"Another popular data collection method involves collecting data from a random sample of people or objects. Percentages based on random samples tend to \"center\" on the population parameter value.","title":"Lesson 10: We\u2019re So Random"},{"location":"unit3/essential/#lesson-11-the-gettysburg-address","text":"Statistics vary from sample to sample. If the typical value across many samples is equal to the population parameter, the statistic is \"unbiased\". Bias means that we tend to \u201cmiss the mark.\u201d If we don't do random sampling, we can get biased estimates.","title":"Lesson 11: The Gettysburg Address"},{"location":"unit3/essential/#lesson-12-bias-in-survey-sampling","text":"Bias concerning survey sampling includes identifying sampling methods that may lead to biased samples, recognizing potential over- or under-representation in samples, and acquiring skills to choose more reliable sampling techniques.","title":"Lesson 12: Bias in Survey Sampling"},{"location":"unit3/essential/#lesson-13-the-confidence-game","text":"There is uncertainty when we estimate population parameters. Because of this, it is better to give a range of plausible values, rather than a single value.","title":"Lesson 13: The Confidence Game"},{"location":"unit3/essential/#lesson-14-how-confident-are-you","text":"The margin of error expresses our uncertainty in an estimate. The estimate, plus or minus the margin of error, gives us an interval in which we are very confident the true value lies.","title":"Lesson 14: How Confident Are You?"},{"location":"unit3/essential/#lesson-15-ready-sense-go","text":"Sensors are another data collection method. Unlike what we have seen so far, sensors do not involve humans (much). They collect data according to an algorithm.","title":"Lesson 15 Ready, Sense, Go!"},{"location":"unit3/essential/#lesson-16-does-it-have-a-trigger","text":"A key feature that distinguishes the way sensors collect data from more traditional approaches is that sensors collect data when a 'trigger' event occurs. In Participatory Sensing, this event is something we humans agree upon beforehand. Every time that trigger happens, we collect data.","title":"Lesson 16: Does it have a Trigger?"},{"location":"unit3/essential/#lesson-17-creating-our-own-participatory-sensing-campaign","text":"Creating a Participatory Sensing Campaign requires that survey questions must be completed whenever they are \u201ctriggered\u201d. Research questions provide an overall direction in a Participatory Sensing Campaign.","title":"Lesson 17: Creating Our Own Participatory Sensing Campaign"},{"location":"unit3/essential/#lesson-18-evaluating-our-own-participatory-sensing-campaign","text":"Statistical investigative questions guide a Participatory Sensing Campaign so that we can learn about a community or ourselves. These campaigns should be evaluated before implementing to make sure they are reasonable and ethically sound.","title":"Lesson 18: Evaluating Our Own Participatory Sensing Campaign"},{"location":"unit3/essential/#lesson-19-implementing-our-own-participatory-sensing-campaign","text":"Practicing data collection prior to implementation allows optimization of a Participatory Sensing Campaign.","title":"Lesson 19: Implementing Our Own Participatory Sensing Campaign"},{"location":"unit3/essential/#lesson-20-online-data-ing","text":"Stretching the conception of data involves seeing that many web pages present information that can be turned into data.","title":"Lesson 20: Online Data-ing"},{"location":"unit3/essential/#lesson-21-learning-to-love-xml","text":"XML is a programming language that we use with our campaigns. We create basic XML \"tags\" in the code, which help us store data in a format we understand.","title":"Lesson 21: Learning to Love XML"},{"location":"unit3/essential/#lesson-22-changing-orientation","text":"Converting XML to spreadsheet format helps us better understand and view our data.","title":"Lesson 22: Changing Orientation"},{"location":"unit3/lab3a/","text":"Lab 3A - The results are in! Directions: Follow along with the slides, completing the questions in blue on your computer, and answering the questions in red in your journal. Conducting experiments Previously in class, you conducted an experiment to gauge how a stimulus affected people's perception of time. \u2013 Some people were given a treatment, others were not. In this lab, we'll use the data cycle to analyze the research question : Does the stimulus your class chose change people's perception of time? Coming up with questions Write down two statistical investigative questions that will help you answer the research question from the previous slide. Then, export , upload , import your experiment data into RStudio. \u2013 If you're having trouble coming up with good statistical investigative questions, try loading the data and looking at the variables. \u2013 Ask yourself, How would I use these variables to answer the research question? Analyzing our data Create appropriate plots to answer your statistical investigative questions. Calculate appropriate numerical summaries to answer your statistical investigative questions. Write down a few sentences interpreting your plots and summaries. Wrapping it up Is it possible your initial results occurred by chance alone? \u2013 Use repeated shuffling to determine how likely the typical difference between the two groups occurred by chance alone. \u2013 Create a plot and use it to justify your answer. What do you conclude about the research question ? \u2013 Write a report using the plots and analysis you conducted to answer the research question . \u2013 Be sure to describe how you conducted your experiment.","title":"Lab 3A: The Results Are In!"},{"location":"unit3/lab3a/#lab-3a-the-results-are-in","text":"Directions: Follow along with the slides, completing the questions in blue on your computer, and answering the questions in red in your journal.","title":"Lab 3A - The results are in!"},{"location":"unit3/lab3a/#conducting-experiments","text":"Previously in class, you conducted an experiment to gauge how a stimulus affected people's perception of time. \u2013 Some people were given a treatment, others were not. In this lab, we'll use the data cycle to analyze the research question : Does the stimulus your class chose change people's perception of time?","title":"Conducting experiments"},{"location":"unit3/lab3a/#coming-up-with-questions","text":"Write down two statistical investigative questions that will help you answer the research question from the previous slide. Then, export , upload , import your experiment data into RStudio. \u2013 If you're having trouble coming up with good statistical investigative questions, try loading the data and looking at the variables. \u2013 Ask yourself, How would I use these variables to answer the research question?","title":"Coming up with questions"},{"location":"unit3/lab3a/#analyzing-our-data","text":"Create appropriate plots to answer your statistical investigative questions. Calculate appropriate numerical summaries to answer your statistical investigative questions. Write down a few sentences interpreting your plots and summaries.","title":"Analyzing our data"},{"location":"unit3/lab3a/#wrapping-it-up","text":"Is it possible your initial results occurred by chance alone? \u2013 Use repeated shuffling to determine how likely the typical difference between the two groups occurred by chance alone. \u2013 Create a plot and use it to justify your answer. What do you conclude about the research question ? \u2013 Write a report using the plots and analysis you conducted to answer the research question . \u2013 Be sure to describe how you conducted your experiment.","title":"Wrapping it up"},{"location":"unit3/lab3b/","text":"Lab 3B - Confound it all! Directions: Follow along with the slides, completing the questions in blue on your computer, and answering the questions in red in your journal. Finding data in new places Since your first forays into doing data science, you've used data from two sources: \u2013 Built-in datasets from RStudio. \u2013 Campaign data from the Campaign Manager. Data can be found in many other places though, especially online. In this lab, we'll read an observational study dataset from a website. \u2013 We'll use this data to then explore what factors are associated with a person's lung capacity. Importing our data Rather than export -ing the data and then upload -ing and importing -ing it, we'll pull the data straight from the webpage into R. You can find the data online here: \u2013 (Right-click and select Open in New Window ) https://raw.githubusercontent.com/IDSUCLA/dataset/main/fev.csv Click on the Import Dataset button under the Environment tab. \u2013 Then click on the From Text (readr) option. \u2013 Type or copy/paste the URL into the box. \u2013 Click Update . Before importing, change the following Import Options : \u2013 Name: lungs \u2013 Uncheck the First Row as Names \u2013 Change Delimiter to Whitespace Our new data Variables that were measured include: \u2013 Age in years. \u2013 Lung capacity, measured in liters. \u2013 The youth's heights, in inches \u2013 Genders; \"1\" for males, \"0\" for females. \u2013 Whether the participant was a smoker, \"1\" , or non-smoker \"0\" . About the data The data come from the Forced Expiratory Volume (FEV) study that took place in the late 1970's. The observations come from a sample of 654 youths, aged 3 to 19, in/around East Boston. Researchers were interested in answering the research question : What is the effect of childhood smoking on lung health? Cleaning your data Now that we've got the data loaded, we need to clean it to get it ready for use ( Look at lab 1F for help ). Specifically: \u2013 We want to name the variables: \"age\" , \"lung_cap\" , \"height\" , \"gender\" , \"smoker\" , in that order. \u2013 Change the type of variable for gender and smoker from numeric to character . After changing the variable types for gender and smoker : \u2013 For gender , use recode to change \"1\" to \"Male\" and \"0\" to \"Female\" . \u2013 For smoker , use recode to change \"1\" to \"Yes\" and \"0\" to \"No\" . Analyzing our data Our lungs data is from an observational study. Write down a reason the researchers couldn't use an experiment to test the effects of smoking on children's lungs. Observational studies are often helpful for analyzing how variables are related: \u2013 Do you think that a person's age affects their lung capacity? Make a sketch of what you think a scatterplot of the two variables would look like and explain. Use the lungs data to create an xyplot of age and lung_cap . \u2013 Interpret the plot and describe why the relationship between the two variables makes sense. Smoking and lung capacity Make a plot that can be used to answer the statistical investigative question: Do people who smoke tend to have lower lung capacity than those who do not smoke? Use your plot to answer the question. \u2013 Were you surprised by the answer? Why? \u2013 Can you suggest a possible confounding factor that might be affecting the result? Let's compare Create three subsets of the data: \u2013 One that includes only 13-year-olds ... \u2013 One that includes only 15-year-olds ... \u2013 and one that includes only 17-year-olds. Make a plot that compares the lung capacity of smokers and non-smokers for each subset. How does the relationship between smoking and lung capacity change as we increase the age from 13 to 15 to 17? Sum it up! Does smoking affect lung capacity? If so, how? \u2013 Support your answers with appropriate plots. \u2013 Explain why you included the variables you used in your plots.","title":"Lab 3B: Confound It All!"},{"location":"unit3/lab3b/#lab-3b-confound-it-all","text":"Directions: Follow along with the slides, completing the questions in blue on your computer, and answering the questions in red in your journal.","title":"Lab 3B - Confound it all!"},{"location":"unit3/lab3b/#finding-data-in-new-places","text":"Since your first forays into doing data science, you've used data from two sources: \u2013 Built-in datasets from RStudio. \u2013 Campaign data from the Campaign Manager. Data can be found in many other places though, especially online. In this lab, we'll read an observational study dataset from a website. \u2013 We'll use this data to then explore what factors are associated with a person's lung capacity.","title":"Finding data in new places"},{"location":"unit3/lab3b/#importing-our-data","text":"Rather than export -ing the data and then upload -ing and importing -ing it, we'll pull the data straight from the webpage into R. You can find the data online here: \u2013 (Right-click and select Open in New Window ) https://raw.githubusercontent.com/IDSUCLA/dataset/main/fev.csv Click on the Import Dataset button under the Environment tab. \u2013 Then click on the From Text (readr) option. \u2013 Type or copy/paste the URL into the box. \u2013 Click Update . Before importing, change the following Import Options : \u2013 Name: lungs \u2013 Uncheck the First Row as Names \u2013 Change Delimiter to Whitespace","title":"Importing our data"},{"location":"unit3/lab3b/#our-new-data","text":"Variables that were measured include: \u2013 Age in years. \u2013 Lung capacity, measured in liters. \u2013 The youth's heights, in inches \u2013 Genders; \"1\" for males, \"0\" for females. \u2013 Whether the participant was a smoker, \"1\" , or non-smoker \"0\" .","title":"Our new data"},{"location":"unit3/lab3b/#about-the-data","text":"The data come from the Forced Expiratory Volume (FEV) study that took place in the late 1970's. The observations come from a sample of 654 youths, aged 3 to 19, in/around East Boston. Researchers were interested in answering the research question : What is the effect of childhood smoking on lung health?","title":"About the data"},{"location":"unit3/lab3b/#cleaning-your-data","text":"Now that we've got the data loaded, we need to clean it to get it ready for use ( Look at lab 1F for help ). Specifically: \u2013 We want to name the variables: \"age\" , \"lung_cap\" , \"height\" , \"gender\" , \"smoker\" , in that order. \u2013 Change the type of variable for gender and smoker from numeric to character . After changing the variable types for gender and smoker : \u2013 For gender , use recode to change \"1\" to \"Male\" and \"0\" to \"Female\" . \u2013 For smoker , use recode to change \"1\" to \"Yes\" and \"0\" to \"No\" .","title":"Cleaning your data"},{"location":"unit3/lab3b/#analyzing-our-data","text":"Our lungs data is from an observational study. Write down a reason the researchers couldn't use an experiment to test the effects of smoking on children's lungs. Observational studies are often helpful for analyzing how variables are related: \u2013 Do you think that a person's age affects their lung capacity? Make a sketch of what you think a scatterplot of the two variables would look like and explain. Use the lungs data to create an xyplot of age and lung_cap . \u2013 Interpret the plot and describe why the relationship between the two variables makes sense.","title":"Analyzing our data"},{"location":"unit3/lab3b/#smoking-and-lung-capacity","text":"Make a plot that can be used to answer the statistical investigative question: Do people who smoke tend to have lower lung capacity than those who do not smoke? Use your plot to answer the question. \u2013 Were you surprised by the answer? Why? \u2013 Can you suggest a possible confounding factor that might be affecting the result?","title":"Smoking and lung capacity"},{"location":"unit3/lab3b/#lets-compare","text":"Create three subsets of the data: \u2013 One that includes only 13-year-olds ... \u2013 One that includes only 15-year-olds ... \u2013 and one that includes only 17-year-olds. Make a plot that compares the lung capacity of smokers and non-smokers for each subset. How does the relationship between smoking and lung capacity change as we increase the age from 13 to 15 to 17?","title":"Let's compare"},{"location":"unit3/lab3b/#sum-it-up","text":"Does smoking affect lung capacity? If so, how? \u2013 Support your answers with appropriate plots. \u2013 Explain why you included the variables you used in your plots.","title":"Sum it up!"},{"location":"unit3/lab3c/","text":"Lab 3C - Random Sampling Directions: Follow along with the slides, completing the questions in blue on your computer, and answering the questions in red in your journal. Learning by sampling In many circumstances, there's simply no feasible way to gather data about everyone in a population . \u2013 For example, the Department of Water & Power (DWP) wants to determine how much water people in Los Angeles use to take a shower. They've created a survey to pass out to collect this information. \u2013 Write down two reasons why getting everyone in Los Angeles to fill out the survey would be difficult. Also, write a sentence why the DWP might consider using a sample of households instead. In this lab, we'll learn how sampling methods affect how representative a sample is of a population . Loading a population In previous labs, we used the cdc data as a sample for young people in the United States. \u2013 In this lab, we'll consider these survey respondents to be our population. Load the cdc data into R and fill in the blanks to take a convenience sample of the first 50 people in the data: s1 <- slice(____, 1:____) Why do you think we call this method a convenience sample? Comparing your convenience sample A convenience sample is a sample from a population where we collect data on subjects because they're easy-to-find. Using your convenience sample, create a bargraph for the number of people in each grade . \u2013 Do you think the distribution of grade for your sample would look similar when compared to the whole cdc data? \u2013 Which groups of people do you think are over or under represented in your convenience sample? Why? Create a bargraph for grade using the cdc data. \u2013 Compare the distributions of the cdc data and your convenience sample and write down how they differ. Using randomness Fill in the blanks below to create a sample by randomly selecting 50 people in the cdc data, without replacement. Call this new sample s2 : ___ <- sample(___, size = ___, replace = ___) Write a sentence that explains why you think the distribution of grade for this random sample will look more or less similar to the distribution from the whole cdc data. Create a bargraph for grade based on this random sample to check your prediction. Increasing sample size Create bargraph s for grade based on each of the following sample sizes: 10, 100, 1,000, 10,000. \u2013 Compare each distribution to that of the population. How do the distributions change as the size of the sample increases? Why do you think this occurs? tally() the proportion of grade s for your convenience sample and all your random samples. \u2013 Which set of proportions looks most similar to the proportions of the population? Lessons learned The mean, or proportion, from a random sample might not always be closer to that of the true population when compared to a convenience sample. However, as sample sizes get larger: \u2013 Random samples will tend to be better estimates for the population. \u2013 With convenience samples, this might not be the case. Write down a reason why estimates based on convenience samples might not improve even as sample size increases.","title":"Lab 3C: Random Sampling"},{"location":"unit3/lab3c/#lab-3c-random-sampling","text":"Directions: Follow along with the slides, completing the questions in blue on your computer, and answering the questions in red in your journal.","title":"Lab 3C - Random Sampling"},{"location":"unit3/lab3c/#learning-by-sampling","text":"In many circumstances, there's simply no feasible way to gather data about everyone in a population . \u2013 For example, the Department of Water & Power (DWP) wants to determine how much water people in Los Angeles use to take a shower. They've created a survey to pass out to collect this information. \u2013 Write down two reasons why getting everyone in Los Angeles to fill out the survey would be difficult. Also, write a sentence why the DWP might consider using a sample of households instead. In this lab, we'll learn how sampling methods affect how representative a sample is of a population .","title":"Learning by sampling"},{"location":"unit3/lab3c/#loading-a-population","text":"In previous labs, we used the cdc data as a sample for young people in the United States. \u2013 In this lab, we'll consider these survey respondents to be our population. Load the cdc data into R and fill in the blanks to take a convenience sample of the first 50 people in the data: s1 <- slice(____, 1:____) Why do you think we call this method a convenience sample?","title":"Loading a population"},{"location":"unit3/lab3c/#comparing-your-convenience-sample","text":"A convenience sample is a sample from a population where we collect data on subjects because they're easy-to-find. Using your convenience sample, create a bargraph for the number of people in each grade . \u2013 Do you think the distribution of grade for your sample would look similar when compared to the whole cdc data? \u2013 Which groups of people do you think are over or under represented in your convenience sample? Why? Create a bargraph for grade using the cdc data. \u2013 Compare the distributions of the cdc data and your convenience sample and write down how they differ.","title":"Comparing your convenience sample"},{"location":"unit3/lab3c/#using-randomness","text":"Fill in the blanks below to create a sample by randomly selecting 50 people in the cdc data, without replacement. Call this new sample s2 : ___ <- sample(___, size = ___, replace = ___) Write a sentence that explains why you think the distribution of grade for this random sample will look more or less similar to the distribution from the whole cdc data. Create a bargraph for grade based on this random sample to check your prediction.","title":"Using randomness"},{"location":"unit3/lab3c/#increasing-sample-size","text":"Create bargraph s for grade based on each of the following sample sizes: 10, 100, 1,000, 10,000. \u2013 Compare each distribution to that of the population. How do the distributions change as the size of the sample increases? Why do you think this occurs? tally() the proportion of grade s for your convenience sample and all your random samples. \u2013 Which set of proportions looks most similar to the proportions of the population?","title":"Increasing sample size"},{"location":"unit3/lab3c/#lessons-learned","text":"The mean, or proportion, from a random sample might not always be closer to that of the true population when compared to a convenience sample. However, as sample sizes get larger: \u2013 Random samples will tend to be better estimates for the population. \u2013 With convenience samples, this might not be the case. Write down a reason why estimates based on convenience samples might not improve even as sample size increases.","title":"Lessons learned"},{"location":"unit3/lab3d/","text":"Lab 3D - Are you sure about that? Directions: Follow along with the slides, completing the questions in blue on your computer, and answering the questions in red in your journal. Confidence and intervals Throughout the year, we've seen that: \u2013 Means are used for describing the typical value in a sample or population, but we usually don't know what they are, because we can't see the entire population. \u2013 Means of samples can be used to estimate means of populations. \u2013 By including a margin of error with our estimate, we create an interval that increases our confidence that we've located the correct value of the population mean. Today, we'll learn how we can calculate margins of error by using a method called the bootstrap . \u2013 Which comes from the phrase, Picking yourself up by your own bootstraps . In this lab Load the built-in atus ( American Time Use Survey ) dataset, which is a survey of how a sample of Americans spent their day. \u2013 The United States has an estimated population of 327,350,075. How many people were surveyed for this particular dataset? The statistical question we wish to investigate is: What is the mean age of people older than 15 living in the United States? Why is it important that the ATUS is a random sample? Use our atus data to calculate an estimate for the average age of people older than 15 living in the U.S. One bootstrap A bootstrapped sample is when we take a random sample() of our original data ( atus ) WITH replacement. \u2013 The size of the sample should be the same size as the original data. We can create a single bootstrapped sample for the mean in three steps: `1. Sample the number of the rows to use in our bootstrap . `2. slice those rows from our original data into our bootstrap data. `3. Calculate the mean of our bootstrapped data. Our first bootstrap Fill in the blanks to sample the row numbers we'll use in our bootstrapped sample. \u2013 Be sure to re-read what a bootstrapped sample is from the previous slide to help you fill in the blanks. \u2013 Use set.seed(123) before taking the sample. bs_rows <- ____(1:____, size = ____, replace = ____) Use the slice function to create a new dataset that includes each row from our sample . bs_atus <- slice(atus, bs_rows) Take a look Look at the values of bs_rows and bs_atus . Write a paragraph that explains to someone that's not familiar with R how you created bs_rows and bs_atus . Be sure to include an explanation of what the values of bs_rows mean and how those values are used to create bs_atus . Also, be sure to explain what each argument of each function does. One strap, two strap Calculate the mean of the age variable in your bootstrapped data, then use a different value of set.seed() to create your own, personal bootstrapped sample. Then calculate its mean . Compare this second bootstrapped sample with three other classmates and write a sentence about how similar or different the bootstrapped sample means were. Many bootstraps To use bootstrapped samples to create confidence intervals , we need to create many bootstrapped samples. \u2013 Normally, the more bootstrapped samples we use, the better the confidence interval . \u2013 In this lab, we'll do() 500 bootstrapped samples. To make do() -ing 500 bootstraps easier, we'll code our 3-step bootstrap method into a function. \u2013 Open a new R script (File -> New File -> R Script) to write your function into. Bootstrap function Fill in the blank space below with the 3 steps needed to create a bootstrapped sample mean for our atus data. \u2013 Each step should be written on its own line between the curly braces. bs_func <- function() { } Highlight and Run the code you write. Visualizing our bootstraps Once your function is created, fill in the blanks to create 500 bootstrapped sample means: bs_means <- do(____) * bs_func() Create a histogram for your bootstrapped samples and describe the center , shape and spread of its distribution. \u2013 These bootstrapped estimates no longer estimate the average age of people in the U.S. \u2013 Instead, they estimate how much the estimate of the average age of people in the U.S. varies. In the next slide, we'll look at how we can use these bootstrapped means to create 90% confidence intervals . Bootstrapped confidence intervals To create a 90% confidence interval, we need to decide between which two ages the middle 90% of our bootstrapped estimates are contained. Using your histogram , fill in the statement below: The lowest 5% of our estimates are below years and the highest 5% of our estimates are above years. Use the quantile() function to check your estimates. Based on your bootstrapped estimates, between which two ages are we 90% confident the actual mean age of people living in the U.S. is contained? On your own Using your bootstrapped sample means, create a 95% confidence interval for the mean age of people living in the U.S. Why is the 95% confidence interval wider than the 90% interval? Write down how you would explain what a 95% confidence interval means to someone not taking Introduction to Data Science .","title":"Lab 3D: Are You Sure about That?"},{"location":"unit3/lab3d/#lab-3d-are-you-sure-about-that","text":"Directions: Follow along with the slides, completing the questions in blue on your computer, and answering the questions in red in your journal.","title":"Lab 3D - Are you sure about that?"},{"location":"unit3/lab3d/#confidence-and-intervals","text":"Throughout the year, we've seen that: \u2013 Means are used for describing the typical value in a sample or population, but we usually don't know what they are, because we can't see the entire population. \u2013 Means of samples can be used to estimate means of populations. \u2013 By including a margin of error with our estimate, we create an interval that increases our confidence that we've located the correct value of the population mean. Today, we'll learn how we can calculate margins of error by using a method called the bootstrap . \u2013 Which comes from the phrase, Picking yourself up by your own bootstraps .","title":"Confidence and intervals"},{"location":"unit3/lab3d/#in-this-lab","text":"Load the built-in atus ( American Time Use Survey ) dataset, which is a survey of how a sample of Americans spent their day. \u2013 The United States has an estimated population of 327,350,075. How many people were surveyed for this particular dataset? The statistical question we wish to investigate is: What is the mean age of people older than 15 living in the United States? Why is it important that the ATUS is a random sample? Use our atus data to calculate an estimate for the average age of people older than 15 living in the U.S.","title":"In this lab"},{"location":"unit3/lab3d/#one-bootstrap","text":"A bootstrapped sample is when we take a random sample() of our original data ( atus ) WITH replacement. \u2013 The size of the sample should be the same size as the original data. We can create a single bootstrapped sample for the mean in three steps: `1. Sample the number of the rows to use in our bootstrap . `2. slice those rows from our original data into our bootstrap data. `3. Calculate the mean of our bootstrapped data.","title":"One bootstrap"},{"location":"unit3/lab3d/#our-first-bootstrap","text":"Fill in the blanks to sample the row numbers we'll use in our bootstrapped sample. \u2013 Be sure to re-read what a bootstrapped sample is from the previous slide to help you fill in the blanks. \u2013 Use set.seed(123) before taking the sample. bs_rows <- ____(1:____, size = ____, replace = ____) Use the slice function to create a new dataset that includes each row from our sample . bs_atus <- slice(atus, bs_rows)","title":"Our first bootstrap"},{"location":"unit3/lab3d/#take-a-look","text":"Look at the values of bs_rows and bs_atus . Write a paragraph that explains to someone that's not familiar with R how you created bs_rows and bs_atus . Be sure to include an explanation of what the values of bs_rows mean and how those values are used to create bs_atus . Also, be sure to explain what each argument of each function does.","title":"Take a look"},{"location":"unit3/lab3d/#one-strap-two-strap","text":"Calculate the mean of the age variable in your bootstrapped data, then use a different value of set.seed() to create your own, personal bootstrapped sample. Then calculate its mean . Compare this second bootstrapped sample with three other classmates and write a sentence about how similar or different the bootstrapped sample means were.","title":"One strap, two strap"},{"location":"unit3/lab3d/#many-bootstraps","text":"To use bootstrapped samples to create confidence intervals , we need to create many bootstrapped samples. \u2013 Normally, the more bootstrapped samples we use, the better the confidence interval . \u2013 In this lab, we'll do() 500 bootstrapped samples. To make do() -ing 500 bootstraps easier, we'll code our 3-step bootstrap method into a function. \u2013 Open a new R script (File -> New File -> R Script) to write your function into.","title":"Many bootstraps"},{"location":"unit3/lab3d/#bootstrap-function","text":"Fill in the blank space below with the 3 steps needed to create a bootstrapped sample mean for our atus data. \u2013 Each step should be written on its own line between the curly braces. bs_func <- function() { } Highlight and Run the code you write.","title":"Bootstrap function"},{"location":"unit3/lab3d/#visualizing-our-bootstraps","text":"Once your function is created, fill in the blanks to create 500 bootstrapped sample means: bs_means <- do(____) * bs_func() Create a histogram for your bootstrapped samples and describe the center , shape and spread of its distribution. \u2013 These bootstrapped estimates no longer estimate the average age of people in the U.S. \u2013 Instead, they estimate how much the estimate of the average age of people in the U.S. varies. In the next slide, we'll look at how we can use these bootstrapped means to create 90% confidence intervals .","title":"Visualizing our bootstraps"},{"location":"unit3/lab3d/#bootstrapped-confidence-intervals","text":"To create a 90% confidence interval, we need to decide between which two ages the middle 90% of our bootstrapped estimates are contained. Using your histogram , fill in the statement below: The lowest 5% of our estimates are below years and the highest 5% of our estimates are above years. Use the quantile() function to check your estimates. Based on your bootstrapped estimates, between which two ages are we 90% confident the actual mean age of people living in the U.S. is contained?","title":"Bootstrapped confidence intervals"},{"location":"unit3/lab3d/#on-your-own","text":"Using your bootstrapped sample means, create a 95% confidence interval for the mean age of people living in the U.S. Why is the 95% confidence interval wider than the 90% interval? Write down how you would explain what a 95% confidence interval means to someone not taking Introduction to Data Science .","title":"On your own"},{"location":"unit3/lab3e/","text":"Lab 3E - Scraping web data Directions: Follow along with the slides, completing the questions in blue on your computer, and answering the questions in red in your journal. The web as a data source The internet contains huge amounts of information. Using computers to gather this information in an automated fashion is referred to as scraping web data . Scraping data from the web can be difficult because each website displays & stores data differently. In this lab, we'll learn how to scrape data in two steps: Step 1: Gather information from the web. Step 2: Clean it up and turn it into a usable data frame for Lab 3F . Our first web scraper Copy and paste the link below into a web browser to view the website of data we'd like to scrape and analyze. https://labs.idsucla.org/extras/webdata/mountains.html Briefly describe what the data on the website is about. \u2013 Then write down 3 questions you'd be interested in answering by analyzing this data. HTML HTML is the code that's used to render every website you've ever visited. The following slide shows the HTML code used to create the first two rows of the web data. \u2013 How is the data table in HTML different than the data tables we're used to seeing in R , for example, when we use the View() function? \u2013 What do you think the tags <TABLE> , <TR> , <TH> , <TD> mean? How does HTML use these tags to display the table? <TABLE> <TR> <TH>peak</TH> <TH>range</TH> <TH>state</TH> <TH>long</TH> <TH>lat</TH> <TH>elev_ft</TH> <TH>elev_m</TH> <TH>prominence_ft</TH> <TH>prominence_m</TH> <TH>rank</TH> </TR> <TR> <TD>Denali (Mount McKinley)</TD> <TD>Alaska Range</TD> <TD>Alaska</TD> <TD>-151.0063</TD> <TD>63.0690</TD> <TD>20236</TD> <TD>6168</TD> <TD>20174</TD> <TD>6149</TD> <TD>1</TD> </TR> </TABLE> Get to scraping! Use your browser to go back to the website with the data we're interested in scraping. Find the URL address for the site and assign it the name data_url in R . Then fill in the blanks below to have R scrape every web table available on the site: tables <- readHTMLTable(____) Find our data Since readHTMLTable() scrapes every table that is on a particular web URL, we need to find out which table has the data we're interested in. \u2013 For example, wikipedia.org often has articles with 3 or more tables. \u2013 This means we need to check all 3 tables to find the one we're interested in. Use the length() function to find out how many tables of data were scraped in our set of tables . Saving tables Now that we know how many tables we've scraped, we can go back and scrape individual tables by adding the which argument to the readHTMLTable() function. Use readHTMLTable() to re-scrape the data from the web but this time use the which argument to scrape just the individual table. \u2013 The which argument should be the integer denoting which table you want scraped. \u2013 Assign the scraped data the name mtns . Check, save and use! After scraping the data, the only thing left to do is to save it and use it. Fill in the blanks to save the data and give it a file name. save(____, file = \"____.Rda\") What is the mean and standard deviation of elev_ft ? Which state has the most mountains in our data?","title":"Lab 3E: Scraping Web Data"},{"location":"unit3/lab3e/#lab-3e-scraping-web-data","text":"Directions: Follow along with the slides, completing the questions in blue on your computer, and answering the questions in red in your journal.","title":"Lab 3E - Scraping web data"},{"location":"unit3/lab3e/#the-web-as-a-data-source","text":"The internet contains huge amounts of information. Using computers to gather this information in an automated fashion is referred to as scraping web data . Scraping data from the web can be difficult because each website displays & stores data differently. In this lab, we'll learn how to scrape data in two steps: Step 1: Gather information from the web. Step 2: Clean it up and turn it into a usable data frame for Lab 3F .","title":"The web as a data source"},{"location":"unit3/lab3e/#our-first-web-scraper","text":"Copy and paste the link below into a web browser to view the website of data we'd like to scrape and analyze. https://labs.idsucla.org/extras/webdata/mountains.html Briefly describe what the data on the website is about. \u2013 Then write down 3 questions you'd be interested in answering by analyzing this data.","title":"Our first web scraper"},{"location":"unit3/lab3e/#html","text":"HTML is the code that's used to render every website you've ever visited. The following slide shows the HTML code used to create the first two rows of the web data. \u2013 How is the data table in HTML different than the data tables we're used to seeing in R , for example, when we use the View() function? \u2013 What do you think the tags <TABLE> , <TR> , <TH> , <TD> mean? How does HTML use these tags to display the table? <TABLE> <TR> <TH>peak</TH> <TH>range</TH> <TH>state</TH> <TH>long</TH> <TH>lat</TH> <TH>elev_ft</TH> <TH>elev_m</TH> <TH>prominence_ft</TH> <TH>prominence_m</TH> <TH>rank</TH> </TR> <TR> <TD>Denali (Mount McKinley)</TD> <TD>Alaska Range</TD> <TD>Alaska</TD> <TD>-151.0063</TD> <TD>63.0690</TD> <TD>20236</TD> <TD>6168</TD> <TD>20174</TD> <TD>6149</TD> <TD>1</TD> </TR> </TABLE>","title":"HTML"},{"location":"unit3/lab3e/#get-to-scraping","text":"Use your browser to go back to the website with the data we're interested in scraping. Find the URL address for the site and assign it the name data_url in R . Then fill in the blanks below to have R scrape every web table available on the site: tables <- readHTMLTable(____)","title":"Get to scraping!"},{"location":"unit3/lab3e/#find-our-data","text":"Since readHTMLTable() scrapes every table that is on a particular web URL, we need to find out which table has the data we're interested in. \u2013 For example, wikipedia.org often has articles with 3 or more tables. \u2013 This means we need to check all 3 tables to find the one we're interested in. Use the length() function to find out how many tables of data were scraped in our set of tables .","title":"Find our data"},{"location":"unit3/lab3e/#saving-tables","text":"Now that we know how many tables we've scraped, we can go back and scrape individual tables by adding the which argument to the readHTMLTable() function. Use readHTMLTable() to re-scrape the data from the web but this time use the which argument to scrape just the individual table. \u2013 The which argument should be the integer denoting which table you want scraped. \u2013 Assign the scraped data the name mtns .","title":"Saving tables"},{"location":"unit3/lab3e/#check-save-and-use","text":"After scraping the data, the only thing left to do is to save it and use it. Fill in the blanks to save the data and give it a file name. save(____, file = \"____.Rda\") What is the mean and standard deviation of elev_ft ? Which state has the most mountains in our data?","title":"Check, save and use!"},{"location":"unit3/lab3f/","text":"Lab 3F - Maps Directions: Follow along with the slides, completing the questions in blue on your computer, and answering the questions in red in your journal. Informative and Fun! Maps are some of the most interesting plots to make because the info represents: Where we live. Where we go. Places that interest us. Maps are also helpful to display geographic information. John Snow (the physician, not the character from Game of Thrones... ) once famously used a map to discover how cholera was transmitted. In this lab, we'll use R to create an interactive map of the mtns data we scraped in Lab 3E . Getting ready to map The map we'll be creating will end up in RStudio's Viewer pane. \u2013 Which means you'll need to alternate between building the map and loading the lab. You'll find it very helpful, for this lab, to write all of the commands, including the load_lab(23) command, as an R Script. \u2013 This way you can edit the code that builds the map and quickly reload the lab. Load your data! In Lab 3E you created a dataset. Load it into Rstudio now by filling in the blank with the file name of the data. load(\"___.Rda\") Didn't finish the lab or save the data file? Ask a friend to share it! Build a Basic Map Let's start by building a basic map! Use the leaflet() function and the mtns data to create the leaf that we can use for mapping. mtns_leaf <- leaflet(____) Then, insert mtns_leaf into the addTiles() function and assign the output the name mtns_map . Run mtns_map in the console to look at your basic map with no data displayed. \u2013 Be sure to try clicking on the map to pan and zoom. Including our data Now we can add markers for the locations of the mountains using the addMarkers() function. Fill in the blanks below with the basic map we've created and the values for latitude and longitude. addMarkers(map = ____, lng = ~____, lat = ~____) Supply the peak variable, in a similar way as we supplied the lat and long variables, to the popup argument and include it in the code above. Click on a marker within California and write down the name of the mountain you clicked on. Colorize Our current map looks pretty good, but what if we wanted to add some colors to our plot? Fill in the blanks below to create a new variable that assigns a color to each mountain based on the state it's located in. mtns <- mutate(____, state_colors = colorize(____)) Now that we've added a new variable, we need to re-build mtns_leaf and mtns_map to use it. \u2013 Create mtns_leaf and mtns_map as you did before. \u2013 Then change addMarkers to addCircleMarkers and keep all of the arguments the same. Showing off our colors To add the colors to our plot, use the addCircleMarkers like before but this time include color = ~state_colors as an argument. It's hard to know just what the different colors mean so let's add a legend. First, assign the map with the circle markers as mtns_map . Then, fill in the blanks below to place a legend in the top-right hand corner. addLegend( _, colors = ~unique( _), labels = ~unique(____)).","title":"Lab 3F: Maps"},{"location":"unit3/lab3f/#lab-3f-maps","text":"Directions: Follow along with the slides, completing the questions in blue on your computer, and answering the questions in red in your journal.","title":"Lab 3F - Maps"},{"location":"unit3/lab3f/#informative-and-fun","text":"Maps are some of the most interesting plots to make because the info represents: Where we live. Where we go. Places that interest us. Maps are also helpful to display geographic information. John Snow (the physician, not the character from Game of Thrones... ) once famously used a map to discover how cholera was transmitted. In this lab, we'll use R to create an interactive map of the mtns data we scraped in Lab 3E .","title":"Informative and Fun!"},{"location":"unit3/lab3f/#getting-ready-to-map","text":"The map we'll be creating will end up in RStudio's Viewer pane. \u2013 Which means you'll need to alternate between building the map and loading the lab. You'll find it very helpful, for this lab, to write all of the commands, including the load_lab(23) command, as an R Script. \u2013 This way you can edit the code that builds the map and quickly reload the lab.","title":"Getting ready to map"},{"location":"unit3/lab3f/#load-your-data","text":"In Lab 3E you created a dataset. Load it into Rstudio now by filling in the blank with the file name of the data. load(\"___.Rda\") Didn't finish the lab or save the data file? Ask a friend to share it!","title":"Load your data!"},{"location":"unit3/lab3f/#build-a-basic-map","text":"Let's start by building a basic map! Use the leaflet() function and the mtns data to create the leaf that we can use for mapping. mtns_leaf <- leaflet(____) Then, insert mtns_leaf into the addTiles() function and assign the output the name mtns_map . Run mtns_map in the console to look at your basic map with no data displayed. \u2013 Be sure to try clicking on the map to pan and zoom.","title":"Build a Basic Map"},{"location":"unit3/lab3f/#including-our-data","text":"Now we can add markers for the locations of the mountains using the addMarkers() function. Fill in the blanks below with the basic map we've created and the values for latitude and longitude. addMarkers(map = ____, lng = ~____, lat = ~____) Supply the peak variable, in a similar way as we supplied the lat and long variables, to the popup argument and include it in the code above. Click on a marker within California and write down the name of the mountain you clicked on.","title":"Including our data"},{"location":"unit3/lab3f/#colorize","text":"Our current map looks pretty good, but what if we wanted to add some colors to our plot? Fill in the blanks below to create a new variable that assigns a color to each mountain based on the state it's located in. mtns <- mutate(____, state_colors = colorize(____)) Now that we've added a new variable, we need to re-build mtns_leaf and mtns_map to use it. \u2013 Create mtns_leaf and mtns_map as you did before. \u2013 Then change addMarkers to addCircleMarkers and keep all of the arguments the same.","title":"Colorize"},{"location":"unit3/lab3f/#showing-off-our-colors","text":"To add the colors to our plot, use the addCircleMarkers like before but this time include color = ~state_colors as an argument. It's hard to know just what the different colors mean so let's add a legend. First, assign the map with the circle markers as mtns_map . Then, fill in the blanks below to place a legend in the top-right hand corner. addLegend( _, colors = ~unique( _), labels = ~unique(____)).","title":"Showing off our colors"},{"location":"unit3/lesson1/","text":"Lesson 1: Anecdotes vs. Data Objective: Students will learn the difference between anecdotes and data. They will begin to read articles critically to discern whether the evidence presented is based on anecdotes or data. Materials: Hans Rosling\u2019s video How Not to Be Ignorant About the World found at https://www.ted.com/talks/hans_and_ola_rosling_how_not_to_be_ignorant_about_the_world Article: Miracle at the KK Caf\u00e9 (also available in the LMR folder) https://web.archive.org/web/20220809045912/https://archives.sfweekly.com/sanfrancisco/miracle-at-the-kk-cafe/Content?oid=2144741 Article: Can Trophy Hunting Actually Help Conservation? (also available in the LMR folder) https://lastwordwildlife.com/2014/01/21/can-trophy-hunting-actually-help-conservation/ Vocabulary: anecdote data Essential Concepts: Essential Concepts: Data beat anecdotes. In science, we need to closely examine the quality of evidence in order to make sound conclusions. Anecdotes can contain personal bias, might be carefully selected to represent a particular point of view, and, in general, may be completely different from the general trend. Lesson: Prepare your video player to show the first 5 minutes and 23 seconds of Hans Rosling\u2019s video How Not to Be Ignorant About the World found at: https://www.ted.com/talks/hans_and_ola_rosling_how_not_to_be_ignorant_about_the_world Ask students to play along as they watch the video. Each time Hans Rosling asks the audience to choose an answer to each of the three questions, pause the video for about 5 seconds and ask students to write down what they think is the answer to each question. After viewing the video, engage students in T-I-P-S (see strategies) with the questions below: Why did the chimps at the zoo score better than the people? Answer: Anyone can select the correct answer just by chance. On the second question, Hans Rosling says that \u201ceveryone is aware that there are countries and there are areas where girls have great difficulties and they are stopped when they go to school.\u201d How could this information influence the answer choice? Answer: Personal knowledge and experiences can influence what we think we know. Why do you think only a few people know the correct answer to these three questions? Answer: People do not know enough about the data that can help them answer these questions. Display the following statements to students: \u201cMy skin glows more\u2026I feel pretty confident.\u201d Melissa for Proactiv\u00ae \u201cWithin four months, I'd lost a grand total of 63 pounds* and was down to my goal weight.\u201d Marianne G. for Nutrisystem\u00ae \u201cThe customer service is obnoxious. The employees are patronizing, smug, and intractable.\u201d Seymour773 for Bank of America\u00ae Discuss each statement with students by asking the following questions: Is a good product? For example, is Proactiv\u00ae a good skin product?, is Nutrisystem\u00ae a good diet program?, is Bank of America\u00ae a good bank? Do you think this person\u2019s experience is \u201ctypical?\u201d Why? Maybe it is typical but maybe not. Their own experience might be very different. Do you think the company chose this person? How do you know? Each company may have chosen the first 2 statements because they were a success. In the case of the Seymour773, a competing company may have chosen his experience to make them appear better. What about all the other people? How many were successes, how many failures? We don\u2019t know for sure. How could we answer such questions? Collect data! Inform students that the statements are called testimonials and they are examples of anecdotes . Anecdotes are stories that someone tells about his/her own experience or the experience of someone he/she knows. Anecdotes are good for some things like witness statements in a police report but are not useful for reaching conclusions about groups of people because the assumptions they are based on are not always true. Their claims are easily debunked. Many anecdotes do not equal data. Note to teacher about witness statements: Lots of evidence suggests that witness testimony needs to be examined very closely. \"As perhaps the single most effective method of proving the elements of a crime, eyewitness testimony has been vital to the trial process for centuries. However, the reliability of eyewitness testimony has recently come into question with the work of organizations such as The Innocence Project, which works to exonerate the wrongfully convicted. This thesis examines previous experiments concerning eyewitness testimony as well as court cases in which eyewitnesses provided vital evidence in order to determine the reliability of eyewitness testimony as well as to determine mitigating or exacerbating factors contributing to a lack of reliability.\" Information gathered from digitalcommons.liberty.edu On the other hand, data are a series of observations, measurements, or facts. Data are information and tell a story. Quickly survey students about whether the video they watched at the beginning of class is based on anecdotes or data. Then inform students that they will analyze two articles to find out if their claims are based on anecdotes or data. Students will read one of two articles, Miracle at the KK Cafe or Can Trophy Hunting Actually Help Conservation ? to analyze whether the claims each makes are based on anecdotes or data. The articles can be found at the following links or in the LMR folder: Miracle at KK Caf\u00e9 https://web.archive.org/web/20220809045912/https://archives.sfweekly.com/sanfrancisco/miracle-at-the-kk-cafe/Content?oid=2144741 Can Trophy Hunting Actually Help Conservation? https://lastwordwildlife.com/2014/01/21/can-trophy-hunting-actually-help-conservation/ Ask students to number themselves off as 1 or 2. Students whose number is 1 will read Miracle at KK Caf\u00e9 and those whose number is 2 will read Can Trophy Hunting Actually Help Conservation ? Ask students to find a partner with the same number. Before reading, ask students what they think their article will be about. Have students pair-share their thoughts. During reading, students will take turns reading each paragraph out loud to each other. The student listening will verbally summarize what his/her partner just read. After reading, student pairs will answer the following questions in their DS Journals: What was the article about? What claim(s) was/were this article making? Cite examples from the article. Was this article based on anecdotes or data? Cite examples from the article. How believable are the claims? After answering the questions, students will find a partner with a different number. Each student will report to their new partner the following information about the article he/she read: The name and publisher of the article. His/her response to the four questions in their DS journal. Quickly survey students about which article was based more on anecdotes and which one was based more on data. Ask a couple of students to explain their choices and give examples. Miracle at KK Caf\u00e9 makes claims that are anecdotal. Students may cite a customer\u2019s claim as an example of an anecdote. Can Trophy Hunting Actually Help Conservation? uses data to make their claims. Students may refer to a statistic used in the article as an example. Class discussion: Data Beat Anecdotes! Ask students to come up with reasons why this statement is true. Have them come up with situations where you have to have an anecdote. For example, if asked what it\u2019s like to walk on the moon, only a few people would be able to tell us. Class Scribes: One team of students will give a brief talk to discuss what they think the 3 most important topics of the day were. Homework Students will do a Last Word Review for the words DATA and ANECDOTE. Last Word Review: Write the word vertically. Students come up with a word or phrase for each letter of the word. Each letter of the word should summarize something about what the students learned about the topic.","title":"Lesson 1: Anecdotes vs. Data"},{"location":"unit3/lesson1/#lesson-1-anecdotes-vs-data","text":"","title":"Lesson 1: Anecdotes vs. Data"},{"location":"unit3/lesson1/#objective","text":"Students will learn the difference between anecdotes and data. They will begin to read articles critically to discern whether the evidence presented is based on anecdotes or data.","title":"Objective:"},{"location":"unit3/lesson1/#materials","text":"Hans Rosling\u2019s video How Not to Be Ignorant About the World found at https://www.ted.com/talks/hans_and_ola_rosling_how_not_to_be_ignorant_about_the_world Article: Miracle at the KK Caf\u00e9 (also available in the LMR folder) https://web.archive.org/web/20220809045912/https://archives.sfweekly.com/sanfrancisco/miracle-at-the-kk-cafe/Content?oid=2144741 Article: Can Trophy Hunting Actually Help Conservation? (also available in the LMR folder) https://lastwordwildlife.com/2014/01/21/can-trophy-hunting-actually-help-conservation/","title":"Materials:"},{"location":"unit3/lesson1/#vocabulary","text":"anecdote data","title":"Vocabulary:"},{"location":"unit3/lesson1/#essential-concepts","text":"Essential Concepts: Data beat anecdotes. In science, we need to closely examine the quality of evidence in order to make sound conclusions. Anecdotes can contain personal bias, might be carefully selected to represent a particular point of view, and, in general, may be completely different from the general trend.","title":"Essential Concepts:"},{"location":"unit3/lesson1/#lesson","text":"Prepare your video player to show the first 5 minutes and 23 seconds of Hans Rosling\u2019s video How Not to Be Ignorant About the World found at: https://www.ted.com/talks/hans_and_ola_rosling_how_not_to_be_ignorant_about_the_world Ask students to play along as they watch the video. Each time Hans Rosling asks the audience to choose an answer to each of the three questions, pause the video for about 5 seconds and ask students to write down what they think is the answer to each question. After viewing the video, engage students in T-I-P-S (see strategies) with the questions below: Why did the chimps at the zoo score better than the people? Answer: Anyone can select the correct answer just by chance. On the second question, Hans Rosling says that \u201ceveryone is aware that there are countries and there are areas where girls have great difficulties and they are stopped when they go to school.\u201d How could this information influence the answer choice? Answer: Personal knowledge and experiences can influence what we think we know. Why do you think only a few people know the correct answer to these three questions? Answer: People do not know enough about the data that can help them answer these questions. Display the following statements to students: \u201cMy skin glows more\u2026I feel pretty confident.\u201d Melissa for Proactiv\u00ae \u201cWithin four months, I'd lost a grand total of 63 pounds* and was down to my goal weight.\u201d Marianne G. for Nutrisystem\u00ae \u201cThe customer service is obnoxious. The employees are patronizing, smug, and intractable.\u201d Seymour773 for Bank of America\u00ae Discuss each statement with students by asking the following questions: Is a good product? For example, is Proactiv\u00ae a good skin product?, is Nutrisystem\u00ae a good diet program?, is Bank of America\u00ae a good bank? Do you think this person\u2019s experience is \u201ctypical?\u201d Why? Maybe it is typical but maybe not. Their own experience might be very different. Do you think the company chose this person? How do you know? Each company may have chosen the first 2 statements because they were a success. In the case of the Seymour773, a competing company may have chosen his experience to make them appear better. What about all the other people? How many were successes, how many failures? We don\u2019t know for sure. How could we answer such questions? Collect data! Inform students that the statements are called testimonials and they are examples of anecdotes . Anecdotes are stories that someone tells about his/her own experience or the experience of someone he/she knows. Anecdotes are good for some things like witness statements in a police report but are not useful for reaching conclusions about groups of people because the assumptions they are based on are not always true. Their claims are easily debunked. Many anecdotes do not equal data. Note to teacher about witness statements: Lots of evidence suggests that witness testimony needs to be examined very closely. \"As perhaps the single most effective method of proving the elements of a crime, eyewitness testimony has been vital to the trial process for centuries. However, the reliability of eyewitness testimony has recently come into question with the work of organizations such as The Innocence Project, which works to exonerate the wrongfully convicted. This thesis examines previous experiments concerning eyewitness testimony as well as court cases in which eyewitnesses provided vital evidence in order to determine the reliability of eyewitness testimony as well as to determine mitigating or exacerbating factors contributing to a lack of reliability.\" Information gathered from digitalcommons.liberty.edu On the other hand, data are a series of observations, measurements, or facts. Data are information and tell a story. Quickly survey students about whether the video they watched at the beginning of class is based on anecdotes or data. Then inform students that they will analyze two articles to find out if their claims are based on anecdotes or data. Students will read one of two articles, Miracle at the KK Cafe or Can Trophy Hunting Actually Help Conservation ? to analyze whether the claims each makes are based on anecdotes or data. The articles can be found at the following links or in the LMR folder: Miracle at KK Caf\u00e9 https://web.archive.org/web/20220809045912/https://archives.sfweekly.com/sanfrancisco/miracle-at-the-kk-cafe/Content?oid=2144741 Can Trophy Hunting Actually Help Conservation? https://lastwordwildlife.com/2014/01/21/can-trophy-hunting-actually-help-conservation/ Ask students to number themselves off as 1 or 2. Students whose number is 1 will read Miracle at KK Caf\u00e9 and those whose number is 2 will read Can Trophy Hunting Actually Help Conservation ? Ask students to find a partner with the same number. Before reading, ask students what they think their article will be about. Have students pair-share their thoughts. During reading, students will take turns reading each paragraph out loud to each other. The student listening will verbally summarize what his/her partner just read. After reading, student pairs will answer the following questions in their DS Journals: What was the article about? What claim(s) was/were this article making? Cite examples from the article. Was this article based on anecdotes or data? Cite examples from the article. How believable are the claims? After answering the questions, students will find a partner with a different number. Each student will report to their new partner the following information about the article he/she read: The name and publisher of the article. His/her response to the four questions in their DS journal. Quickly survey students about which article was based more on anecdotes and which one was based more on data. Ask a couple of students to explain their choices and give examples. Miracle at KK Caf\u00e9 makes claims that are anecdotal. Students may cite a customer\u2019s claim as an example of an anecdote. Can Trophy Hunting Actually Help Conservation? uses data to make their claims. Students may refer to a statistic used in the article as an example. Class discussion: Data Beat Anecdotes! Ask students to come up with reasons why this statement is true. Have them come up with situations where you have to have an anecdote. For example, if asked what it\u2019s like to walk on the moon, only a few people would be able to tell us.","title":"Lesson:"},{"location":"unit3/lesson1/#class-scribes","text":"One team of students will give a brief talk to discuss what they think the 3 most important topics of the day were.","title":"Class Scribes:"},{"location":"unit3/lesson1/#homework","text":"Students will do a Last Word Review for the words DATA and ANECDOTE. Last Word Review: Write the word vertically. Students come up with a word or phrase for each letter of the word. Each letter of the word should summarize something about what the students learned about the topic.","title":"Homework"},{"location":"unit3/lesson10/","text":"Lesson 10: We\u2019re So Random Objective: Students will learn how to collect random samples from a population in order to estimate a parameter. Materials: Populations & Samples handout ( LMR_3.5_Populations and Samples ) RStudio Projector for RStudio functions Dotplot titled \u201cPercent of Students Who Have Met Friends Online\u201d Parameters & Statistics handout ( LMR_3.6_Parameters and Statistics ) Vocabulary: population sample representative sample random sample parameter statistic Essential Concepts: Essential Concepts: Another popular data collection method involves collecting data from a random sample of people or objects. Percentages based on random samples tend to \"center\" on the population parameter value. Lesson: Introduce today\u2019s lesson by displaying the following statement made by the Pew Research Center in their August 2015 report titled Teens, Technology & Friendships : \u201cFor today\u2019s teens, friendships can start digitally: 57% of teens have met new friends online. The margin of error is plus or minus 3.7 percentage points. Social media and online gameplay are the most common digital venues for meeting friends.\u201d Note: The data for this report were collected via interviews of 1,060 teenagers between the ages of 13 and 17. Discuss the results of the Pew poll with the following prompting questions: The report says that 57% of teens have met new friends online. Since the report was based on a sample of 1,060 teens, how many of the teens reported that they have met friends online? 0.57(1060) = 604.2. This means that approximately 604 teens in the sample have met friends online. Do you think 57% of students in our class have met friends online? Why or why not? Answers will vary by class. The discussion should include points about how similar and different samples can be. The sample of students in the Pew poll may not represent the students in our class. What percent of students in our class have met friends online while a teenager? Answers will vary by class. Calculate the percentage by dividing the number of students who have met friends online by the total number of students who came to class today. What if [absent person] were in class today? Would that change our percentage? What effect would it have on the percentage if [absent person] answered \u201cyes?\u201d What effect would it have if [absent person] answered \u201cno?\u201d Answers will vary by class. If a student who was absent for today\u2019s lesson had actually come to class, we would have a different sample of students. It would change the percentage because our sample size now includes 1 more person. If the person answered \u201cyes,\u201d the values in the numerator and denominator of the percentage would change. If the person answered \u201cno,\u201d the value in the denominator would change. (Students should calculate these values.) If we were able to interview every single teenager in the United States, would exactly 57% of them say they have met friends online? Probably not because there are many more teenagers in the US than the 1,060 they interviewed. It would be unlikely for a group of 1,060 teenagers to exactly represent all teenagers in the entire country. Why do you think the Pew Research Center only interviewed 1,060 teenagers, and not all teenagers in the US? It would be impossible to talk to all teenagers in the US in a short period of time, or even a fairly long period of time. Introduce students to the terms population and sample . Explain that a population consists of all of the people we want to learn something about. A sample consists of people (or objects) that are selected from the population. In pairs, ask students to discuss and record answers to the following two questions: What was the population of interest to the researchers for the Pew poll above? All teenagers in the US right now. Based on your answer in (a), what characteristics should people have in order to be included in a sample for this poll? People would need to be in the US and be between the ages of 13 and 17. People could be from many states, but we would not want to sample only people from California, or only people from Los Angeles. It would be impossible to survey every single person in the US; this is why we create a random representative sample of the population. Note: Steer the discussion so that students see that a sample has to be \u201clike\u201d or \u201csimilar to\u201d or \u201crepresentative of\u201d the population. Select pairs to share their responses to the questions and let students revise their responses. Distribute the Populations & Samples handout ( LMR_3.5 ), which contains survey results from other Pew Research Center reports. Give students time to determine the population and sample for each scenario, and then have them verify their results with a partner. LMR_3.5 State that we want to know the percentage of students in our class that have made friends online, but we don\u2019t want to ask every single student. Instead, we would like to ask only 10 students and then make some guesses about our class from those 10. Ask: What is the population of interest? The students in our classroom. How can we select 10 students to be part of our sample? Answers will vary by class. There may be a variety of suggestions; here are some examples of what may be given: (1) put every student\u2019s name in a hat and pick out 10; (2) select the 10 students sitting closest to the teacher\u2019s desk; (3) have 10 students volunteer to be in the sample. Inform students that, in general, we want samples to \u201clook like\u201d the population. One way to get a representative sample like this is to take a random sample . Ask the students: Would the selection techniques we came up with in Step 6 result in random samples of our class. Answers will vary by class. Using the examples from Step 6: (1) putting each student\u2019s name in a hat and then picking out 10 would be a random sample as long as each piece of paper is the same size; (2) selecting the 10 students sitting closest to the teacher\u2019s desk would not be a random sample because those students might not represent the whole class; (3) if 10 students volunteer, we would not have a random sample because those students selected themselves to be part of the group and may not represent everyone in the class. Next, assign each student in the class a number by having them count off from 1 to N ( N being the total number of students in the class). Show students that we can use RStudio to create random samples with the following function: > sample(1:N, size = 10, replace = FALSE) Note: we use replace = FALSE because we only want each student to be selected once. Using the results given in the output of RStudio, ask the students whose numbers were chosen to stand. Inform them that they are \u201cin\u201d the sample. Then, determine what percent of the sample (these 10 students) have made friends online. How does this percentage compare to the overall class percentage we found in Step 2(c)? Answers will vary by class. Create a dotplot on the board titled \u201cPercent of Students Who Have Met Friends Online.\u201d Record the sample percentage from Step 9 on this dotplot. Have the students return to their seats so that we can select a new sample. Before we do this, ask: What do you predict the percentage to be for the next sample of 10 students? Answers will vary by class. They might say the expected percentage will be close to the class\u2019s overall percentage. Using RStudio, create a new sample, calculate the percentage of those 10 students who have met friends online, and record the value in the dotplot. Repeat Step 11 for a few more rounds (at least 5 samples should be taken). Be sure that the students give a prediction before finding each new sample. Display the following questions. Refer to the dotplot of sample percentages. Ask students to discuss the questions in teams: What do you notice about the sample percentages? What is the \u201ctypical\u201d value? Answers will vary by class. The typical value should be close to the class\u2019s overall percent calculated in Step 2(c) since it is the population percent. What is the smallest value? What is the biggest value? Answers will vary by class. There might be a lot of variability in the dotplot based on the selected samples. Most sample percentages will be within 30% of the population value, so that really gives a wide variety of possible sample values. If we took a larger sample \u2013 maybe of size 15 or 20 \u2013 would there be more or less variability in the dotplot? There will be less variability because adding more people gets us closer to the population size. Be sure to point out that if the sample were exactly the same as the population, then there would be no variability in the plot. Select teams at random to share their responses to the questions above with the whole class. The rest of the teams should be in full agreement with the responses before moving on to the next question. Explain that the population percentage (the percentage of all students in the class who met friends online) we have been using is called a parameter . A parameter is any number that summarizes a population. So, our class has been the population, and the percentage of students that have met friends online is the parameter. Similarly, the term statistics is used for numbers that summarize a sample. Ask students what sample statistics they have seen today? Each value we included in the dotplot is a statistic. Be sure to point out that there can be multiple values for a sample statistic (i.e. \u201cWe had 5 sample percentages in our dotplot.\u201d), but there is always only one parameter value. Using these new definitions, ask students to consider the original Pew poll data, which found that 57% of teens have met friends online. Ask the students: Is 57% a parameter or statistic? This is a statistic because it is based on a sample. Remind them that the population was ALL US teenagers and the sample included 1,060 teens. What is the population parameter then? We don\u2019t know! We would have to interview every teenager in the US to determine the parameter, and that is not possible. Conclude the lesson by telling the students: although we cannot determine the actual population parameter for the percent of teens that have made friends online, we can estimate it using random samples. Class Scribes: One team of students will give a brief talk to discuss what they think the 3 most important topics of the day were. Homework Students should complete the Parameters & Statistics handout ( LMR_3.6Q LMR_3.6 ) for homework. Note: Page 2 of the handout is an answer key for teacher reference only! LMR_3.6","title":"Lesson 10: We\u2019re So Random"},{"location":"unit3/lesson10/#lesson-10-were-so-random","text":"","title":"Lesson 10: We\u2019re So Random"},{"location":"unit3/lesson10/#objective","text":"Students will learn how to collect random samples from a population in order to estimate a parameter.","title":"Objective:"},{"location":"unit3/lesson10/#materials","text":"Populations & Samples handout ( LMR_3.5_Populations and Samples ) RStudio Projector for RStudio functions Dotplot titled \u201cPercent of Students Who Have Met Friends Online\u201d Parameters & Statistics handout ( LMR_3.6_Parameters and Statistics )","title":"Materials:"},{"location":"unit3/lesson10/#vocabulary","text":"population sample representative sample random sample parameter statistic","title":"Vocabulary:"},{"location":"unit3/lesson10/#essential-concepts","text":"Essential Concepts: Another popular data collection method involves collecting data from a random sample of people or objects. Percentages based on random samples tend to \"center\" on the population parameter value.","title":"Essential Concepts:"},{"location":"unit3/lesson10/#lesson","text":"Introduce today\u2019s lesson by displaying the following statement made by the Pew Research Center in their August 2015 report titled Teens, Technology & Friendships : \u201cFor today\u2019s teens, friendships can start digitally: 57% of teens have met new friends online. The margin of error is plus or minus 3.7 percentage points. Social media and online gameplay are the most common digital venues for meeting friends.\u201d Note: The data for this report were collected via interviews of 1,060 teenagers between the ages of 13 and 17. Discuss the results of the Pew poll with the following prompting questions: The report says that 57% of teens have met new friends online. Since the report was based on a sample of 1,060 teens, how many of the teens reported that they have met friends online? 0.57(1060) = 604.2. This means that approximately 604 teens in the sample have met friends online. Do you think 57% of students in our class have met friends online? Why or why not? Answers will vary by class. The discussion should include points about how similar and different samples can be. The sample of students in the Pew poll may not represent the students in our class. What percent of students in our class have met friends online while a teenager? Answers will vary by class. Calculate the percentage by dividing the number of students who have met friends online by the total number of students who came to class today. What if [absent person] were in class today? Would that change our percentage? What effect would it have on the percentage if [absent person] answered \u201cyes?\u201d What effect would it have if [absent person] answered \u201cno?\u201d Answers will vary by class. If a student who was absent for today\u2019s lesson had actually come to class, we would have a different sample of students. It would change the percentage because our sample size now includes 1 more person. If the person answered \u201cyes,\u201d the values in the numerator and denominator of the percentage would change. If the person answered \u201cno,\u201d the value in the denominator would change. (Students should calculate these values.) If we were able to interview every single teenager in the United States, would exactly 57% of them say they have met friends online? Probably not because there are many more teenagers in the US than the 1,060 they interviewed. It would be unlikely for a group of 1,060 teenagers to exactly represent all teenagers in the entire country. Why do you think the Pew Research Center only interviewed 1,060 teenagers, and not all teenagers in the US? It would be impossible to talk to all teenagers in the US in a short period of time, or even a fairly long period of time. Introduce students to the terms population and sample . Explain that a population consists of all of the people we want to learn something about. A sample consists of people (or objects) that are selected from the population. In pairs, ask students to discuss and record answers to the following two questions: What was the population of interest to the researchers for the Pew poll above? All teenagers in the US right now. Based on your answer in (a), what characteristics should people have in order to be included in a sample for this poll? People would need to be in the US and be between the ages of 13 and 17. People could be from many states, but we would not want to sample only people from California, or only people from Los Angeles. It would be impossible to survey every single person in the US; this is why we create a random representative sample of the population. Note: Steer the discussion so that students see that a sample has to be \u201clike\u201d or \u201csimilar to\u201d or \u201crepresentative of\u201d the population. Select pairs to share their responses to the questions and let students revise their responses. Distribute the Populations & Samples handout ( LMR_3.5 ), which contains survey results from other Pew Research Center reports. Give students time to determine the population and sample for each scenario, and then have them verify their results with a partner. LMR_3.5 State that we want to know the percentage of students in our class that have made friends online, but we don\u2019t want to ask every single student. Instead, we would like to ask only 10 students and then make some guesses about our class from those 10. Ask: What is the population of interest? The students in our classroom. How can we select 10 students to be part of our sample? Answers will vary by class. There may be a variety of suggestions; here are some examples of what may be given: (1) put every student\u2019s name in a hat and pick out 10; (2) select the 10 students sitting closest to the teacher\u2019s desk; (3) have 10 students volunteer to be in the sample. Inform students that, in general, we want samples to \u201clook like\u201d the population. One way to get a representative sample like this is to take a random sample . Ask the students: Would the selection techniques we came up with in Step 6 result in random samples of our class. Answers will vary by class. Using the examples from Step 6: (1) putting each student\u2019s name in a hat and then picking out 10 would be a random sample as long as each piece of paper is the same size; (2) selecting the 10 students sitting closest to the teacher\u2019s desk would not be a random sample because those students might not represent the whole class; (3) if 10 students volunteer, we would not have a random sample because those students selected themselves to be part of the group and may not represent everyone in the class. Next, assign each student in the class a number by having them count off from 1 to N ( N being the total number of students in the class). Show students that we can use RStudio to create random samples with the following function: > sample(1:N, size = 10, replace = FALSE) Note: we use replace = FALSE because we only want each student to be selected once. Using the results given in the output of RStudio, ask the students whose numbers were chosen to stand. Inform them that they are \u201cin\u201d the sample. Then, determine what percent of the sample (these 10 students) have made friends online. How does this percentage compare to the overall class percentage we found in Step 2(c)? Answers will vary by class. Create a dotplot on the board titled \u201cPercent of Students Who Have Met Friends Online.\u201d Record the sample percentage from Step 9 on this dotplot. Have the students return to their seats so that we can select a new sample. Before we do this, ask: What do you predict the percentage to be for the next sample of 10 students? Answers will vary by class. They might say the expected percentage will be close to the class\u2019s overall percentage. Using RStudio, create a new sample, calculate the percentage of those 10 students who have met friends online, and record the value in the dotplot. Repeat Step 11 for a few more rounds (at least 5 samples should be taken). Be sure that the students give a prediction before finding each new sample. Display the following questions. Refer to the dotplot of sample percentages. Ask students to discuss the questions in teams: What do you notice about the sample percentages? What is the \u201ctypical\u201d value? Answers will vary by class. The typical value should be close to the class\u2019s overall percent calculated in Step 2(c) since it is the population percent. What is the smallest value? What is the biggest value? Answers will vary by class. There might be a lot of variability in the dotplot based on the selected samples. Most sample percentages will be within 30% of the population value, so that really gives a wide variety of possible sample values. If we took a larger sample \u2013 maybe of size 15 or 20 \u2013 would there be more or less variability in the dotplot? There will be less variability because adding more people gets us closer to the population size. Be sure to point out that if the sample were exactly the same as the population, then there would be no variability in the plot. Select teams at random to share their responses to the questions above with the whole class. The rest of the teams should be in full agreement with the responses before moving on to the next question. Explain that the population percentage (the percentage of all students in the class who met friends online) we have been using is called a parameter . A parameter is any number that summarizes a population. So, our class has been the population, and the percentage of students that have met friends online is the parameter. Similarly, the term statistics is used for numbers that summarize a sample. Ask students what sample statistics they have seen today? Each value we included in the dotplot is a statistic. Be sure to point out that there can be multiple values for a sample statistic (i.e. \u201cWe had 5 sample percentages in our dotplot.\u201d), but there is always only one parameter value. Using these new definitions, ask students to consider the original Pew poll data, which found that 57% of teens have met friends online. Ask the students: Is 57% a parameter or statistic? This is a statistic because it is based on a sample. Remind them that the population was ALL US teenagers and the sample included 1,060 teens. What is the population parameter then? We don\u2019t know! We would have to interview every teenager in the US to determine the parameter, and that is not possible. Conclude the lesson by telling the students: although we cannot determine the actual population parameter for the percent of teens that have made friends online, we can estimate it using random samples.","title":"Lesson:"},{"location":"unit3/lesson10/#class-scribes","text":"One team of students will give a brief talk to discuss what they think the 3 most important topics of the day were.","title":"Class Scribes:"},{"location":"unit3/lesson10/#homework","text":"Students should complete the Parameters & Statistics handout ( LMR_3.6Q LMR_3.6 ) for homework. Note: Page 2 of the handout is an answer key for teacher reference only! LMR_3.6","title":"Homework"},{"location":"unit3/lesson11/","text":"Lesson 11: The Gettysburg Address Objective: Students will learn the definition of sampling bias and will learn that random samples reduce bias when estimating a population parameter. They will gain practice collecting a random sample from a small population and estimating the population parameter. Materials: Gettysburg Address handout ( LMR_3.7_Gettysburg_Address ) Sampling the Gettysburg Address handout ( LMR_3.8_Sampling the Gettysburg Address ) Dotplot titled \u201cMean Word Length, Self-Selected Sample, Size = 10\u201d \u2013 on board or poster paper Gettysburg Address \u2013 Word Length Histogram file ( LMR_3.9_Gettysburg Histogram ) Gettysburg Word Lengths handout ( LMR_3.10_Gettysburg_Words ) RStudio Projector for RStudio functions Dotplot titled \u201cMean Word Length, Random Sample, Size = 10\u201d \u2013 on board or poster paper Note: This dotplot will be used again during Lesson 13 , so the results need to be kept in some way (this can be either on poster paper or by simply taking a photo). Vocabulary: sampling bias Essential Concepts: Essential Concepts: Statistics vary from sample to sample. If the typical value across many samples is equal to the population parameter, the statistic is \"unbiased\". Bias means that we tend to \u201cmiss the mark.\u201d If we don't do random sampling, we can get biased estimates. Lesson: Introduce the lesson by describing the Gettysburg Address: President Lincoln delivered the Gettysburg Address in November 1863. It is one of the most famous speeches in the United States. In it, Lincoln invoked the principles of human equality contained in the U.S. Constitution and Declaration of Independence. Read the Gettysburg Address aloud to the class OR have students read it aloud. The text of the speech can be found in the Gettysburg Address handout ( LMR_3.7 ). LMR_3.7 Today we will use the Gettysburg Address to learn about different sampling techniques. Inform students that the Gettysburg Address contains 272 words. We can consider these 272 words to be our population because it includes all words in the entire speech. From the population, we can sample 10 words that we think represent the speech. It is ok for this step to be vague \u2013 students can come up with their own concept for what they think \u201crepresentative\u201d means in this case. Distribute the Sampling the Gettysburg Address handout ( LMR_3.8 ), which includes the actual speech, as well as 2 sampling activities. For this part of the lesson, we will only be looking at Sampling Activity 1 on page 1 of the handout. Note: This activity was originally created by Allan Rossman and Beth Chance, and has been modified for the IDS curriculum. LMR_3.8 Inform students that they will get 30 seconds to select 10 words that they think are representative of all words in the speech. This is our first sampling plan. Note: It is important that students work fast so they are forced to choose based on first impressions and don't have time to reflect. Also, this activity tends to not work well if students are informed of the punch line (that random samples are unbiased) before they begin. At this point, explain to students that we are actually interested in answering a specific question: What is the typical word length in the Gettysburg Address? Next, students should record each circled word, as well as the number of letters each word has (this is the word length) in the table on the handout. Then, they should summarize the data in a dotplot and calculate the mean word length of the sample. On the board, create a class dotplot (may also be done on poster paper) titled \u201cMean Word Length, Self-Selected Sample, Size = 10.\u201d Once all students have completed the first page of the Gettysburg Address handout ( LMR_3.8 ), ask each student to record the mean word length of his or her sample on the class\u2019s dotplot. When all students have recorded their sample statistics in the dotplot, conduct a class discussion based on the questions listed below. Note: You might need to do a reality check. Students will often make mistakes when adding the word lengths and when dividing. Be suspicious (and double-check) extreme values. What does each point on the plot represent? Each point represents one student's estimate of the mean length of all of words in the Gettysburg address. What is the typical value represented in the dotplot? Answers will vary by class. You should indicate the approximate location of the mean of the distribution (the balancing point, on the dotplot. Remind students that when we ask for the 'typical' value we mean the value in the center of the distribution. How much variability is there in the distribution? Answers will vary by class. One reasonable approach is for students to give the range (the difference between the largest and smallest values). What is the shape of the distribution? Answers will vary by class. Often, the shape is right-skewed, but it might not be for you. Note that outliers here will often be arithmetic errors, but not always. Next, display the histogram from the Gettysburg Address \u2013 Word Length Histogram file ( LMR_3.9 ), which shows the distribution of word lengths for the entire population of words in the Gettysburg Address. LMR_3.9 Remind students that the population is the 272 words from the speech, and inform them that the mean word length of the population, or the population parameter, is 4.22. Using Think-Pair-Share , ask: How does the typical value of our class\u2019s sample means compare to the actual population mean of 4.22? Almost always, the class\u2019s typical mean will be higher (sometimes much higher) than 4.22. Some students will be close to 4.22. But point out that we are talking about the \"trend\" or typical outcome. The typical outcome is usually too high. Explain that our sampling plan was a self-selected sample, which often produces biased results. Sampling bias occurs when the resulting samples tend to produce results that are influenced in one particular direction. Note: Bias is NOT the same as prejudice. Bias is a tendency to lean towards a certain belief or viewpoint, and is mostly unconscious. Prejudice is a very conscious phenomenon though, where a person actively makes a decision to dislike something based on unfounded facts. Refer back to the dotplot of sample means and point out how it is biased. Ask: Why was our original sampling procedure biased? When we look for 'representative' words, and do so quickly, our eyes are drawn by the larger, more unusual words, and we tend to overlook small words such as \u201cin,\u201d \u201ca,\u201d \u201cwe,\u201d etc. Go back to the Gettysburg Address handout ( LMR_3.8 ), and direct students to page 2 for Sampling Activity 2. Inform students that they will now do a sampling procedure that results in a better representation of the population of words in the speech. Explain that a random sample tends to produce unbiased sample results. Before students begin the activity, demonstrate how to generate 10 random numbers from a possible 272 using RStudio. > sample((1:272), size = 10, replace = FALSE) Each student should generate his or her own set of 10 random numbers. Once students have created their random numbers, distribute the Gettysburg Address Word Lengths handout ( LMR_3.10 ). LMR_3.10 Explain that the table contains the word number, word, and length of each word in the Gettysburg Address. Demonstrate how to find a word that corresponds to one of the random numbers generated by RStudio, and explain that this word is now part of our random sample. Then, each student will complete the handout by creating a dotplot and calculating the mean of their random sample. On the board, near the first dotplot, create another class dotplot (may also be done on poster paper) titled \u201cMean Word Length, Random Sample, Size = 10.\u201d Once all students have completed the second page of the Gettysburg Address handout ( LMR_3.8 ), ask each student to record the mean word length of his or her random sample on the class\u2019s dotplot. Note: As in Step 9, be sure to check arithmetic for outliers! When all students have recorded their sample statistics in the dotplot, conduct a class discussion based on the following questions: What does each point in the dotplot represent? Each dot represents one student's estimate of the mean word length. But this time, the estimates are based on a random sample of 10 words. What do you notice about the typical value in this distribution? Answers will vary by class. They should notice that the means of the random samples are fairly close to the population mean of 4.22. (Again, you might have to discard or correct outliers.) What shape does this distribution have? What does that tell us? Typically, the distribution of sample means for random samples will be symmetric and unimodal. What does this distribution tell us about the benefits of random samples? We can reduce bias by collecting random samples instead of self-selected samples. Why do we need sampling? Why can\u2019t we just get the information from the actual population? It is usually impossible to include every person or object from a population. Even for the population of size 272 words in the Gettysburg Address, it would take a long time to calculate the word lengths of every single word. Conclusions and takeaways: It turns out that there are approximately 5.17\u202210 17 different possible samples of 10 words from the Gettysburg Address. If we could determine the mean for each of these samples and produce a dotplot for all of these means, then the center of the dotplot would lie exactly at 4.22 and the shape of the dotplot is symmetric and unimodal. The resulting distribution of the means from all possible samples is called the sampling distribution for the sample mean (for samples of size 10 from this population). The above dotplot is an approximation to the actual sampling distribution. Class Scribes: One team of students will give a brief talk to discuss what they think the 3 most important topics of the day were. Homework & Next Day Students should write a reflection about why random sampling is better at reducing bias than other sampling procedures. Lab 3C: Random Sampling Complete Lab 3C prior to Lesson 12 .","title":"Lesson 11: The Gettysburg Address"},{"location":"unit3/lesson11/#lesson-11-the-gettysburg-address","text":"","title":"Lesson 11: The Gettysburg Address"},{"location":"unit3/lesson11/#objective","text":"Students will learn the definition of sampling bias and will learn that random samples reduce bias when estimating a population parameter. They will gain practice collecting a random sample from a small population and estimating the population parameter.","title":"Objective:"},{"location":"unit3/lesson11/#materials","text":"Gettysburg Address handout ( LMR_3.7_Gettysburg_Address ) Sampling the Gettysburg Address handout ( LMR_3.8_Sampling the Gettysburg Address ) Dotplot titled \u201cMean Word Length, Self-Selected Sample, Size = 10\u201d \u2013 on board or poster paper Gettysburg Address \u2013 Word Length Histogram file ( LMR_3.9_Gettysburg Histogram ) Gettysburg Word Lengths handout ( LMR_3.10_Gettysburg_Words ) RStudio Projector for RStudio functions Dotplot titled \u201cMean Word Length, Random Sample, Size = 10\u201d \u2013 on board or poster paper Note: This dotplot will be used again during Lesson 13 , so the results need to be kept in some way (this can be either on poster paper or by simply taking a photo).","title":"Materials:"},{"location":"unit3/lesson11/#vocabulary","text":"sampling bias","title":"Vocabulary:"},{"location":"unit3/lesson11/#essential-concepts","text":"Essential Concepts: Statistics vary from sample to sample. If the typical value across many samples is equal to the population parameter, the statistic is \"unbiased\". Bias means that we tend to \u201cmiss the mark.\u201d If we don't do random sampling, we can get biased estimates.","title":"Essential Concepts:"},{"location":"unit3/lesson11/#lesson","text":"Introduce the lesson by describing the Gettysburg Address: President Lincoln delivered the Gettysburg Address in November 1863. It is one of the most famous speeches in the United States. In it, Lincoln invoked the principles of human equality contained in the U.S. Constitution and Declaration of Independence. Read the Gettysburg Address aloud to the class OR have students read it aloud. The text of the speech can be found in the Gettysburg Address handout ( LMR_3.7 ). LMR_3.7 Today we will use the Gettysburg Address to learn about different sampling techniques. Inform students that the Gettysburg Address contains 272 words. We can consider these 272 words to be our population because it includes all words in the entire speech. From the population, we can sample 10 words that we think represent the speech. It is ok for this step to be vague \u2013 students can come up with their own concept for what they think \u201crepresentative\u201d means in this case. Distribute the Sampling the Gettysburg Address handout ( LMR_3.8 ), which includes the actual speech, as well as 2 sampling activities. For this part of the lesson, we will only be looking at Sampling Activity 1 on page 1 of the handout. Note: This activity was originally created by Allan Rossman and Beth Chance, and has been modified for the IDS curriculum. LMR_3.8 Inform students that they will get 30 seconds to select 10 words that they think are representative of all words in the speech. This is our first sampling plan. Note: It is important that students work fast so they are forced to choose based on first impressions and don't have time to reflect. Also, this activity tends to not work well if students are informed of the punch line (that random samples are unbiased) before they begin. At this point, explain to students that we are actually interested in answering a specific question: What is the typical word length in the Gettysburg Address? Next, students should record each circled word, as well as the number of letters each word has (this is the word length) in the table on the handout. Then, they should summarize the data in a dotplot and calculate the mean word length of the sample. On the board, create a class dotplot (may also be done on poster paper) titled \u201cMean Word Length, Self-Selected Sample, Size = 10.\u201d Once all students have completed the first page of the Gettysburg Address handout ( LMR_3.8 ), ask each student to record the mean word length of his or her sample on the class\u2019s dotplot. When all students have recorded their sample statistics in the dotplot, conduct a class discussion based on the questions listed below. Note: You might need to do a reality check. Students will often make mistakes when adding the word lengths and when dividing. Be suspicious (and double-check) extreme values. What does each point on the plot represent? Each point represents one student's estimate of the mean length of all of words in the Gettysburg address. What is the typical value represented in the dotplot? Answers will vary by class. You should indicate the approximate location of the mean of the distribution (the balancing point, on the dotplot. Remind students that when we ask for the 'typical' value we mean the value in the center of the distribution. How much variability is there in the distribution? Answers will vary by class. One reasonable approach is for students to give the range (the difference between the largest and smallest values). What is the shape of the distribution? Answers will vary by class. Often, the shape is right-skewed, but it might not be for you. Note that outliers here will often be arithmetic errors, but not always. Next, display the histogram from the Gettysburg Address \u2013 Word Length Histogram file ( LMR_3.9 ), which shows the distribution of word lengths for the entire population of words in the Gettysburg Address. LMR_3.9 Remind students that the population is the 272 words from the speech, and inform them that the mean word length of the population, or the population parameter, is 4.22. Using Think-Pair-Share , ask: How does the typical value of our class\u2019s sample means compare to the actual population mean of 4.22? Almost always, the class\u2019s typical mean will be higher (sometimes much higher) than 4.22. Some students will be close to 4.22. But point out that we are talking about the \"trend\" or typical outcome. The typical outcome is usually too high. Explain that our sampling plan was a self-selected sample, which often produces biased results. Sampling bias occurs when the resulting samples tend to produce results that are influenced in one particular direction. Note: Bias is NOT the same as prejudice. Bias is a tendency to lean towards a certain belief or viewpoint, and is mostly unconscious. Prejudice is a very conscious phenomenon though, where a person actively makes a decision to dislike something based on unfounded facts. Refer back to the dotplot of sample means and point out how it is biased. Ask: Why was our original sampling procedure biased? When we look for 'representative' words, and do so quickly, our eyes are drawn by the larger, more unusual words, and we tend to overlook small words such as \u201cin,\u201d \u201ca,\u201d \u201cwe,\u201d etc. Go back to the Gettysburg Address handout ( LMR_3.8 ), and direct students to page 2 for Sampling Activity 2. Inform students that they will now do a sampling procedure that results in a better representation of the population of words in the speech. Explain that a random sample tends to produce unbiased sample results. Before students begin the activity, demonstrate how to generate 10 random numbers from a possible 272 using RStudio. > sample((1:272), size = 10, replace = FALSE) Each student should generate his or her own set of 10 random numbers. Once students have created their random numbers, distribute the Gettysburg Address Word Lengths handout ( LMR_3.10 ). LMR_3.10 Explain that the table contains the word number, word, and length of each word in the Gettysburg Address. Demonstrate how to find a word that corresponds to one of the random numbers generated by RStudio, and explain that this word is now part of our random sample. Then, each student will complete the handout by creating a dotplot and calculating the mean of their random sample. On the board, near the first dotplot, create another class dotplot (may also be done on poster paper) titled \u201cMean Word Length, Random Sample, Size = 10.\u201d Once all students have completed the second page of the Gettysburg Address handout ( LMR_3.8 ), ask each student to record the mean word length of his or her random sample on the class\u2019s dotplot. Note: As in Step 9, be sure to check arithmetic for outliers! When all students have recorded their sample statistics in the dotplot, conduct a class discussion based on the following questions: What does each point in the dotplot represent? Each dot represents one student's estimate of the mean word length. But this time, the estimates are based on a random sample of 10 words. What do you notice about the typical value in this distribution? Answers will vary by class. They should notice that the means of the random samples are fairly close to the population mean of 4.22. (Again, you might have to discard or correct outliers.) What shape does this distribution have? What does that tell us? Typically, the distribution of sample means for random samples will be symmetric and unimodal. What does this distribution tell us about the benefits of random samples? We can reduce bias by collecting random samples instead of self-selected samples. Why do we need sampling? Why can\u2019t we just get the information from the actual population? It is usually impossible to include every person or object from a population. Even for the population of size 272 words in the Gettysburg Address, it would take a long time to calculate the word lengths of every single word. Conclusions and takeaways: It turns out that there are approximately 5.17\u202210 17 different possible samples of 10 words from the Gettysburg Address. If we could determine the mean for each of these samples and produce a dotplot for all of these means, then the center of the dotplot would lie exactly at 4.22 and the shape of the dotplot is symmetric and unimodal. The resulting distribution of the means from all possible samples is called the sampling distribution for the sample mean (for samples of size 10 from this population). The above dotplot is an approximation to the actual sampling distribution.","title":"Lesson:"},{"location":"unit3/lesson11/#class-scribes","text":"One team of students will give a brief talk to discuss what they think the 3 most important topics of the day were.","title":"Class Scribes:"},{"location":"unit3/lesson11/#homework-next-day","text":"Students should write a reflection about why random sampling is better at reducing bias than other sampling procedures. Lab 3C: Random Sampling Complete Lab 3C prior to Lesson 12 .","title":"Homework &amp; Next Day"},{"location":"unit3/lesson12/","text":"Lesson 12: Bias in Survey Sampling Objective: Students will learn about bias in relation to survey sampling. More specifically, they will learn what types of sampling methods could result in a biased sample, who might be over/under-represented in the sample, and how to select a better sample. Materials: Identifying Biased Samples handout ( LMR_3.11_Identifying Biased Samples ) Poster paper Vocabulary: survey sample over-represented under-represented random sampling Essential Concepts: Essential Concepts: Bias concerning survey sampling includes identifying sampling methods that may lead to biased samples, recognizing potential over- or under-representation in samples, and acquiring skills to choose more reliable sampling techniques. Lesson: Remind students that they learned about biased samples during the last few lessons. Today, they will continue with this topic and discuss how people are selected to be in a sample. The people who are asked to participate in a survey are known as the survey sample . Ideally, the people who are included in the survey sample are a representative group of the target population, or the population we would like to make inferences about. Propose the following scenario to the class: \u201cAn elementary school is going to start serving ice cream in the cafeteria every Friday during lunch, and needs to know the favorite flavor of its students.\" In pairs, ask students to come up with two examples of samples that might be biased. For instance, one biased sample might include only the four 3 rd grade classes at the school. For each biased sample, the students should answer the following questions in their DS journals: Who is the target population? All students at the elementary school. Who is included in your biased sample? Only 3 rd grade students. These students are overrepresented in the sample. Who is not included in your biased sample? All other students in the school (Kindergartners, 1 st , 2 nd , 4 th , and 5 th graders). These students are underrepresented in the sample. Is your sample representative of the target population? No! We\u2019re only including 3 rd graders, and they may not have the same preferences as other students. Once pairs have come up with their biased samples and answered the questions in Step 4, they should share out with their student teams and answer the following questions: How is your biased sample different from the samples created by other pairs in your team? Answers will vary by class. An example might be that one pair sampled only 3 rd graders and the other pair sampled only girls. Which do you think is more representative of the target population? Why? Answers will vary by class. Using the example above, we could argue that either one is more representative. We could maybe say that, since 3 rd graders include both boys and girls, we have a more representative sample than if we just sampled girls. Or, we could say that since girls come from all grade levels, they\u2019re more representative of the entire school than just 3 rd graders. After the teams have discussed their samples, they should select one pair\u2019s biased sample to share with the rest of the class. Record each team\u2019s biased sample on a sheet of poster paper with the following layout: Biased Sample Who is overrepresented? Who is underrepresented? All 3 rd grade students 3 rd grade students All other students (kindergartners, 1 st , 2 nd , 4 th , and 5 th graders) Distribute the Identifying Biased Samples handout ( LMR_3.11 ). In this activity, students will explain why a particular sampling method might result in a biased sample \u2013 a sample that is not representative of the population of interest. Note: It is NOT enough for students to say that the \u201csample is not random.\u201d They need to explain how the sample is biased. Note: Page 2 of the handout provides sample answers for teacher reference ONLY. Do NOT distribute page 2 to students. LMR_3.11Q LMR_3.11 Each student should complete the handout independently. Afterwards, conduct a whole-class discussion to compare and contrast different students\u2019 explanations of how the samples might be biased. For each example given in the handout, discuss who is most likely over-represented and who is most likely under-represented in the sample. Ask the students: Now that we have learned about sampling biases, how can we eliminate this type of bias? Answers will vary by class. Note: Allow students to collaborate and come up with a few ideas on their own of how to eliminate sampling bias. If desired, ideas can be written on the board for discussion and comparison. Conclude with the actual answer: random sampling . If we randomly sample people from our population of interest, we can reduce the bias of any sample statistics obtained from the survey responses. If we have a biased sample, we can only give descriptions about that particular sample; we CANNOT generalize to the population of interest. Class Scribes: One team of students will give a brief talk to discuss what they think the 3 most important topics of the day were. Homework Students will complete the Survey Sampling handout ( LMR_3.12 ) for homework. LMR_3.12","title":"Lesson 12: Bias in Survey Sampling"},{"location":"unit3/lesson12/#lesson-12-bias-in-survey-sampling","text":"","title":"Lesson 12: Bias in Survey Sampling"},{"location":"unit3/lesson12/#objective","text":"Students will learn about bias in relation to survey sampling. More specifically, they will learn what types of sampling methods could result in a biased sample, who might be over/under-represented in the sample, and how to select a better sample.","title":"Objective:"},{"location":"unit3/lesson12/#materials","text":"Identifying Biased Samples handout ( LMR_3.11_Identifying Biased Samples ) Poster paper","title":"Materials:"},{"location":"unit3/lesson12/#vocabulary","text":"survey sample over-represented under-represented random sampling","title":"Vocabulary:"},{"location":"unit3/lesson12/#essential-concepts","text":"Essential Concepts: Bias concerning survey sampling includes identifying sampling methods that may lead to biased samples, recognizing potential over- or under-representation in samples, and acquiring skills to choose more reliable sampling techniques.","title":"Essential Concepts:"},{"location":"unit3/lesson12/#lesson","text":"Remind students that they learned about biased samples during the last few lessons. Today, they will continue with this topic and discuss how people are selected to be in a sample. The people who are asked to participate in a survey are known as the survey sample . Ideally, the people who are included in the survey sample are a representative group of the target population, or the population we would like to make inferences about. Propose the following scenario to the class: \u201cAn elementary school is going to start serving ice cream in the cafeteria every Friday during lunch, and needs to know the favorite flavor of its students.\" In pairs, ask students to come up with two examples of samples that might be biased. For instance, one biased sample might include only the four 3 rd grade classes at the school. For each biased sample, the students should answer the following questions in their DS journals: Who is the target population? All students at the elementary school. Who is included in your biased sample? Only 3 rd grade students. These students are overrepresented in the sample. Who is not included in your biased sample? All other students in the school (Kindergartners, 1 st , 2 nd , 4 th , and 5 th graders). These students are underrepresented in the sample. Is your sample representative of the target population? No! We\u2019re only including 3 rd graders, and they may not have the same preferences as other students. Once pairs have come up with their biased samples and answered the questions in Step 4, they should share out with their student teams and answer the following questions: How is your biased sample different from the samples created by other pairs in your team? Answers will vary by class. An example might be that one pair sampled only 3 rd graders and the other pair sampled only girls. Which do you think is more representative of the target population? Why? Answers will vary by class. Using the example above, we could argue that either one is more representative. We could maybe say that, since 3 rd graders include both boys and girls, we have a more representative sample than if we just sampled girls. Or, we could say that since girls come from all grade levels, they\u2019re more representative of the entire school than just 3 rd graders. After the teams have discussed their samples, they should select one pair\u2019s biased sample to share with the rest of the class. Record each team\u2019s biased sample on a sheet of poster paper with the following layout: Biased Sample Who is overrepresented? Who is underrepresented? All 3 rd grade students 3 rd grade students All other students (kindergartners, 1 st , 2 nd , 4 th , and 5 th graders) Distribute the Identifying Biased Samples handout ( LMR_3.11 ). In this activity, students will explain why a particular sampling method might result in a biased sample \u2013 a sample that is not representative of the population of interest. Note: It is NOT enough for students to say that the \u201csample is not random.\u201d They need to explain how the sample is biased. Note: Page 2 of the handout provides sample answers for teacher reference ONLY. Do NOT distribute page 2 to students. LMR_3.11Q LMR_3.11 Each student should complete the handout independently. Afterwards, conduct a whole-class discussion to compare and contrast different students\u2019 explanations of how the samples might be biased. For each example given in the handout, discuss who is most likely over-represented and who is most likely under-represented in the sample. Ask the students: Now that we have learned about sampling biases, how can we eliminate this type of bias? Answers will vary by class. Note: Allow students to collaborate and come up with a few ideas on their own of how to eliminate sampling bias. If desired, ideas can be written on the board for discussion and comparison. Conclude with the actual answer: random sampling . If we randomly sample people from our population of interest, we can reduce the bias of any sample statistics obtained from the survey responses. If we have a biased sample, we can only give descriptions about that particular sample; we CANNOT generalize to the population of interest.","title":"Lesson:"},{"location":"unit3/lesson12/#class-scribes","text":"One team of students will give a brief talk to discuss what they think the 3 most important topics of the day were.","title":"Class Scribes:"},{"location":"unit3/lesson12/#homework","text":"Students will complete the Survey Sampling handout ( LMR_3.12 ) for homework. LMR_3.12","title":"Homework"},{"location":"unit3/lesson13/","text":"Lesson 13: The Confidence Game Objective: Students will learn about informal confidence intervals for making estimates about population parameters based on statistics from random samples. Materials: The Confidence Game handout ( LMR_3.13_Confidence Game ) Dotplot titled \u201cNumber Correct\u201d displayed on the board (or on poster paper) Vocabulary: inferences interval confidence interval Essential Concepts: Essential Concepts: There is uncertainty when we estimate population parameters. Because of this, it is better to give a range of plausible values, rather than a single value. Lesson: Remind students that they have been learning about why sampling allows us to make inferences about a population. Some methods of sampling produce biased sample statistics, which does not allow us to generalize the results from a sample to the population of interest. To obtain unbiased statistics, random sampling methods need to be used. Conduct a class brainstorm about what it means to \u201cestimate\u201d something. Have students come up with possible synonyms for the word \u201cestimate.\u201d Some example synonyms include: guess, approximation, projection, opinion, impression, etc. Inform students that, in statistics, to provide an estimate means that we can give a range of values that we are confident include the population parameter value. In today\u2019s lesson, explain that the students will be playing a game, called The Confidence Game , in which they will be asked a series of questions that each have one numerical answer. However, instead of guessing what the exact answer is, the students will create a range of possible values that they think might include the real answer. They should be 90% confident that the true value is within their interval. Introduce The Confidence Game to students by first going through an example using the question: How tall is the Empire State Building, in feet (including the spire at the very top)? Ask the students to write down an interval , or range, of values that they think contains the true height of the building. Have a few students share their intervals with the class and discuss any obvious similarities or differences between them. For example: If Student A gives an interval from 500 to 2000 feet and Student B gives an interval from 1100 to 1400 feet, one discussion could stem from asking Student A why he or she isn\u2019t as sure of the answer as Student B is (since Student B gave a narrower interval). Then see if Student A wants to change his or her interval. After the discussion, tell the students that the actual height of the Empire State Building is 1,454 feet tall. Take a poll to see how many students\u2019 intervals contained this value. We will learn what it means to have the true value in our intervals after we play the game. Now, we can actually play the game! Distribute The Confidence Game handout ( LMR_3.13 ) and explain the rules. Students will have about 5 minutes to complete the handout, which gives them approximately 30 seconds per question. Note: The rules are printed at the beginning of the handout. They are included here for your convenience. Each question must be answered WITH AN INTERVAL. You should choose your interval so that you are \u201c90% confident\u201d (whatever that means to you). You CANNOT use any reference tools (i.e. no cell phones or computers to find answers). A question is \u201ccorrect\u201d if the true answer is inside your interval. The winner is determined by who got the most questions correct. In the case of a tie, the winner is chosen by whose intervals were narrower. LMR_3.13 Once each student has completed The Confidence Game handout ( LMR_3.13 ), have students choose partners and exchange handouts so that they can grade each other. Remind them that a question is marked as \u201ccorrect\u201d if the actual value (see answers in Step 8) falls within the interval. Display the answers for each of the 10 questions from the handout found below: 1) In what year did Mickey Mouse make his film debut? 1928 2) What is the lowest temperature (in degrees Fahrenheit) ever recorded in California? -45 degrees Fahrenheit 3) During the year 2014, how many television series were aired? 1,715 TV shows 4) How far away, in miles, is Earth from the moon? 238,900 miles 5) What is the greatest number of children officially recorded that were all born to one mother? 69 children 6) In what year did Orville and Wilbur Wright, more commonly known as the Wright brothers, make the first-ever powered flight in a plane? 1903 7) As of June 2015, how many of Rihanna\u2019s songs have reached the Number 1 spot on Billboard\u2019s \u201cDance Club Hits\u201d chart? 23 songs 8) How many years have actors Will Smith and Jada Pinkett-Smith been married? As of January 2016, it was 18 years - they were married December 31, 1997. 9) How many hours will it take to complete a cross-country road trip from Los Angeles to New York City according to Google Maps? 41 hours (2,789.5 miles) 10) How many baseball fans can attend game at Dodger Stadium during any given day? 56,000 fans Each student should write the total number of \u201ccorrect\u201d responses at the top of his or her partner\u2019s handout, and then return it. Engage the students in a discussion about how well they did at estimating the true values with their intervals. The following questions can be used to steer the discussion: Remember that we were aiming to be 90% confident for each question. Based on this, how many of the 10 questions should we each have gotten correct? If we are 90% confident, then we would expect 90% of the 10 intervals to include the true value, which is 9 intervals. Did anyone in the class get exactly 9 correct? Did anyone get all 10 correct? Answers will vary by class. However, it is very unlikely that many students will have gotten 9 or 10 correct responses on this first round. Create a dotplot on the board (or on poster paper) titled \u201cNumber Correct\u201d and have each student record his or her value. Then, ask: How many students got 9 correct? In other words, how many students were actually 90% confident of their intervals? Answers will vary by class. What is the typical number of correct responses for our class? Does it seem too high or too low? Explain. Answers will vary by class. Most likely, the typical number of correct responses will be fairly low (maybe even 4 or less). Why is our typical score so much lower than 9? We tend to be more confident than we should be, so we create narrower intervals. It looks like, even though we thought we were 90% confident, most of us (or all of us) did not succeed 90% of the time. How could we increase our level of confidence? We could use wider intervals. Recall from Step 3 that, in statistics, to estimate something means that we can give a range of values that we are confident include the population parameter value. This range of values, like the ones the students created during The Confidence Game activity, is known as a confidence interval . Students will continue to learn about confidence intervals during the next lesson . Class Scribes: One team of students will give a brief talk to discuss what they think the 3 most important topics of the day were. Homework In your own words, write a description of what a confidence interval is and why it is used in statistics.","title":"Lesson 13: The Confidence Game"},{"location":"unit3/lesson13/#lesson-13-the-confidence-game","text":"","title":"Lesson 13: The Confidence Game"},{"location":"unit3/lesson13/#objective","text":"Students will learn about informal confidence intervals for making estimates about population parameters based on statistics from random samples.","title":"Objective:"},{"location":"unit3/lesson13/#materials","text":"The Confidence Game handout ( LMR_3.13_Confidence Game ) Dotplot titled \u201cNumber Correct\u201d displayed on the board (or on poster paper)","title":"Materials:"},{"location":"unit3/lesson13/#vocabulary","text":"inferences interval confidence interval","title":"Vocabulary:"},{"location":"unit3/lesson13/#essential-concepts","text":"Essential Concepts: There is uncertainty when we estimate population parameters. Because of this, it is better to give a range of plausible values, rather than a single value.","title":"Essential Concepts:"},{"location":"unit3/lesson13/#lesson","text":"Remind students that they have been learning about why sampling allows us to make inferences about a population. Some methods of sampling produce biased sample statistics, which does not allow us to generalize the results from a sample to the population of interest. To obtain unbiased statistics, random sampling methods need to be used. Conduct a class brainstorm about what it means to \u201cestimate\u201d something. Have students come up with possible synonyms for the word \u201cestimate.\u201d Some example synonyms include: guess, approximation, projection, opinion, impression, etc. Inform students that, in statistics, to provide an estimate means that we can give a range of values that we are confident include the population parameter value. In today\u2019s lesson, explain that the students will be playing a game, called The Confidence Game , in which they will be asked a series of questions that each have one numerical answer. However, instead of guessing what the exact answer is, the students will create a range of possible values that they think might include the real answer. They should be 90% confident that the true value is within their interval. Introduce The Confidence Game to students by first going through an example using the question: How tall is the Empire State Building, in feet (including the spire at the very top)? Ask the students to write down an interval , or range, of values that they think contains the true height of the building. Have a few students share their intervals with the class and discuss any obvious similarities or differences between them. For example: If Student A gives an interval from 500 to 2000 feet and Student B gives an interval from 1100 to 1400 feet, one discussion could stem from asking Student A why he or she isn\u2019t as sure of the answer as Student B is (since Student B gave a narrower interval). Then see if Student A wants to change his or her interval. After the discussion, tell the students that the actual height of the Empire State Building is 1,454 feet tall. Take a poll to see how many students\u2019 intervals contained this value. We will learn what it means to have the true value in our intervals after we play the game. Now, we can actually play the game! Distribute The Confidence Game handout ( LMR_3.13 ) and explain the rules. Students will have about 5 minutes to complete the handout, which gives them approximately 30 seconds per question. Note: The rules are printed at the beginning of the handout. They are included here for your convenience. Each question must be answered WITH AN INTERVAL. You should choose your interval so that you are \u201c90% confident\u201d (whatever that means to you). You CANNOT use any reference tools (i.e. no cell phones or computers to find answers). A question is \u201ccorrect\u201d if the true answer is inside your interval. The winner is determined by who got the most questions correct. In the case of a tie, the winner is chosen by whose intervals were narrower. LMR_3.13 Once each student has completed The Confidence Game handout ( LMR_3.13 ), have students choose partners and exchange handouts so that they can grade each other. Remind them that a question is marked as \u201ccorrect\u201d if the actual value (see answers in Step 8) falls within the interval. Display the answers for each of the 10 questions from the handout found below: 1) In what year did Mickey Mouse make his film debut? 1928 2) What is the lowest temperature (in degrees Fahrenheit) ever recorded in California? -45 degrees Fahrenheit 3) During the year 2014, how many television series were aired? 1,715 TV shows 4) How far away, in miles, is Earth from the moon? 238,900 miles 5) What is the greatest number of children officially recorded that were all born to one mother? 69 children 6) In what year did Orville and Wilbur Wright, more commonly known as the Wright brothers, make the first-ever powered flight in a plane? 1903 7) As of June 2015, how many of Rihanna\u2019s songs have reached the Number 1 spot on Billboard\u2019s \u201cDance Club Hits\u201d chart? 23 songs 8) How many years have actors Will Smith and Jada Pinkett-Smith been married? As of January 2016, it was 18 years - they were married December 31, 1997. 9) How many hours will it take to complete a cross-country road trip from Los Angeles to New York City according to Google Maps? 41 hours (2,789.5 miles) 10) How many baseball fans can attend game at Dodger Stadium during any given day? 56,000 fans Each student should write the total number of \u201ccorrect\u201d responses at the top of his or her partner\u2019s handout, and then return it. Engage the students in a discussion about how well they did at estimating the true values with their intervals. The following questions can be used to steer the discussion: Remember that we were aiming to be 90% confident for each question. Based on this, how many of the 10 questions should we each have gotten correct? If we are 90% confident, then we would expect 90% of the 10 intervals to include the true value, which is 9 intervals. Did anyone in the class get exactly 9 correct? Did anyone get all 10 correct? Answers will vary by class. However, it is very unlikely that many students will have gotten 9 or 10 correct responses on this first round. Create a dotplot on the board (or on poster paper) titled \u201cNumber Correct\u201d and have each student record his or her value. Then, ask: How many students got 9 correct? In other words, how many students were actually 90% confident of their intervals? Answers will vary by class. What is the typical number of correct responses for our class? Does it seem too high or too low? Explain. Answers will vary by class. Most likely, the typical number of correct responses will be fairly low (maybe even 4 or less). Why is our typical score so much lower than 9? We tend to be more confident than we should be, so we create narrower intervals. It looks like, even though we thought we were 90% confident, most of us (or all of us) did not succeed 90% of the time. How could we increase our level of confidence? We could use wider intervals. Recall from Step 3 that, in statistics, to estimate something means that we can give a range of values that we are confident include the population parameter value. This range of values, like the ones the students created during The Confidence Game activity, is known as a confidence interval . Students will continue to learn about confidence intervals during the next lesson .","title":"Lesson:"},{"location":"unit3/lesson13/#class-scribes","text":"One team of students will give a brief talk to discuss what they think the 3 most important topics of the day were.","title":"Class Scribes:"},{"location":"unit3/lesson13/#homework","text":"In your own words, write a description of what a confidence interval is and why it is used in statistics.","title":"Homework"},{"location":"unit3/lesson14/","text":"Lesson 14: How Confident Are You? Objective: Students will learn about informal confidence intervals and estimates for the margin of error. Materials: Dotplot titled \u201cMean Word Length, Random Sample, Size = 10\u201d \u2013 from Lesson 11 Vocabulary: margin of error bootstrapping Essential Concepts: Essential Concepts: The margin of error expresses our uncertainty in an estimate. The estimate, plus or minus the margin of error, gives us an interval in which we are very confident the true value lies. Lesson: In this lesson, students will learn about confidence intervals in more detail. Display the dotplot the class created during Lesson 11 (The Gettysburg Address) titled \u201cMean Word Length, Random Sample, Size = 10.\u201d Have students recall that each dot represents one student\u2019s calculation of the mean word length of a sample of 10 randomly selected words from the Gettysburg Address. Also remind them that the population parameter, which is the mean word length of all words in the speech, was 4.22. There should already be a vertical line on the dotplot to indicate this value, but if it is not present, please add it during this step. Ask: What vocabulary word was used to describe each of the sample means we each created during Lesson 11 ? The sample statistic. Every dot on the graph represents one sample statistic, more specifically each dot corresponds to a different sample mean. How many of us got exactly the right value? Probably none. Thinking back on The Confidence Game we played yesterday, what approach could we do so that 90% of us would be correct? We could give an interval. Show students that they can give an interval in the form: Your sample statistic plus or minus AMOUNT Ask them to calculate what their AMOUNT must be so that their interval includes the parameter value. Ask them to write this as an interval. Choose one student to illustrate what is to be done. Ask them for their AMOUNT. On the dotplot, find their value, and use bars to go out plus and minus the AMOUNT. Confirm that it includes the parameter value. For example: The purple dot represents a sample mean of 3.5. The AMOUNT we have chosen for this particular case is 0.8, so the lower bracket is 0.8 below the sample mean, and the upper bracket is 0.8 above the sample mean. Notice that the population parameter is included within the brackets. Now, convert this to an interval of the form [lowest value, highest value] by subtracting the amount from the sample statistic to get the lowest value, and adding to get the highest. Inform students that this AMOUNT is called the margin of error . Explain that the students all now have different margins of error because in this unusual 'game' they know the population value. But in real life we do not, and so we have to choose one single margin of error that will work 90% of the time. Ask the students what margin of error they should use so that 90% of the estimates will have a 'successful' interval. You might want to tell them how many estimates that is for your class. A ballpark figure for the margin of error is 1.3. Explain: If we were to start all over, we could imagine picking one of these sample statistics at random. What's the probability that the sample statistic plus or minus the margin of error would include the parameter value? 90%. Because of this, we call these 'confidence intervals.' When we report an interval, for example 2.7 to 4.3, we say \"We are 90% confident that the population parameter value is between 2.7 and 4.3.\" This is another way of saying \"We don't know what the exact true value is, but we're confident it is somewhere in this interval.\" Remind students of the Pew Poll they discussed during Lesson 10 . For reference, the Pew Research Center made the following statement in their August 2015 report titled Teens, Technology & Friendships: Pew Poll \u201cFor today\u2019s teens, friendships can start digitally: 57% of teens have met new friends online. The margin of error is plus or minus 3.7 percentage points. Social media and online gameplay are the most common digital venues for meeting friends.\u201d Note: The data for this report were collected via interviews of 1,060 teenagers between the ages of 13 and 17. Now that students have learned about the margin of error, have them write an Exit Slip about what the margin of error means in context of the Pew Poll. Conclusions and takeaways: Estimates that are based on random samples vary. We can measure this variation. The margin of error can tell us how much estimates vary. We can use the estimate from our random sample, along with the margin of error, to give us a range of plausible values for the population parameter. This is called a confidence interval. If time allows, introduce students to the idea of bootstrapping , which is where we take random samples of really large samples. For example, if we were looking at Twitter data, it would be almost impossible to compile every single tweet that exists in the population. Instead, we might be able to access 500,000 tweets, which is a very large sample. From this sample, we could create smaller random samples of size 100 and make inferences about the overall population of tweets from these samples. This will be discussed further in Lab 3D . Class Scribes: One team of students will give a brief talk to discuss what they think the 3 most important topics of the day were. Next Day LAB 3D: Are You Sure about That? Complete Lab 3D prior to the Let\u2019s Build a Survey! Practicum .","title":"Lesson 14: How Confident Are You?"},{"location":"unit3/lesson14/#lesson-14-how-confident-are-you","text":"","title":"Lesson 14: How Confident Are You?"},{"location":"unit3/lesson14/#objective","text":"Students will learn about informal confidence intervals and estimates for the margin of error.","title":"Objective:"},{"location":"unit3/lesson14/#materials","text":"Dotplot titled \u201cMean Word Length, Random Sample, Size = 10\u201d \u2013 from Lesson 11","title":"Materials:"},{"location":"unit3/lesson14/#vocabulary","text":"margin of error bootstrapping","title":"Vocabulary:"},{"location":"unit3/lesson14/#essential-concepts","text":"Essential Concepts: The margin of error expresses our uncertainty in an estimate. The estimate, plus or minus the margin of error, gives us an interval in which we are very confident the true value lies.","title":"Essential Concepts:"},{"location":"unit3/lesson14/#lesson","text":"In this lesson, students will learn about confidence intervals in more detail. Display the dotplot the class created during Lesson 11 (The Gettysburg Address) titled \u201cMean Word Length, Random Sample, Size = 10.\u201d Have students recall that each dot represents one student\u2019s calculation of the mean word length of a sample of 10 randomly selected words from the Gettysburg Address. Also remind them that the population parameter, which is the mean word length of all words in the speech, was 4.22. There should already be a vertical line on the dotplot to indicate this value, but if it is not present, please add it during this step. Ask: What vocabulary word was used to describe each of the sample means we each created during Lesson 11 ? The sample statistic. Every dot on the graph represents one sample statistic, more specifically each dot corresponds to a different sample mean. How many of us got exactly the right value? Probably none. Thinking back on The Confidence Game we played yesterday, what approach could we do so that 90% of us would be correct? We could give an interval. Show students that they can give an interval in the form: Your sample statistic plus or minus AMOUNT Ask them to calculate what their AMOUNT must be so that their interval includes the parameter value. Ask them to write this as an interval. Choose one student to illustrate what is to be done. Ask them for their AMOUNT. On the dotplot, find their value, and use bars to go out plus and minus the AMOUNT. Confirm that it includes the parameter value. For example: The purple dot represents a sample mean of 3.5. The AMOUNT we have chosen for this particular case is 0.8, so the lower bracket is 0.8 below the sample mean, and the upper bracket is 0.8 above the sample mean. Notice that the population parameter is included within the brackets. Now, convert this to an interval of the form [lowest value, highest value] by subtracting the amount from the sample statistic to get the lowest value, and adding to get the highest. Inform students that this AMOUNT is called the margin of error . Explain that the students all now have different margins of error because in this unusual 'game' they know the population value. But in real life we do not, and so we have to choose one single margin of error that will work 90% of the time. Ask the students what margin of error they should use so that 90% of the estimates will have a 'successful' interval. You might want to tell them how many estimates that is for your class. A ballpark figure for the margin of error is 1.3. Explain: If we were to start all over, we could imagine picking one of these sample statistics at random. What's the probability that the sample statistic plus or minus the margin of error would include the parameter value? 90%. Because of this, we call these 'confidence intervals.' When we report an interval, for example 2.7 to 4.3, we say \"We are 90% confident that the population parameter value is between 2.7 and 4.3.\" This is another way of saying \"We don't know what the exact true value is, but we're confident it is somewhere in this interval.\" Remind students of the Pew Poll they discussed during Lesson 10 . For reference, the Pew Research Center made the following statement in their August 2015 report titled Teens, Technology & Friendships: Pew Poll \u201cFor today\u2019s teens, friendships can start digitally: 57% of teens have met new friends online. The margin of error is plus or minus 3.7 percentage points. Social media and online gameplay are the most common digital venues for meeting friends.\u201d Note: The data for this report were collected via interviews of 1,060 teenagers between the ages of 13 and 17. Now that students have learned about the margin of error, have them write an Exit Slip about what the margin of error means in context of the Pew Poll. Conclusions and takeaways: Estimates that are based on random samples vary. We can measure this variation. The margin of error can tell us how much estimates vary. We can use the estimate from our random sample, along with the margin of error, to give us a range of plausible values for the population parameter. This is called a confidence interval. If time allows, introduce students to the idea of bootstrapping , which is where we take random samples of really large samples. For example, if we were looking at Twitter data, it would be almost impossible to compile every single tweet that exists in the population. Instead, we might be able to access 500,000 tweets, which is a very large sample. From this sample, we could create smaller random samples of size 100 and make inferences about the overall population of tweets from these samples. This will be discussed further in Lab 3D .","title":"Lesson:"},{"location":"unit3/lesson14/#class-scribes","text":"One team of students will give a brief talk to discuss what they think the 3 most important topics of the day were.","title":"Class Scribes:"},{"location":"unit3/lesson14/#next-day","text":"LAB 3D: Are You Sure about That? Complete Lab 3D prior to the Let\u2019s Build a Survey! Practicum .","title":"Next Day"},{"location":"unit3/lesson15/","text":"Lesson 15: Ready, Sense, Go! Objective: Students will learn what sensors are and how they are used to collect data. Materials: Video: Play Like Nadal With a Smart Tennis Racket https://youtu.be/lcBnzddQECc Computers (see Step 5) Poster paper Flags in 3 different colors Advanced preparation required (see Step 10 below) Vocabulary: sensor trigger algorithm Essential Concepts: Essential Concepts: Sensors are another data collection method. Unlike what we have seen so far, sensors do not involve humans (much). They collect data according to an algorithm. Lesson: Entrance Ticket: What are some of the data collection methods we have learned about so far in this unit? We have learned about experiments, observational studies, surveys, and getting data from a URL (in Lab 3B ). Inform students that, in this lesson, they will be introduced to another data collection method known as sensors. With a partner, ask students to discuss what they think a sensor is. Ask each pair to write down their ideas. Show the Play Like Nadal With a Smart Tennis Racket video found at: https://youtu.be/lcBnzddQECc . As students watch the video, they should think about other sensors they may have come across, particularly ones used with smartphones. After watching the video, ask students to add to their definition of a sensor. Now, inform students that they will work in teams to compile a list of data-collecting sensors. They may use computers to conduct online research for this part of the lesson. Challenge each team to generate the longest list in the class. After students have had time to research and create their lists, ask students in each team to number off one through four (or five, depending on team sizes). Share out in rounds. First, ask students in each team whose number is one to share one sensor from their list. On the poster paper, create a class list of sensors as shared by the students. Repeat with the rest of the numbers. Score keeping: Each person gets five seconds to respond. You may hold up your hand with the palm facing the students and count down. The rules for teams are as follows: add a sensor to the list, get 1 point repeat an answer, lose 1 point do not answer in five seconds, lose a turn do not have an answer to contribute, may pass Note: You may reward the winning team with extra credit points, if desired. Next, students will engage in an activity to see sensors in action. Create 3 groups of students: Group 1 \u2013 Triggers (3 students) Provide each Trigger a different colored flag (for example: Pink , Purple , Green ). The teacher will call out a color, at random, and the Trigger assigned to that color will raise his or her flag. Each flag corresponds to a research question of interest. Pink \u2013 Who is in our class? Purple \u2013 What is on our classroom walls? Green \u2013 What do we like to do after school? Group 2 \u2013 Sensors (2 students) Each Sensor should be assigned to one Trigger, or colored flag ( Pink = Sensor A, Purple = Sensor B, Green = no sensor assigned to it). When the Sensors see their assigned Trigger, they send a signal to the Collector (see below) telling him or her to collect data from another student in the class. The Sensors are basically gobetweens for the Triggers and the Collector. Group 3 \u2013 Collector (1 student) One student is the Collector of all the data. The Collector is in charge of asking survey questions related to the research question of the original Trigger. Survey questions are provided here: Survey questions related to the Pink trigger: (1) How did you get to school today (bus, car, walking, etc.)? (2) What size shoe do you wear? (3) What is your favorite pizza topping? Survey questions related to the Purple trigger: (1) What is your favorite wall decoration? (2) What type of poster is it (motivational, reference, class work, etc.)? (3) What color is most prevalent in the poster? Survey questions related to the Green trigger: (1) Do you have a sports team practice or club meeting today? (2) Are you hanging out with friends today after school? (3) Will you be working today after school? Each time a sensor is active, the Collector must ask a new student in the class the appropriate survey questions. Explain the activity to your students. Then, call out a flag color at random. Repeat several times. Make sure you call out the flag that has no assignment at least once so that students see that no action took place. Reflect on the activity with the following discussion questions: What data were missed? Why? Data about what our class likes to do after school. They were missed because there was no Sensor connected to the Green trigger, so the Collector never knew to collect this type of data. Grocery stores keep track of customer data when purchases are made with a loyalty card. What is the trigger in this case? What data are being collected? The trigger is checking out at a grocery store. There are lots of data that are collected, including: items bought, cost of items, number of items on sale, etc. After they engage in the sensor activity, ask students to revisit their definition of a sensor (see Step 3). Have them revise their definition based on the following concepts: A sensor is a converter that measures a physical quantity and converts it into a signal, which can be read by an observer or by an instrument. A sensor collects data continuously, or whenever a trigger is activated. A trigger is a something that responds to an event so that an action can occur. Sensors collect data according to an algorithm . An algorithm is a process or set of rules that are followed (just like the rules followed during the activity). Sensors may also collect data automatically, without anyone\u2019s knowledge or input. Examples include GPS location, time, and date. Class Scribes: One team of students will give a brief talk to discuss what they think the 3 most important topics of the day were. Homework Now that students learned what sensors are, ask them what data they would they like to see collected on a sensor that they couldn't collect in an experiment or survey. They must explain why it is difficult to collect that data in an experiment or survey, and how a sensor would make it easier to collect that data.","title":"Lesson 15: Ready, Sense, Go!"},{"location":"unit3/lesson15/#lesson-15-ready-sense-go","text":"","title":"Lesson 15: Ready, Sense, Go!"},{"location":"unit3/lesson15/#objective","text":"Students will learn what sensors are and how they are used to collect data.","title":"Objective:"},{"location":"unit3/lesson15/#materials","text":"Video: Play Like Nadal With a Smart Tennis Racket https://youtu.be/lcBnzddQECc Computers (see Step 5) Poster paper Flags in 3 different colors Advanced preparation required (see Step 10 below)","title":"Materials:"},{"location":"unit3/lesson15/#vocabulary","text":"sensor trigger algorithm","title":"Vocabulary:"},{"location":"unit3/lesson15/#essential-concepts","text":"Essential Concepts: Sensors are another data collection method. Unlike what we have seen so far, sensors do not involve humans (much). They collect data according to an algorithm.","title":"Essential Concepts:"},{"location":"unit3/lesson15/#lesson","text":"Entrance Ticket: What are some of the data collection methods we have learned about so far in this unit? We have learned about experiments, observational studies, surveys, and getting data from a URL (in Lab 3B ). Inform students that, in this lesson, they will be introduced to another data collection method known as sensors. With a partner, ask students to discuss what they think a sensor is. Ask each pair to write down their ideas. Show the Play Like Nadal With a Smart Tennis Racket video found at: https://youtu.be/lcBnzddQECc . As students watch the video, they should think about other sensors they may have come across, particularly ones used with smartphones. After watching the video, ask students to add to their definition of a sensor. Now, inform students that they will work in teams to compile a list of data-collecting sensors. They may use computers to conduct online research for this part of the lesson. Challenge each team to generate the longest list in the class. After students have had time to research and create their lists, ask students in each team to number off one through four (or five, depending on team sizes). Share out in rounds. First, ask students in each team whose number is one to share one sensor from their list. On the poster paper, create a class list of sensors as shared by the students. Repeat with the rest of the numbers. Score keeping: Each person gets five seconds to respond. You may hold up your hand with the palm facing the students and count down. The rules for teams are as follows: add a sensor to the list, get 1 point repeat an answer, lose 1 point do not answer in five seconds, lose a turn do not have an answer to contribute, may pass Note: You may reward the winning team with extra credit points, if desired. Next, students will engage in an activity to see sensors in action. Create 3 groups of students: Group 1 \u2013 Triggers (3 students) Provide each Trigger a different colored flag (for example: Pink , Purple , Green ). The teacher will call out a color, at random, and the Trigger assigned to that color will raise his or her flag. Each flag corresponds to a research question of interest. Pink \u2013 Who is in our class? Purple \u2013 What is on our classroom walls? Green \u2013 What do we like to do after school? Group 2 \u2013 Sensors (2 students) Each Sensor should be assigned to one Trigger, or colored flag ( Pink = Sensor A, Purple = Sensor B, Green = no sensor assigned to it). When the Sensors see their assigned Trigger, they send a signal to the Collector (see below) telling him or her to collect data from another student in the class. The Sensors are basically gobetweens for the Triggers and the Collector. Group 3 \u2013 Collector (1 student) One student is the Collector of all the data. The Collector is in charge of asking survey questions related to the research question of the original Trigger. Survey questions are provided here: Survey questions related to the Pink trigger: (1) How did you get to school today (bus, car, walking, etc.)? (2) What size shoe do you wear? (3) What is your favorite pizza topping? Survey questions related to the Purple trigger: (1) What is your favorite wall decoration? (2) What type of poster is it (motivational, reference, class work, etc.)? (3) What color is most prevalent in the poster? Survey questions related to the Green trigger: (1) Do you have a sports team practice or club meeting today? (2) Are you hanging out with friends today after school? (3) Will you be working today after school? Each time a sensor is active, the Collector must ask a new student in the class the appropriate survey questions. Explain the activity to your students. Then, call out a flag color at random. Repeat several times. Make sure you call out the flag that has no assignment at least once so that students see that no action took place. Reflect on the activity with the following discussion questions: What data were missed? Why? Data about what our class likes to do after school. They were missed because there was no Sensor connected to the Green trigger, so the Collector never knew to collect this type of data. Grocery stores keep track of customer data when purchases are made with a loyalty card. What is the trigger in this case? What data are being collected? The trigger is checking out at a grocery store. There are lots of data that are collected, including: items bought, cost of items, number of items on sale, etc. After they engage in the sensor activity, ask students to revisit their definition of a sensor (see Step 3). Have them revise their definition based on the following concepts: A sensor is a converter that measures a physical quantity and converts it into a signal, which can be read by an observer or by an instrument. A sensor collects data continuously, or whenever a trigger is activated. A trigger is a something that responds to an event so that an action can occur. Sensors collect data according to an algorithm . An algorithm is a process or set of rules that are followed (just like the rules followed during the activity). Sensors may also collect data automatically, without anyone\u2019s knowledge or input. Examples include GPS location, time, and date.","title":"Lesson:"},{"location":"unit3/lesson15/#class-scribes","text":"One team of students will give a brief talk to discuss what they think the 3 most important topics of the day were.","title":"Class Scribes:"},{"location":"unit3/lesson15/#homework","text":"Now that students learned what sensors are, ask them what data they would they like to see collected on a sensor that they couldn't collect in an experiment or survey. They must explain why it is difficult to collect that data in an experiment or survey, and how a sensor would make it easier to collect that data.","title":"Homework"},{"location":"unit3/lesson16/","text":"Lesson 16: Does It Have a Trigger? Objective: Students will learn to identify and categorize survey questions versus sensor questions, and will practice writing sensor questions. Materials: Poster paper (one per student team) Sticky notes Sensor or Survey? handout ( LMR_3.14_Sensor or Survey ) Vocabulary: Participatory Sensing Essential Concepts: Essential Concepts: A key feature that distinguishes the way sensors collect data from more traditional approaches is that sensors collect data when a 'trigger' event occurs. In Participatory Sensing, this event is something we humans agree upon beforehand. Every time that trigger happens, we collect data. Lesson: Refer back to the list of sensors the class created during the previous lesson. Distribute a piece of poster paper to each student team, and have them create the following table: Assign each student team 3 sensors from the class\u2019s list. Then, each team should complete the table using their knowledge of triggers discussed during the previous lesson. Remind students that when a trigger occurs, a sensor reacts to it and sends a signal to a data collector. Conduct a Gallery Walk of the posters. Each team will get to write one reaction or question about what they see on each poster. After the Gallery Walk, ask each team to return to their posters. If the posters include questions, have teams take turns responding to the questions. Quickwrite : In their DS Journals, ask student to respond to the following questions. They will have two minutes to write as much as they can: a. When you learned about survey questions, what were the two categories of questions you learned about? Answer: Open-ended and Closed-ended are the categories. b. What are some examples of these types of questions? Open-ended: write a paragraph, comments, essays, write a sentence, single answer. Closed-ended: multiple or single choice, yes/no, scales (e.g. 1-5), choose from a list, check a box. In teams, ask students to share their responses using the Give One/Get One strategy. You may use a timer to keep track of time. Remind students that one of the most important things they learned about sensors is that there is a trigger that reminds either a device or a person to answer a question or to collect data. For this class, students have already had experience with using sensors as a data collection tool \u2013 all the Participatory Sensing campaigns. Explain that survey questions are asked in Participatory Sensing campaigns. There is no difference in the type of questions that are asked when collecting data via surveys and when collecting data via PS campaigns. When deciding whether to use a survey or a PS campaign for data collection, we have to look at the research question of interest. Some questions are better answered with survey data, while others with PS campaigns. Research questions that include variation across time or across locations are good candidates for PS. Some questions might be answered by both. For example: Consider the research question: How does my sense of safety and security change as I go about my daily routine? This question would best be answered via a PS campaign because students could collect data in real time about their sense of security. A possible trigger could be \"whenever you change locations\" or \"once at the start of every hour\" or perhaps whenever a random alarm goes off. Consider the research question: What proportion of high school students are superstitious? This question could be done with a survey based on a random sample from the population of all high school students. Distribute the Sensor or Survey? ( LMR_3.14 ) handout. In teams, students will determine whether a sensor or survey is better for a given research scenario. LMR_3.14 Once the teams have completed the handout, assign each team one research scenario from the Sensor or Survey activity. Conduct a Whip Around and have each team share their responses with the class. Allow students time to revise any incorrect responses. Summarize the lesson by highlighting that PS campaigns and surveys use similar questions. However, depending on the research topic of interest, the decision to use one or the other relies on whether or not a trigger is involved. Class Scribes: One team of students will give a brief talk to discuss what they think the 3 most important topics of the day were. Homework Suppose we wish to know more about whether people behave superstitiously. Write two research scenarios, using the following questions as a guide: a. How would you collect data to address this using PS? Include the trigger event you would use, and the data you would like to collect when the trigger happens. b. How would you collect data to collect this using a survey based on a random sample of people in California? c. Describe the differences between these two approaches. What can you learn in one approach that you can't in the other?","title":"Lesson 16: Does It Have a Trigger?"},{"location":"unit3/lesson16/#lesson-16-does-it-have-a-trigger","text":"","title":"Lesson 16: Does It Have a Trigger?"},{"location":"unit3/lesson16/#objective","text":"Students will learn to identify and categorize survey questions versus sensor questions, and will practice writing sensor questions.","title":"Objective:"},{"location":"unit3/lesson16/#materials","text":"Poster paper (one per student team) Sticky notes Sensor or Survey? handout ( LMR_3.14_Sensor or Survey )","title":"Materials:"},{"location":"unit3/lesson16/#vocabulary","text":"Participatory Sensing","title":"Vocabulary:"},{"location":"unit3/lesson16/#essential-concepts","text":"Essential Concepts: A key feature that distinguishes the way sensors collect data from more traditional approaches is that sensors collect data when a 'trigger' event occurs. In Participatory Sensing, this event is something we humans agree upon beforehand. Every time that trigger happens, we collect data.","title":"Essential Concepts:"},{"location":"unit3/lesson16/#lesson","text":"Refer back to the list of sensors the class created during the previous lesson. Distribute a piece of poster paper to each student team, and have them create the following table: Assign each student team 3 sensors from the class\u2019s list. Then, each team should complete the table using their knowledge of triggers discussed during the previous lesson. Remind students that when a trigger occurs, a sensor reacts to it and sends a signal to a data collector. Conduct a Gallery Walk of the posters. Each team will get to write one reaction or question about what they see on each poster. After the Gallery Walk, ask each team to return to their posters. If the posters include questions, have teams take turns responding to the questions. Quickwrite : In their DS Journals, ask student to respond to the following questions. They will have two minutes to write as much as they can: a. When you learned about survey questions, what were the two categories of questions you learned about? Answer: Open-ended and Closed-ended are the categories. b. What are some examples of these types of questions? Open-ended: write a paragraph, comments, essays, write a sentence, single answer. Closed-ended: multiple or single choice, yes/no, scales (e.g. 1-5), choose from a list, check a box. In teams, ask students to share their responses using the Give One/Get One strategy. You may use a timer to keep track of time. Remind students that one of the most important things they learned about sensors is that there is a trigger that reminds either a device or a person to answer a question or to collect data. For this class, students have already had experience with using sensors as a data collection tool \u2013 all the Participatory Sensing campaigns. Explain that survey questions are asked in Participatory Sensing campaigns. There is no difference in the type of questions that are asked when collecting data via surveys and when collecting data via PS campaigns. When deciding whether to use a survey or a PS campaign for data collection, we have to look at the research question of interest. Some questions are better answered with survey data, while others with PS campaigns. Research questions that include variation across time or across locations are good candidates for PS. Some questions might be answered by both. For example: Consider the research question: How does my sense of safety and security change as I go about my daily routine? This question would best be answered via a PS campaign because students could collect data in real time about their sense of security. A possible trigger could be \"whenever you change locations\" or \"once at the start of every hour\" or perhaps whenever a random alarm goes off. Consider the research question: What proportion of high school students are superstitious? This question could be done with a survey based on a random sample from the population of all high school students. Distribute the Sensor or Survey? ( LMR_3.14 ) handout. In teams, students will determine whether a sensor or survey is better for a given research scenario. LMR_3.14 Once the teams have completed the handout, assign each team one research scenario from the Sensor or Survey activity. Conduct a Whip Around and have each team share their responses with the class. Allow students time to revise any incorrect responses. Summarize the lesson by highlighting that PS campaigns and surveys use similar questions. However, depending on the research topic of interest, the decision to use one or the other relies on whether or not a trigger is involved.","title":"Lesson:"},{"location":"unit3/lesson16/#class-scribes","text":"One team of students will give a brief talk to discuss what they think the 3 most important topics of the day were.","title":"Class Scribes:"},{"location":"unit3/lesson16/#homework","text":"Suppose we wish to know more about whether people behave superstitiously. Write two research scenarios, using the following questions as a guide: a. How would you collect data to address this using PS? Include the trigger event you would use, and the data you would like to collect when the trigger happens. b. How would you collect data to collect this using a survey based on a random sample of people in California? c. Describe the differences between these two approaches. What can you learn in one approach that you can't in the other?","title":"Homework"},{"location":"unit3/lesson17/","text":"Lesson 17: Creating Our Own Participatory Sensing Campaign Objective: Students will be guided through the creation of a new Participatory Sensing campaign and survey on a topic of interest chosen by the class. Materials: Food Habits Campaign Questions handout ( LMR_3.15_Food Habits Qs ) Campaign Creation Brainstorm handout ( LMR_3.16_Campaign Creation ) Essential Concepts: Essential Concepts: Creating a Participatory Sensing Campaign requires that survey questions must be completed whenever they are \u201ctriggered\u201d. Research questions provide an overall direction in a Participatory Sensing Campaign. Lesson: Review homework questions by asking a couple of students to share their responses. The rest of students will engage in Agree/Disagree as the questions are shared. Display the following definition of Participatory Sensing that some computer scientists have agreed to and ask students to read and record this definition in their DS journals: At its heart, Participatory Sensing is data collection and interpretation. Participatory Sensing emphasizes the involvement of citizens and community groups in the process of sensing and documenting where they live, work, and play. It can range from private personal observations to the combination of data from hundreds, or even thousands, of individuals that reveals patterns across an entire city. Most important, Participatory Sensing begins and ends with people, both as individuals and members of communities. The type of information collected, how it is organized, and how it is ultimately used, may be determined in a traditional manner by a centrally organized body, or in a deliberative manner by the collection of participants themselves. The latter case, in particular, emphasizes the novelty of Participatory Sensing as an approach and underscores the importance of using widely available and familiar technology. [Source: \"Participatory Sensing: A citizen-powered approach to illuminating the patterns that shape our world.\"] Activate prior knowledge: Based on this definition, ask students to recall the Participatory Sensing campaigns in which they have engaged thus far. Answer: Food Habits, Time Use, Stress/Chill. Note: Personality Color and Time Perception were surveys, not Participatory Sensing campaigns because they were only completed once. Their data was not collected over time. Inform students that they will be creating a new, whole class Participatory Sensing campaign, but before they do that, they will analyze the Food Habits Campaign Questions handout ( LMR_3.15 ). LMR_3.15 In teams, allow students two minutes to discuss the following as they analyze the Food Habits Campaign questions: How many questions does the campaign have and what do they notice about the questions? Answers will vary. Students may notice that they are survey type of questions and may identify the type of questions such as open-ended, single-choice, etc. When do these questions need to be answered? Each time they eat a snack. Who collects the data for this campaign? The participants collect their own data. Ask a few teams to share their insights about the discussion. In the share-out, guide students to see that the questions are in fact survey questions. Although survey questions are answered once, when we collect data every time a 'trigger' event occurs, then we are engaging in Participatory Sensing. Ensure that team roles have defined duties to keep teams on task for the rest of this lesson. Creating this class campaign will follow a process in which consensus (or a majority rule) will be reached in each step of the campaign development within each team. Inform students that they will be creating a Participatory Sensing campaign on a topic of their interest using LMR_3.16 . LMR_3.16 Round 1: First, teams will discuss their hobbies, areas of interest, or places or processes they want to know more about. Prompt students to think about whether they want to learn about \"where they live, work, or play.\" All students within the group must agree on a hobby or area of interest to be their topic of interest to create a campaign for. An example of a hobby is practicing cello. An area of interest might be 'the environment.' A place of interest might be \"our school\" or \"my church\" or \"Disneyland.\" Once teams have decided on a topic for their group, have teams share out their topic of interest. As a class, decide on one topic that will be used for creating a new Campaign. Round 2: Now have teams consider what research questions you might ask about this topic of interest. An example of a research question for practicing cello is \u201cHow can I improve my playing?\u201d or \"How can I practice more effectively?\" Once teams have decided on a research question for their group, have teams share out their research question. As a class, decide on one research question that will be used for creating a new Campaign. Round 3: Next, they will examine what kind of data needs to be collected in order to answer this research question. They should discuss possible triggers that will determine when data should be collected. Allow teams to engage in a discussion about when is the best time to trigger the data collection/completion of the survey. For example: every day at 8am; whenever they practice the cello; whenever they see an advertisement; etc. They should record it in their DS Journals. An example of a trigger for practicing cello is whenever you play the cello. In this case, it could be any time of day or even multiple times of the day. Once teams have decided on a trigger for their group, have teams share out their possible trigger. As a class, decide on one trigger that will be used for creating a new Campaign. Class Scribes: One team of students will give a brief talk to discuss what they think the 3 most important topics of the day were. Homework Using the class topic, research question, trigger event, and discussion of the data they plan to collect. Classify our class campaign under the appropriate category with your justification: (A) Individual; (B) Groups of people; (C) Community Note to teacher: To determine which category a campaign should be placed under, consider the question \"Who or what will we learn about?\" If the answer is \"only one person\", then place in the Individual category. The cello campaign is an example of this. If we might learn about lots of people, put it in the Groups of People category. The Food Habits, Stress/Chill, and Time Use campaigns fit into this category (they learn about the students in the class). A campaign that wanted to know where all of the churches in the neighborhood were located, or wanted to try to keep people from littering, or wasting water, these should go into the \"community\" category.","title":"Lesson 17: Creating Our Own Participatory Sensing Campaign"},{"location":"unit3/lesson17/#lesson-17-creating-our-own-participatory-sensing-campaign","text":"","title":"Lesson 17: Creating Our Own Participatory Sensing Campaign"},{"location":"unit3/lesson17/#objective","text":"Students will be guided through the creation of a new Participatory Sensing campaign and survey on a topic of interest chosen by the class.","title":"Objective:"},{"location":"unit3/lesson17/#materials","text":"Food Habits Campaign Questions handout ( LMR_3.15_Food Habits Qs ) Campaign Creation Brainstorm handout ( LMR_3.16_Campaign Creation )","title":"Materials:"},{"location":"unit3/lesson17/#essential-concepts","text":"Essential Concepts: Creating a Participatory Sensing Campaign requires that survey questions must be completed whenever they are \u201ctriggered\u201d. Research questions provide an overall direction in a Participatory Sensing Campaign.","title":"Essential Concepts:"},{"location":"unit3/lesson17/#lesson","text":"Review homework questions by asking a couple of students to share their responses. The rest of students will engage in Agree/Disagree as the questions are shared. Display the following definition of Participatory Sensing that some computer scientists have agreed to and ask students to read and record this definition in their DS journals: At its heart, Participatory Sensing is data collection and interpretation. Participatory Sensing emphasizes the involvement of citizens and community groups in the process of sensing and documenting where they live, work, and play. It can range from private personal observations to the combination of data from hundreds, or even thousands, of individuals that reveals patterns across an entire city. Most important, Participatory Sensing begins and ends with people, both as individuals and members of communities. The type of information collected, how it is organized, and how it is ultimately used, may be determined in a traditional manner by a centrally organized body, or in a deliberative manner by the collection of participants themselves. The latter case, in particular, emphasizes the novelty of Participatory Sensing as an approach and underscores the importance of using widely available and familiar technology. [Source: \"Participatory Sensing: A citizen-powered approach to illuminating the patterns that shape our world.\"] Activate prior knowledge: Based on this definition, ask students to recall the Participatory Sensing campaigns in which they have engaged thus far. Answer: Food Habits, Time Use, Stress/Chill. Note: Personality Color and Time Perception were surveys, not Participatory Sensing campaigns because they were only completed once. Their data was not collected over time. Inform students that they will be creating a new, whole class Participatory Sensing campaign, but before they do that, they will analyze the Food Habits Campaign Questions handout ( LMR_3.15 ). LMR_3.15 In teams, allow students two minutes to discuss the following as they analyze the Food Habits Campaign questions: How many questions does the campaign have and what do they notice about the questions? Answers will vary. Students may notice that they are survey type of questions and may identify the type of questions such as open-ended, single-choice, etc. When do these questions need to be answered? Each time they eat a snack. Who collects the data for this campaign? The participants collect their own data. Ask a few teams to share their insights about the discussion. In the share-out, guide students to see that the questions are in fact survey questions. Although survey questions are answered once, when we collect data every time a 'trigger' event occurs, then we are engaging in Participatory Sensing. Ensure that team roles have defined duties to keep teams on task for the rest of this lesson. Creating this class campaign will follow a process in which consensus (or a majority rule) will be reached in each step of the campaign development within each team. Inform students that they will be creating a Participatory Sensing campaign on a topic of their interest using LMR_3.16 . LMR_3.16 Round 1: First, teams will discuss their hobbies, areas of interest, or places or processes they want to know more about. Prompt students to think about whether they want to learn about \"where they live, work, or play.\" All students within the group must agree on a hobby or area of interest to be their topic of interest to create a campaign for. An example of a hobby is practicing cello. An area of interest might be 'the environment.' A place of interest might be \"our school\" or \"my church\" or \"Disneyland.\" Once teams have decided on a topic for their group, have teams share out their topic of interest. As a class, decide on one topic that will be used for creating a new Campaign. Round 2: Now have teams consider what research questions you might ask about this topic of interest. An example of a research question for practicing cello is \u201cHow can I improve my playing?\u201d or \"How can I practice more effectively?\" Once teams have decided on a research question for their group, have teams share out their research question. As a class, decide on one research question that will be used for creating a new Campaign. Round 3: Next, they will examine what kind of data needs to be collected in order to answer this research question. They should discuss possible triggers that will determine when data should be collected. Allow teams to engage in a discussion about when is the best time to trigger the data collection/completion of the survey. For example: every day at 8am; whenever they practice the cello; whenever they see an advertisement; etc. They should record it in their DS Journals. An example of a trigger for practicing cello is whenever you play the cello. In this case, it could be any time of day or even multiple times of the day. Once teams have decided on a trigger for their group, have teams share out their possible trigger. As a class, decide on one trigger that will be used for creating a new Campaign.","title":"Lesson:"},{"location":"unit3/lesson17/#class-scribes","text":"One team of students will give a brief talk to discuss what they think the 3 most important topics of the day were.","title":"Class Scribes:"},{"location":"unit3/lesson17/#homework","text":"Using the class topic, research question, trigger event, and discussion of the data they plan to collect. Classify our class campaign under the appropriate category with your justification: (A) Individual; (B) Groups of people; (C) Community Note to teacher: To determine which category a campaign should be placed under, consider the question \"Who or what will we learn about?\" If the answer is \"only one person\", then place in the Individual category. The cello campaign is an example of this. If we might learn about lots of people, put it in the Groups of People category. The Food Habits, Stress/Chill, and Time Use campaigns fit into this category (they learn about the students in the class). A campaign that wanted to know where all of the churches in the neighborhood were located, or wanted to try to keep people from littering, or wasting water, these should go into the \"community\" category.","title":"Homework"},{"location":"unit3/lesson18/","text":"Lesson 18: Evaluating Our Own Participatory Sensing Campaign Objective: Students will create statistical questions and evaluate their Participatory Sensing Campaign. Materials: Campaign Creation handout ( LMR_3.16_Campaign Creation ) from previous lesson Class Campaign Information from Lesson 16 Essential Concepts: Essential Concepts: Statistical investigative questions guide a Participatory Sensing Campaign so that we can learn about a community or ourselves. These campaigns should be evaluated before implementing to make sure they are reasonable and ethically sound. Lesson: Review homework by giving students about five minutes to share their classifications in their teams. They will decide as a team which classification is the most fitting. Once the five minutes have passed, have a class discussion of classifications and their justifications. Explain to the class that the campaign must be carried out by the whole class so if it has been classified in the Individual category, it must be revised. Also discuss whether the campaign is feasible. (For example, is the trigger so rare that no one will collect data? Are the questions too intrusive?). Inform students that one of the promises of PS is its potential for helping people bring about social and civic change. Ask teams to consider the following questions and report back: Does our campaign try to do this? Could it be changed or modified to do this? Note: Feasible campaigns fall under the groups of people or community categories. If a campaign is in the individual category, it should be modified to fall under the other categories before moving to round 4. Display the campaign information students generated (and selected as a class) the previous day or revised today: Topic, Research question, Trigger, and Type of Data needed. Now they will continue the rounds using the Campaign Creation handout LMR 3.16 from the previous lesson. Round 4: Now that the class has decided on a trigger and the type of data needed, they will create survey questions to ask when the trigger is set. The questions should consider all of the possible data they might collect at this trigger event. It's ok if the list is long; the goal is to be creative and think of lots of different ideas. Examples of survey questions for practicing cello are: \u201cHow long did you practice?\u201d \u201cWhat did you play?\u201d \u201cHow would you rate your practice session: 1 to 5?\u201d \u201cAny thoughts or comments about your practice?\u201d Once teams have created 4 survey questions for their group, have teams share out their survey questions. As a class, decide on no more than 10 survey questions that will be used for creating a new Campaign. Then, evaluate each survey question. For each question they should consider: What type of data will this question collect? (Numerical, discrete numerical, text, categories, photos, location). How does this question help address the research question? Does the question need to be reworded? (Is it clear what is being asked for? Do they know how to answer it?) If the survey questions need to be rewritten, assign teams to rewrite survey questions. Then, as a class, decide on the changes. Once finalized, write the survey question that goes along with that data variable, being cognizant of question bias. Round 5: In teams, now generate two to three statistical questions that they might answer with these data. Make sure your statistical questions are interesting and relevant to the class topic of interest. They may keep a record in their DS Journals. Remind students that they will also have data about the date, time, and place of data collection. Examples of statistical questions that can be answered for practicing cello are: \u201cHow frequently do I practice?\u201d \u201cWhen I practice more frequently, do I rate my sessions higher?\u201d \u201cAre higher-rated sessions associated with time of day?\u201d Once teams have generated their statistical questions, have them share out with the class. Confirm that the questions are statistical and that they can be answered with the data the students propose to collect. As a class, decide on no more than 3 statistical questions to guide your campaign. Now that they have all the pieces of the campaign, evaluate whether it\u2019s a reasonable and ethically sound campaign. Engage the class in a whole group discussion on the following questions: Are answers to your survey questions likely to vary when the trigger occurs? (If not, you'll get bored entering the same data again and again) Can the entire class carry out the campaign? Do triggers occur so rarely that you'll have very little data? Do they occur so often that you'll get frustrated entering too much data? Ethics: Would sharing these data with strangers or friends be embarrassing or undermine someone's privacy? Can you change your trigger or survey questions to improve your evaluation? Will you be able to gather enough relevant data from your survey questions to be able to answer your statistical questions? Students have collaboratively created their first Participatory Sensing campaign. Inform them that you will be demonstrating one tool used to create the campaigns that they see on their smart devices or the computer. Students should take notes in their DS journals, as they will be using the tool later. Class Scribes: One team of students will give a brief talk to discuss what they think the 3 most important topics of the day were.","title":"Lesson 18: Evaluating Our Own Participatory Sensing Campaign"},{"location":"unit3/lesson18/#lesson-18-evaluating-our-own-participatory-sensing-campaign","text":"","title":"Lesson 18: Evaluating Our Own Participatory Sensing Campaign"},{"location":"unit3/lesson18/#objective","text":"Students will create statistical questions and evaluate their Participatory Sensing Campaign.","title":"Objective:"},{"location":"unit3/lesson18/#materials","text":"Campaign Creation handout ( LMR_3.16_Campaign Creation ) from previous lesson Class Campaign Information from Lesson 16","title":"Materials:"},{"location":"unit3/lesson18/#essential-concepts","text":"Essential Concepts: Statistical investigative questions guide a Participatory Sensing Campaign so that we can learn about a community or ourselves. These campaigns should be evaluated before implementing to make sure they are reasonable and ethically sound.","title":"Essential Concepts:"},{"location":"unit3/lesson18/#lesson","text":"Review homework by giving students about five minutes to share their classifications in their teams. They will decide as a team which classification is the most fitting. Once the five minutes have passed, have a class discussion of classifications and their justifications. Explain to the class that the campaign must be carried out by the whole class so if it has been classified in the Individual category, it must be revised. Also discuss whether the campaign is feasible. (For example, is the trigger so rare that no one will collect data? Are the questions too intrusive?). Inform students that one of the promises of PS is its potential for helping people bring about social and civic change. Ask teams to consider the following questions and report back: Does our campaign try to do this? Could it be changed or modified to do this? Note: Feasible campaigns fall under the groups of people or community categories. If a campaign is in the individual category, it should be modified to fall under the other categories before moving to round 4. Display the campaign information students generated (and selected as a class) the previous day or revised today: Topic, Research question, Trigger, and Type of Data needed. Now they will continue the rounds using the Campaign Creation handout LMR 3.16 from the previous lesson. Round 4: Now that the class has decided on a trigger and the type of data needed, they will create survey questions to ask when the trigger is set. The questions should consider all of the possible data they might collect at this trigger event. It's ok if the list is long; the goal is to be creative and think of lots of different ideas. Examples of survey questions for practicing cello are: \u201cHow long did you practice?\u201d \u201cWhat did you play?\u201d \u201cHow would you rate your practice session: 1 to 5?\u201d \u201cAny thoughts or comments about your practice?\u201d Once teams have created 4 survey questions for their group, have teams share out their survey questions. As a class, decide on no more than 10 survey questions that will be used for creating a new Campaign. Then, evaluate each survey question. For each question they should consider: What type of data will this question collect? (Numerical, discrete numerical, text, categories, photos, location). How does this question help address the research question? Does the question need to be reworded? (Is it clear what is being asked for? Do they know how to answer it?) If the survey questions need to be rewritten, assign teams to rewrite survey questions. Then, as a class, decide on the changes. Once finalized, write the survey question that goes along with that data variable, being cognizant of question bias. Round 5: In teams, now generate two to three statistical questions that they might answer with these data. Make sure your statistical questions are interesting and relevant to the class topic of interest. They may keep a record in their DS Journals. Remind students that they will also have data about the date, time, and place of data collection. Examples of statistical questions that can be answered for practicing cello are: \u201cHow frequently do I practice?\u201d \u201cWhen I practice more frequently, do I rate my sessions higher?\u201d \u201cAre higher-rated sessions associated with time of day?\u201d Once teams have generated their statistical questions, have them share out with the class. Confirm that the questions are statistical and that they can be answered with the data the students propose to collect. As a class, decide on no more than 3 statistical questions to guide your campaign. Now that they have all the pieces of the campaign, evaluate whether it\u2019s a reasonable and ethically sound campaign. Engage the class in a whole group discussion on the following questions: Are answers to your survey questions likely to vary when the trigger occurs? (If not, you'll get bored entering the same data again and again) Can the entire class carry out the campaign? Do triggers occur so rarely that you'll have very little data? Do they occur so often that you'll get frustrated entering too much data? Ethics: Would sharing these data with strangers or friends be embarrassing or undermine someone's privacy? Can you change your trigger or survey questions to improve your evaluation? Will you be able to gather enough relevant data from your survey questions to be able to answer your statistical questions? Students have collaboratively created their first Participatory Sensing campaign. Inform them that you will be demonstrating one tool used to create the campaigns that they see on their smart devices or the computer. Students should take notes in their DS journals, as they will be using the tool later.","title":"Lesson:"},{"location":"unit3/lesson18/#class-scribes","text":"One team of students will give a brief talk to discuss what they think the 3 most important topics of the day were.","title":"Class Scribes:"},{"location":"unit3/lesson19/","text":"Lesson 19: Implementing Our Own Participatory Sensing Campaign Objective: Students will mock-implement, create their Participatory Sensing campaign, survey on their topic of interest, then begin data collection. Materials: Campaign Creation handout ( LMR_3.16_Campaign Creation ) from previous lesson Campaign Authoring Tool ( https://portal.idsucla.org ) Video showing how to create a Campaign found here . Essential Concepts: Essential Concepts: Practicing data collection prior to implementation allows optimization of a Participatory Sensing Campaign. Lesson: Display the class generated campaign information for the class to clearly see. In teams, have students mock-implement the campaign they have created. They can do this by asking each other the survey questions to make sure they make sense/ will generate relevant data to their research question and statistical questions. They can use the evaluative questions from Lesson 17 step #10. If there are suggestions for improvement, have teams propose them to the class and make final changes to the campaign. Inform students that you will now demonstrate the tool used to create the campaigns that is displayed on their mobile devices or computers. Login to the IDS Home Page found at https://portal.idsucla.org . Click on the Campaigns tab on the navigation bar at the top of the page. Then, follow the steps in the tool: Campaign Info Window: Campaign Name: Give your campaign a name. A name related to the topic is recommended. Select your class/period. Description: Provide a one-sentence description of your campaign. Data Sharing: Select Disabled in order to monitor for improper responses. Campaign Status: Select Running. Click the | +Add Survey | button. Survey Window: Title: Give the survey a title (again, it may or may not be the same as the campaign name). Users see the title and the all the prompts that follow. ID: Give the survey a name (it may or may not be the same as the campaign name). Users do not see the survey ID. Description: Provide a short description of the survey for display. Submit Text: Provide a brief message to be displayed after survey submission. Anytime: Select the checkbox if you want the survey to be available at anytime. Click the | +Add Prompt | button and select the prompt type for your first survey question. Note: You should only select from the following choices: Single choice, number, photo, and text. Multiple-choice does not mean select one choice; it means select many choices. It is not recommended that multiple-choice be used at this point. Prompt Information: Click the new prompt bar . Prompt ID: This will be your first variable. A short one-word name or short two-word name separated by an underscore is recommended. Prompt Label: This is the variable name that will be displayed (it may be the same as the prompt ID without the underscore, if used). Question Text: Type the survey question about which you want to collect data. Additional Prompt Information: Depending on the prompt type, you will be asked to enter additional information. For example, if your prompt is Text, you will be asked a minimum and a maximum value for the number of characters the participant can enter. Skippable: Select the checkbox if you would like the prompt to be skipped. It is recommended that photo prompts be skippable, since some users will submit their responses via a browser. Repeat step c for the remaining survey questions by clicking the | +Add Prompt | button. XML Code: As you create the campaign, the code that creates it will be displayed. You may select the checkbox titled Enable Syntax Highlighting so that students can keep track of where the information you are adding is embedded in the code. Inform students that they will be learning about XML syntax in the next several lessons. Click the | Submit Campaign | button on the top, right hand side of the page once all prompts have been added. This action will send the campaign to the server for users to see. Once all prompts have been created, students may use their smart devices or login to the IDS Home Page to view the new campaign. Remember to Refresh Campaigns . Students should go through the entire participatory sensing survey to see how their questions are displayed. They do not have to upload the survey. Class Scribes: One team of students will give a brief talk to discuss what they think the 3 most important topics of the day were. Homework For the next 5 days, students will collect data using their newly created Participatory Sensing campaign.","title":"Lesson 19: Implementing Our Own Participatory Sensing Campaign"},{"location":"unit3/lesson19/#lesson-19-implementing-our-own-participatory-sensing-campaign","text":"","title":"Lesson 19: Implementing Our Own Participatory Sensing Campaign"},{"location":"unit3/lesson19/#objective","text":"Students will mock-implement, create their Participatory Sensing campaign, survey on their topic of interest, then begin data collection.","title":"Objective:"},{"location":"unit3/lesson19/#materials","text":"Campaign Creation handout ( LMR_3.16_Campaign Creation ) from previous lesson Campaign Authoring Tool ( https://portal.idsucla.org ) Video showing how to create a Campaign found here .","title":"Materials:"},{"location":"unit3/lesson19/#essential-concepts","text":"Essential Concepts: Practicing data collection prior to implementation allows optimization of a Participatory Sensing Campaign.","title":"Essential Concepts:"},{"location":"unit3/lesson19/#lesson","text":"Display the class generated campaign information for the class to clearly see. In teams, have students mock-implement the campaign they have created. They can do this by asking each other the survey questions to make sure they make sense/ will generate relevant data to their research question and statistical questions. They can use the evaluative questions from Lesson 17 step #10. If there are suggestions for improvement, have teams propose them to the class and make final changes to the campaign. Inform students that you will now demonstrate the tool used to create the campaigns that is displayed on their mobile devices or computers. Login to the IDS Home Page found at https://portal.idsucla.org . Click on the Campaigns tab on the navigation bar at the top of the page. Then, follow the steps in the tool: Campaign Info Window: Campaign Name: Give your campaign a name. A name related to the topic is recommended. Select your class/period. Description: Provide a one-sentence description of your campaign. Data Sharing: Select Disabled in order to monitor for improper responses. Campaign Status: Select Running. Click the | +Add Survey | button. Survey Window: Title: Give the survey a title (again, it may or may not be the same as the campaign name). Users see the title and the all the prompts that follow. ID: Give the survey a name (it may or may not be the same as the campaign name). Users do not see the survey ID. Description: Provide a short description of the survey for display. Submit Text: Provide a brief message to be displayed after survey submission. Anytime: Select the checkbox if you want the survey to be available at anytime. Click the | +Add Prompt | button and select the prompt type for your first survey question. Note: You should only select from the following choices: Single choice, number, photo, and text. Multiple-choice does not mean select one choice; it means select many choices. It is not recommended that multiple-choice be used at this point. Prompt Information: Click the new prompt bar . Prompt ID: This will be your first variable. A short one-word name or short two-word name separated by an underscore is recommended. Prompt Label: This is the variable name that will be displayed (it may be the same as the prompt ID without the underscore, if used). Question Text: Type the survey question about which you want to collect data. Additional Prompt Information: Depending on the prompt type, you will be asked to enter additional information. For example, if your prompt is Text, you will be asked a minimum and a maximum value for the number of characters the participant can enter. Skippable: Select the checkbox if you would like the prompt to be skipped. It is recommended that photo prompts be skippable, since some users will submit their responses via a browser. Repeat step c for the remaining survey questions by clicking the | +Add Prompt | button. XML Code: As you create the campaign, the code that creates it will be displayed. You may select the checkbox titled Enable Syntax Highlighting so that students can keep track of where the information you are adding is embedded in the code. Inform students that they will be learning about XML syntax in the next several lessons. Click the | Submit Campaign | button on the top, right hand side of the page once all prompts have been added. This action will send the campaign to the server for users to see. Once all prompts have been created, students may use their smart devices or login to the IDS Home Page to view the new campaign. Remember to Refresh Campaigns . Students should go through the entire participatory sensing survey to see how their questions are displayed. They do not have to upload the survey.","title":"Lesson:"},{"location":"unit3/lesson19/#class-scribes","text":"One team of students will give a brief talk to discuss what they think the 3 most important topics of the day were.","title":"Class Scribes:"},{"location":"unit3/lesson19/#homework","text":"For the next 5 days, students will collect data using their newly created Participatory Sensing campaign.","title":"Homework"},{"location":"unit3/lesson2/","text":"Lesson 2: What is an Experiment? Objective: Students will learn about the elements of an experiment and the meaning of \"causation\". Students will learn to distinguish claims of causation from claims of association. Materials: Video: MythBusters\u2019 Is Yawning Contagious? Note: Please use a search engine (e.g., Google Video) and type \u201cMythBusters Is Yawning Contagious\u201d to find it. The clip is a little over 5 minutes in length. If you cannot access it, an alternate experiment (MythBusters' How Does Music Affect Plants ) can be found at https://youtu.be/C5dNhNfGyWQ Vocabulary: experiment subjects treatment treatment group control group random assignment outcome research question confounding factors Essential Concepts: Essential Concepts: Science is often concerned with the question \"What causes things to happen?\". To answer this, controlled experiments are required. Controlled experiments have several key features: (1) there is a treatment variable and a response variable, and we wish to see if the treatment causes a change that we can measure with the response variable; (2) There is a comparison/control group; (3) Subjects are assigned randomly to treatment or control (randomized assignment); (4) Subjects are not aware of which group they are in (a 'blind'). This may require the use of a placebo for those in the control group; and (5) those who measure the response variable do not know which group the subjects were in (if both 4 and 5 are satisfied, this is a 'double blind' experiment). Lesson: Display the following headlines to students: Stop Global Warming: Become a Pirate Lack of sleep may shrink your brain Early language skills reduce preschool tantrums Dogs walked by men are more aggressive Discuss each headline by asking the following questions: What is the headline implying with its wording? 1a is implying that you can stop global warming by becoming a pirate, 1b is implying that it\u2019s possible to shrink your brain if you aren\u2019t getting enough sleep, 1c is implying that having early language skills will decrease preschool tantrums, 1d is implying that dogs are more aggressive when they\u2019ve been walked by men. Is it implying causation or association? Discuss definitions of causation and association. Causation means there is a cause and effect relationship between variables. For example, heat causes water to boil; whereas association or correlation means that high values of one variable tend to be associated with high values of the other (or high values tend to be with low values). However, this is not necessarily cause-and-effect at play. For example, blanket sales in Canada are associated with brush fires in Australia - not because Canadian blankets cause the fires, but because Canadian winters cause blanket sales, and Canadian winters are Australian summers, which cause fires. 1a, 1c and 1d are implying causation and 1b is implying association. How can you tell the difference between causation and correlation? What words stand out in these headlines? Answers will vary but some terms for causation include: cause, increase/ decrease, benefits, impacts, effect/ affect, etc.; and for correlation include: get, have, linked, more/ less, tied, connected, etc. In 1a, \u201cbecome\u201d stands out; in 1b, \u201cmay\u201d stands out; in 1c, \u201creduce\u201d stands out; in 1d, \u201care\u201d stands out. Change each causal version of a headline into a non-causal version and vice versa. Answers will vary but an example for 1a is to instead say Global Warming linked to increase of pirates. Introduce the MythBusters video clip by answering the following questions, in teams, for their headline \u201cIs Yawning Contagious?\u201d What is the headline implying with its wording? That yawning may cause other people to yawn. Is it implying causation or correlation? How do you know? Causation because \u201ccontagious\u201d yawns means that you are yawning because someone else has yawned. How can we determine if this is true? Split the class into groups and have each team come up with a way to determine if this is true. Each group should assume that they get to examine 50 people. Show the MythBusters video clip called Is Yawning Contagious? The links to the clips can be found in the Materials above. Focus students on the following guiding questions and ask them to take notes as they watch the video clip: How did the MythBusters design the investigation? What steps did they take? How is this different than your team\u2019s headline responses? After viewing the clip, inform students that the MythBusters have just conducted an experiment , which is one method of data collection. We begin with a brief introduction into \u201cwhat is an experiment\u201d but the definition will be developed over the next several lessons. Guide students to identify the elements of an experiment by referring back to the video clip: Research Question \u2014 the question to be answered by the experiment ( Is Yawning Contagious? ) Subjects \u2013 people or objects that are participating in the experiment ( the 50 adults ) Treatment \u2013 the variable that is deliberately manipulated to investigate its influence on the outcome; this is sometimes known as the explanatory, or independent, variable ( Kari yawned before subject entered the room ) Treatment group \u2013 the group of subjects that receive the treatment ( two out of every three subjects who were placed into rooms \u2013 yawn from Kari ) Control group \u2013 the group that does not receive a treatment ( one out of every three subjects who were placed into rooms \u2013 no yawn from Kari ) Random assignment \u2013 subjects are randomly assigned to either the treatment or control group ( two out of every three subjects received the treatment - in this experiment, random assignment was not used (or if it was, we were not told so) ) Outcome \u2013 the variable that the treatment is meant to influence; this is sometimes known as the response, or dependent, variable ( whether or not a person yawned ) Statistic \u2014 a method for comparing the outcomes of the control and treatment groups (this is needed) In this case, the MythBusters used the difference between the percent of subjects that yawned in the treatment group, which was 4% higher than the control group. Note: In this experiment, and in those found in the IDS curriculum, we use a treatment and a control group. However, a control group is not a necessary element of an experiment. Sometimes it is more appropriate to have two treatment groups with no control group (e.g., medical professionals testing different doses of drugs). The effect that is being studied will dictate whether to feature a control group or not. Display the following questions on the board or projector. Using T-I-P-S , ask students to discuss them. Why did the MythBusters follow all of these steps to design their experiment? In order to determine if watching someone yawn can cause you to yawn. We don't know how MythBusters chose who would be in the treatment group and who would be in the control group. Suppose that the people who showed up first, early in the morning, were assigned to the treatment group, and the last few people, later in the day, ended up in the control group. Would you believe in the conclusions? No, because the two groups were different. The first group might have been sleepier, and so more likely to yawn anyways. Explain that this --another explanation for the cause-and-effect-- is called a confounding variable. Explain that in order to make the two groups as similar as possible, experimenters usually assign subjects randomly. How might we randomly assign about half of the subjects to the treatment and half to the control? We might flip a coin, and those who get Heads go to Treatment. Why would random assignment improve the MythBusters experiment? Because then the two groups would be more similar, so we wouldn't have a confounding variable to worry about. Emphasize that without random assignment, we cannot determine causation because we are not comparing two similar groups. Class Scribes: One team of students will give a brief talk to discuss what they think the 3 most important topics of the day were.","title":"Lesson 2: What Is an Experiment?"},{"location":"unit3/lesson2/#lesson-2-what-is-an-experiment","text":"","title":"Lesson 2: What is an Experiment?"},{"location":"unit3/lesson2/#objective","text":"Students will learn about the elements of an experiment and the meaning of \"causation\". Students will learn to distinguish claims of causation from claims of association.","title":"Objective:"},{"location":"unit3/lesson2/#materials","text":"Video: MythBusters\u2019 Is Yawning Contagious? Note: Please use a search engine (e.g., Google Video) and type \u201cMythBusters Is Yawning Contagious\u201d to find it. The clip is a little over 5 minutes in length. If you cannot access it, an alternate experiment (MythBusters' How Does Music Affect Plants ) can be found at https://youtu.be/C5dNhNfGyWQ","title":"Materials:"},{"location":"unit3/lesson2/#vocabulary","text":"experiment subjects treatment treatment group control group random assignment outcome research question confounding factors","title":"Vocabulary:"},{"location":"unit3/lesson2/#essential-concepts","text":"Essential Concepts: Science is often concerned with the question \"What causes things to happen?\". To answer this, controlled experiments are required. Controlled experiments have several key features: (1) there is a treatment variable and a response variable, and we wish to see if the treatment causes a change that we can measure with the response variable; (2) There is a comparison/control group; (3) Subjects are assigned randomly to treatment or control (randomized assignment); (4) Subjects are not aware of which group they are in (a 'blind'). This may require the use of a placebo for those in the control group; and (5) those who measure the response variable do not know which group the subjects were in (if both 4 and 5 are satisfied, this is a 'double blind' experiment).","title":"Essential Concepts:"},{"location":"unit3/lesson2/#lesson","text":"Display the following headlines to students: Stop Global Warming: Become a Pirate Lack of sleep may shrink your brain Early language skills reduce preschool tantrums Dogs walked by men are more aggressive Discuss each headline by asking the following questions: What is the headline implying with its wording? 1a is implying that you can stop global warming by becoming a pirate, 1b is implying that it\u2019s possible to shrink your brain if you aren\u2019t getting enough sleep, 1c is implying that having early language skills will decrease preschool tantrums, 1d is implying that dogs are more aggressive when they\u2019ve been walked by men. Is it implying causation or association? Discuss definitions of causation and association. Causation means there is a cause and effect relationship between variables. For example, heat causes water to boil; whereas association or correlation means that high values of one variable tend to be associated with high values of the other (or high values tend to be with low values). However, this is not necessarily cause-and-effect at play. For example, blanket sales in Canada are associated with brush fires in Australia - not because Canadian blankets cause the fires, but because Canadian winters cause blanket sales, and Canadian winters are Australian summers, which cause fires. 1a, 1c and 1d are implying causation and 1b is implying association. How can you tell the difference between causation and correlation? What words stand out in these headlines? Answers will vary but some terms for causation include: cause, increase/ decrease, benefits, impacts, effect/ affect, etc.; and for correlation include: get, have, linked, more/ less, tied, connected, etc. In 1a, \u201cbecome\u201d stands out; in 1b, \u201cmay\u201d stands out; in 1c, \u201creduce\u201d stands out; in 1d, \u201care\u201d stands out. Change each causal version of a headline into a non-causal version and vice versa. Answers will vary but an example for 1a is to instead say Global Warming linked to increase of pirates. Introduce the MythBusters video clip by answering the following questions, in teams, for their headline \u201cIs Yawning Contagious?\u201d What is the headline implying with its wording? That yawning may cause other people to yawn. Is it implying causation or correlation? How do you know? Causation because \u201ccontagious\u201d yawns means that you are yawning because someone else has yawned. How can we determine if this is true? Split the class into groups and have each team come up with a way to determine if this is true. Each group should assume that they get to examine 50 people. Show the MythBusters video clip called Is Yawning Contagious? The links to the clips can be found in the Materials above. Focus students on the following guiding questions and ask them to take notes as they watch the video clip: How did the MythBusters design the investigation? What steps did they take? How is this different than your team\u2019s headline responses? After viewing the clip, inform students that the MythBusters have just conducted an experiment , which is one method of data collection. We begin with a brief introduction into \u201cwhat is an experiment\u201d but the definition will be developed over the next several lessons. Guide students to identify the elements of an experiment by referring back to the video clip: Research Question \u2014 the question to be answered by the experiment ( Is Yawning Contagious? ) Subjects \u2013 people or objects that are participating in the experiment ( the 50 adults ) Treatment \u2013 the variable that is deliberately manipulated to investigate its influence on the outcome; this is sometimes known as the explanatory, or independent, variable ( Kari yawned before subject entered the room ) Treatment group \u2013 the group of subjects that receive the treatment ( two out of every three subjects who were placed into rooms \u2013 yawn from Kari ) Control group \u2013 the group that does not receive a treatment ( one out of every three subjects who were placed into rooms \u2013 no yawn from Kari ) Random assignment \u2013 subjects are randomly assigned to either the treatment or control group ( two out of every three subjects received the treatment - in this experiment, random assignment was not used (or if it was, we were not told so) ) Outcome \u2013 the variable that the treatment is meant to influence; this is sometimes known as the response, or dependent, variable ( whether or not a person yawned ) Statistic \u2014 a method for comparing the outcomes of the control and treatment groups (this is needed) In this case, the MythBusters used the difference between the percent of subjects that yawned in the treatment group, which was 4% higher than the control group. Note: In this experiment, and in those found in the IDS curriculum, we use a treatment and a control group. However, a control group is not a necessary element of an experiment. Sometimes it is more appropriate to have two treatment groups with no control group (e.g., medical professionals testing different doses of drugs). The effect that is being studied will dictate whether to feature a control group or not. Display the following questions on the board or projector. Using T-I-P-S , ask students to discuss them. Why did the MythBusters follow all of these steps to design their experiment? In order to determine if watching someone yawn can cause you to yawn. We don't know how MythBusters chose who would be in the treatment group and who would be in the control group. Suppose that the people who showed up first, early in the morning, were assigned to the treatment group, and the last few people, later in the day, ended up in the control group. Would you believe in the conclusions? No, because the two groups were different. The first group might have been sleepier, and so more likely to yawn anyways. Explain that this --another explanation for the cause-and-effect-- is called a confounding variable. Explain that in order to make the two groups as similar as possible, experimenters usually assign subjects randomly. How might we randomly assign about half of the subjects to the treatment and half to the control? We might flip a coin, and those who get Heads go to Treatment. Why would random assignment improve the MythBusters experiment? Because then the two groups would be more similar, so we wouldn't have a confounding variable to worry about. Emphasize that without random assignment, we cannot determine causation because we are not comparing two similar groups.","title":"Lesson:"},{"location":"unit3/lesson2/#class-scribes","text":"One team of students will give a brief talk to discuss what they think the 3 most important topics of the day were.","title":"Class Scribes:"},{"location":"unit3/lesson20/","text":"Lesson 20: Online Data-ing Objective: Students will discover that data exists on the Internet in a variety of areas, formats, and for a variety of purposes. Materials: Video: Explore a Google Data Center with Street View found at: https://www.engadget.com/2012-10-17-google-inside-data-centers.html Wikipedia \u2013 Video Games handout ( LMR_3.17_Wikipedia - Video Games ) Wikipedia \u2013 Video Games \u2013 CSV Format handout ( LMR_3.18_Video Games - CSV ) Online Data-ing handout ( LMR_3.19_Online Data-ing ) Vocabulary: data farm tags HTML Essential Concepts: Essential Concepts: Stretching the conception of data involves seeing that many web pages present information that can be turned into data. Lesson: By a show of hands, ask students if they have ever heard of the term data farm . If any of them have, ask him or her to share what they know about it. Inform students that a data farm is a physical space where high capacity servers are placed to store large amounts of data. Introduce the video titled Explore a Google data center with Street View found at https://www.engadget.com/2012-10-17-google-inside-data-centers.html by explaining that the data center they are about to see is one of these large data farms used to store vast amounts of data. After students watch the video, have a class discussion using the following questions: We have been talking about data for a few months now. How would you respond if someone asked you, \u201cWhat are data?\u201d Answers will vary by class. What are some ways that we have stored data? Data frames in R, Excel spreadsheets, .csv files. Explain that one of the main ways data are distributed is through the Internet. Storing and sharing data on the Internet requires a different format than what we have seen. For example, Wikipedia has a page dedicated to the top video games. Distribute the Wikipedia \u2013 Video Games handout ( LMR_3.17 ), and have students explain the information that the data table provides. LMR_3.17 Once the students understand what the data table describes, walk them through the first portion of the HTML , or Hypertext Markup Language, source code (on page 1). Notice that the first header on the table is denoted as \u201cGame.\u201d Ask: How is \u201cGame\u201d represented in the source code? <th>Game</th> What do you think the <th> code represents? The <th> is a tag for \u201ctable header\u201d If this were in RStudio, what would we call this header? A variable . Assign each student team one video game from the Wikipedia data table. Each team will compare how the information is stored in the table with its corresponding HTML source code. Each team should answer the following questions in the DS journals. Each group was given HTML code for a different game. Which one did your group get? Answers will vary. The variable names are stored at the beginning of the code, in between <th> and </th> and are called tags \u2013 they tell the browser to represent the information between them as a header in the table. Between what tags are the different values of the variables stored? Values are stored between the <td> and </td> tags. Why do you think the data are stored in such a complex way? Why can\u2019t we just put them in a spreadsheet? Answers may vary by class. One reason is that the data must be displayed in a way that allows a browser to make it look pretty (and readable) on a computer screen. How could we get this into an R dataframe so we can analyze it? In its current form, this would be very difficult. We would need to represent the data in a different format in order for R to understand it. Distribute the Wikipedia \u2013 Video Games \u2013 CSV Format handout ( LMR_3.18 ) and explain that this is yet another way to represent the same video game data. Note: The handout only provides information on the first 5 rows of the Wikipedia table. A full version of the file (including all video games in the table) is located on the server with the title bestgames.csv. LMR_3.18 Inform students that a file with the CSV format is easily readable by R. Then ask: a. Where are the variable names stored? The variable names are stored in the first row b. How are values of the variables separated? The values are separated by commas. c. If we were interested in using the online data, how would we obtain it? This is a challenging problem \u2013 one which students may not know how to answer at this point. The objective is for them to struggle with how they would obtain data and recognize that it is not always as simple as \u201cexport, upload, import.\u201d Split the class into their student teams and distribute the Online Data-ing handout ( LMR_3.19 ). Assign each team a different website (each page of the handout lists a different site) and have them use this site to complete the questions in the handout. LMR_3.19 Have each student team share their findings with one other team. They should have their website displayed while discussing their results. Class Scribes: One team of students will give a brief talk to discuss what they think the 3 most important topics of the day were. Homework & Next 2 Days For the next 4 days, students will collect data using their newly created Participatory Sensing campaign. Lab 3E: Scraping Web Data Lab 3F: Maps Complete Labs 3E and 3F prior to Lesson 21 .","title":"Lesson 20: Online Data-ing"},{"location":"unit3/lesson20/#lesson-20-online-data-ing","text":"","title":"Lesson 20: Online Data-ing"},{"location":"unit3/lesson20/#objective","text":"Students will discover that data exists on the Internet in a variety of areas, formats, and for a variety of purposes.","title":"Objective:"},{"location":"unit3/lesson20/#materials","text":"Video: Explore a Google Data Center with Street View found at: https://www.engadget.com/2012-10-17-google-inside-data-centers.html Wikipedia \u2013 Video Games handout ( LMR_3.17_Wikipedia - Video Games ) Wikipedia \u2013 Video Games \u2013 CSV Format handout ( LMR_3.18_Video Games - CSV ) Online Data-ing handout ( LMR_3.19_Online Data-ing )","title":"Materials:"},{"location":"unit3/lesson20/#vocabulary","text":"data farm tags HTML","title":"Vocabulary:"},{"location":"unit3/lesson20/#essential-concepts","text":"Essential Concepts: Stretching the conception of data involves seeing that many web pages present information that can be turned into data.","title":"Essential Concepts:"},{"location":"unit3/lesson20/#lesson","text":"By a show of hands, ask students if they have ever heard of the term data farm . If any of them have, ask him or her to share what they know about it. Inform students that a data farm is a physical space where high capacity servers are placed to store large amounts of data. Introduce the video titled Explore a Google data center with Street View found at https://www.engadget.com/2012-10-17-google-inside-data-centers.html by explaining that the data center they are about to see is one of these large data farms used to store vast amounts of data. After students watch the video, have a class discussion using the following questions: We have been talking about data for a few months now. How would you respond if someone asked you, \u201cWhat are data?\u201d Answers will vary by class. What are some ways that we have stored data? Data frames in R, Excel spreadsheets, .csv files. Explain that one of the main ways data are distributed is through the Internet. Storing and sharing data on the Internet requires a different format than what we have seen. For example, Wikipedia has a page dedicated to the top video games. Distribute the Wikipedia \u2013 Video Games handout ( LMR_3.17 ), and have students explain the information that the data table provides. LMR_3.17 Once the students understand what the data table describes, walk them through the first portion of the HTML , or Hypertext Markup Language, source code (on page 1). Notice that the first header on the table is denoted as \u201cGame.\u201d Ask: How is \u201cGame\u201d represented in the source code? <th>Game</th> What do you think the <th> code represents? The <th> is a tag for \u201ctable header\u201d If this were in RStudio, what would we call this header? A variable . Assign each student team one video game from the Wikipedia data table. Each team will compare how the information is stored in the table with its corresponding HTML source code. Each team should answer the following questions in the DS journals. Each group was given HTML code for a different game. Which one did your group get? Answers will vary. The variable names are stored at the beginning of the code, in between <th> and </th> and are called tags \u2013 they tell the browser to represent the information between them as a header in the table. Between what tags are the different values of the variables stored? Values are stored between the <td> and </td> tags. Why do you think the data are stored in such a complex way? Why can\u2019t we just put them in a spreadsheet? Answers may vary by class. One reason is that the data must be displayed in a way that allows a browser to make it look pretty (and readable) on a computer screen. How could we get this into an R dataframe so we can analyze it? In its current form, this would be very difficult. We would need to represent the data in a different format in order for R to understand it. Distribute the Wikipedia \u2013 Video Games \u2013 CSV Format handout ( LMR_3.18 ) and explain that this is yet another way to represent the same video game data. Note: The handout only provides information on the first 5 rows of the Wikipedia table. A full version of the file (including all video games in the table) is located on the server with the title bestgames.csv. LMR_3.18 Inform students that a file with the CSV format is easily readable by R. Then ask: a. Where are the variable names stored? The variable names are stored in the first row b. How are values of the variables separated? The values are separated by commas. c. If we were interested in using the online data, how would we obtain it? This is a challenging problem \u2013 one which students may not know how to answer at this point. The objective is for them to struggle with how they would obtain data and recognize that it is not always as simple as \u201cexport, upload, import.\u201d Split the class into their student teams and distribute the Online Data-ing handout ( LMR_3.19 ). Assign each team a different website (each page of the handout lists a different site) and have them use this site to complete the questions in the handout. LMR_3.19 Have each student team share their findings with one other team. They should have their website displayed while discussing their results.","title":"Lesson:"},{"location":"unit3/lesson20/#class-scribes","text":"One team of students will give a brief talk to discuss what they think the 3 most important topics of the day were.","title":"Class Scribes:"},{"location":"unit3/lesson20/#homework-next-2-days","text":"For the next 4 days, students will collect data using their newly created Participatory Sensing campaign. Lab 3E: Scraping Web Data Lab 3F: Maps Complete Labs 3E and 3F prior to Lesson 21 .","title":"Homework &amp; Next 2 Days"},{"location":"unit3/lesson21/","text":"Lesson 21: Learning to Love XML Objective: Students will understand the need for data to be stored in different ways - specifically, why it makes sense for web data to be formatted as XML. Materials: Online Data-ing handout ( LMR_3.19_Online Data-ing ) Note: This should have been completed during the previous class. Mountain Peak XML data found at: https://labs.idsucla.org/extras/webdata/mountains.html Note: Open with Google Chrome or Firefox browsers, NOT with Safari. Projector Mountains \u2013 HTML vs. XML handout ( LMR_3.20_Mountains - HTML vs. XML ) Vocabulary: XML Essential Concepts: Essential Concepts: XML is a programming language that we use with our campaigns. We create basic XML \"tags\" in the code, which help us store data in a format we understand. Lesson: Allow time for student teams to present their findings from the Online Data-ing handout ( LMR_3.19 ) if there was not sufficient time during the previous lesson. Remind students that in the previous lesson they learned about a variety of ways that data can be presented online. They've been working with comma separated (CSV) files and R data frames. Last time and in the lab, they worked with HTML tables. Today they are going to learn how HTML can be displayed as an XML table. XML , or Extensible Mark up Language, is a popular format for storing data on the Internet. It is useful because it creates readable web pages, and also because it allows programmers to easily update values in the data table if those values change. In pairs, ask students to brainstorm ways in which data that is found online is different than the way we see data in RStudio. Then, create a class brainstorm from the student pair responses. After the brainstorm, emphasize the following: RStudio\u2019s default way to work with data is as large data frames (tables) where rows represent observations and columns represent variables. Data that is viewed online often has a different structure. Data structures found on the web might be displayed in tables, such as those on Wikipedia, or streams, such as Twitter, and might even include data spread across multiple sections of a web page, such as Yelp. Show students, on a projector, the Mountain Peak XML data found at https://labs.idsucla.org/extras/webdata/mountains.html Ask students to look at the data and determine if they have seen it before. Hint: They have! It was the data they scraped during Lab 3E . Once students figure out that the XML is just the same data as the website they scraped during Lab 3E , distribute the Mountains \u2013 HTML vs. XML handout ( LMR_3.20 ), which displays both HTML and XML versions of the data. Note: The handout only includes the first 3 mountains. LMR_3.20 Ask student pairs to answer the following: Why are certain XML tags indented in the XML version of the data? The indentations tell us how to structure the HTML table. For example, all the mountains are contained in the <data> section, but are further tagged by each particular mountain within the <mountain> and </mountain> tags. All information stored between those two tags will be displayed as one row of the HTML table. What are the role of tags (ex. <state>) and end tags (ex. </state>) in the XML code? Tags tell us when a certain type of data begins, and end tags tell us when the data should end. In other words, it tells us where to find the specific values of a variable (ex. Alaska would be the value of the \u201cstate\u201d variable since it is between the <state> and </state> tags. Where are the variable names? The variable names can be found between each <mountain> and </mountain> tags. Specifically, the first variable is \u201cpeak\u201d and the last variable is \u201crank.\u201d Where are the observations? The observations are located within each of the variable tags. For example, the observation \u201cMount McKinley (Denali)\u201d is found between the <peak> and </peak> tags. Assign student pairs one of the above questions to share out with the class. Student pairs that did not receive an assignment must participate using the Agree/Disagree strategy. As a class, discuss the answers to the questions above. XML formats make it easier to display data on the web in a pleasant matter and make it easier for programmers to find and alter data if the values change or if, for example, they wish to add a new row to a table. Class Scribes: One team of students will give a brief talk to discuss what they think the 3 most important topics of the day were. Homework For the next 3 days, students will collect data using the class\u2019s newly created Participatory Sensing campaign (see Lessons 16 - 18 ). For homework, students should reflect about how XML and HTML data are displayed. They should discuss when each format is appropriate.","title":"Lesson 21: Learning to Love XML"},{"location":"unit3/lesson21/#lesson-21-learning-to-love-xml","text":"","title":"Lesson 21: Learning to Love XML"},{"location":"unit3/lesson21/#objective","text":"Students will understand the need for data to be stored in different ways - specifically, why it makes sense for web data to be formatted as XML.","title":"Objective:"},{"location":"unit3/lesson21/#materials","text":"Online Data-ing handout ( LMR_3.19_Online Data-ing ) Note: This should have been completed during the previous class. Mountain Peak XML data found at: https://labs.idsucla.org/extras/webdata/mountains.html Note: Open with Google Chrome or Firefox browsers, NOT with Safari. Projector Mountains \u2013 HTML vs. XML handout ( LMR_3.20_Mountains - HTML vs. XML )","title":"Materials:"},{"location":"unit3/lesson21/#vocabulary","text":"XML","title":"Vocabulary:"},{"location":"unit3/lesson21/#essential-concepts","text":"Essential Concepts: XML is a programming language that we use with our campaigns. We create basic XML \"tags\" in the code, which help us store data in a format we understand.","title":"Essential Concepts:"},{"location":"unit3/lesson21/#lesson","text":"Allow time for student teams to present their findings from the Online Data-ing handout ( LMR_3.19 ) if there was not sufficient time during the previous lesson. Remind students that in the previous lesson they learned about a variety of ways that data can be presented online. They've been working with comma separated (CSV) files and R data frames. Last time and in the lab, they worked with HTML tables. Today they are going to learn how HTML can be displayed as an XML table. XML , or Extensible Mark up Language, is a popular format for storing data on the Internet. It is useful because it creates readable web pages, and also because it allows programmers to easily update values in the data table if those values change. In pairs, ask students to brainstorm ways in which data that is found online is different than the way we see data in RStudio. Then, create a class brainstorm from the student pair responses. After the brainstorm, emphasize the following: RStudio\u2019s default way to work with data is as large data frames (tables) where rows represent observations and columns represent variables. Data that is viewed online often has a different structure. Data structures found on the web might be displayed in tables, such as those on Wikipedia, or streams, such as Twitter, and might even include data spread across multiple sections of a web page, such as Yelp. Show students, on a projector, the Mountain Peak XML data found at https://labs.idsucla.org/extras/webdata/mountains.html Ask students to look at the data and determine if they have seen it before. Hint: They have! It was the data they scraped during Lab 3E . Once students figure out that the XML is just the same data as the website they scraped during Lab 3E , distribute the Mountains \u2013 HTML vs. XML handout ( LMR_3.20 ), which displays both HTML and XML versions of the data. Note: The handout only includes the first 3 mountains. LMR_3.20 Ask student pairs to answer the following: Why are certain XML tags indented in the XML version of the data? The indentations tell us how to structure the HTML table. For example, all the mountains are contained in the <data> section, but are further tagged by each particular mountain within the <mountain> and </mountain> tags. All information stored between those two tags will be displayed as one row of the HTML table. What are the role of tags (ex. <state>) and end tags (ex. </state>) in the XML code? Tags tell us when a certain type of data begins, and end tags tell us when the data should end. In other words, it tells us where to find the specific values of a variable (ex. Alaska would be the value of the \u201cstate\u201d variable since it is between the <state> and </state> tags. Where are the variable names? The variable names can be found between each <mountain> and </mountain> tags. Specifically, the first variable is \u201cpeak\u201d and the last variable is \u201crank.\u201d Where are the observations? The observations are located within each of the variable tags. For example, the observation \u201cMount McKinley (Denali)\u201d is found between the <peak> and </peak> tags. Assign student pairs one of the above questions to share out with the class. Student pairs that did not receive an assignment must participate using the Agree/Disagree strategy. As a class, discuss the answers to the questions above. XML formats make it easier to display data on the web in a pleasant matter and make it easier for programmers to find and alter data if the values change or if, for example, they wish to add a new row to a table.","title":"Lesson:"},{"location":"unit3/lesson21/#class-scribes","text":"One team of students will give a brief talk to discuss what they think the 3 most important topics of the day were.","title":"Class Scribes:"},{"location":"unit3/lesson21/#homework","text":"For the next 3 days, students will collect data using the class\u2019s newly created Participatory Sensing campaign (see Lessons 16 - 18 ). For homework, students should reflect about how XML and HTML data are displayed. They should discuss when each format is appropriate.","title":"Homework"},{"location":"unit3/lesson22/","text":"Lesson 22: Changing Format Objective: Students will learn how to convert XML files to the more familiar data table format and vice versa. Materials: There and Back Again: From XML to Data Tables handout ( LMR_3.21_From XML to Data Tables ) There and Back Again: From Data Tables to XML handout ( LMR_3.22_From Data Tables to XML ) Essential Concepts: Essential Concepts: Converting XML to spreadsheet format helps us better understand and view our data. Lesson: Take a few minutes to compare the structure of XML code to HTML data tables (refer to Step 7 from Lesson 21 ). Inform students that in today\u2019s lesson, they will learn how to translate information from XML code into a data table. Distribute the There and Back Again: From XML to Data Tables handout ( LMR_3.21 ) to students. LMR_3.21 Inform the students that XML code is provided on page 1 of the handout, and their goal is to transfer all the information to the empty data table. As a guide, ask a volunteer to find and name one of the variables in the XML code and then have all the students write the name of the variable in the first column of the top row in the data table. Next, ask another student to find the first value of the variable named in Step 5. This value should be placed in the correct column and row of the data table. Provide time for students to complete the handout individually. Using the Anonymous Author strategy, share a couple of the completed data tables. Ask teams to discuss how they are alike and how they are different. Note: Most tables will probably be the same, but could vary slightly based on which columns each variable name was placed in, and in what order the observations were listed in the rows. Ultimately, the information contained in the data tables is the same. Then, conduct a whole class discussion regarding student responses to the questions on page 2 of the handout. Distribute the There and Back Again: From Data Tables to XML ( LMR_3.22 ) to student teams and allow them time to complete it. LMR_3.22 Once teams have finished, teams will guide you to write the correct XML code. Using a Whip Around , teams will tell you the first line of the XML code you need to write. Teams waiting their turn will check if the team is guiding you correctly. If not, they need to stop you and propose their line of code. You may not continue writing the lines of code until all teams are in agreement. Class Scribes: One team of students will give a brief talk to discuss what they think the 3 most important topics of the day were. Homework & Next Day Students will continue to collect data using the class\u2019s Participatory Sensing campaign (see Lessons 17 - 19 ). They will analyze the data the next day during the practicum.","title":"Lesson 22: Changing Orientation"},{"location":"unit3/lesson22/#lesson-22-changing-format","text":"","title":"Lesson 22: Changing Format"},{"location":"unit3/lesson22/#objective","text":"Students will learn how to convert XML files to the more familiar data table format and vice versa.","title":"Objective:"},{"location":"unit3/lesson22/#materials","text":"There and Back Again: From XML to Data Tables handout ( LMR_3.21_From XML to Data Tables ) There and Back Again: From Data Tables to XML handout ( LMR_3.22_From Data Tables to XML )","title":"Materials:"},{"location":"unit3/lesson22/#essential-concepts","text":"Essential Concepts: Converting XML to spreadsheet format helps us better understand and view our data.","title":"Essential Concepts:"},{"location":"unit3/lesson22/#lesson","text":"Take a few minutes to compare the structure of XML code to HTML data tables (refer to Step 7 from Lesson 21 ). Inform students that in today\u2019s lesson, they will learn how to translate information from XML code into a data table. Distribute the There and Back Again: From XML to Data Tables handout ( LMR_3.21 ) to students. LMR_3.21 Inform the students that XML code is provided on page 1 of the handout, and their goal is to transfer all the information to the empty data table. As a guide, ask a volunteer to find and name one of the variables in the XML code and then have all the students write the name of the variable in the first column of the top row in the data table. Next, ask another student to find the first value of the variable named in Step 5. This value should be placed in the correct column and row of the data table. Provide time for students to complete the handout individually. Using the Anonymous Author strategy, share a couple of the completed data tables. Ask teams to discuss how they are alike and how they are different. Note: Most tables will probably be the same, but could vary slightly based on which columns each variable name was placed in, and in what order the observations were listed in the rows. Ultimately, the information contained in the data tables is the same. Then, conduct a whole class discussion regarding student responses to the questions on page 2 of the handout. Distribute the There and Back Again: From Data Tables to XML ( LMR_3.22 ) to student teams and allow them time to complete it. LMR_3.22 Once teams have finished, teams will guide you to write the correct XML code. Using a Whip Around , teams will tell you the first line of the XML code you need to write. Teams waiting their turn will check if the team is guiding you correctly. If not, they need to stop you and propose their line of code. You may not continue writing the lines of code until all teams are in agreement.","title":"Lesson:"},{"location":"unit3/lesson22/#class-scribes","text":"One team of students will give a brief talk to discuss what they think the 3 most important topics of the day were.","title":"Class Scribes:"},{"location":"unit3/lesson22/#homework-next-day","text":"Students will continue to collect data using the class\u2019s Participatory Sensing campaign (see Lessons 17 - 19 ). They will analyze the data the next day during the practicum.","title":"Homework &amp; Next Day"},{"location":"unit3/lesson3/","text":"Lesson 3: Let\u2019s Try an Experiment! Objective: Students will explore the importance of randomized assignment in experiments. They will understand that without random assignment, there might be confounding variables and will be able to suggest possible confounding variables. Materials: Measuring Tape Essential Concepts: Essential Concepts: Randomized assignment is required to determine cause-and-effect. Lesson: Inform students that they will be exploring the question \u201cWhy do we need randomized assignment?\u201d by conducting an experiment. Tell students that you have a treatment that can make people taller. Explain that the class will be divided into two groups, one group will get the treatment, and one group will not. The group that does not receive the treatment will be the control group. After the treatment, they will measure the groups to see which is taller. Now divide the class into two groups by placing the boys in the treatment group and the girls in the control group. Remember that in an experiment we typically have a treatment group and a control group. In the MythBusters experiment, they compared number of yawns after treatment, and not any measurements before treatment, because they were comparing the treatment group to the control group (the control group is specifically here because it is a comparable untreated group - this allows us to not need \u201cbefore\u201d measurements). Therefore, in this case, we will run the experiment and then compare average height of the treatment group to the control group. Tell them that after the treatment group takes the treatment, your statistic to compare groups will be to measure the heights. If the treatment group is taller, then the treatment must have worked. There are two possible outcomes to dividing the class this way: The students will protest (as they should) and you can start a discussion as to why this is not a good way to divide the class. OR the students don\u2019t protest and you continue with the experiment. The treatment should be something silly, like waving a ruler in front of the person\u2019s face or by asking them to chant \u201cgrow, grow, grow!\u201d three times. After treatment, measure the heights of each group and ask them if they think this is good evidence ( do not say \u201cproves\u201d ) that the treatment is effective. Regardless of the outcome, students should recognize that by putting the boys in one group, the outcome was pre-determined, since boys tend to be taller than girls to begin with. This is an example of a confounding factor . Confounding factors are variables that provide an alternative explanation of the effect of the treatment on the outcome variable. Ask students: \u201cHow should students be put into groups?\u201d Discuss various other methods of grouping students. Someone will probably say to split the groups into equal numbers of boys and girls. At this suggestion, divide the class into two groups by placing the tallest boys and tallest girls in the treatment group, and the shorter boys and shorter girls in the control group. Students should be able to recognize that you shouldn\u2019t use any characteristics to decide the groups. Continue discussion of other ways to decide the groups. Use the following questions as a guide: What about flipping a coin? What will the gender balance look like? Each group should have about the same balance as the class, though not exactly. Why is it important that the groups be similar? Because otherwise, something else might be the cause of the response changing. Inform students that today the class will begin to design their own experiment using what they have learned over the last few lessons. The question they will investigate is: How does our perception of time change when exposed to a stimulus? They will be trying to determine the length of one minute without the use of time-aids. In their experiment, they will subject some students to a stimulus and others to no stimulus. They will then analyze the data to determine if subjecting students to a stimulus affects the perception of how long a minute of time lasts. In their DS journals, ask students to answer the following questions about the elements of their experiment: What is the research question we\u2019re interested in addressing? Who are the subjects that will be participating in the experiment? How should we randomly assign the subjects into treatment and control groups? (See step 12 for an RStudio method that the teacher can use) What is the outcome variable that we will be measuring? What unit of measurement should we use? Note: Students will decide on a treatment to apply to each group on the following day. As a class, discuss the responses to the questions above (step #9, a-d) and come to a consensus for each question\u2019s answer. Inform the class that they will be using the answers they have agreed upon as the final design of the class\u2019s experiment. At the end of the class, the students should be assigned to the treatment or control groups using the randomization method they chose as a class in step #9c. Note: One method to determine group assignment would be to use the class roster and the sample() function in RStudio. The students have a number that corresponds to their placement on the roster (i.e. student 1\u2019s last name most likely starts with an A, and then we move alphabetically through the roster). You can then use RStudio to randomly select which half of the numbers/students will be assigned to the treatment group. > sample(1:30, size = 15, replace = FALSE) Students will conduct the experiment in the next lesson . Class Scribes: One team of students will give a brief talk to discuss what they think the 3 most important topics of the day were.","title":"Lesson 3: Let\u2019s Try an Experiment!"},{"location":"unit3/lesson3/#lesson-3-lets-try-an-experiment","text":"","title":"Lesson 3: Let\u2019s Try an Experiment!"},{"location":"unit3/lesson3/#objective","text":"Students will explore the importance of randomized assignment in experiments. They will understand that without random assignment, there might be confounding variables and will be able to suggest possible confounding variables.","title":"Objective:"},{"location":"unit3/lesson3/#materials","text":"Measuring Tape","title":"Materials:"},{"location":"unit3/lesson3/#essential-concepts","text":"Essential Concepts: Randomized assignment is required to determine cause-and-effect.","title":"Essential Concepts:"},{"location":"unit3/lesson3/#lesson","text":"Inform students that they will be exploring the question \u201cWhy do we need randomized assignment?\u201d by conducting an experiment. Tell students that you have a treatment that can make people taller. Explain that the class will be divided into two groups, one group will get the treatment, and one group will not. The group that does not receive the treatment will be the control group. After the treatment, they will measure the groups to see which is taller. Now divide the class into two groups by placing the boys in the treatment group and the girls in the control group. Remember that in an experiment we typically have a treatment group and a control group. In the MythBusters experiment, they compared number of yawns after treatment, and not any measurements before treatment, because they were comparing the treatment group to the control group (the control group is specifically here because it is a comparable untreated group - this allows us to not need \u201cbefore\u201d measurements). Therefore, in this case, we will run the experiment and then compare average height of the treatment group to the control group. Tell them that after the treatment group takes the treatment, your statistic to compare groups will be to measure the heights. If the treatment group is taller, then the treatment must have worked. There are two possible outcomes to dividing the class this way: The students will protest (as they should) and you can start a discussion as to why this is not a good way to divide the class. OR the students don\u2019t protest and you continue with the experiment. The treatment should be something silly, like waving a ruler in front of the person\u2019s face or by asking them to chant \u201cgrow, grow, grow!\u201d three times. After treatment, measure the heights of each group and ask them if they think this is good evidence ( do not say \u201cproves\u201d ) that the treatment is effective. Regardless of the outcome, students should recognize that by putting the boys in one group, the outcome was pre-determined, since boys tend to be taller than girls to begin with. This is an example of a confounding factor . Confounding factors are variables that provide an alternative explanation of the effect of the treatment on the outcome variable. Ask students: \u201cHow should students be put into groups?\u201d Discuss various other methods of grouping students. Someone will probably say to split the groups into equal numbers of boys and girls. At this suggestion, divide the class into two groups by placing the tallest boys and tallest girls in the treatment group, and the shorter boys and shorter girls in the control group. Students should be able to recognize that you shouldn\u2019t use any characteristics to decide the groups. Continue discussion of other ways to decide the groups. Use the following questions as a guide: What about flipping a coin? What will the gender balance look like? Each group should have about the same balance as the class, though not exactly. Why is it important that the groups be similar? Because otherwise, something else might be the cause of the response changing. Inform students that today the class will begin to design their own experiment using what they have learned over the last few lessons. The question they will investigate is: How does our perception of time change when exposed to a stimulus? They will be trying to determine the length of one minute without the use of time-aids. In their experiment, they will subject some students to a stimulus and others to no stimulus. They will then analyze the data to determine if subjecting students to a stimulus affects the perception of how long a minute of time lasts. In their DS journals, ask students to answer the following questions about the elements of their experiment: What is the research question we\u2019re interested in addressing? Who are the subjects that will be participating in the experiment? How should we randomly assign the subjects into treatment and control groups? (See step 12 for an RStudio method that the teacher can use) What is the outcome variable that we will be measuring? What unit of measurement should we use? Note: Students will decide on a treatment to apply to each group on the following day. As a class, discuss the responses to the questions above (step #9, a-d) and come to a consensus for each question\u2019s answer. Inform the class that they will be using the answers they have agreed upon as the final design of the class\u2019s experiment. At the end of the class, the students should be assigned to the treatment or control groups using the randomization method they chose as a class in step #9c. Note: One method to determine group assignment would be to use the class roster and the sample() function in RStudio. The students have a number that corresponds to their placement on the roster (i.e. student 1\u2019s last name most likely starts with an A, and then we move alphabetically through the roster). You can then use RStudio to randomly select which half of the numbers/students will be assigned to the treatment group. > sample(1:30, size = 15, replace = FALSE) Students will conduct the experiment in the next lesson .","title":"Lesson:"},{"location":"unit3/lesson3/#class-scribes","text":"One team of students will give a brief talk to discuss what they think the 3 most important topics of the day were.","title":"Class Scribes:"},{"location":"unit3/lesson4/","text":"Lesson 4: Predictions, Predictions Objective: Students will continue to read articles critically. They will anticipate visualizations about the data that will be collected from the class experiment and make predictions about the outcome. Materials: Article: PsyBlog\u2019s 10 Ways Our Minds Warp Time found at: http://www.spring.org.uk/2011/06/10-ways-our-minds-warp-time.php Experiment Predictions handout ( LMR_3.1_Experiment Predictions ) Vocabulary: theory Essential Concepts: Essential Concepts: Designing an experiment requires making many decisions, including what to measure and how to measure it. Lesson: Students will read the article 10 Ways Our Minds Warp Time found at: http://www.spring.org.uk/2011/06/10-ways-our-minds-warp-time.php They will read the article critically to answer the following questions (displayed or written on the board): Who was observed and what were the variables measured? People and their perceptions of time. What statistical questions were the researchers trying to answer? How is time perception affected by different stimuli? Who collected the data? Researchers such as cave expert Michel Siffre collected data. How were the data collected? Data were collected through various experiments/studies (13 were cited). What claim(s) did the article make? There were 10 claims made regarding time perception. What are some statistics that the article used to make the claim(s)? Answers may vary. Article has several percentage statistics. In their teams, ask students to share their responses from reading the 10 Ways Our Minds Warp Time article and agree on the responses as a team. Do a quick Whip Around of the responses (see step #2 for possible responses). Remind students that they designed a class experiment during the previous lesson but did not select an actual treatment. As a class, decide on a treatment to use for the experiment. Students can use the methods found in the article for inspiration, or come up with something novel on their own. Note: Stimuli examples include music (genres determined by the class), lights off, physical activity (e.g., holding arms out), relaxation/meditation techniques, heads down, eyes closed, etc. Ensure that the experiment can be completed in one 50-60 minute class period. Treatments requiring excessive preparation time (e.g., running a mile) are less than ideal. Before they conduct the experiment, students will test their theories by making predictions about the data and the outcomes. A theory is an idea used to explain a situation. Display the class experiment\u2019s research question: How does our perception of time change when exposed to a stimulus? Take a poll of the students who believe that there will be differences in the estimate of the length of a minute between the treatment and control groups. The remaining students, then, do not believe that there will be differences. Then, ask those students who believe there are differences, how small or large they think the difference will be. Distribute the Experiment Predictions handout ( LMR_3.1 ) and, in pairs, have students discuss and complete the answers for the handout. Note: What will the distribution of time perceptions look like? The distributions will likely have more points that are closer to 60 seconds, but will also have values that are shorter and longer than 60 seconds. Appropriate plots to use will include histograms, dotplots or boxplots. LMR_3.1 Using Anonymous Author , select student work to share with the whole class. Give student teams time (about 2 minutes) to discuss each product that is shared/presented. Teams will offer their thoughts using a modified Two Cents strategy where, instead of two cents, each team will receive one cent (or a token) and, in order to turn it in, the team will have to make comments or ask questions about the student work that is being shared. Call on teams until you have collected every cent. This ensures that all teams contribute to the discussion. Inform students that they will conduct the experiment in which they will estimate the length of time of one minute during the next lesson . Class Scribes: One team of students will give a brief talk to discuss what they think the 3 most important topics of the day were.","title":"Lesson 4: Predictions, Predictions"},{"location":"unit3/lesson4/#lesson-4-predictions-predictions","text":"","title":"Lesson 4: Predictions, Predictions"},{"location":"unit3/lesson4/#objective","text":"Students will continue to read articles critically. They will anticipate visualizations about the data that will be collected from the class experiment and make predictions about the outcome.","title":"Objective:"},{"location":"unit3/lesson4/#materials","text":"Article: PsyBlog\u2019s 10 Ways Our Minds Warp Time found at: http://www.spring.org.uk/2011/06/10-ways-our-minds-warp-time.php Experiment Predictions handout ( LMR_3.1_Experiment Predictions )","title":"Materials:"},{"location":"unit3/lesson4/#vocabulary","text":"theory","title":"Vocabulary:"},{"location":"unit3/lesson4/#essential-concepts","text":"Essential Concepts: Designing an experiment requires making many decisions, including what to measure and how to measure it.","title":"Essential Concepts:"},{"location":"unit3/lesson4/#lesson","text":"Students will read the article 10 Ways Our Minds Warp Time found at: http://www.spring.org.uk/2011/06/10-ways-our-minds-warp-time.php They will read the article critically to answer the following questions (displayed or written on the board): Who was observed and what were the variables measured? People and their perceptions of time. What statistical questions were the researchers trying to answer? How is time perception affected by different stimuli? Who collected the data? Researchers such as cave expert Michel Siffre collected data. How were the data collected? Data were collected through various experiments/studies (13 were cited). What claim(s) did the article make? There were 10 claims made regarding time perception. What are some statistics that the article used to make the claim(s)? Answers may vary. Article has several percentage statistics. In their teams, ask students to share their responses from reading the 10 Ways Our Minds Warp Time article and agree on the responses as a team. Do a quick Whip Around of the responses (see step #2 for possible responses). Remind students that they designed a class experiment during the previous lesson but did not select an actual treatment. As a class, decide on a treatment to use for the experiment. Students can use the methods found in the article for inspiration, or come up with something novel on their own. Note: Stimuli examples include music (genres determined by the class), lights off, physical activity (e.g., holding arms out), relaxation/meditation techniques, heads down, eyes closed, etc. Ensure that the experiment can be completed in one 50-60 minute class period. Treatments requiring excessive preparation time (e.g., running a mile) are less than ideal. Before they conduct the experiment, students will test their theories by making predictions about the data and the outcomes. A theory is an idea used to explain a situation. Display the class experiment\u2019s research question: How does our perception of time change when exposed to a stimulus? Take a poll of the students who believe that there will be differences in the estimate of the length of a minute between the treatment and control groups. The remaining students, then, do not believe that there will be differences. Then, ask those students who believe there are differences, how small or large they think the difference will be. Distribute the Experiment Predictions handout ( LMR_3.1 ) and, in pairs, have students discuss and complete the answers for the handout. Note: What will the distribution of time perceptions look like? The distributions will likely have more points that are closer to 60 seconds, but will also have values that are shorter and longer than 60 seconds. Appropriate plots to use will include histograms, dotplots or boxplots. LMR_3.1 Using Anonymous Author , select student work to share with the whole class. Give student teams time (about 2 minutes) to discuss each product that is shared/presented. Teams will offer their thoughts using a modified Two Cents strategy where, instead of two cents, each team will receive one cent (or a token) and, in order to turn it in, the team will have to make comments or ask questions about the student work that is being shared. Call on teams until you have collected every cent. This ensures that all teams contribute to the discussion. Inform students that they will conduct the experiment in which they will estimate the length of time of one minute during the next lesson .","title":"Lesson:"},{"location":"unit3/lesson4/#class-scribes","text":"One team of students will give a brief talk to discuss what they think the 3 most important topics of the day were.","title":"Class Scribes:"},{"location":"unit3/lesson5/","text":"Lesson 5: Time Perception Experiment Objective: Students will engage in a collectively designed experiment. Materials: RStudio\u2019s stopwatch() function IDS UCLA App or Browser-Based survey-taking tool Essential Concepts: Essential Concepts: Designing and carrying out an experiment helps us answer specific statistical questions of interest. Lesson: Begin the lesson by eliciting the elements of an experiment from students (they may refer back to their DS journals for their responses from Lesson 2 ). Inform students that they will be using RStudio to get a precise measurement of their estimate. Ask for a student volunteer. Demonstrate the stopwatch function using RStudio by typing in the following code: > stopwatch() Then, ask the student volunteer to stand in front of your computer and get ready to estimate the length of time of one minute without looking at a clock. Once he/she thinks a minute has passed, ask him/her to press the enter/return key on the keyboard to see the result of the estimate. Inform students that you have just demonstrated how they will measure their one-minute estimates. Begin conducting the experiment by reviewing the research question: How does our perception of time change when exposed to a stimulus? Refer back to the experiment design. Review the specific treatment that the subjects in the treatment group will receive. If necessary, demonstrate to the treatment group how to do the experiment. For example, if standing with open arms is the stimulus, the estimate begins when the student starts the stopwatch() function and engages in the stimulus, and ends when the subject presses enter/return in RStudio to stop the timer. For the control group, the students can simply sit at their desks with their eyes closed. Each student will run the stopwatch() function and stop the timer when they believe a minute has elapsed. Conduct the experiment in its entirety. Use team roles effectively to ensure the experiment is done correctly. Have each student use a computer and the stopwatch() function to record her/his estimate of one minute. Ensure each student records her/his estimate in the DS journal. When the experiment is completed, have students enter their data in the Time Perception survey found in the Survey Taking Tool at https://portal.idsucla.org or by using the IDS UCLA App in their iOS or Android devices. Inform students that they will be analyzing the results from the experiment in Lab 3A: The results are in! Class Scribes: One team of students will give a brief talk to discuss what they think the 3 most important topics of the day were. Next Day LAB 3A: The results are in! Complete Lab 3A prior to Practicum .","title":"Lesson 5: Time Perception Experiment"},{"location":"unit3/lesson5/#lesson-5-time-perception-experiment","text":"","title":"Lesson 5: Time Perception Experiment"},{"location":"unit3/lesson5/#objective","text":"Students will engage in a collectively designed experiment.","title":"Objective:"},{"location":"unit3/lesson5/#materials","text":"RStudio\u2019s stopwatch() function IDS UCLA App or Browser-Based survey-taking tool","title":"Materials:"},{"location":"unit3/lesson5/#essential-concepts","text":"Essential Concepts: Designing and carrying out an experiment helps us answer specific statistical questions of interest.","title":"Essential Concepts:"},{"location":"unit3/lesson5/#lesson","text":"Begin the lesson by eliciting the elements of an experiment from students (they may refer back to their DS journals for their responses from Lesson 2 ). Inform students that they will be using RStudio to get a precise measurement of their estimate. Ask for a student volunteer. Demonstrate the stopwatch function using RStudio by typing in the following code: > stopwatch() Then, ask the student volunteer to stand in front of your computer and get ready to estimate the length of time of one minute without looking at a clock. Once he/she thinks a minute has passed, ask him/her to press the enter/return key on the keyboard to see the result of the estimate. Inform students that you have just demonstrated how they will measure their one-minute estimates. Begin conducting the experiment by reviewing the research question: How does our perception of time change when exposed to a stimulus? Refer back to the experiment design. Review the specific treatment that the subjects in the treatment group will receive. If necessary, demonstrate to the treatment group how to do the experiment. For example, if standing with open arms is the stimulus, the estimate begins when the student starts the stopwatch() function and engages in the stimulus, and ends when the subject presses enter/return in RStudio to stop the timer. For the control group, the students can simply sit at their desks with their eyes closed. Each student will run the stopwatch() function and stop the timer when they believe a minute has elapsed. Conduct the experiment in its entirety. Use team roles effectively to ensure the experiment is done correctly. Have each student use a computer and the stopwatch() function to record her/his estimate of one minute. Ensure each student records her/his estimate in the DS journal. When the experiment is completed, have students enter their data in the Time Perception survey found in the Survey Taking Tool at https://portal.idsucla.org or by using the IDS UCLA App in their iOS or Android devices. Inform students that they will be analyzing the results from the experiment in Lab 3A: The results are in!","title":"Lesson:"},{"location":"unit3/lesson5/#class-scribes","text":"One team of students will give a brief talk to discuss what they think the 3 most important topics of the day were.","title":"Class Scribes:"},{"location":"unit3/lesson5/#next-day","text":"LAB 3A: The results are in! Complete Lab 3A prior to Practicum .","title":"Next Day"},{"location":"unit3/lesson6/","text":"Lesson 6: Observational Studies Objective: Students will learn that an observational study is a data collection method in which subjects are observed and outcomes are recorded. They will learn how to collect this type of data and make informal inferences about the results. Materials: Stick Figures Cutouts ( LMR_1.2_Stick Figures ) from Unit 1 , Lesson 2 Note: Advanced preparation required (see step 1 below). Turning Observations into Data handout ( LMR_3.2_Observations_to_Data ) Vocabulary: observational study Essential Concepts: Essential Concepts: Observational studies are those for which there is no intervention applied by researchers. Lesson: From Unit 1 , Lesson 2 , redistribute one full set of 8 cards from the Stick Figures handout ( LMR_1.2 ) to each student team. Advanced preparation required: Print the Stick Figures handout ( LMR_1.2 ). The handout can then be cut into the 8 cards. You will need enough sets of the cards for each student team to share a full set. For example, if there are 5 student teams in a class, then 5 copies of the file will need to be printed so that each team gets all 8 cards. Have students recall that they used these cards in Unit 1 , Lesson 2 . When they used them in Lesson 2 , the data was collected, recorded, and organized, but without particular structure to it. LMR_1.2 Then, distribute one copy per student of the Turning Observations into Data handout ( LMR_3.2 ). LMR_3.2 Every student from the team will then select one of the cards from the team\u2019s pile of 8, and should begin working through the Turning Observations into Data handout individually. As the students finish each part of the handout, they should compare their responses with their student teams. Go over the names of the variables in Part 1 by doing a quick Whip Around by teams. Then, select a couple of teams to share the information on the first row and one of the columns. Part 3 of the handout asks the students to consider the following research question: What determines the number of friends a person has on social media? Once the students have completed the handout, discuss the variable that they thought was best associated with the number of friends on social media. They should have seen that a person\u2019s GPA was related to the number of friends. More specifically, the higher a person\u2019s GPA, the more friends he/she had. Ask a few students to share out their responses to the very last question: \u201cCan you think of another variable (not necessarily given in the pictures) that might impact both the number of friends AND the variable you selected? Give an example and explain how it might impact each of the variables.\u201d Answers will vary, but one example could be: a person\u2019s self-esteem level (if he/she is confident in school, his/her grades might be higher; higher confidence could also be a reason for a person having more friends). Remind students that in the previous section, they learned about the elements of an experiment. In teams, ask students to discuss how collecting this data is similar or different from experiments. Then have a whole class discussion about this comparison, guiding students to realize that there were no assignments to groups and no treatment was applied. The subjects (i.e. the people displayed on the cards) were simply observed, and then information about them was recorded. Inform students that an observational study is a data collection method in which subjects are observed and outcomes are recorded. No treatment is applied to the subjects. Instead, researchers are simply watching something happen and have absolutely no control over it. In lesson 7 , students will learn more about the differences between experiments and observational studies and what conclusions they can make about each. Class Scribes: One team of students will give a brief talk to discuss what they think the 3 most important topics of the day were.","title":"Lesson 6: Observational Studies"},{"location":"unit3/lesson6/#lesson-6-observational-studies","text":"","title":"Lesson 6: Observational Studies"},{"location":"unit3/lesson6/#objective","text":"Students will learn that an observational study is a data collection method in which subjects are observed and outcomes are recorded. They will learn how to collect this type of data and make informal inferences about the results.","title":"Objective:"},{"location":"unit3/lesson6/#materials","text":"Stick Figures Cutouts ( LMR_1.2_Stick Figures ) from Unit 1 , Lesson 2 Note: Advanced preparation required (see step 1 below). Turning Observations into Data handout ( LMR_3.2_Observations_to_Data )","title":"Materials:"},{"location":"unit3/lesson6/#vocabulary","text":"observational study","title":"Vocabulary:"},{"location":"unit3/lesson6/#essential-concepts","text":"Essential Concepts: Observational studies are those for which there is no intervention applied by researchers.","title":"Essential Concepts:"},{"location":"unit3/lesson6/#lesson","text":"From Unit 1 , Lesson 2 , redistribute one full set of 8 cards from the Stick Figures handout ( LMR_1.2 ) to each student team. Advanced preparation required: Print the Stick Figures handout ( LMR_1.2 ). The handout can then be cut into the 8 cards. You will need enough sets of the cards for each student team to share a full set. For example, if there are 5 student teams in a class, then 5 copies of the file will need to be printed so that each team gets all 8 cards. Have students recall that they used these cards in Unit 1 , Lesson 2 . When they used them in Lesson 2 , the data was collected, recorded, and organized, but without particular structure to it. LMR_1.2 Then, distribute one copy per student of the Turning Observations into Data handout ( LMR_3.2 ). LMR_3.2 Every student from the team will then select one of the cards from the team\u2019s pile of 8, and should begin working through the Turning Observations into Data handout individually. As the students finish each part of the handout, they should compare their responses with their student teams. Go over the names of the variables in Part 1 by doing a quick Whip Around by teams. Then, select a couple of teams to share the information on the first row and one of the columns. Part 3 of the handout asks the students to consider the following research question: What determines the number of friends a person has on social media? Once the students have completed the handout, discuss the variable that they thought was best associated with the number of friends on social media. They should have seen that a person\u2019s GPA was related to the number of friends. More specifically, the higher a person\u2019s GPA, the more friends he/she had. Ask a few students to share out their responses to the very last question: \u201cCan you think of another variable (not necessarily given in the pictures) that might impact both the number of friends AND the variable you selected? Give an example and explain how it might impact each of the variables.\u201d Answers will vary, but one example could be: a person\u2019s self-esteem level (if he/she is confident in school, his/her grades might be higher; higher confidence could also be a reason for a person having more friends). Remind students that in the previous section, they learned about the elements of an experiment. In teams, ask students to discuss how collecting this data is similar or different from experiments. Then have a whole class discussion about this comparison, guiding students to realize that there were no assignments to groups and no treatment was applied. The subjects (i.e. the people displayed on the cards) were simply observed, and then information about them was recorded. Inform students that an observational study is a data collection method in which subjects are observed and outcomes are recorded. No treatment is applied to the subjects. Instead, researchers are simply watching something happen and have absolutely no control over it. In lesson 7 , students will learn more about the differences between experiments and observational studies and what conclusions they can make about each.","title":"Lesson:"},{"location":"unit3/lesson6/#class-scribes","text":"One team of students will give a brief talk to discuss what they think the 3 most important topics of the day were.","title":"Class Scribes:"},{"location":"unit3/lesson7/","text":"Lesson 7: Observational Studies vs. Experiments Objective: Students will learn how observational studies differ from experiments, and will classify different research scenarios based on which method would be most appropriate. They will also learn about the roles of ethics, cost limitations, and feasibility when deciding between the two data collection methods. Materials: What Should We Do? handout ( LMR_3.3_ObsStudies vs Experiments ) Vocabulary: ethics cost limitations feasibility Essential Concepts: Essential Concepts: Experiments are not always possible because of various factors such as ethics, cost limitations, and feasibility. Lesson: Remind students that in observational studies, we can never randomly assign subjects to treatment and control groups. Conversely, in experiments, we always need to have random assignment into these groups. Pose the following question to students: Why can\u2019t we just always do experiments? Have students discuss this question in their student teams and write down a few responses in their DS journals. Inform students that a researcher wants to perform studies to answer the research questions below. In teams, have students come up with reasons for why an experiment would not be possible for each scenario. Does smoking cause lung cancer? Unethical. You cannot make people smoke cigarettes and then see if they have lung cancer later in life. Does drinking water from Mars keep you healthier than drinking water from Earth? Cost. It would be incredibly expensive to design a space shuttle that can successfully transport people to Mars and have them live there for an extended period of time and most researchers would not have the funding to do this. Do people with higher IQ scores score better on the SAT than people with lower IQ scores? Not feasible/not possible. You cannot randomly assign IQ scores to people because it is a measurement based on aptitude. Select three teams and assign a scenario above to each team. Ask each team to report out on their assigned scenario. As teams share, be sure to discuss the following issues regarding why we cannot always to experiments: Ethics: Sometimes, experiments cannot be performed because it would be unethical to give certain treatments to subjects. For example, we could not inject an HIV infection into participants because the long-term effects might lead to death. Cost Limitations: Sometimes, experiments would be very costly and much too expensive to perform. Some possibilities could be with technology. Feasibility , impossible to randomize: In certain cases, you cannot perform an experiment because it is impossible to randomly assign people to particular groups. For example, you cannot assign a gender to a person. Distribute What Should We Do? handout ( LMR_3.3 ). In teams, students will identify whether the research question could best be answered via an experiment or an observational study. Once all student teams have completed the handout, assign one research question to each team to report out. As each response is shared, conduct a whole-class discussion to compare which data collection method was most appropriate for each research question. Ensure everyone understands the reasons each method was chosen before moving on to the next scenario. Note: Page 2 of the handout is an answer key for teacher reference only! LMR_3.3 Next, student teams will generate three research questions on their own. They need to identify the best data collection method for answering their question and should provide an explanation. At least one of the three research questions should use an observational study for data collection. Using a share-out strategy, have the reporter of each team share one of their investigation questions with the rest of the class. Class Scribes: One team of students will give a brief talk to discuss what they think the 3 most important topics of the day were.","title":"Lesson 7: Observational Studies vs. Experiments"},{"location":"unit3/lesson7/#lesson-7-observational-studies-vs-experiments","text":"","title":"Lesson 7: Observational Studies vs. Experiments"},{"location":"unit3/lesson7/#objective","text":"Students will learn how observational studies differ from experiments, and will classify different research scenarios based on which method would be most appropriate. They will also learn about the roles of ethics, cost limitations, and feasibility when deciding between the two data collection methods.","title":"Objective:"},{"location":"unit3/lesson7/#materials","text":"What Should We Do? handout ( LMR_3.3_ObsStudies vs Experiments )","title":"Materials:"},{"location":"unit3/lesson7/#vocabulary","text":"ethics cost limitations feasibility","title":"Vocabulary:"},{"location":"unit3/lesson7/#essential-concepts","text":"Essential Concepts: Experiments are not always possible because of various factors such as ethics, cost limitations, and feasibility.","title":"Essential Concepts:"},{"location":"unit3/lesson7/#lesson","text":"Remind students that in observational studies, we can never randomly assign subjects to treatment and control groups. Conversely, in experiments, we always need to have random assignment into these groups. Pose the following question to students: Why can\u2019t we just always do experiments? Have students discuss this question in their student teams and write down a few responses in their DS journals. Inform students that a researcher wants to perform studies to answer the research questions below. In teams, have students come up with reasons for why an experiment would not be possible for each scenario. Does smoking cause lung cancer? Unethical. You cannot make people smoke cigarettes and then see if they have lung cancer later in life. Does drinking water from Mars keep you healthier than drinking water from Earth? Cost. It would be incredibly expensive to design a space shuttle that can successfully transport people to Mars and have them live there for an extended period of time and most researchers would not have the funding to do this. Do people with higher IQ scores score better on the SAT than people with lower IQ scores? Not feasible/not possible. You cannot randomly assign IQ scores to people because it is a measurement based on aptitude. Select three teams and assign a scenario above to each team. Ask each team to report out on their assigned scenario. As teams share, be sure to discuss the following issues regarding why we cannot always to experiments: Ethics: Sometimes, experiments cannot be performed because it would be unethical to give certain treatments to subjects. For example, we could not inject an HIV infection into participants because the long-term effects might lead to death. Cost Limitations: Sometimes, experiments would be very costly and much too expensive to perform. Some possibilities could be with technology. Feasibility , impossible to randomize: In certain cases, you cannot perform an experiment because it is impossible to randomly assign people to particular groups. For example, you cannot assign a gender to a person. Distribute What Should We Do? handout ( LMR_3.3 ). In teams, students will identify whether the research question could best be answered via an experiment or an observational study. Once all student teams have completed the handout, assign one research question to each team to report out. As each response is shared, conduct a whole-class discussion to compare which data collection method was most appropriate for each research question. Ensure everyone understands the reasons each method was chosen before moving on to the next scenario. Note: Page 2 of the handout is an answer key for teacher reference only! LMR_3.3 Next, student teams will generate three research questions on their own. They need to identify the best data collection method for answering their question and should provide an explanation. At least one of the three research questions should use an observational study for data collection. Using a share-out strategy, have the reporter of each team share one of their investigation questions with the rest of the class.","title":"Lesson:"},{"location":"unit3/lesson7/#class-scribes","text":"One team of students will give a brief talk to discuss what they think the 3 most important topics of the day were.","title":"Class Scribes:"},{"location":"unit3/lesson8/","text":"Lesson 8: Monsters that Hide in Observational Studies Objective: Students will learn about confounding factors that may impact the results of an observational study, which is why causation can never be concluded with observational studies, only associations between variables. Materials: Computers Spurious Correlations website ( tylervigen.com ) Vocabulary: cause confounding factors associated Essential Concepts: Essential Concepts: Confounding factors/variables make it difficult to determine a cause-and-effect relationship between two variables. Lesson: Ask students to recall that they looked at the relationship between a student\u2019s GPA and the number of friends that person has on social media during lesson 6. It seemed that students with higher GPAs had more friends than students with lower GPAs. But did this mean that the cause of a person\u2019s GPA is the amount of friends they have? NO! They also identified other variables that could have contributed to the relationship, these outside variables are called confounding factors . Confounding factors are variables that are related to both the explanatory variable and the response variable in an observational study. Propose the following statement to students: \u201cResearch suggests that a rise in umbrella sales leads to decreased crime rates.\u201d Allow the students to work in teams to think about possible confounding factors. They should choose a variable that is related to umbrella sales, and that might lead to decreased crime rates. After they\u2019ve come up with a few possibilities, use the following diagram progression to further explain the impact of confounding factors. Step 1: Draw an arrow showing that \u201ca rise in umbrella sales leads to decreased crime rates\u201d since that is what researchers have stated. Step 2: Include the variable that might be related to people buying more umbrellas (i.e., the confounding factor). For example, when the weather is rainy, people buy more umbrellas. Step 3: Draw an additional arrow from \u201cWeather\u201d to \u201cCrime Rates Down\u201d because it is well known that when the weather is bad, people are less likely to be outside committing crimes. Step 4: Remind students that the original claim was that \u201ca rise in umbrella sales leads to decreased crime rates\u201d.\u201d However, we\u2019ve now shown that maybe buying umbrellas is not the only thing that could be contributing to a decrease in crime, which makes us question the link between the two variables. Step 5: Therefore, we have found a confounding factor with the variable \u201ccrime rates.\u201d This means we can erase the original \u201clink\u201d between a rise in umbrella sales and decreased crime rates since there are outside variables interfering. We can\u2019t say buying umbrellas causes decreased crime rates, but we can say that a rise in umbrella sales are associated with decreased crime rates. Once the students grasp what confounding factors are and how to identify them, introduce them to the website Spurious Correlations by Tyler Vigen. This site shows many explanatory and response variables that are randomly associated with each other. Spurious Correlations can be found at: http://www.tylervigen.com/spurious-correlations . For the example given above, we see that as the US spends more money on science, space, and technology, more people are dying by way of suicide. Clearly, it does not make sense that if the US keeps spending money on science, then more people are going to commit suicide. It simply happened by chance (or a bizarre chain of confounding factors) that the two variables are related to each other. Allow the students to explore the website on their own (Note: there are multiple pages of graphs, so they are not restricted to simply the homepage). They should choose a graph that interests them and answer the following questions in their DS journals: What are the two variables shown in your graph? Is there a positive association or a negative association between the variables? Write an interpretation of this plot in the context of the data. Write the data points in a \"spreadsheet format\" in a form that RStudio could read. Each row should represent a point on the graph, and each column one of the two variables. By hand, make a scatterplot of the association. Describe whether the association seems strong or weak or moderate to you. Do you think that the explanatory variable causes the response variable? Explain. If you answered 'no' to f, then draw a diagram like in #4 with possible confounding factors. Note: this can be difficult, depending on the graph chosen. Some factors to consider: weather, economy, fashion trends. Example answers to Step 7 are given below: What are the two variables shown in your graph? Total revenue generated by arcades in the US and the number of computer science doctorates awarded. Is there a positive association or a negative association between the variables? There is a direct relationship because the lines have the same shape (they follow the same pattern). Write an interpretation of this plot in the context of the data. It seems that as more doctorates are awarded to computer scientists, arcades are generating more revenue. Arcade Revenue CS doctorates 1196 861 1176 830 etc. Answers will vary. Can you conclude that the one variable causes the other? No. Although the two variables are associated with one another, we do not have evidence to say that more doctorate awards cause arcades to make more money because the data do not come from a controlled experiment. Draw a diagram like the one we did together earlier (in step 4 of lesson) with possible confounding factors. Student\u2019s diagram should look like the one below: Once all students have selected a graph and have answered the above questions, have them share their responses with a partner. They should explain why they thought their particular graph was interesting, how the two variables are related (directly or inversely), and whether or not there is a causal link between the variables. At the end of this lesson, students should understand that causation can only be concluded when an experiment is performed, but associations can be concluded for observational studies. Class Scribes: One team of students will give a brief talk to discuss what they think the 3 most important topics of the day were. Next Day LAB 3B: Confound it all! Complete Lab 3B prior to Lesson 9 .","title":"Lesson 8: Monsters That Hide in Observational Studies"},{"location":"unit3/lesson8/#lesson-8-monsters-that-hide-in-observational-studies","text":"","title":"Lesson 8: Monsters that Hide in Observational Studies"},{"location":"unit3/lesson8/#objective","text":"Students will learn about confounding factors that may impact the results of an observational study, which is why causation can never be concluded with observational studies, only associations between variables.","title":"Objective:"},{"location":"unit3/lesson8/#materials","text":"Computers Spurious Correlations website ( tylervigen.com )","title":"Materials:"},{"location":"unit3/lesson8/#vocabulary","text":"cause confounding factors associated","title":"Vocabulary:"},{"location":"unit3/lesson8/#essential-concepts","text":"Essential Concepts: Confounding factors/variables make it difficult to determine a cause-and-effect relationship between two variables.","title":"Essential Concepts:"},{"location":"unit3/lesson8/#lesson","text":"Ask students to recall that they looked at the relationship between a student\u2019s GPA and the number of friends that person has on social media during lesson 6. It seemed that students with higher GPAs had more friends than students with lower GPAs. But did this mean that the cause of a person\u2019s GPA is the amount of friends they have? NO! They also identified other variables that could have contributed to the relationship, these outside variables are called confounding factors . Confounding factors are variables that are related to both the explanatory variable and the response variable in an observational study. Propose the following statement to students: \u201cResearch suggests that a rise in umbrella sales leads to decreased crime rates.\u201d Allow the students to work in teams to think about possible confounding factors. They should choose a variable that is related to umbrella sales, and that might lead to decreased crime rates. After they\u2019ve come up with a few possibilities, use the following diagram progression to further explain the impact of confounding factors. Step 1: Draw an arrow showing that \u201ca rise in umbrella sales leads to decreased crime rates\u201d since that is what researchers have stated. Step 2: Include the variable that might be related to people buying more umbrellas (i.e., the confounding factor). For example, when the weather is rainy, people buy more umbrellas. Step 3: Draw an additional arrow from \u201cWeather\u201d to \u201cCrime Rates Down\u201d because it is well known that when the weather is bad, people are less likely to be outside committing crimes. Step 4: Remind students that the original claim was that \u201ca rise in umbrella sales leads to decreased crime rates\u201d.\u201d However, we\u2019ve now shown that maybe buying umbrellas is not the only thing that could be contributing to a decrease in crime, which makes us question the link between the two variables. Step 5: Therefore, we have found a confounding factor with the variable \u201ccrime rates.\u201d This means we can erase the original \u201clink\u201d between a rise in umbrella sales and decreased crime rates since there are outside variables interfering. We can\u2019t say buying umbrellas causes decreased crime rates, but we can say that a rise in umbrella sales are associated with decreased crime rates. Once the students grasp what confounding factors are and how to identify them, introduce them to the website Spurious Correlations by Tyler Vigen. This site shows many explanatory and response variables that are randomly associated with each other. Spurious Correlations can be found at: http://www.tylervigen.com/spurious-correlations . For the example given above, we see that as the US spends more money on science, space, and technology, more people are dying by way of suicide. Clearly, it does not make sense that if the US keeps spending money on science, then more people are going to commit suicide. It simply happened by chance (or a bizarre chain of confounding factors) that the two variables are related to each other. Allow the students to explore the website on their own (Note: there are multiple pages of graphs, so they are not restricted to simply the homepage). They should choose a graph that interests them and answer the following questions in their DS journals: What are the two variables shown in your graph? Is there a positive association or a negative association between the variables? Write an interpretation of this plot in the context of the data. Write the data points in a \"spreadsheet format\" in a form that RStudio could read. Each row should represent a point on the graph, and each column one of the two variables. By hand, make a scatterplot of the association. Describe whether the association seems strong or weak or moderate to you. Do you think that the explanatory variable causes the response variable? Explain. If you answered 'no' to f, then draw a diagram like in #4 with possible confounding factors. Note: this can be difficult, depending on the graph chosen. Some factors to consider: weather, economy, fashion trends. Example answers to Step 7 are given below: What are the two variables shown in your graph? Total revenue generated by arcades in the US and the number of computer science doctorates awarded. Is there a positive association or a negative association between the variables? There is a direct relationship because the lines have the same shape (they follow the same pattern). Write an interpretation of this plot in the context of the data. It seems that as more doctorates are awarded to computer scientists, arcades are generating more revenue. Arcade Revenue CS doctorates 1196 861 1176 830 etc. Answers will vary. Can you conclude that the one variable causes the other? No. Although the two variables are associated with one another, we do not have evidence to say that more doctorate awards cause arcades to make more money because the data do not come from a controlled experiment. Draw a diagram like the one we did together earlier (in step 4 of lesson) with possible confounding factors. Student\u2019s diagram should look like the one below: Once all students have selected a graph and have answered the above questions, have them share their responses with a partner. They should explain why they thought their particular graph was interesting, how the two variables are related (directly or inversely), and whether or not there is a causal link between the variables. At the end of this lesson, students should understand that causation can only be concluded when an experiment is performed, but associations can be concluded for observational studies.","title":"Lesson:"},{"location":"unit3/lesson8/#class-scribes","text":"One team of students will give a brief talk to discuss what they think the 3 most important topics of the day were.","title":"Class Scribes:"},{"location":"unit3/lesson8/#next-day","text":"LAB 3B: Confound it all! Complete Lab 3B prior to Lesson 9 .","title":"Next Day"},{"location":"unit3/lesson9/","text":"Lesson 9: Survey Says\u2026 Objective: Students will learn that a survey is another data collection method. They will learn what a survey is, what types of questions are used in a survey, and how a survey is conducted. Materials: Video: Family Feud\u2019s \u201cShocking Fast Money\u201d found at: https://www.youtube.com/watch_popup?v=-3Nk9t7-rCs (good quality, but sad ending) OR Video: Family Feud video clip titled \u201cFamily Feud \u2013 Comeback of the Century\u201d found at: https://www.youtube.com/watch_popup?v=ofQkOfeg60g (bad quality, but happy ending) Designing a Survey handout ( LMR_3.4_Designing a Survey ) Vocabulary: survey self-reported open-ended questions closed-ended questions Essential Concepts: Essential Concepts: Surveys ask simple, straightforward questions in order to collect data that can be used to answer statistical investigative questions. Writing such questions can be hard (but fun)! Lesson: Introduce one of the videos listed above by informing students that they will be watching a clip from the television game show Family Feud . This segment of the show is called Fast Money , where the winning family plays for an additional $20,000 (in the older video, they are playing for an additional $10,000 instead of $20,000). Two family members are chosen to play and must reach a combined score of 200 points to win the money. The goal is to guess the most common responses to five questions. For example, if the question \u201cWhat animal is a common pet?\u201d were asked, each family member might answer with \u201cdog\u201d or \u201ccat\u201d since these are popular household pets. The first person accumulates as many points as possible during the 20-second first round. The second person is given 25 seconds to earn points with different answers. As students watch the video, have them answer the following questions in their DS journals: How many people were surveyed? 100 Who was represented in the survey? Single men How many survey questions were asked? 5 When the host says \u201csurvey said\u201d and we see the response, what does it mean? It means that X number of people out of the 100 gave that response to the survey question. Family Feud uses surveys as its main data collection tool. In their DS journals, students should write down what they know about surveys individually. Then, with a partner, students will share what each one of them knows about surveys. Select a couple of students to either share their response or their partner\u2019s response with the whole class. Inform students that a survey is a data collection method where the data are self-reported , meaning that participants answer questions themselves. Surveys are composed of: Questionnaires or a series of questions A representative sample of the population of interest Carefully worded questions Surveys rely on questions. There are two types of questions that can be asked in a survey: open-ended questions and closed-ended questions . Open-ended questions offer a free-response/ text approach, whereas closed-ended questions give a fixed set of choices. Display the following list to the class. With a partner, have the students categorize the following types of questions as either open-ended or closed-ended: Multiple choice (closed) Write a paragraph (open) Yes/No (closed) Comments (open) Essays (open) On a scale from 1 to 5 (closed) Choose from a list (closed) Write a sentence (open) Check a box (closed) Do a quick Whip Around to share the categorization for each type of question. Be sure that students make corrections to the list if any items were miscategorized. Quickly review the Data Cycle. To give students an introduction to conducting surveys, they will first go through a practice scenario as a class to try to answer the following research question: What are \u2018families\u2019 in the United States? Distribute the Designing a Survey handout ( LMR_3.4 ) and let students fill in the boxes for \u201cResearch Topic\u201d (Families) and \u201cResearch Question\u201d (What are \u2018families\u2019 in the United States?) . LMR_3.4 Inform students that the left side of the handout will be completed as a class, and then student teams will work together to complete the right side. Using the Data Cycle as a guide, students should brainstorm a statistical investigative question that is related to the research question. One might be: What is the typical family size in the United States? Note: This requires a definition of \u201cfamily,\u201d which can have a variety of meanings to different people. Different definitions will likely guide the discussion of possible survey questions in the following step. Next, students need to determine 3 survey questions to help answer the statistical investigative question. The goal in creating survey questions is to make sure they (1) are unambiguous, and (2) address the statistical investigative question. Some examples are listed below (which come from different definitions of \u201cfamily\u201d): Note: Survey questions MUST match the statistical investigative question. How many siblings do you have? How many people live with you? It may help to actually collect data once the first survey question has been created. For example: \u201cHow many siblings do you have?\u201d \u2013 each student would give a response and the values could be recorded in a dotplot (if desired). If the question is too vague (do we include half-siblings, step-siblings, etc.?), students can revise the question. Once the class has agreed upon 3 survey questions for the first statistical investigative question, allow students to join their student teams for the remainder of the activity. Each team should come up with a statistical investigative question that might answer the research question, then determine 3 survey questions that match their statistical investigative question. Have the students create both open- and closed-ended questions in the handout. Each survey question should be a different type (see Step 8). Have student teams share out their statistical investigative questions and related survey questions with the rest of the class. Class Scribes: One team of students will give a brief talk to discuss what they think the 3 most important topics of the day were. Homework & Next Day For homework, students should choose one of their team\u2019s survey questions and rewrite it 3 ways, using 3 different question types (see Step 8). Example rewrites for the survey question \u201cHow many siblings do you have?\u201d are given below for reference. (a.) Multiple choice: How many siblings do you have? Select one option. (a) 0 siblings (b) 1 sibling (c) 2 siblings (d) 3 siblings (e) more than 3 siblings (b.) Write a paragraph: In your own words, describe your siblings. (c.) Yes/No: Do you have any siblings?","title":"Lesson 9: Survey Says\u2026"},{"location":"unit3/lesson9/#lesson-9-survey-says","text":"","title":"Lesson 9: Survey Says\u2026"},{"location":"unit3/lesson9/#objective","text":"Students will learn that a survey is another data collection method. They will learn what a survey is, what types of questions are used in a survey, and how a survey is conducted.","title":"Objective:"},{"location":"unit3/lesson9/#materials","text":"Video: Family Feud\u2019s \u201cShocking Fast Money\u201d found at: https://www.youtube.com/watch_popup?v=-3Nk9t7-rCs (good quality, but sad ending) OR Video: Family Feud video clip titled \u201cFamily Feud \u2013 Comeback of the Century\u201d found at: https://www.youtube.com/watch_popup?v=ofQkOfeg60g (bad quality, but happy ending) Designing a Survey handout ( LMR_3.4_Designing a Survey )","title":"Materials:"},{"location":"unit3/lesson9/#vocabulary","text":"survey self-reported open-ended questions closed-ended questions","title":"Vocabulary:"},{"location":"unit3/lesson9/#essential-concepts","text":"Essential Concepts: Surveys ask simple, straightforward questions in order to collect data that can be used to answer statistical investigative questions. Writing such questions can be hard (but fun)!","title":"Essential Concepts:"},{"location":"unit3/lesson9/#lesson","text":"Introduce one of the videos listed above by informing students that they will be watching a clip from the television game show Family Feud . This segment of the show is called Fast Money , where the winning family plays for an additional $20,000 (in the older video, they are playing for an additional $10,000 instead of $20,000). Two family members are chosen to play and must reach a combined score of 200 points to win the money. The goal is to guess the most common responses to five questions. For example, if the question \u201cWhat animal is a common pet?\u201d were asked, each family member might answer with \u201cdog\u201d or \u201ccat\u201d since these are popular household pets. The first person accumulates as many points as possible during the 20-second first round. The second person is given 25 seconds to earn points with different answers. As students watch the video, have them answer the following questions in their DS journals: How many people were surveyed? 100 Who was represented in the survey? Single men How many survey questions were asked? 5 When the host says \u201csurvey said\u201d and we see the response, what does it mean? It means that X number of people out of the 100 gave that response to the survey question. Family Feud uses surveys as its main data collection tool. In their DS journals, students should write down what they know about surveys individually. Then, with a partner, students will share what each one of them knows about surveys. Select a couple of students to either share their response or their partner\u2019s response with the whole class. Inform students that a survey is a data collection method where the data are self-reported , meaning that participants answer questions themselves. Surveys are composed of: Questionnaires or a series of questions A representative sample of the population of interest Carefully worded questions Surveys rely on questions. There are two types of questions that can be asked in a survey: open-ended questions and closed-ended questions . Open-ended questions offer a free-response/ text approach, whereas closed-ended questions give a fixed set of choices. Display the following list to the class. With a partner, have the students categorize the following types of questions as either open-ended or closed-ended: Multiple choice (closed) Write a paragraph (open) Yes/No (closed) Comments (open) Essays (open) On a scale from 1 to 5 (closed) Choose from a list (closed) Write a sentence (open) Check a box (closed) Do a quick Whip Around to share the categorization for each type of question. Be sure that students make corrections to the list if any items were miscategorized. Quickly review the Data Cycle. To give students an introduction to conducting surveys, they will first go through a practice scenario as a class to try to answer the following research question: What are \u2018families\u2019 in the United States? Distribute the Designing a Survey handout ( LMR_3.4 ) and let students fill in the boxes for \u201cResearch Topic\u201d (Families) and \u201cResearch Question\u201d (What are \u2018families\u2019 in the United States?) . LMR_3.4 Inform students that the left side of the handout will be completed as a class, and then student teams will work together to complete the right side. Using the Data Cycle as a guide, students should brainstorm a statistical investigative question that is related to the research question. One might be: What is the typical family size in the United States? Note: This requires a definition of \u201cfamily,\u201d which can have a variety of meanings to different people. Different definitions will likely guide the discussion of possible survey questions in the following step. Next, students need to determine 3 survey questions to help answer the statistical investigative question. The goal in creating survey questions is to make sure they (1) are unambiguous, and (2) address the statistical investigative question. Some examples are listed below (which come from different definitions of \u201cfamily\u201d): Note: Survey questions MUST match the statistical investigative question. How many siblings do you have? How many people live with you? It may help to actually collect data once the first survey question has been created. For example: \u201cHow many siblings do you have?\u201d \u2013 each student would give a response and the values could be recorded in a dotplot (if desired). If the question is too vague (do we include half-siblings, step-siblings, etc.?), students can revise the question. Once the class has agreed upon 3 survey questions for the first statistical investigative question, allow students to join their student teams for the remainder of the activity. Each team should come up with a statistical investigative question that might answer the research question, then determine 3 survey questions that match their statistical investigative question. Have the students create both open- and closed-ended questions in the handout. Each survey question should be a different type (see Step 8). Have student teams share out their statistical investigative questions and related survey questions with the rest of the class.","title":"Lesson:"},{"location":"unit3/lesson9/#class-scribes","text":"One team of students will give a brief talk to discuss what they think the 3 most important topics of the day were.","title":"Class Scribes:"},{"location":"unit3/lesson9/#homework-next-day","text":"For homework, students should choose one of their team\u2019s survey questions and rewrite it 3 ways, using 3 different question types (see Step 8). Example rewrites for the survey question \u201cHow many siblings do you have?\u201d are given below for reference. (a.) Multiple choice: How many siblings do you have? Select one option. (a) 0 siblings (b) 1 sibling (c) 2 siblings (d) 3 siblings (e) more than 3 siblings (b.) Write a paragraph: In your own words, describe your siblings. (c.) Yes/No: Do you have any siblings?","title":"Homework &amp; Next Day"},{"location":"unit3/overview/","text":"Introduction to Data Science Daily Overview: Unit 3 Unit 3 Daily Overview: Unit 3 .tg {border-collapse:collapse;border-spacing:0;} .tg td{font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;} .tg th{font-family:Arial, sans-serif;font-size:14px;font-weight:normal;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;} .tg .tg-88nc{font-weight:bold;border-color:inherit;text-align:center} .tg .tg-yj5y{background-color:#efefef;border-color:inherit;text-align:center;vertical-align:top} .tg .tg-c3ow{border-color:inherit;text-align:center;vertical-align:top} .tg .tg-uys7{border-color:inherit;text-align:center} .tg .tg-pwj7{background-color:#efefef;border-color:inherit;text-align:left} .tg .tg-5e9r{background-color:#efefef;border-color:inherit;text-align:center} .tg .tg-xldj{border-color:inherit;text-align:left} .tg .tg-y698{background-color:#efefef;border-color:inherit;text-align:left;vertical-align:top} .tg .tg-0pky{border-color:inherit;text-align:left;vertical-align:top} Theme Day Lessons and Labs Campaign Topics Page Testing, Testing\u2026 1, 2, 3\u2026 (7 days) 1 Lesson 1: Anecdotes vs. Data Reading articles critically, data 221 2 Lesson 2: What is an Experiment? Experiments, causation 224 3 Lesson 3: Let\u2019s Try an Experiment! Random assignments, confounding factors 227 4 Lesson 4: Predictions, Predictions Visualizations, predictions 229 5 Lesson 5: Time Perception Experiment Elements of an experiment 231 6 Lab 3A: The results are in! Analyzing experiment data 233 7 Practicum: Music to my Ears Design an experiment 234 Would You Look at That? (4 days) 8 Lesson 6: Observational Studies Observational study 237 9 Lesson 7: Observational Studies vs. Experiments Observational study, experiment 239 10 Lesson 8: Monsters that Hide in Observational Studies Observational study, confounding factors 241 11 Lab 3B: Confound it all! Confounding factors 245 Are You Asking Me? (9 days) 12 Lesson 9: Survey Says\u2026 Survey 249 13 Lesson 10: We\u2019re So Random Data collection, random samples 252 14 Lesson 11: The Gettysburg Address Sampling bias 256 15 Lab 3C: Random Sampling Random sampling 261 16 Lesson 12: Bias in Survey Sampling Bias, sampling methods 263 16 Lesson 13: The Confidence Game Confidence intervals 266 17 Lesson 14: How Confident Are You? Confidence intervals, margin of error 269 18 Lab 3D: Are You Sure about That? Bootstrapping 271 19 Practicum: Let\u2019s Build a Survey! Non-biased survey design 274 What\u2019s the Trigger? (5 days) 20 Lesson 15 Ready, Sense, Go! Sensors, data collection 277 21 Lesson 16: Does it have a Trigger? Survey questions, sensor questions 280 22 Lesson 17: Creating Our Own Participatory Sensing Campaign Participatory sensing campaign creation 283 23 Lesson 18: Evaluating Our Own Participatory Sensing Campaign Statistical questions, evaluate campaign 286 24^ Lesson 19: Implementing Our Own Participatory Sensing Campaign Class Campaign\u2014data Mock-implement campaign, campaign creation, data collection 288 Webpages (6 days) 29 Lesson 20: Online Data-ing Class Campaign\u2014data Data on the internet 292 30 Lab 3E: Scraping web data Class Campaign\u2014data Scraping data from the internet 296 31 Lab 3F: Maps Class Campaign\u2014data Making maps with data from the internet 299 32 Lesson 21: Learning to Love XML Class Campaign\u2014data Data storage, XML 301 33+ Lesson 22: Changing Orientation Class Campaign\u2014data Converting XML files 303 34 Practicum: What Does Our Campaign Data Say? Class Campaign Statistical questions, visualizations, numerical summaries 305 End of Unit Project (5 days) 35- 40 End of Unit Project: TB or Not TB Class Campaign Simulation using experiment data 306 ^=Data collection window begins. +=Data collection window ends.","title":"Daily Overview"},{"location":"unit3/overview/#introduction-to-data-science-daily-overview-unit-3","text":"","title":"Introduction to Data Science Daily Overview: Unit 3"},{"location":"unit3/overview/#daily-overview-unit-3","text":".tg {border-collapse:collapse;border-spacing:0;} .tg td{font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;} .tg th{font-family:Arial, sans-serif;font-size:14px;font-weight:normal;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;} .tg .tg-88nc{font-weight:bold;border-color:inherit;text-align:center} .tg .tg-yj5y{background-color:#efefef;border-color:inherit;text-align:center;vertical-align:top} .tg .tg-c3ow{border-color:inherit;text-align:center;vertical-align:top} .tg .tg-uys7{border-color:inherit;text-align:center} .tg .tg-pwj7{background-color:#efefef;border-color:inherit;text-align:left} .tg .tg-5e9r{background-color:#efefef;border-color:inherit;text-align:center} .tg .tg-xldj{border-color:inherit;text-align:left} .tg .tg-y698{background-color:#efefef;border-color:inherit;text-align:left;vertical-align:top} .tg .tg-0pky{border-color:inherit;text-align:left;vertical-align:top} Theme Day Lessons and Labs Campaign Topics Page Testing, Testing\u2026 1, 2, 3\u2026 (7 days) 1 Lesson 1: Anecdotes vs. Data Reading articles critically, data 221 2 Lesson 2: What is an Experiment? Experiments, causation 224 3 Lesson 3: Let\u2019s Try an Experiment! Random assignments, confounding factors 227 4 Lesson 4: Predictions, Predictions Visualizations, predictions 229 5 Lesson 5: Time Perception Experiment Elements of an experiment 231 6 Lab 3A: The results are in! Analyzing experiment data 233 7 Practicum: Music to my Ears Design an experiment 234 Would You Look at That? (4 days) 8 Lesson 6: Observational Studies Observational study 237 9 Lesson 7: Observational Studies vs. Experiments Observational study, experiment 239 10 Lesson 8: Monsters that Hide in Observational Studies Observational study, confounding factors 241 11 Lab 3B: Confound it all! Confounding factors 245 Are You Asking Me? (9 days) 12 Lesson 9: Survey Says\u2026 Survey 249 13 Lesson 10: We\u2019re So Random Data collection, random samples 252 14 Lesson 11: The Gettysburg Address Sampling bias 256 15 Lab 3C: Random Sampling Random sampling 261 16 Lesson 12: Bias in Survey Sampling Bias, sampling methods 263 16 Lesson 13: The Confidence Game Confidence intervals 266 17 Lesson 14: How Confident Are You? Confidence intervals, margin of error 269 18 Lab 3D: Are You Sure about That? Bootstrapping 271 19 Practicum: Let\u2019s Build a Survey! Non-biased survey design 274 What\u2019s the Trigger? (5 days) 20 Lesson 15 Ready, Sense, Go! Sensors, data collection 277 21 Lesson 16: Does it have a Trigger? Survey questions, sensor questions 280 22 Lesson 17: Creating Our Own Participatory Sensing Campaign Participatory sensing campaign creation 283 23 Lesson 18: Evaluating Our Own Participatory Sensing Campaign Statistical questions, evaluate campaign 286 24^ Lesson 19: Implementing Our Own Participatory Sensing Campaign Class Campaign\u2014data Mock-implement campaign, campaign creation, data collection 288 Webpages (6 days) 29 Lesson 20: Online Data-ing Class Campaign\u2014data Data on the internet 292 30 Lab 3E: Scraping web data Class Campaign\u2014data Scraping data from the internet 296 31 Lab 3F: Maps Class Campaign\u2014data Making maps with data from the internet 299 32 Lesson 21: Learning to Love XML Class Campaign\u2014data Data storage, XML 301 33+ Lesson 22: Changing Orientation Class Campaign\u2014data Converting XML files 303 34 Practicum: What Does Our Campaign Data Say? Class Campaign Statistical questions, visualizations, numerical summaries 305 End of Unit Project (5 days) 35- 40 End of Unit Project: TB or Not TB Class Campaign Simulation using experiment data 306 ^=Data collection window begins. +=Data collection window ends.","title":"Daily Overview: Unit 3"},{"location":"unit3/practicum1/","text":"Practicum: Music to my Ears Objective: Students will design a simple experiment. Materials: Practicum: Music to my ears ( LMR_U3_Practicum_Music to My Ears ) Note to Teacher: Before assigning the practicum to your students, engage the class in a discussion about experiments. Use the following questions as a guide to assess student understanding. When is random assignment used? Why is it important? Random assignment is used when you wish to determine whether a treatment causes changes in an outcome variable. It\u2019s important because it creates a \u201cbalance\u201d of the groups so that the only way the groups differ, on average, is that one gets the treatment and one does not. Thus, if there is a change in the outcome variable, only the treatment could have caused it. Below are some headlines, determine if they are causal or not. If not causal, re-write so that it is. If causal, state why it\u2019s causal. \u2022 Straight A\u2019s in high school may mean better health later in life. not causal, re-writing answers will vary \u2022 Murder rates affect IQ test scores: Study. causal, explanations will vary \u2022 Microbe linked to Alzheimer\u2019s Disease. not causal, re-writing answers will vary \u2022 Luckiest people \u201cborn in summer\u201d. causal, explanations will vary Why is a control group important? The control group is important because it allows us to measure the effects of the treatment group with an untreated comparable group. Without the control group, we don't know what would have happened if we had done nothing. [Think of a new vaccine for the flu. If there is no control group, and we see the treatment group improving, we will never know if they would have improved anyways, without the vaccine.] Practicum Music to my Ears In class, you designed and conducted the Time Perception experiment to find out if a person\u2019s perception of time changed when exposed to a stimulus. This experiment was designed so that it used random assignment, which is the process of using a chance device (e.g., dice, RStudio, etc.) to determine the placement of subjects into the treatment and control groups. By randomizing, you are removing other possible explanations for why the results happened the way they did. Now we are asking you to design an experiment to determine whether doing math homework with music playing in the background affects student\u2019s test scores. Work with your team to design this experiment. Submit a paper that clearly lays out your team\u2019s design plan. Be sure to include: Descriptions of each element of the experiment by answering the following questions: a. What is the research question we are interested in addressing? b. Who are the subjects that would be participating in the experiment? How should we select them? c. What could be possible treatments? What treatment do you choose and why? What will the control group do in your study? d. Describe how to randomize the subjects into the treatment and control groups. e. What is the outcome variable that we are measuring? Is it categorical or numerical? What other variables will you measure for each subject? An analysis plan: a. What statistical questions will you ask to address your research question? b. What analyses (graphical and numerical) will you use to answer these questions? c. An explanation of how you will determine whether the treatment affects test scores.","title":"Practicum: Music to my Ears"},{"location":"unit3/practicum1/#practicum-music-to-my-ears","text":"","title":"Practicum: Music to my Ears"},{"location":"unit3/practicum1/#objective","text":"Students will design a simple experiment.","title":"Objective:"},{"location":"unit3/practicum1/#materials","text":"Practicum: Music to my ears ( LMR_U3_Practicum_Music to My Ears ) Note to Teacher: Before assigning the practicum to your students, engage the class in a discussion about experiments. Use the following questions as a guide to assess student understanding. When is random assignment used? Why is it important? Random assignment is used when you wish to determine whether a treatment causes changes in an outcome variable. It\u2019s important because it creates a \u201cbalance\u201d of the groups so that the only way the groups differ, on average, is that one gets the treatment and one does not. Thus, if there is a change in the outcome variable, only the treatment could have caused it. Below are some headlines, determine if they are causal or not. If not causal, re-write so that it is. If causal, state why it\u2019s causal. \u2022 Straight A\u2019s in high school may mean better health later in life. not causal, re-writing answers will vary \u2022 Murder rates affect IQ test scores: Study. causal, explanations will vary \u2022 Microbe linked to Alzheimer\u2019s Disease. not causal, re-writing answers will vary \u2022 Luckiest people \u201cborn in summer\u201d. causal, explanations will vary Why is a control group important? The control group is important because it allows us to measure the effects of the treatment group with an untreated comparable group. Without the control group, we don't know what would have happened if we had done nothing. [Think of a new vaccine for the flu. If there is no control group, and we see the treatment group improving, we will never know if they would have improved anyways, without the vaccine.] Practicum Music to my Ears In class, you designed and conducted the Time Perception experiment to find out if a person\u2019s perception of time changed when exposed to a stimulus. This experiment was designed so that it used random assignment, which is the process of using a chance device (e.g., dice, RStudio, etc.) to determine the placement of subjects into the treatment and control groups. By randomizing, you are removing other possible explanations for why the results happened the way they did. Now we are asking you to design an experiment to determine whether doing math homework with music playing in the background affects student\u2019s test scores. Work with your team to design this experiment. Submit a paper that clearly lays out your team\u2019s design plan. Be sure to include: Descriptions of each element of the experiment by answering the following questions: a. What is the research question we are interested in addressing? b. Who are the subjects that would be participating in the experiment? How should we select them? c. What could be possible treatments? What treatment do you choose and why? What will the control group do in your study? d. Describe how to randomize the subjects into the treatment and control groups. e. What is the outcome variable that we are measuring? Is it categorical or numerical? What other variables will you measure for each subject? An analysis plan: a. What statistical questions will you ask to address your research question? b. What analyses (graphical and numerical) will you use to answer these questions? c. An explanation of how you will determine whether the treatment affects test scores.","title":"Materials:"},{"location":"unit3/practicum2/","text":"Practicum: Let\u2019s Build a Survey! Objective: Students will design a non-biased survey. Materials: Practicum: Let\u2019s Build a Survey! ( LMR_U3_Practicum_Build a Survey ) Practicum Let\u2019s Build a Survey! Based on what you have learned in Lessons 9 through 14 , you will now design a survey. You and your team members must do all of the following: Select a topic from the list below: a. Social Media b. Entertainment c. Sports d. The Environment e. Health f. Education g. Other topic of interest Create a research question about your topic of interest. Create a statistical question that is related to the research question. Identify the population of interest. Describe how you will select your sample from the population so that you'll be able to make generalizations about your population of interest. Identify the number of people who will be in your sample. Create five survey questions that will try to answer your statistical question and describe how you have made sure that they are non-leading questions. Identify a statistic that can be used to summarize the responses from this survey. Can you identify a parameter? Submit a typed paper that details the survey you just designed.","title":"Practicum: Let\u2019s Build a Survey!"},{"location":"unit3/practicum2/#practicum-lets-build-a-survey","text":"","title":"Practicum: Let\u2019s Build a Survey!"},{"location":"unit3/practicum2/#objective","text":"Students will design a non-biased survey.","title":"Objective:"},{"location":"unit3/practicum2/#materials","text":"Practicum: Let\u2019s Build a Survey! ( LMR_U3_Practicum_Build a Survey ) Practicum Let\u2019s Build a Survey! Based on what you have learned in Lessons 9 through 14 , you will now design a survey. You and your team members must do all of the following: Select a topic from the list below: a. Social Media b. Entertainment c. Sports d. The Environment e. Health f. Education g. Other topic of interest Create a research question about your topic of interest. Create a statistical question that is related to the research question. Identify the population of interest. Describe how you will select your sample from the population so that you'll be able to make generalizations about your population of interest. Identify the number of people who will be in your sample. Create five survey questions that will try to answer your statistical question and describe how you have made sure that they are non-leading questions. Identify a statistic that can be used to summarize the responses from this survey. Can you identify a parameter? Submit a typed paper that details the survey you just designed.","title":"Materials:"},{"location":"unit3/practicum3/","text":"Practicum: What Does Our Campaign Data Say? Objective: Students will answer the statistical question they generated at the beginning of the Participatory Sensing campaign creation lesson. They will use RStudio to make graphical representations or numerical summaries of their data to answer their question. Materials: Our Own Campaign ( LMR_U3_Practicum_Our Own Campaign ) Practicum Our Own Campaign At the start of the Participatory Sensing campaign creation in lesson 16 , the class developed a research question about your class\u2019s topic of interest. It is now time to analyze and interpret your class campaign data. You will use the data from your class-created campaign only. Based on the analysis, you can also wonder about what other data would be necessary to better answer your question, if any. Based on the class\u2019s campaign data collected: Refer back to the statistical questions your class generated in lesson 16 - 18 that address the research question. Choose one of these statistical questions and determine which variables will answer this question. Analyze the data to answer the question you've chosen. Your analysis should include graphs and numerical summaries. You should: a. Provide the plot and numerical summary. b. Describe what the plot shows. c. Explain why you chose to make that particular plot. d. Explain how the plot and numerical summary answers your statistical question. e. Include the code you used in RStudio to make your plot. After analyzing your data, determine if additional data would better answer your statistical question. If so, propose what that data would be. Different variables? Different data collection approach? Same variables, but more people? Same variables and people but more time? Now, choose two more statistical questions that address the research question. Analyze and interpret the data to answer these questions. Sometimes, when analyzing data, we think of new statistical questions to ask, or we realize that the data need to be cleaned before we can answer. Explain whether this is the case with any of your statistical questions. Write a one-page report and present it to another member of the class who is not in your team.","title":"Practicum: What Does Our Campaign Data Say?"},{"location":"unit3/practicum3/#practicum-what-does-our-campaign-data-say","text":"","title":"Practicum: What Does Our Campaign Data Say?"},{"location":"unit3/practicum3/#objective","text":"Students will answer the statistical question they generated at the beginning of the Participatory Sensing campaign creation lesson. They will use RStudio to make graphical representations or numerical summaries of their data to answer their question.","title":"Objective:"},{"location":"unit3/practicum3/#materials","text":"Our Own Campaign ( LMR_U3_Practicum_Our Own Campaign ) Practicum Our Own Campaign At the start of the Participatory Sensing campaign creation in lesson 16 , the class developed a research question about your class\u2019s topic of interest. It is now time to analyze and interpret your class campaign data. You will use the data from your class-created campaign only. Based on the analysis, you can also wonder about what other data would be necessary to better answer your question, if any. Based on the class\u2019s campaign data collected: Refer back to the statistical questions your class generated in lesson 16 - 18 that address the research question. Choose one of these statistical questions and determine which variables will answer this question. Analyze the data to answer the question you've chosen. Your analysis should include graphs and numerical summaries. You should: a. Provide the plot and numerical summary. b. Describe what the plot shows. c. Explain why you chose to make that particular plot. d. Explain how the plot and numerical summary answers your statistical question. e. Include the code you used in RStudio to make your plot. After analyzing your data, determine if additional data would better answer your statistical question. If so, propose what that data would be. Different variables? Different data collection approach? Same variables, but more people? Same variables and people but more time? Now, choose two more statistical questions that address the research question. Analyze and interpret the data to answer these questions. Sometimes, when analyzing data, we think of new statistical questions to ask, or we realize that the data need to be cleaned before we can answer. Explain whether this is the case with any of your statistical questions. Write a one-page report and present it to another member of the class who is not in your team.","title":"Materials:"},{"location":"unit3/section1/","text":"Unit 3, Section 1: Testing, Testing\u2026 1, 2, 3\u2026 Instructional Days: 7 Enduring Understandings An experiment is a data collection method in which the effects of different treatments on an outcome of interest are measured. In an experiment, a treatment is applied to subjects and then observations about the effect of the treatment are made. To isolate the effects from unexplained variation, randomization (or chance) assignment to treatments is applied. Engagement Students will view Hans Rosling\u2019s video How Not to Be Ignorant About the World and will participate in his interactive quiz in order to learn how anecdotes and personal experience can influence what we know and, alternatively, how data provides basis for evidence. The video can be found at: https://www.ted.com/talks/hans_and_ola_rosling_how_not_to_be_ignorant_about_the_world Learning Objectives Statistical/Mathematical: S-IC 1: Understand statistics as a process for making inferences about population parameters based on a random sample from that population. S-IC 3: Recognize the purposes of and differences among sample surveys, experiments, and observational studies; explain how randomization relates to each. S-IC 6: Evaluate reports based on data. S-IC B.5 Use data from a randomized experiment to compare two treatments; use simulations to decide if differences between parameters are significant. Focus Standards for Mathematical Practice for All of Unit 3: SMP-1: Make sense of problems and persevere in solving them. SMP-4: Model with mathematics. SMP-8: Look for and express regularity in repeated reasoning. Data Science: Understand that differences between the measured outcomes of the treatment and control groups in an experiment can be tested. Understand the roles of randomization and of random sampling in statistical inference. Applied Computational Thinking: \u2022 Test for differences between experimental groups. \u2022 Create graphical representations to compare data between experimental groups. \u2022 Write code to randomly assign subjects to treatment groups Real-World Connections: Experiments are used to ensure safety and efficacy of medicines, reliability of electronics and structural materials and find patterns in human behavior. Language Objectives Students will use complex sentences to construct summary statements about their understanding of data, how it is collected, how it is used, and how to work with it. Students will engage in partner and whole group discussions and presentations to express their understanding of data science concepts. Students will use complex sentences to write informative short reports that use data science concepts and skills. Students will read informative texts to evaluate claims based on data. Data File or Data Collection Method Data Collection Method: Students will gather data generated through a simple experiment. Data File: Students\u2019 Time Perception experiment data. Legend for Activity Icons","title":"Testing, Testing\u20261, 2, 3\u2026"},{"location":"unit3/section1/#unit-3-section-1-testing-testing-1-2-3","text":"Instructional Days: 7","title":"Unit 3, Section 1: Testing, Testing\u2026 1, 2, 3\u2026"},{"location":"unit3/section1/#enduring-understandings","text":"An experiment is a data collection method in which the effects of different treatments on an outcome of interest are measured. In an experiment, a treatment is applied to subjects and then observations about the effect of the treatment are made. To isolate the effects from unexplained variation, randomization (or chance) assignment to treatments is applied.","title":"Enduring Understandings"},{"location":"unit3/section1/#engagement","text":"Students will view Hans Rosling\u2019s video How Not to Be Ignorant About the World and will participate in his interactive quiz in order to learn how anecdotes and personal experience can influence what we know and, alternatively, how data provides basis for evidence. The video can be found at: https://www.ted.com/talks/hans_and_ola_rosling_how_not_to_be_ignorant_about_the_world","title":"Engagement"},{"location":"unit3/section1/#learning-objectives","text":"Statistical/Mathematical: S-IC 1: Understand statistics as a process for making inferences about population parameters based on a random sample from that population. S-IC 3: Recognize the purposes of and differences among sample surveys, experiments, and observational studies; explain how randomization relates to each. S-IC 6: Evaluate reports based on data. S-IC B.5 Use data from a randomized experiment to compare two treatments; use simulations to decide if differences between parameters are significant. Focus Standards for Mathematical Practice for All of Unit 3: SMP-1: Make sense of problems and persevere in solving them. SMP-4: Model with mathematics. SMP-8: Look for and express regularity in repeated reasoning. Data Science: Understand that differences between the measured outcomes of the treatment and control groups in an experiment can be tested. Understand the roles of randomization and of random sampling in statistical inference. Applied Computational Thinking: \u2022 Test for differences between experimental groups. \u2022 Create graphical representations to compare data between experimental groups. \u2022 Write code to randomly assign subjects to treatment groups Real-World Connections: Experiments are used to ensure safety and efficacy of medicines, reliability of electronics and structural materials and find patterns in human behavior.","title":"Learning Objectives"},{"location":"unit3/section1/#language-objectives","text":"Students will use complex sentences to construct summary statements about their understanding of data, how it is collected, how it is used, and how to work with it. Students will engage in partner and whole group discussions and presentations to express their understanding of data science concepts. Students will use complex sentences to write informative short reports that use data science concepts and skills. Students will read informative texts to evaluate claims based on data.","title":"Language Objectives"},{"location":"unit3/section1/#data-file-or-data-collection-method","text":"Data Collection Method: Students will gather data generated through a simple experiment. Data File: Students\u2019 Time Perception experiment data.","title":"Data File or Data Collection Method"},{"location":"unit3/section1/#legend-for-activity-icons","text":"","title":"Legend for Activity Icons"},{"location":"unit3/section2/","text":"Unit3, Section2: Would You Look at That? Instructional Days: 4 Enduring Understandings An observational study is a data collection method in which subjects are observed and outcomes are recorded. Unlike experiments, it may not be possible to assign subjects to treatment and control groups in observational studies, which impedes our ability to control for confounding factors. This means that researchers must rely on existing control and treatment groups to observe the outcomes. Observational studies can show associations in the data, but cause and effect relationships can only be concluded with experiments. Engagement Students will participate in the Observational Studies Activity described in Lesson 5 . They will record information that can be obtained through pictures. The data will then be analyzed to see if there are any variables related to the number of friends a person has on social media. Learning Objectives Statistical/Mathematical: S-IC 1. Understand statistics as a process for making inferences about population parameters based on a random sample from that population. S-IC 3. Recognize the purposes of and differences among sample surveys, experiments, and observational studies; explain how randomization relates to each. S-IC 6: Evaluate reports based on data. Data Science: Understand that data from observational studies can help us find associations among variables. Explain why some variables that are not related in reality might look as though they are due to the presence of confounding factors. Applied Computational Thinking using RStudio: \u2022 Download data from the Internet that was collected via an observational study. \u2022 Clean data set by adding variable names. \u2022 Create scatterplots of two variables and determine possible relationships between them, as well as identify potential confounding variables. Real-World Connections: Economists, psychologists, and biologists conduct observational studies to study human behavior. For example, observational studies are used in epidemiology to study outbreaks of illnesses and people\u2019s behavioral patterns. Language Objectives Students will use complex sentences to construct summary statements about their understanding of data, how it is collected, how it used, and how to work with it. Students will engage in partner and whole group discussions and presentations to express their understanding of data science concepts. Students will use complex sentences to write informative short reports that use data science concepts and skills. Students will read informative texts to evaluate claims based on data. Data File or Data Collection Method Data Collection Method: Students will record information about a set of high school students by observing characteristics given in a picture. Data File: Lung Capacity of Children data set information found at https://jse.amstat.org/v13n2/datasets.kahn.html NOTE: The raw data set can be found at https://jse.amstat.org/datasets/fev.dat.txt Legend for Activity Icons","title":"Would You Look at That?"},{"location":"unit3/section2/#unit3-section2-would-you-look-at-that","text":"Instructional Days: 4","title":"Unit3, Section2: Would You Look at That?"},{"location":"unit3/section2/#enduring-understandings","text":"An observational study is a data collection method in which subjects are observed and outcomes are recorded. Unlike experiments, it may not be possible to assign subjects to treatment and control groups in observational studies, which impedes our ability to control for confounding factors. This means that researchers must rely on existing control and treatment groups to observe the outcomes. Observational studies can show associations in the data, but cause and effect relationships can only be concluded with experiments.","title":"Enduring Understandings"},{"location":"unit3/section2/#engagement","text":"Students will participate in the Observational Studies Activity described in Lesson 5 . They will record information that can be obtained through pictures. The data will then be analyzed to see if there are any variables related to the number of friends a person has on social media.","title":"Engagement"},{"location":"unit3/section2/#learning-objectives","text":"Statistical/Mathematical: S-IC 1. Understand statistics as a process for making inferences about population parameters based on a random sample from that population. S-IC 3. Recognize the purposes of and differences among sample surveys, experiments, and observational studies; explain how randomization relates to each. S-IC 6: Evaluate reports based on data. Data Science: Understand that data from observational studies can help us find associations among variables. Explain why some variables that are not related in reality might look as though they are due to the presence of confounding factors. Applied Computational Thinking using RStudio: \u2022 Download data from the Internet that was collected via an observational study. \u2022 Clean data set by adding variable names. \u2022 Create scatterplots of two variables and determine possible relationships between them, as well as identify potential confounding variables. Real-World Connections: Economists, psychologists, and biologists conduct observational studies to study human behavior. For example, observational studies are used in epidemiology to study outbreaks of illnesses and people\u2019s behavioral patterns.","title":"Learning Objectives"},{"location":"unit3/section2/#language-objectives","text":"Students will use complex sentences to construct summary statements about their understanding of data, how it is collected, how it used, and how to work with it. Students will engage in partner and whole group discussions and presentations to express their understanding of data science concepts. Students will use complex sentences to write informative short reports that use data science concepts and skills. Students will read informative texts to evaluate claims based on data.","title":"Language Objectives"},{"location":"unit3/section2/#data-file-or-data-collection-method","text":"Data Collection Method: Students will record information about a set of high school students by observing characteristics given in a picture. Data File: Lung Capacity of Children data set information found at https://jse.amstat.org/v13n2/datasets.kahn.html NOTE: The raw data set can be found at https://jse.amstat.org/datasets/fev.dat.txt","title":"Data File or Data Collection Method"},{"location":"unit3/section2/#legend-for-activity-icons","text":"","title":"Legend for Activity Icons"},{"location":"unit3/section3/","text":"Unit3, Section3: Are You Asking Me? Instructional Days: 9 Enduring Understandings A survey is a data collection method that is administered to a sample. The sample is fraction of the target population. The sample must be representative of the population and random sampling is used to ensure an equal chance of being selected. A census is a special survey that collects data from the entire population. Sampling error and bias cause problems in analysis made from survey data. Engagement Students will view a video clip from the game show Family Feud to begin to think about survey components. The video can be found at: https://www.youtube.com/watch?v=-3Nk9t7-rCs https://www.youtube.com/embed/-3Nk9t7-rCs Learning Objectives Statistical/Mathematical: S-IC 1: Understand statistics as a process for making inferences about population parameters based on a random sample from that population. S-IC 3: Recognize the purposes of and differences among sample surveys, experiments, and observational studies; explain how randomization relates to each. S-IC 6: Evaluate reports based on data. S-IC B.4 Use data from a sample survey to estimate a population mean or proportion; develop a margin of error through the use of simulation models for random sampling. Data Science: Understand that bias and sampling error should be minimized when conducting surveys. The wording of questions, as well as who is asked to participate in a survey, can lead to bias. Learn that sampling error can be minimized when larger random samples are collected from a population. Applied Computational Thinking Using RStudio: \u2022 Create random samples of different sizes to make estimates about a population. \u2022 Create informal confidence intervals based on sample medians. Language Objectives Students will use complex sentences to construct summary statements about their understanding of data, how it is collected, how it is used, and how to work with it. Students will engage in partner and whole group discussions and presentations to express their understanding of data science concepts. Students will write questions that emphasize differences in data science concepts and skills. Data File or Data Collection Method Data File: American Time-Use Survey (ATUS) data Legend for Activity Icons","title":"Are You Asking Me?"},{"location":"unit3/section3/#unit3-section3-are-you-asking-me","text":"Instructional Days: 9","title":"Unit3, Section3: Are You Asking Me?"},{"location":"unit3/section3/#enduring-understandings","text":"A survey is a data collection method that is administered to a sample. The sample is fraction of the target population. The sample must be representative of the population and random sampling is used to ensure an equal chance of being selected. A census is a special survey that collects data from the entire population. Sampling error and bias cause problems in analysis made from survey data.","title":"Enduring Understandings"},{"location":"unit3/section3/#engagement","text":"Students will view a video clip from the game show Family Feud to begin to think about survey components. The video can be found at: https://www.youtube.com/watch?v=-3Nk9t7-rCs https://www.youtube.com/embed/-3Nk9t7-rCs","title":"Engagement"},{"location":"unit3/section3/#learning-objectives","text":"Statistical/Mathematical: S-IC 1: Understand statistics as a process for making inferences about population parameters based on a random sample from that population. S-IC 3: Recognize the purposes of and differences among sample surveys, experiments, and observational studies; explain how randomization relates to each. S-IC 6: Evaluate reports based on data. S-IC B.4 Use data from a sample survey to estimate a population mean or proportion; develop a margin of error through the use of simulation models for random sampling. Data Science: Understand that bias and sampling error should be minimized when conducting surveys. The wording of questions, as well as who is asked to participate in a survey, can lead to bias. Learn that sampling error can be minimized when larger random samples are collected from a population. Applied Computational Thinking Using RStudio: \u2022 Create random samples of different sizes to make estimates about a population. \u2022 Create informal confidence intervals based on sample medians.","title":"Learning Objectives"},{"location":"unit3/section3/#language-objectives","text":"Students will use complex sentences to construct summary statements about their understanding of data, how it is collected, how it is used, and how to work with it. Students will engage in partner and whole group discussions and presentations to express their understanding of data science concepts. Students will write questions that emphasize differences in data science concepts and skills.","title":"Language Objectives"},{"location":"unit3/section3/#data-file-or-data-collection-method","text":"Data File: American Time-Use Survey (ATUS) data","title":"Data File or Data Collection Method"},{"location":"unit3/section3/#legend-for-activity-icons","text":"","title":"Legend for Activity Icons"},{"location":"unit3/section4/","text":"Unit3, Section4: What\u2019s the Trigger? Instructional Days: 5 Enduring Understandings Sensors are data collection devices that collect data either continuously or whenever they are triggered. A sensor is a converter that measures a physical quantity and converts it into a signal, which can be read by an observer or by an instrument. Participatory Sensing is a specific data collection method that uses sensor technology. This method emphasizes the involvement of citizens and community groups in the process of sensing and documenting where they live, work, and play. Triggers play an important role in the Participatory Sensing data collection process. The response to the triggers may or may not be the same each time. Engagement Students will view and discuss a video clip called Play Like Nadal With a Smart Tennis Racket to begin to think about the sensors as data collection devices found ubiquitously in today\u2019s world. The video can be found at: https://youtu.be/lcBnzddQECc Learning Objectives Statistical/Mathematical: S-IC 3: Recognize the purposes of and differences among sample surveys, experiments, and observational studies; explain how randomization relates to each. S-IC 6: Evaluate reports based on data. Data Science: Understand that sensors provide a continuous stream of data. Participatory Sensing provides real-time data from a user who is willingly providing the data. What differentiates a sensor as a data gathering method is the use of a trigger that signals a data collection session. Applied Computational Thinking: \u2022 Create a Participatory Sensing campaign using a campaign Authoring Tool. Real-World Connections: Sensors are found everywhere in today\u2019s world. They can provide data about environmental conditions as well as personal habits. More and more, sensors are being used for personal tracking, especially in the medical field, to inform people about what they do. Language Objectives Students will use complex sentences to construct summary statements about their understanding of data, how it is collected, how it used, and how to work with it. Students will engage in partner and whole group discussions and presentations to express their understanding of data science concepts. Students will use write questions that use emphasize differences in data science concepts and skills. Data File or Data Collection Method Data Collection Method: Students will gather data generated through a class-generated Participatory Sensing campaign. Data File: Students\u2019 Participatory Sensing campaign data Legend for Activity Icons","title":"What\u2019s the Trigger?"},{"location":"unit3/section4/#unit3-section4-whats-the-trigger","text":"Instructional Days: 5","title":"Unit3, Section4: What\u2019s the Trigger?"},{"location":"unit3/section4/#enduring-understandings","text":"Sensors are data collection devices that collect data either continuously or whenever they are triggered. A sensor is a converter that measures a physical quantity and converts it into a signal, which can be read by an observer or by an instrument. Participatory Sensing is a specific data collection method that uses sensor technology. This method emphasizes the involvement of citizens and community groups in the process of sensing and documenting where they live, work, and play. Triggers play an important role in the Participatory Sensing data collection process. The response to the triggers may or may not be the same each time.","title":"Enduring Understandings"},{"location":"unit3/section4/#engagement","text":"Students will view and discuss a video clip called Play Like Nadal With a Smart Tennis Racket to begin to think about the sensors as data collection devices found ubiquitously in today\u2019s world. The video can be found at: https://youtu.be/lcBnzddQECc","title":"Engagement"},{"location":"unit3/section4/#learning-objectives","text":"Statistical/Mathematical: S-IC 3: Recognize the purposes of and differences among sample surveys, experiments, and observational studies; explain how randomization relates to each. S-IC 6: Evaluate reports based on data. Data Science: Understand that sensors provide a continuous stream of data. Participatory Sensing provides real-time data from a user who is willingly providing the data. What differentiates a sensor as a data gathering method is the use of a trigger that signals a data collection session. Applied Computational Thinking: \u2022 Create a Participatory Sensing campaign using a campaign Authoring Tool. Real-World Connections: Sensors are found everywhere in today\u2019s world. They can provide data about environmental conditions as well as personal habits. More and more, sensors are being used for personal tracking, especially in the medical field, to inform people about what they do.","title":"Learning Objectives"},{"location":"unit3/section4/#language-objectives","text":"Students will use complex sentences to construct summary statements about their understanding of data, how it is collected, how it used, and how to work with it. Students will engage in partner and whole group discussions and presentations to express their understanding of data science concepts. Students will use write questions that use emphasize differences in data science concepts and skills.","title":"Language Objectives"},{"location":"unit3/section4/#data-file-or-data-collection-method","text":"Data Collection Method: Students will gather data generated through a class-generated Participatory Sensing campaign. Data File: Students\u2019 Participatory Sensing campaign data","title":"Data File or Data Collection Method"},{"location":"unit3/section4/#legend-for-activity-icons","text":"","title":"Legend for Activity Icons"},{"location":"unit3/section5/","text":"Unit3, Section5: Webpages Instructional Days: 6 Enduring Understandings Data takes on a variety of forms online and requires a different style of representation. Engagement Students will view a video clip about a data farm, specifically, Google\u2019s Street View Data Center to begin thinking about data formats and accessing data online. The video can be found at: https://www.engadget.com/2012-10-17-google-inside-data-centers.html Learning Objectives Statistical/Mathematical: S-IC 3: Recognize the purposes of and differences among sample surveys, experiments, and observational studies; explain how randomization relates to each. S-IC 6: Evaluate reports based on data. DS: Use different techniques to access data from the web and understand why different data representations are useful for different software platforms. Applied Computational Thinking using RStudio: \u2022 Read data from xml and html table and convert to R data frames \u2022 Use latitude and longitude coordinates of mountain data and overlay it on a map Real-World Connections: Data from the web has been used to predict outbreaks of the flu and is a source of extremely rich data sets. Language Objectives Students will use complex sentences to construct summary statements about their understanding of data, how it is collected, how it used, and how to work with it. Students will engage in partner and whole group discussions and presentations to express their understanding of data science concepts. Students will engage in discussions regarding internet research as it applies to data science. Data File or Data Collection Method Data Collection Method: Students will scrape data from online HTML and XML sources. Legend for Activity Icons","title":"Webpages"},{"location":"unit3/section5/#unit3-section5-webpages","text":"Instructional Days: 6","title":"Unit3, Section5: Webpages"},{"location":"unit3/section5/#enduring-understandings","text":"Data takes on a variety of forms online and requires a different style of representation.","title":"Enduring Understandings"},{"location":"unit3/section5/#engagement","text":"Students will view a video clip about a data farm, specifically, Google\u2019s Street View Data Center to begin thinking about data formats and accessing data online. The video can be found at: https://www.engadget.com/2012-10-17-google-inside-data-centers.html","title":"Engagement"},{"location":"unit3/section5/#learning-objectives","text":"Statistical/Mathematical: S-IC 3: Recognize the purposes of and differences among sample surveys, experiments, and observational studies; explain how randomization relates to each. S-IC 6: Evaluate reports based on data. DS: Use different techniques to access data from the web and understand why different data representations are useful for different software platforms. Applied Computational Thinking using RStudio: \u2022 Read data from xml and html table and convert to R data frames \u2022 Use latitude and longitude coordinates of mountain data and overlay it on a map Real-World Connections: Data from the web has been used to predict outbreaks of the flu and is a source of extremely rich data sets.","title":"Learning Objectives"},{"location":"unit3/section5/#language-objectives","text":"Students will use complex sentences to construct summary statements about their understanding of data, how it is collected, how it used, and how to work with it. Students will engage in partner and whole group discussions and presentations to express their understanding of data science concepts. Students will engage in discussions regarding internet research as it applies to data science.","title":"Language Objectives"},{"location":"unit3/section5/#data-file-or-data-collection-method","text":"Data Collection Method: Students will scrape data from online HTML and XML sources.","title":"Data File or Data Collection Method"},{"location":"unit3/section5/#legend-for-activity-icons","text":"","title":"Legend for Activity Icons"},{"location":"unit4/end/","text":"End of Unit 4 Modeling Activity Project and Presentation Objective: Students will apply their learning of the third and fourth units of the curriculum by completing an end of unit modeling activity project. Materials: Computers IDS Unit 4 \u2013 Project and Presentation ( LMR_U4_Modeling Activity Project ) End of Unit 4 Project and Presentation: Community Issue At the beginning of this unit, you explored data from the Trash Participatory Sensing campaign as well as from drought.gov (U.S. drought). You also created a Participatory Sensing campaign to investigate an issue in your community. For this assignment, you will use the results of your Participatory Sensing campaign, and possibly an official dataset, to apply what you have learned in unit 4 and to answer the research question you chose with your group at the beginning of the unit. Your assignment is as follows: Research which government entity is responsible for the community issue that your team chose \u2013 it could be local, state, federal, or even an international entity. Propose one or two recommendations to the government entity in step 1 to help raise public awareness and/or alleviate the issue. Specifically, you will create a presentation in which you answer the following questions: What is/are the specific recommendation(s) you are proposing to increase public awareness and/or alleviate the issue? Why do you think this will work? What evidence do you have to support this? Include any necessary plots and analysis. Your 5-minute presentation comprising 4-5 slides should include: An introduction - Who are you? Introduce yourself and your team. The issue - What is the issue? Why is this issue important? Why should we care about it? The Participatory Sensing campaign - Explain your campaign. What was your research question? What statistical investigative question(s) were you hoping to answer? Your recommendation - What is you recommendation? How will it raise public awareness and/or alleviate the issue? Why do you think this will work? This is where you include your evidence: How does your article connect to your recommendation? How does your Participatory Sensing campaign data support your recommendation? Or how does it show that there is a lack of something needed? If you have an official dataset, how does it support your recommendation? Include visualizations, numerical summaries, and/or statistics. A closing - Summarize your point into a few closing sentences. Each person must participate in the presentation. In addition to the presentation, submit a 2-4 page, double-spaced summary of your analysis including plots/graphs.","title":"End of Unit 4 Modeling Activity Project and Presentation"},{"location":"unit4/end/#end-of-unit-4-modeling-activity-project-and-presentation","text":"","title":"End of Unit 4 Modeling Activity Project and Presentation"},{"location":"unit4/end/#objective","text":"Students will apply their learning of the third and fourth units of the curriculum by completing an end of unit modeling activity project.","title":"Objective:"},{"location":"unit4/end/#materials","text":"Computers IDS Unit 4 \u2013 Project and Presentation ( LMR_U4_Modeling Activity Project ) End of Unit 4 Project and Presentation: Community Issue At the beginning of this unit, you explored data from the Trash Participatory Sensing campaign as well as from drought.gov (U.S. drought). You also created a Participatory Sensing campaign to investigate an issue in your community. For this assignment, you will use the results of your Participatory Sensing campaign, and possibly an official dataset, to apply what you have learned in unit 4 and to answer the research question you chose with your group at the beginning of the unit. Your assignment is as follows: Research which government entity is responsible for the community issue that your team chose \u2013 it could be local, state, federal, or even an international entity. Propose one or two recommendations to the government entity in step 1 to help raise public awareness and/or alleviate the issue. Specifically, you will create a presentation in which you answer the following questions: What is/are the specific recommendation(s) you are proposing to increase public awareness and/or alleviate the issue? Why do you think this will work? What evidence do you have to support this? Include any necessary plots and analysis. Your 5-minute presentation comprising 4-5 slides should include: An introduction - Who are you? Introduce yourself and your team. The issue - What is the issue? Why is this issue important? Why should we care about it? The Participatory Sensing campaign - Explain your campaign. What was your research question? What statistical investigative question(s) were you hoping to answer? Your recommendation - What is you recommendation? How will it raise public awareness and/or alleviate the issue? Why do you think this will work? This is where you include your evidence: How does your article connect to your recommendation? How does your Participatory Sensing campaign data support your recommendation? Or how does it show that there is a lack of something needed? If you have an official dataset, how does it support your recommendation? Include visualizations, numerical summaries, and/or statistics. A closing - Summarize your point into a few closing sentences. Each person must participate in the presentation. In addition to the presentation, submit a 2-4 page, double-spaced summary of your analysis including plots/graphs.","title":"Materials:"},{"location":"unit4/essential/","text":"IDS Unit 4: Essential Concepts Lesson 1: Trash Exploring different datasets can give us insight about the same processes. Data from our Participatory Sensing campaigns rely on human sensors and limit the ability to generalize to the greater population. Lesson 2: Drought Data can be used to make predictions. Official datasets rely on censuses or random samples and can be used to make generalizations. Lesson 3: Community Connection Data collected through Participatory Sensing campaigns will be used to create models that answer real-world problems related to our community. Lesson 4: Evaluate and Implement the Campaign Statistical investigative questions guide a Participatory Sensing campaign so that we can learn about a community or ourselves. These campaigns should be evaluated before implementing to make sure they are reasonable and ethically sound. Lesson 5: Refine and Create the Campaign Statistical investigative questions guide a Participatory Sensing campaign so that we can learn about a community or ourselves. These campaigns should be tried before implementing to make sure they are collecting the data they are meant to collect and refined accordingly. Lesson 6: Statistical Predictions using One Variable Anyone can make a prediction. But statisticians measure the success of their predictions. This lesson encourages the classroom to consider different measures of success. Lesson 7: Statistical Predictions by Applying the Rule If we use the mean squared errors rule, then the mean of our current data is the best prediction of future values. If we use the mean absolute errors rule, then the median of the current data is the best prediction of future values. Lesson 8: Statistical Predictions Using Two Variables When predicting values of a variable y , and if y is linearly associated with x , then we can get improved predictions by using our knowledge about x . For every value of x , find the mean of the y values for that value of x . If the resulting mean follows a trend, we can model this trend to generalize to unseen values of x . Lesson 9: Spaghetti Line We can often use a straight line to summarize a trend. \u201cEyeballing\u201d a straight line to a scatterplot is one way to do this. Lesson 10: What\u2019s the Best Line? The regression line can be used to make good predictions about values of y for any given value of x . This works for exactly the same reason the mean works well for one variable: the predictions will make your score on the mean squared errors as small as possible. Lesson 11: What\u2019s the Trend? A positive or negative association between variables provides valuable insights into increasing or decreasing trends, particularly in making predictions. By understanding these associations, we can anticipate future outcomes or behaviors more accurately. Lesson 12: How Strong Is It? A high absolute value for correlation means a strong linear trend. A value close to 0 means a weak linear trend. Lesson 13: Improving your Model If a linear model is fit to a non-linear trend, it will not do a good job of predicting. For this reason, we need to identify non-linear trends by looking at a scatterplot or the model needs to match the trend. Lesson 14: More Variables to Make Better Predictions We can use scatterplots to assess which variables might lead to strong predictive models. Sometimes using several predictors in one model can produce stronger models. Lesson 15: Combination of Variables If multiple predictors are associated with the response variable, a better predictive model will be produced, as measured by the mean squared error. Lesson 16: Footbal or Futbol? Some trends are not linear, so the approaches we\u2019ve done so far won\u2019t be helpful. We need to model such trends differently. Decision trees are a non-linear tool for classifying observations into groups when the trend is non-linear. Lesson 17: Grow Your Own Decision Tree We can determine the usefulness of decision trees by comparing the number of misclassifications in each. Lesson 18: Where Do I Belong? We can identify groups, or \u201cclusters\u201d, in data based on a few characteristics. For example, it is easy to classify a group of people into football players and swimmers, but what if you only knew each person\u2019s arm span? How well could you classify them into football players and swimmers now? Lesson 19: Our Class Network Networks are made when observations are interconnected. In a social setting, we can examine how different people are connected by finding relationships between other people in a network.","title":"Essential Concepts"},{"location":"unit4/essential/#ids-unit-4-essential-concepts","text":"","title":"IDS Unit 4: Essential Concepts"},{"location":"unit4/essential/#lesson-1-trash","text":"Exploring different datasets can give us insight about the same processes. Data from our Participatory Sensing campaigns rely on human sensors and limit the ability to generalize to the greater population.","title":"Lesson 1: Trash"},{"location":"unit4/essential/#lesson-2-drought","text":"Data can be used to make predictions. Official datasets rely on censuses or random samples and can be used to make generalizations.","title":"Lesson 2: Drought"},{"location":"unit4/essential/#lesson-3-community-connection","text":"Data collected through Participatory Sensing campaigns will be used to create models that answer real-world problems related to our community.","title":"Lesson 3: Community Connection"},{"location":"unit4/essential/#lesson-4-evaluate-and-implement-the-campaign","text":"Statistical investigative questions guide a Participatory Sensing campaign so that we can learn about a community or ourselves. These campaigns should be evaluated before implementing to make sure they are reasonable and ethically sound.","title":"Lesson 4: Evaluate and Implement the Campaign"},{"location":"unit4/essential/#lesson-5-refine-and-create-the-campaign","text":"Statistical investigative questions guide a Participatory Sensing campaign so that we can learn about a community or ourselves. These campaigns should be tried before implementing to make sure they are collecting the data they are meant to collect and refined accordingly.","title":"Lesson 5: Refine and Create the Campaign"},{"location":"unit4/essential/#lesson-6-statistical-predictions-using-one-variable","text":"Anyone can make a prediction. But statisticians measure the success of their predictions. This lesson encourages the classroom to consider different measures of success.","title":"Lesson 6: Statistical Predictions using One Variable"},{"location":"unit4/essential/#lesson-7-statistical-predictions-by-applying-the-rule","text":"If we use the mean squared errors rule, then the mean of our current data is the best prediction of future values. If we use the mean absolute errors rule, then the median of the current data is the best prediction of future values.","title":"Lesson 7: Statistical Predictions by Applying the Rule"},{"location":"unit4/essential/#lesson-8-statistical-predictions-using-two-variables","text":"When predicting values of a variable y , and if y is linearly associated with x , then we can get improved predictions by using our knowledge about x . For every value of x , find the mean of the y values for that value of x . If the resulting mean follows a trend, we can model this trend to generalize to unseen values of x .","title":"Lesson 8: Statistical Predictions Using Two Variables"},{"location":"unit4/essential/#lesson-9-spaghetti-line","text":"We can often use a straight line to summarize a trend. \u201cEyeballing\u201d a straight line to a scatterplot is one way to do this.","title":"Lesson 9: Spaghetti Line"},{"location":"unit4/essential/#lesson-10-whats-the-best-line","text":"The regression line can be used to make good predictions about values of y for any given value of x . This works for exactly the same reason the mean works well for one variable: the predictions will make your score on the mean squared errors as small as possible.","title":"Lesson 10: What\u2019s the Best Line?"},{"location":"unit4/essential/#lesson-11-whats-the-trend","text":"A positive or negative association between variables provides valuable insights into increasing or decreasing trends, particularly in making predictions. By understanding these associations, we can anticipate future outcomes or behaviors more accurately.","title":"Lesson 11: What\u2019s the Trend?"},{"location":"unit4/essential/#lesson-12-how-strong-is-it","text":"A high absolute value for correlation means a strong linear trend. A value close to 0 means a weak linear trend.","title":"Lesson 12: How Strong Is It?"},{"location":"unit4/essential/#lesson-13-improving-your-model","text":"If a linear model is fit to a non-linear trend, it will not do a good job of predicting. For this reason, we need to identify non-linear trends by looking at a scatterplot or the model needs to match the trend.","title":"Lesson 13: Improving your Model"},{"location":"unit4/essential/#lesson-14-more-variables-to-make-better-predictions","text":"We can use scatterplots to assess which variables might lead to strong predictive models. Sometimes using several predictors in one model can produce stronger models.","title":"Lesson 14: More Variables to Make Better Predictions"},{"location":"unit4/essential/#lesson-15-combination-of-variables","text":"If multiple predictors are associated with the response variable, a better predictive model will be produced, as measured by the mean squared error.","title":"Lesson 15: Combination of Variables"},{"location":"unit4/essential/#lesson-16-footbal-or-futbol","text":"Some trends are not linear, so the approaches we\u2019ve done so far won\u2019t be helpful. We need to model such trends differently. Decision trees are a non-linear tool for classifying observations into groups when the trend is non-linear.","title":"Lesson 16: Footbal or Futbol?"},{"location":"unit4/essential/#lesson-17-grow-your-own-decision-tree","text":"We can determine the usefulness of decision trees by comparing the number of misclassifications in each.","title":"Lesson 17: Grow Your Own Decision Tree"},{"location":"unit4/essential/#lesson-18-where-do-i-belong","text":"We can identify groups, or \u201cclusters\u201d, in data based on a few characteristics. For example, it is easy to classify a group of people into football players and swimmers, but what if you only knew each person\u2019s arm span? How well could you classify them into football players and swimmers now?","title":"Lesson 18: Where Do I Belong?"},{"location":"unit4/essential/#lesson-19-our-class-network","text":"Networks are made when observations are interconnected. In a social setting, we can examine how different people are connected by finding relationships between other people in a network.","title":"Lesson 19: Our Class Network"},{"location":"unit4/lab4a/","text":"Lab 4A - If the line fits ... Directions: Follow along with the slides, completing the questions in blue on your computer, and answering the questions in red in your journal. How to make predictions Anyone can make predictions. \u2013 Data scientists use data to inform their predictions by using the information learned from the sample to make predictions for the whole population. In this lab, we'll learn how to make predictions by finding the line of best fit . \u2013 You will also learn how to use the information from one variable to make predictions about another variable. Predicting heights Use the data() function to load the arm_span data. This data comes from a sample of 90 people in the Los Angeles area. \u2013 The measurements of height and armspan are in inches. \u2013 A person's armspan is the maximum distance between their fingertips when they spread their arms out wide. Make a plot of the height variable. \u2013 If you had to predict the height of someone in the LA area, what single height would you choose and why? \u2013 Would you describe this as a good guess? What might you try to improve your predictions? Predicting heights knowing arm spans Create two subsets of our arm_span data: \u2013 One for armspan >= 61 and armspan <= 63 . \u2013 A second for armspan >= 64 and armspan <= 66 . Create a histogram for the height of people in each subset. Answer the following based on the data: \u2013 What height would you predict if you knew a person had an armspan around 62 inches? \u2013 What height would you predict if you knew a person had an armspan around 65 inches? \u2013 Does knowing someone's armspan help you predict their height? Why or why not? Fitting lines Notice that there is a trend that people with a larger armspan also tend to have a larger mean height . \u2013 One way of describing this sort of trend is with a line. Data scientists often fit lines to their data to make predictions. \u2013 What we mean by fit is to come up with a line that's close to as many of the data points as possible. Create a scatterplot for height and armspan . Then run the following code. add_line() On the Plot pane, click two data points to draw a line through. NOTE: If your line does not appear or it appears but is above the points you selected, zoom out on your browser (typically 50% if you have a Mac, 80% on Windows). Or if your line appears below the points you selected, zoom in on your browser. Then run the add_line() function again and click on two points. Zoom out (or in) until your line appears through the points you selected. Predicting with lines Draw a line that you think is a good fit and write down its equation. Using this equation: \u2013 Predict how tall a person with a 62-inch armspan and a person with a 65-inch armspan would be. Using a line to make predictions also lets us make predictions for armspan s that aren't in our data. \u2013 How tall would you predict a person with a 63.5-inch armspan to be? Compare your answers with a neighbor. Did both of you come up with the same equation for a line? If not, can you tell which line fits the data best?","title":"LAB 4A: If the Line Fits\u2026"},{"location":"unit4/lab4a/#lab-4a-if-the-line-fits","text":"Directions: Follow along with the slides, completing the questions in blue on your computer, and answering the questions in red in your journal.","title":"Lab 4A - If the line fits ..."},{"location":"unit4/lab4a/#how-to-make-predictions","text":"Anyone can make predictions. \u2013 Data scientists use data to inform their predictions by using the information learned from the sample to make predictions for the whole population. In this lab, we'll learn how to make predictions by finding the line of best fit . \u2013 You will also learn how to use the information from one variable to make predictions about another variable.","title":"How to make predictions"},{"location":"unit4/lab4a/#predicting-heights","text":"Use the data() function to load the arm_span data. This data comes from a sample of 90 people in the Los Angeles area. \u2013 The measurements of height and armspan are in inches. \u2013 A person's armspan is the maximum distance between their fingertips when they spread their arms out wide. Make a plot of the height variable. \u2013 If you had to predict the height of someone in the LA area, what single height would you choose and why? \u2013 Would you describe this as a good guess? What might you try to improve your predictions?","title":"Predicting heights"},{"location":"unit4/lab4a/#predicting-heights-knowing-arm-spans","text":"Create two subsets of our arm_span data: \u2013 One for armspan >= 61 and armspan <= 63 . \u2013 A second for armspan >= 64 and armspan <= 66 . Create a histogram for the height of people in each subset. Answer the following based on the data: \u2013 What height would you predict if you knew a person had an armspan around 62 inches? \u2013 What height would you predict if you knew a person had an armspan around 65 inches? \u2013 Does knowing someone's armspan help you predict their height? Why or why not?","title":"Predicting heights knowing arm spans"},{"location":"unit4/lab4a/#fitting-lines","text":"Notice that there is a trend that people with a larger armspan also tend to have a larger mean height . \u2013 One way of describing this sort of trend is with a line. Data scientists often fit lines to their data to make predictions. \u2013 What we mean by fit is to come up with a line that's close to as many of the data points as possible. Create a scatterplot for height and armspan . Then run the following code. add_line() On the Plot pane, click two data points to draw a line through. NOTE: If your line does not appear or it appears but is above the points you selected, zoom out on your browser (typically 50% if you have a Mac, 80% on Windows). Or if your line appears below the points you selected, zoom in on your browser. Then run the add_line() function again and click on two points. Zoom out (or in) until your line appears through the points you selected.","title":"Fitting lines"},{"location":"unit4/lab4a/#predicting-with-lines","text":"Draw a line that you think is a good fit and write down its equation. Using this equation: \u2013 Predict how tall a person with a 62-inch armspan and a person with a 65-inch armspan would be. Using a line to make predictions also lets us make predictions for armspan s that aren't in our data. \u2013 How tall would you predict a person with a 63.5-inch armspan to be? Compare your answers with a neighbor. Did both of you come up with the same equation for a line? If not, can you tell which line fits the data best?","title":"Predicting with lines"},{"location":"unit4/lab4b/","text":"Lab 4B - What's the score? Directions: Follow along with the slides, completing the questions in blue on your computer, and answering the questions in red in your journal. Previously In the previous lab, we learned we could make predictions about one variable by utilizing the information of another. In this lab, we will learn how to measure the accuracy of our predictions. \u2013 This in turn will let us evaluate how well a model performs at making predictions. \u2013 We'll also use this information later to compare different models to find which model makes the best predictions. Predictions using a line Load the arm_span data again. \u2013 Create an xyplot with height on the y-axis and armspan on the x-axis. \u2013 Type add_line() to run the add_line function; you'll be prompted to click twice in the plot window to create a line that you think fits the data well. Fill in the blanks below to create a function that will make predictions of people's height s based on their armspan : predict_height <- function(armspan) { ____ * armspan + ____ } Make your predictions Fill in the blanks to include your predictions in the arm_span data. ____ <- mutate(____, predicted_height = ____(____)) Now that we've made our predictions, we'll need to figure out a way to decide how accurate our predictions are. \u2013 We'll want to compare our predicted heights to the actual heights . \u2013 At the end, we'll want to come up with a single number summary that describes our model's accuracy. Sums of differences A residual is the difference between the actual and predicted value of a quantity of interest. Fill in the blanks below to add a column of residucals to arm_span . ____ <- mutate(____, residual = ____ - ____) What do the residuals measure? One method we might consider to measure our model's accuracy is to sum the residuals. Fill in the blanks below to calculate our accuracy summary. summarize(____, sum(____)) Hint: Like mutate , the first argument of summarize is a dataframe, and the second argument is the action to perform on a column of the dataframe. Whereas the output of mutate is a column, the output of summarize is (usually) a single number summary. Describe and interpret, in words, what the output of your accuracy summary means. Write down why adding positive and negative errors together is problematic for assessing prediction accuracy. Mean squared error When adding residuals, the positive errors in our predictions (underestimates) are cancelled out by negative errors (overestimates) which lead to the impression that our model is making better predictions than it actually is. To solve this problem we calculate the squared values of the errors because squared values are always positive. The mean squared error (MSE) is calculated by squaring all of the residuals, and then taking the mean of the squared residuals. Fill in the blanks below to calculate the MSE of your line. summarize(____, mean((____))^2) Compare your MSE with a neighbor. Whose line was more accurate and why? Regression lines If you were to go around your class, each student would have created a different line that they feel fit the data best. Which is a problem because everyone's line will make slightly different predictions. To avoid this variation in predictions, data scientists will use regression lines . We also refer to regression lines as lienar models . This line connects the mean height of people with similar armspan s. Fill in the blanks below to create a regression line using lm , which stands for linear model . best_fit <- lm( _ ~ _, data = arm_span) Predicting wiht regression lines Making predictions with models R is familiar with is simpler than with lines, or models, we come up with ourselves. \u2013 Fill in the blanks to make predictions using best_fit : ____ <- mutate(____, predicted_height = predict(____)) Hint: the predict function takes a linear model as input, and outputs the predictions of that model. The magic of lm() The lm() function creates the line of best fit equation by finding the line that minimizes the mean squared error . Meaning, it's the best fitting line possible . Calculate the MSE for the values predicted using the regression line. Compare the MSE of the linear model you fitted with add_line() to the MSE of the linear model obtained with lm . Which linear model performed better? Ask your neighbors if any of their lines beat the lm line in terms of the MSE. Were any of them successful?","title":"LAB 4B: What\u2019s the Score?"},{"location":"unit4/lab4b/#lab-4b-whats-the-score","text":"Directions: Follow along with the slides, completing the questions in blue on your computer, and answering the questions in red in your journal.","title":"Lab 4B - What's the score?"},{"location":"unit4/lab4b/#previously","text":"In the previous lab, we learned we could make predictions about one variable by utilizing the information of another. In this lab, we will learn how to measure the accuracy of our predictions. \u2013 This in turn will let us evaluate how well a model performs at making predictions. \u2013 We'll also use this information later to compare different models to find which model makes the best predictions.","title":"Previously"},{"location":"unit4/lab4b/#predictions-using-a-line","text":"Load the arm_span data again. \u2013 Create an xyplot with height on the y-axis and armspan on the x-axis. \u2013 Type add_line() to run the add_line function; you'll be prompted to click twice in the plot window to create a line that you think fits the data well. Fill in the blanks below to create a function that will make predictions of people's height s based on their armspan : predict_height <- function(armspan) { ____ * armspan + ____ }","title":"Predictions using a line"},{"location":"unit4/lab4b/#make-your-predictions","text":"Fill in the blanks to include your predictions in the arm_span data. ____ <- mutate(____, predicted_height = ____(____)) Now that we've made our predictions, we'll need to figure out a way to decide how accurate our predictions are. \u2013 We'll want to compare our predicted heights to the actual heights . \u2013 At the end, we'll want to come up with a single number summary that describes our model's accuracy.","title":"Make your predictions"},{"location":"unit4/lab4b/#sums-of-differences","text":"A residual is the difference between the actual and predicted value of a quantity of interest. Fill in the blanks below to add a column of residucals to arm_span . ____ <- mutate(____, residual = ____ - ____) What do the residuals measure? One method we might consider to measure our model's accuracy is to sum the residuals. Fill in the blanks below to calculate our accuracy summary. summarize(____, sum(____)) Hint: Like mutate , the first argument of summarize is a dataframe, and the second argument is the action to perform on a column of the dataframe. Whereas the output of mutate is a column, the output of summarize is (usually) a single number summary. Describe and interpret, in words, what the output of your accuracy summary means. Write down why adding positive and negative errors together is problematic for assessing prediction accuracy.","title":"Sums of differences"},{"location":"unit4/lab4b/#mean-squared-error","text":"When adding residuals, the positive errors in our predictions (underestimates) are cancelled out by negative errors (overestimates) which lead to the impression that our model is making better predictions than it actually is. To solve this problem we calculate the squared values of the errors because squared values are always positive. The mean squared error (MSE) is calculated by squaring all of the residuals, and then taking the mean of the squared residuals. Fill in the blanks below to calculate the MSE of your line. summarize(____, mean((____))^2) Compare your MSE with a neighbor. Whose line was more accurate and why?","title":"Mean squared error"},{"location":"unit4/lab4b/#regression-lines","text":"If you were to go around your class, each student would have created a different line that they feel fit the data best. Which is a problem because everyone's line will make slightly different predictions. To avoid this variation in predictions, data scientists will use regression lines . We also refer to regression lines as lienar models . This line connects the mean height of people with similar armspan s. Fill in the blanks below to create a regression line using lm , which stands for linear model . best_fit <- lm( _ ~ _, data = arm_span)","title":"Regression lines"},{"location":"unit4/lab4b/#predicting-wiht-regression-lines","text":"Making predictions with models R is familiar with is simpler than with lines, or models, we come up with ourselves. \u2013 Fill in the blanks to make predictions using best_fit : ____ <- mutate(____, predicted_height = predict(____)) Hint: the predict function takes a linear model as input, and outputs the predictions of that model.","title":"Predicting wiht regression lines"},{"location":"unit4/lab4b/#the-magic-of-lm","text":"The lm() function creates the line of best fit equation by finding the line that minimizes the mean squared error . Meaning, it's the best fitting line possible . Calculate the MSE for the values predicted using the regression line. Compare the MSE of the linear model you fitted with add_line() to the MSE of the linear model obtained with lm . Which linear model performed better? Ask your neighbors if any of their lines beat the lm line in terms of the MSE. Were any of them successful?","title":"The magic of lm()"},{"location":"unit4/lab4c/","text":"Lab 4C - Cross-Validation Directions: Follow along with the slides, completing the questions in blue on your computer, and answering the questions in red in your journal. What is cross-validation? In the previous two labs, we learned how to: \u2013 Create a linear model predicting height from the arm_span data (4A). \u2013 See how well our model predicts height on the arm_span data by computing mean squared error (MSE)(4B). In this lab, we will see how well our model predicts the heights of people we haven't yet measured . To do this, we will use a method called cross-validation . Cross-validation consists of three steps: \u2013 Step 1: Split the data into training and test sets. \u2013 Step 2: Create a model using the training set. \u2013 Step 3: Use this model to make predictions on the test set. Step 1: train-test split Waiting for new observations can take a long time. The U.S. takes a census of its population once every 10 years, for example. Instead of waiting for new observations, data scientists will take their current data and divide it into two distinct sets. Split the arm_span data into training and test sets using the following two steps. First, fill in the blanks below to randomly select which rows of arm_span will go into the training set. set.seed(123) train_rows <- sample(1:____, size = 85) Second, use the slice function to create two dataframes: one called train consisting of the train_rows , and another called test consisting of the remaining rows of arm_span . train <- slice(arm_span, ____) test <- slice(____, - ____) Explain these lines of code and describe the train and test datasets. Aside: set.seed() When we split data, we're randomly separating our observations into training and testing sets. \u2013 It's important to notice that no single observation will be placed in both sets. Because we're splitting the data sets randomly, our models can also vary slightly, person-to-person. \u2013 This is why it's important to use set.seed . By using set.seed , we're able to reproduce the random splitting so that each person's model outputs the same results. Whenever you split data into training and testing, always use set.seed first. Aside: train-test ratio When splitting data into training and testing sets, we need to have enough observations in our data so that we can build a good model. \u2013 This is why we kept 85 observations in our training data. As datasets grow larger, we can use a larger proportion of the data to test with. Step 2: train the model Step 2 is to create a linear model relating height and armspan using the training data. Fit a line of best fit model to our training data and assign it the name best_train . Recall that the slope and intercept of our linear model are chosen to minimize MSE. Since the MSE being minimized is from the training data, we can call it training MSE . Step 3: test the model Step 3 is to use the model we built on the training data to make predictions on the test data. Note that we are NOT recomputing the slope and intercept to fit the test data best. We use the same slope and intercept that were computed in step 2. Because we're using the line of best fit , we can use the predict() function we introduced in the last lab to make predictions. \u2013 Fill in the blanks below to add predicted heights to our test data: test <- mutate(test, ____ = predict(best_train, newdata = ____)) Hint: the predict function without the argument newdata will output predictions on the training data. To output predictions on the test data, supply the test data to the newdata argument. Calculate the test MSE in the same way as you did in the previous lab (test MSE is simply MSE of the predictions on the test data). Recap Another way to describe the three steps is Step 1: Split the data into training and test sets. Step 2: Choose a slope and intercept that minimize training MSE. Step 3: Using the same slope and intercept from step 2, make predictions on the test set, and use these predictions to compute test MSE. This begs the question, why do we care about test MSE? Why cross-validate? Why go to all this trouble to compute test MSE when we could just compute MSE on the original dataset? When we compute MSE on the original dataset, we are measuring the ability of a model to make predictions on the current batch of data . Relying on a single dataset can lead to models that are so specific to the current batch of data that they're unable to make good predictions for future observations. \u2013 This phenomenon is known as overfitting . By splitting the data into a training and test set, we are hiding a proportion of the data from the model. This emulates future observations, which are unseen. Test MSE estimates the ability of a model to make predictions of future observations . Example of overfitting The following example motivates cross-validation by illustrating the dangers of overfitting. We randomly select 7 points from the arm_span dataset and fit two models: a linear model, and a polynomial model . \u2013 You will learn how to fit a polynomial model in the next lab. Below is a plot of these 7 training points, and two curves representing the value of height each model would predict given a value of armspan. Which model does a better job of predicting the 7 training points? Which model do you think will do a better job of predicting the rest of the data? Example of overfitting, continued Below is a plot of the rest of the arm_span dataset, along with the predictions each model would make. Which model does a better job of generalizing to the rest of the arm_span dataset?","title":"LAB 4C: Cross-Validation"},{"location":"unit4/lab4c/#lab-4c-cross-validation","text":"Directions: Follow along with the slides, completing the questions in blue on your computer, and answering the questions in red in your journal.","title":"Lab 4C - Cross-Validation"},{"location":"unit4/lab4c/#what-is-cross-validation","text":"In the previous two labs, we learned how to: \u2013 Create a linear model predicting height from the arm_span data (4A). \u2013 See how well our model predicts height on the arm_span data by computing mean squared error (MSE)(4B). In this lab, we will see how well our model predicts the heights of people we haven't yet measured . To do this, we will use a method called cross-validation . Cross-validation consists of three steps: \u2013 Step 1: Split the data into training and test sets. \u2013 Step 2: Create a model using the training set. \u2013 Step 3: Use this model to make predictions on the test set.","title":"What is cross-validation?"},{"location":"unit4/lab4c/#step-1-train-test-split","text":"Waiting for new observations can take a long time. The U.S. takes a census of its population once every 10 years, for example. Instead of waiting for new observations, data scientists will take their current data and divide it into two distinct sets. Split the arm_span data into training and test sets using the following two steps. First, fill in the blanks below to randomly select which rows of arm_span will go into the training set. set.seed(123) train_rows <- sample(1:____, size = 85) Second, use the slice function to create two dataframes: one called train consisting of the train_rows , and another called test consisting of the remaining rows of arm_span . train <- slice(arm_span, ____) test <- slice(____, - ____) Explain these lines of code and describe the train and test datasets.","title":"Step 1: train-test split"},{"location":"unit4/lab4c/#aside-setseed","text":"When we split data, we're randomly separating our observations into training and testing sets. \u2013 It's important to notice that no single observation will be placed in both sets. Because we're splitting the data sets randomly, our models can also vary slightly, person-to-person. \u2013 This is why it's important to use set.seed . By using set.seed , we're able to reproduce the random splitting so that each person's model outputs the same results. Whenever you split data into training and testing, always use set.seed first.","title":"Aside: set.seed()"},{"location":"unit4/lab4c/#aside-train-test-ratio","text":"When splitting data into training and testing sets, we need to have enough observations in our data so that we can build a good model. \u2013 This is why we kept 85 observations in our training data. As datasets grow larger, we can use a larger proportion of the data to test with.","title":"Aside: train-test ratio"},{"location":"unit4/lab4c/#step-2-train-the-model","text":"Step 2 is to create a linear model relating height and armspan using the training data. Fit a line of best fit model to our training data and assign it the name best_train . Recall that the slope and intercept of our linear model are chosen to minimize MSE. Since the MSE being minimized is from the training data, we can call it training MSE .","title":"Step 2: train the model"},{"location":"unit4/lab4c/#step-3-test-the-model","text":"Step 3 is to use the model we built on the training data to make predictions on the test data. Note that we are NOT recomputing the slope and intercept to fit the test data best. We use the same slope and intercept that were computed in step 2. Because we're using the line of best fit , we can use the predict() function we introduced in the last lab to make predictions. \u2013 Fill in the blanks below to add predicted heights to our test data: test <- mutate(test, ____ = predict(best_train, newdata = ____)) Hint: the predict function without the argument newdata will output predictions on the training data. To output predictions on the test data, supply the test data to the newdata argument. Calculate the test MSE in the same way as you did in the previous lab (test MSE is simply MSE of the predictions on the test data).","title":"Step 3: test the model"},{"location":"unit4/lab4c/#recap","text":"Another way to describe the three steps is Step 1: Split the data into training and test sets. Step 2: Choose a slope and intercept that minimize training MSE. Step 3: Using the same slope and intercept from step 2, make predictions on the test set, and use these predictions to compute test MSE. This begs the question, why do we care about test MSE?","title":"Recap"},{"location":"unit4/lab4c/#why-cross-validate","text":"Why go to all this trouble to compute test MSE when we could just compute MSE on the original dataset? When we compute MSE on the original dataset, we are measuring the ability of a model to make predictions on the current batch of data . Relying on a single dataset can lead to models that are so specific to the current batch of data that they're unable to make good predictions for future observations. \u2013 This phenomenon is known as overfitting . By splitting the data into a training and test set, we are hiding a proportion of the data from the model. This emulates future observations, which are unseen. Test MSE estimates the ability of a model to make predictions of future observations .","title":"Why cross-validate?"},{"location":"unit4/lab4c/#example-of-overfitting","text":"The following example motivates cross-validation by illustrating the dangers of overfitting. We randomly select 7 points from the arm_span dataset and fit two models: a linear model, and a polynomial model . \u2013 You will learn how to fit a polynomial model in the next lab. Below is a plot of these 7 training points, and two curves representing the value of height each model would predict given a value of armspan. Which model does a better job of predicting the 7 training points? Which model do you think will do a better job of predicting the rest of the data?","title":"Example of overfitting"},{"location":"unit4/lab4c/#example-of-overfitting-continued","text":"Below is a plot of the rest of the arm_span dataset, along with the predictions each model would make. Which model does a better job of generalizing to the rest of the arm_span dataset?","title":"Example of overfitting, continued"},{"location":"unit4/lab4d/","text":"Lab 4D - Interpreting correlations Directions: Follow along with the slides, completing the questions in blue on your computer, and answering the questions in red in your journal. Some background... So far, we\u2019ve learned about measuring the success of a model based on how close its predictions come to the actual observations. The correlation coefficient is a tool that gives us a fairly good idea of how these predictions will turn out without having to make predictions on future observations. For this lab, we will be using the movie data set to investigate the following questions: Which variables are better predictors of a movie's critics_rating when the predictions are made using a line of best fit? Correlation coefficients The correlation coefficient describes the strength and direction of the linear trend. It's only useful when the trend is linear and both variables are numeric. Are these variables linearly related? Why or why not? Correlation review I Correlation coefficients with values close to 1 are very strong with a positive slope. Values close to -1 means the correlation is very strong with a negative slope. Does this plot have a positive or negative correlation? Correlation review II Recall that if there is no linear relation between two numerical variables, the correlation coefficient is close to 0. What do you guess the correlation coefficient will be for these two variables? The movie data Load the movie data using the data command. The data comes from a variety of sources like IMDB and Rotten Tomatoes . \u2013 The critics_rating contains values between 0 and 100, 100 being the best. \u2013 The audience_rating contains values that range between 0 and 10, 10 being the best. \u2013 n_critics and n_audience describe the number of reviews used for the ratings. \u2013 gross and budget descibes the amount of money the film made and took to make. Calculating Correlation Coefficients! We can use the cor() function to find the particular correlation coefficient of the variables from the previous plot, which happen to be audience_rating and critics_rating . \u2013 But note, the cor() function removes any observations which contain an NA value in either variable. \u2013 Calculate the correlation coefficient for these variables using the cor function. The inputs to the functions work just like the inputs of the xyplot function. Now answer the following. What was the value of the correlation coefficient you calculated? How does this actual value compare with the one you estimated previously? >/span Does this indicate a strong, weak, or moderate association? Why? How would the scatterplot need to change in order for the correlation to be stronger? How would it need to change in order for the correlation to be weaker? Correlation and Predictions Find the two variables that look to have the strongest correlation with critics_rating . \u2013 Compute the correlation coefficients for critics_rating and each of the two variables. \u2013 Use the correlation coefficient to determine which variable has a stronger linear relationship with critics_rating . Fit two lm models to predict critics_rating with each variable and compute the MSE for each. \u2013 Use the MSE to determine which variable is a better predictor of critics_rating . How are the correlation coefficient and the MSE related? On your own Select two different numerical variables from the movie data. Plot the variables using the xyplot() function. \u2013 Would calculating a correlation coefficient for the two variables be appropriate? Justify your answer. \u2013 Predict what value you think the correlation coefficient will be. Compare this value to the actual value. Finally, interpret what the actual correlation coefficient means. Work with your classmates to determine which two variables have the strongest correlation coefficient. Why do you think these variables are so strongly related? Is using the correlation coefficient to describe the relationship appropriate and why/why not?","title":"LAB 4D: Interpreting Correlations"},{"location":"unit4/lab4d/#lab-4d-interpreting-correlations","text":"Directions: Follow along with the slides, completing the questions in blue on your computer, and answering the questions in red in your journal.","title":"Lab 4D - Interpreting correlations"},{"location":"unit4/lab4d/#some-background","text":"So far, we\u2019ve learned about measuring the success of a model based on how close its predictions come to the actual observations. The correlation coefficient is a tool that gives us a fairly good idea of how these predictions will turn out without having to make predictions on future observations. For this lab, we will be using the movie data set to investigate the following questions: Which variables are better predictors of a movie's critics_rating when the predictions are made using a line of best fit?","title":"Some background..."},{"location":"unit4/lab4d/#correlation-coefficients","text":"The correlation coefficient describes the strength and direction of the linear trend. It's only useful when the trend is linear and both variables are numeric. Are these variables linearly related? Why or why not?","title":"Correlation coefficients"},{"location":"unit4/lab4d/#correlation-review-i","text":"Correlation coefficients with values close to 1 are very strong with a positive slope. Values close to -1 means the correlation is very strong with a negative slope. Does this plot have a positive or negative correlation?","title":"Correlation review I"},{"location":"unit4/lab4d/#correlation-review-ii","text":"Recall that if there is no linear relation between two numerical variables, the correlation coefficient is close to 0. What do you guess the correlation coefficient will be for these two variables?","title":"Correlation review II"},{"location":"unit4/lab4d/#the-movie-data","text":"Load the movie data using the data command. The data comes from a variety of sources like IMDB and Rotten Tomatoes . \u2013 The critics_rating contains values between 0 and 100, 100 being the best. \u2013 The audience_rating contains values that range between 0 and 10, 10 being the best. \u2013 n_critics and n_audience describe the number of reviews used for the ratings. \u2013 gross and budget descibes the amount of money the film made and took to make.","title":"The movie data"},{"location":"unit4/lab4d/#calculating-correlation-coefficients","text":"We can use the cor() function to find the particular correlation coefficient of the variables from the previous plot, which happen to be audience_rating and critics_rating . \u2013 But note, the cor() function removes any observations which contain an NA value in either variable. \u2013 Calculate the correlation coefficient for these variables using the cor function. The inputs to the functions work just like the inputs of the xyplot function.","title":"Calculating Correlation Coefficients!"},{"location":"unit4/lab4d/#now-answer-the-following","text":"What was the value of the correlation coefficient you calculated? How does this actual value compare with the one you estimated previously? >/span Does this indicate a strong, weak, or moderate association? Why? How would the scatterplot need to change in order for the correlation to be stronger? How would it need to change in order for the correlation to be weaker?","title":"Now answer the following."},{"location":"unit4/lab4d/#correlation-and-predictions","text":"Find the two variables that look to have the strongest correlation with critics_rating . \u2013 Compute the correlation coefficients for critics_rating and each of the two variables. \u2013 Use the correlation coefficient to determine which variable has a stronger linear relationship with critics_rating . Fit two lm models to predict critics_rating with each variable and compute the MSE for each. \u2013 Use the MSE to determine which variable is a better predictor of critics_rating . How are the correlation coefficient and the MSE related?","title":"Correlation and Predictions"},{"location":"unit4/lab4d/#on-your-own","text":"Select two different numerical variables from the movie data. Plot the variables using the xyplot() function. \u2013 Would calculating a correlation coefficient for the two variables be appropriate? Justify your answer. \u2013 Predict what value you think the correlation coefficient will be. Compare this value to the actual value. Finally, interpret what the actual correlation coefficient means. Work with your classmates to determine which two variables have the strongest correlation coefficient. Why do you think these variables are so strongly related? Is using the correlation coefficient to describe the relationship appropriate and why/why not?","title":"On your own"},{"location":"unit4/lab4e/","text":"Lab 4E - Some models have curves Directions: Follow along with the slides, completing the questions in blue on your computer, and answering the questions in red in your journal. Making models do yoga So far, we have only worked with prediction models that fit the line of best fit to the data. What happens if the true relationship between the data is nonlinear? In this lab, we will learn about prediction models that fit best fitting curves to data. Before moving on, load the movie data and split it into two sets: \u2013 A set named training that includes 75% of the data. \u2013 And a set named testing that includes the remaining 25%. \u2013 Remember to use set.seed . Problems with lines Before learning how to fit curves, let's first fit a linear model for reference. Train a linear model predicting audience_rating based on critics_rating for the training data. Assign this model to movie_linear . Fill in the blanks below to create a scatterplot with audience_rating on the y-axis and critics_rating on the x-axis using your testing data. xyplot(____ ~ ____, data = ____) Previously, you used add_line to plot the line of best fit . An alternative function for plotting the line of best fit is add_curve , which takes the name of the model as an argument. Run the code below to add the line of best fit for the training data to the plot. add_curve(movie_linear) Describe, in words, how the line fits the data. Are there any values for critics_rating that would make obviously poor predictions? \u2013 Hint: how does the linear model perform on very low and very high values of critics_rating ? Compute the MSE of the linear model for the testing data and write it down for later. \u2013 Hint: refer to lab 4B. Adding flexibility You don't need to be a full-fledged Data Scientist to realize that trying to fit a line to curved data is a poor modeling choice. If our data is curved, we should try to model it with a curve. Instead of fitting a line, with equation of the form we might consider fitting a quadratic curve , with equation of the form or even a cubic curve , with equation of the form In general, the more coefficients in the model, the more flexible its predictions can be. Making bend-y models To fit a quadratic model in R , we can use the poly() function. \u2013 Fill in the blanks below to train a quadratic model predicting audience_rating from critics_rating , and assign that model to movie_quad . movie_quad <- lm(____ ~ poly(____, 2), data = training) What is the role of the number 2 in the poly() function? Comparing lines and curves Fill in the blanks below to \u2013 create a scatterplot with audience_rating on the y-axis and critics_rating on the x-axis using your testing data, and \u2013 add the line of best fit and best fitting quadratic curve . \u2013 Hint: the col argument is added to the add_curve functions to help distinguish the two curves. xyplot(____ ~ ____, data = ____) add_curve(____, col = \"blue\") add_curve(____, col = \"red\") Compare how the line of best fit and the quadratic model fit the data. Which do you think has a lower test MSE? Compute the MSE of the quadratic model for the test data and write it down for later. Use the difference in each model's test MSE to describe why one model fits better than the other. On your own Create a model that predicts audience_rating using a cubic curve (polynomial with degree 3 ), and assign this model to movie_cubic . Create a scatterplot with audience_rating on the y-axis and critics_rating on the x-axis using your test data. Using the names of the three models you have trained, add the line of best fit , best fitting quadratic curve , and best fitting cubic curve for the training data to the plot. Based on the plot, which model do you think is the best at predicting the testing data? Use the test MSE to verify which model is the best at predicting the testing data.","title":"LAB 4E: Some Models Have Curves"},{"location":"unit4/lab4e/#lab-4e-some-models-have-curves","text":"Directions: Follow along with the slides, completing the questions in blue on your computer, and answering the questions in red in your journal.","title":"Lab 4E - Some models have curves"},{"location":"unit4/lab4e/#making-models-do-yoga","text":"So far, we have only worked with prediction models that fit the line of best fit to the data. What happens if the true relationship between the data is nonlinear? In this lab, we will learn about prediction models that fit best fitting curves to data. Before moving on, load the movie data and split it into two sets: \u2013 A set named training that includes 75% of the data. \u2013 And a set named testing that includes the remaining 25%. \u2013 Remember to use set.seed .","title":"Making models do yoga"},{"location":"unit4/lab4e/#problems-with-lines","text":"Before learning how to fit curves, let's first fit a linear model for reference. Train a linear model predicting audience_rating based on critics_rating for the training data. Assign this model to movie_linear . Fill in the blanks below to create a scatterplot with audience_rating on the y-axis and critics_rating on the x-axis using your testing data. xyplot(____ ~ ____, data = ____) Previously, you used add_line to plot the line of best fit . An alternative function for plotting the line of best fit is add_curve , which takes the name of the model as an argument. Run the code below to add the line of best fit for the training data to the plot. add_curve(movie_linear) Describe, in words, how the line fits the data. Are there any values for critics_rating that would make obviously poor predictions? \u2013 Hint: how does the linear model perform on very low and very high values of critics_rating ? Compute the MSE of the linear model for the testing data and write it down for later. \u2013 Hint: refer to lab 4B.","title":"Problems with lines"},{"location":"unit4/lab4e/#adding-flexibility","text":"You don't need to be a full-fledged Data Scientist to realize that trying to fit a line to curved data is a poor modeling choice. If our data is curved, we should try to model it with a curve. Instead of fitting a line, with equation of the form we might consider fitting a quadratic curve , with equation of the form or even a cubic curve , with equation of the form In general, the more coefficients in the model, the more flexible its predictions can be.","title":"Adding flexibility"},{"location":"unit4/lab4e/#making-bend-y-models","text":"To fit a quadratic model in R , we can use the poly() function. \u2013 Fill in the blanks below to train a quadratic model predicting audience_rating from critics_rating , and assign that model to movie_quad . movie_quad <- lm(____ ~ poly(____, 2), data = training) What is the role of the number 2 in the poly() function?","title":"Making bend-y models"},{"location":"unit4/lab4e/#comparing-lines-and-curves","text":"Fill in the blanks below to \u2013 create a scatterplot with audience_rating on the y-axis and critics_rating on the x-axis using your testing data, and \u2013 add the line of best fit and best fitting quadratic curve . \u2013 Hint: the col argument is added to the add_curve functions to help distinguish the two curves. xyplot(____ ~ ____, data = ____) add_curve(____, col = \"blue\") add_curve(____, col = \"red\") Compare how the line of best fit and the quadratic model fit the data. Which do you think has a lower test MSE? Compute the MSE of the quadratic model for the test data and write it down for later. Use the difference in each model's test MSE to describe why one model fits better than the other.","title":"Comparing lines and curves"},{"location":"unit4/lab4e/#on-your-own","text":"Create a model that predicts audience_rating using a cubic curve (polynomial with degree 3 ), and assign this model to movie_cubic . Create a scatterplot with audience_rating on the y-axis and critics_rating on the x-axis using your test data. Using the names of the three models you have trained, add the line of best fit , best fitting quadratic curve , and best fitting cubic curve for the training data to the plot. Based on the plot, which model do you think is the best at predicting the testing data? Use the test MSE to verify which model is the best at predicting the testing data.","title":"On your own"},{"location":"unit4/lab4f/","text":"Lab 4F - This model is big enough for all of us! Directions: Follow along with the slides, completing the questions in blue on your computer, and answering the questions in red in your journal. Building better models So far, in the labs, we've learned how to make predictions using the line of best fit , also knowns as linear models or regression models . We've also learned how to measure our model's prediction accuracy by cross-validation. In this lab, we'll investigate the following question: Will including more variables in our model improve its predictions? Divide & Conquer Start by loading the movie data and split it into two sets (See Lab 4C for help). \u2013 A set named training that includes 75% of the data. \u2013 A set named test that includes the remaining 25%. Remember to use set.seed . Create a linear model, using the training data, that predicts gross using runtime . \u2013 Compute the MSE of the model by making predictions for the test data. Do you think that a movie's runtime is the only factor that goes into how much a movie will make? What else might affect a movie's gross ? Including more info Data scientists often find that including more relevant information in their models leads to better predictions. \u2013 Fill in the blanks below to predict gross using runtime and reviews_num . lm(____ ~ ____ + ____, data = training) Does this new model make more or less accurate predictions? Describe the process you used to arrive at your conclusion. Write down the code you would use to include a 3rd variable, of your choosing, in your lm() . Own your own Write down which other variables in the movie data you think would help you make better predictions. \u2013 Are there any variables that you think would not improve our predictions? Create a model for all of the variables you think are relevant. \u2013 Assess whether your model makes more accurate predictions for the test data than the model that included only runtime and reviews_num With your neighbors, determine which combination of variables leads to the best predictions for the test data.","title":"LAB 4F: This Model Is Big Enough for All of Us"},{"location":"unit4/lab4f/#lab-4f-this-model-is-big-enough-for-all-of-us","text":"Directions: Follow along with the slides, completing the questions in blue on your computer, and answering the questions in red in your journal.","title":"Lab 4F - This model is big enough for all of us!"},{"location":"unit4/lab4f/#building-better-models","text":"So far, in the labs, we've learned how to make predictions using the line of best fit , also knowns as linear models or regression models . We've also learned how to measure our model's prediction accuracy by cross-validation. In this lab, we'll investigate the following question: Will including more variables in our model improve its predictions?","title":"Building better models"},{"location":"unit4/lab4f/#divide-conquer","text":"Start by loading the movie data and split it into two sets (See Lab 4C for help). \u2013 A set named training that includes 75% of the data. \u2013 A set named test that includes the remaining 25%. Remember to use set.seed . Create a linear model, using the training data, that predicts gross using runtime . \u2013 Compute the MSE of the model by making predictions for the test data. Do you think that a movie's runtime is the only factor that goes into how much a movie will make? What else might affect a movie's gross ?","title":"Divide &amp; Conquer"},{"location":"unit4/lab4f/#including-more-info","text":"Data scientists often find that including more relevant information in their models leads to better predictions. \u2013 Fill in the blanks below to predict gross using runtime and reviews_num . lm(____ ~ ____ + ____, data = training) Does this new model make more or less accurate predictions? Describe the process you used to arrive at your conclusion. Write down the code you would use to include a 3rd variable, of your choosing, in your lm() .","title":"Including more info"},{"location":"unit4/lab4f/#own-your-own","text":"Write down which other variables in the movie data you think would help you make better predictions. \u2013 Are there any variables that you think would not improve our predictions? Create a model for all of the variables you think are relevant. \u2013 Assess whether your model makes more accurate predictions for the test data than the model that included only runtime and reviews_num With your neighbors, determine which combination of variables leads to the best predictions for the test data.","title":"Own your own"},{"location":"unit4/lab4g/","text":"Lab 4G - Growing trees Directions: Follow along with the slides, completing the questions in blue on your computer, and answering the questions in red in your journal. Trees vs. Lines So far in the labs, we've learned how we can fit linear models to our data and use them to make predictions. In this lab, we'll learn how to make predictions by growing trees. \u2013 Instead of creating a line, we split our data into branches based on a series of yes or no questions. \u2013 The branches help sort our data into leaves which can then be used to make predictions. Start, by loading the titanic data. Our first tree Use the tree() function to create a classification tree that predicts whether a person survived the Titanic based on their gender . \u2013 A classification tree tries to predict which category a categorical variable would belong to based on other variables. \u2013 The syntax for tree is similar to that of the lm() function. \u2013 Assign this model the name tree1 . Why can't we just use a linear model to predict whether a passenger on the Titanic survived or not based on their gender ? Viewing trees To actually look at and interpret our tree1 , place the model into the treeplot function. \u2013 Write down the labels of the two branches . \u2013 Write down the labels of the two leaves . Answer the following, based on the treeplot : \u2013 Which gender does the model predict will survive? \u2013 Where does the plot tell you the number of people that get sorted into each leaf? How do you know? \u2013 Where does the plot tell you the number of people that have been sorted incorrectly in each leaf? Leafier trees Similar to how you included multiple variables for a linear model, create a tree that predicts whether a person survived based on their gender , age , class , and where they embarked . \u2013 Call this model tree2 . Create a treeplot for this model and answer the following question: \u2013 Mrs. Cumings was a 38-year-old female with a 1st class ticket from Cherbourg. Does the model predict that she survived? \u2013 Which variable ended up not being used by tree ? Tree complexity By default, the tree() function will fit a tree model that will make good predictions without needing lots of branches. We can increase the complexity of our trees by changing the complexity parameter, cp , which equals 0.01 by default. We can also change the minimum number of observations needed in a leaf before we split it into a new branch using minsplit , which equals 20 by default. Using the same variables that you used in tree2 , create a model named tree3 but include cp = 0.005 and minsplit = 10 as arguments. \u2013 How is tree3 different from tree2 ? Predictions and Cross-validation Just like with linear models , we can use cross-validation to measure how well our classification trees perform on unseen data. First, we need to compute the predictions that our model makes on test data. \u2013 Use the data function to load the titanic_test data. \u2013 Fill in the blanks below to predict whether people in the titanic_test data survived or not using tree1 . Note: the argument type = \"class\" tells the predict function that we are classifying a categorical variable instead of predicting a numerical variable. titanic_test <- mutate( _, prediction = predict( _, newdata = ____, type = \"class\")) Measuring model performance Similar to how we use the mean squared error to describe how well our model predicts numerical variables, we use the misclassification rate to describe how well our model predicts categorical variables. \u2013 The misclassification rate (MCR) is the number of people who were predicted to be in one category but were actually in another. Run the following command to see a side-by-side comparison of the actual outcome and the predicted outcome: View(select(titanic_test, survived, prediction)) Where does the first misclassification occur? Misclassification rate In order to tally up the total number of misclassifications, we need to create a function that compares the actual outcome with the predicted outcome. The not equal to operator (!=) will be useful here. Fill in the blanks to create a function to calculate the MCR. Hint: sum( _!= _) will count the number of times that the left-hand side does not equal the right-hand side. We want to count the number of times that actual does not equal predicted and then divide by the total number of observations. calc_mcr <- function(actual, predicted) { sum( _ != _) / length(____) } Then run the following to calculate the MCR. summarize(titanic_test, mcr = calc_mcr(survived, prediction)) On your own In your own words, explain what the misclassification rate is. Which model ( tree1 , tree2 or tree3 ) had the lowest misclassification rate for the titanic_test data? Create a 4th model using the same variables used in tree2 . This time though, change the complexity parameter to 0.0001 . Then answer the following. \u2013 Does creating a more complex classification tree always lead to better predictions? Why not? A regression tree is a tree model that predicts a numerical variable. Create a regression tree model to predict the Titanic's passenger's ages and calculate the MSE. \u2013 Plots of regression trees are often too complex to plot.","title":"LAB 4G: Growing Trees"},{"location":"unit4/lab4g/#lab-4g-growing-trees","text":"Directions: Follow along with the slides, completing the questions in blue on your computer, and answering the questions in red in your journal.","title":"Lab 4G - Growing trees"},{"location":"unit4/lab4g/#trees-vs-lines","text":"So far in the labs, we've learned how we can fit linear models to our data and use them to make predictions. In this lab, we'll learn how to make predictions by growing trees. \u2013 Instead of creating a line, we split our data into branches based on a series of yes or no questions. \u2013 The branches help sort our data into leaves which can then be used to make predictions. Start, by loading the titanic data.","title":"Trees vs. Lines"},{"location":"unit4/lab4g/#our-first-tree","text":"Use the tree() function to create a classification tree that predicts whether a person survived the Titanic based on their gender . \u2013 A classification tree tries to predict which category a categorical variable would belong to based on other variables. \u2013 The syntax for tree is similar to that of the lm() function. \u2013 Assign this model the name tree1 . Why can't we just use a linear model to predict whether a passenger on the Titanic survived or not based on their gender ?","title":"Our first tree"},{"location":"unit4/lab4g/#viewing-trees","text":"To actually look at and interpret our tree1 , place the model into the treeplot function. \u2013 Write down the labels of the two branches . \u2013 Write down the labels of the two leaves . Answer the following, based on the treeplot : \u2013 Which gender does the model predict will survive? \u2013 Where does the plot tell you the number of people that get sorted into each leaf? How do you know? \u2013 Where does the plot tell you the number of people that have been sorted incorrectly in each leaf?","title":"Viewing trees"},{"location":"unit4/lab4g/#leafier-trees","text":"Similar to how you included multiple variables for a linear model, create a tree that predicts whether a person survived based on their gender , age , class , and where they embarked . \u2013 Call this model tree2 . Create a treeplot for this model and answer the following question: \u2013 Mrs. Cumings was a 38-year-old female with a 1st class ticket from Cherbourg. Does the model predict that she survived? \u2013 Which variable ended up not being used by tree ?","title":"Leafier trees"},{"location":"unit4/lab4g/#tree-complexity","text":"By default, the tree() function will fit a tree model that will make good predictions without needing lots of branches. We can increase the complexity of our trees by changing the complexity parameter, cp , which equals 0.01 by default. We can also change the minimum number of observations needed in a leaf before we split it into a new branch using minsplit , which equals 20 by default. Using the same variables that you used in tree2 , create a model named tree3 but include cp = 0.005 and minsplit = 10 as arguments. \u2013 How is tree3 different from tree2 ?","title":"Tree complexity"},{"location":"unit4/lab4g/#predictions-and-cross-validation","text":"Just like with linear models , we can use cross-validation to measure how well our classification trees perform on unseen data. First, we need to compute the predictions that our model makes on test data. \u2013 Use the data function to load the titanic_test data. \u2013 Fill in the blanks below to predict whether people in the titanic_test data survived or not using tree1 . Note: the argument type = \"class\" tells the predict function that we are classifying a categorical variable instead of predicting a numerical variable. titanic_test <- mutate( _, prediction = predict( _, newdata = ____, type = \"class\"))","title":"Predictions and Cross-validation"},{"location":"unit4/lab4g/#measuring-model-performance","text":"Similar to how we use the mean squared error to describe how well our model predicts numerical variables, we use the misclassification rate to describe how well our model predicts categorical variables. \u2013 The misclassification rate (MCR) is the number of people who were predicted to be in one category but were actually in another. Run the following command to see a side-by-side comparison of the actual outcome and the predicted outcome: View(select(titanic_test, survived, prediction)) Where does the first misclassification occur?","title":"Measuring model performance"},{"location":"unit4/lab4g/#misclassification-rate","text":"In order to tally up the total number of misclassifications, we need to create a function that compares the actual outcome with the predicted outcome. The not equal to operator (!=) will be useful here. Fill in the blanks to create a function to calculate the MCR. Hint: sum( _!= _) will count the number of times that the left-hand side does not equal the right-hand side. We want to count the number of times that actual does not equal predicted and then divide by the total number of observations. calc_mcr <- function(actual, predicted) { sum( _ != _) / length(____) } Then run the following to calculate the MCR. summarize(titanic_test, mcr = calc_mcr(survived, prediction))","title":"Misclassification rate"},{"location":"unit4/lab4g/#on-your-own","text":"In your own words, explain what the misclassification rate is. Which model ( tree1 , tree2 or tree3 ) had the lowest misclassification rate for the titanic_test data? Create a 4th model using the same variables used in tree2 . This time though, change the complexity parameter to 0.0001 . Then answer the following. \u2013 Does creating a more complex classification tree always lead to better predictions? Why not? A regression tree is a tree model that predicts a numerical variable. Create a regression tree model to predict the Titanic's passenger's ages and calculate the MSE. \u2013 Plots of regression trees are often too complex to plot.","title":"On your own"},{"location":"unit4/lab4h/","text":"Lab 4H - Finding clusters Directions: Follow along with the slides, completing the questions in blue on your computer, and answering the questions in red in your journal. Clustering data We've seen previously that data scientists have methods to predict values of specific variables. \u2013 We used regression to predict numerical values and classification to predict categories. Clustering is similar to classification in that we want to group people into categories. But there's one important difference: \u2013 In clustering , we don't know how many groups to use because we're not predicting the value of a known variable! In this lab, we'll learn how to use the k-means clustering algorithm to group our data into clusters. The k-means algorithm The k-means algorithm works by splitting our data into k different clusters. \u2013 The number of clusters, the value of k , is chosen by the data scientist. The algorithm works only for numerical variables and only when we have no missing data. To start, use the data function to load the futbol data set. \u2013 This data contains 23 players from the US Men's National Soccer team (USMNT) and 22 quarterbacks from the National Football League (NFL). Create a scatterplot of the players ht_inches and wt_lbs and color each dot based on the league they play for. Running k-means After plotting the player's heights and weights, we can see that there are two clusters, or different types, of players: \u2013 Players in the NFL tend to be taller and weigh more than the shorter and lighter USMNT players. Fill in the blanks below to use k-means to cluster the same height and weight data into two groups: kclusters(____~____, data = futbol, k = ____) Use this code and the mutate function to add the values from kclusters to the futbol data. Call the variable clusters . k-means vs. ground-truth In comparing our football and soccer players, we know for certain which league each player plays in. \u2013 We call this knowledge ground-truth . Knowing the ground-truth for this example is helpful to illustrate how k-means works, but in reality, data-scientists would run k-means not knowing the ground-truth . Compare the clusters chosen by k-means to the ground-truth. How successful was k-means at recovering the league information? On your own Load your class' timeuse data (remember to run timeuse_format so each row represents the mean time each student spent participating in the various activities). Create a scatterplot of homework and videogames variables. \u2013 Based on this graph, identify and remove any outliers by using the filter function. Use kclusters with k=2 for homework and videogames . \u2013 Describe how the groups differ from each other in terms of how long each group spends playing videogames and doing homework .","title":"LAB 4H: Finding Clusters"},{"location":"unit4/lab4h/#lab-4h-finding-clusters","text":"Directions: Follow along with the slides, completing the questions in blue on your computer, and answering the questions in red in your journal.","title":"Lab 4H - Finding clusters"},{"location":"unit4/lab4h/#clustering-data","text":"We've seen previously that data scientists have methods to predict values of specific variables. \u2013 We used regression to predict numerical values and classification to predict categories. Clustering is similar to classification in that we want to group people into categories. But there's one important difference: \u2013 In clustering , we don't know how many groups to use because we're not predicting the value of a known variable! In this lab, we'll learn how to use the k-means clustering algorithm to group our data into clusters.","title":"Clustering data"},{"location":"unit4/lab4h/#the-k-means-algorithm","text":"The k-means algorithm works by splitting our data into k different clusters. \u2013 The number of clusters, the value of k , is chosen by the data scientist. The algorithm works only for numerical variables and only when we have no missing data. To start, use the data function to load the futbol data set. \u2013 This data contains 23 players from the US Men's National Soccer team (USMNT) and 22 quarterbacks from the National Football League (NFL). Create a scatterplot of the players ht_inches and wt_lbs and color each dot based on the league they play for.","title":"The k-means algorithm"},{"location":"unit4/lab4h/#running-k-means","text":"After plotting the player's heights and weights, we can see that there are two clusters, or different types, of players: \u2013 Players in the NFL tend to be taller and weigh more than the shorter and lighter USMNT players. Fill in the blanks below to use k-means to cluster the same height and weight data into two groups: kclusters(____~____, data = futbol, k = ____) Use this code and the mutate function to add the values from kclusters to the futbol data. Call the variable clusters .","title":"Running k-means"},{"location":"unit4/lab4h/#k-means-vs-ground-truth","text":"In comparing our football and soccer players, we know for certain which league each player plays in. \u2013 We call this knowledge ground-truth . Knowing the ground-truth for this example is helpful to illustrate how k-means works, but in reality, data-scientists would run k-means not knowing the ground-truth . Compare the clusters chosen by k-means to the ground-truth. How successful was k-means at recovering the league information?","title":"k-means vs. ground-truth"},{"location":"unit4/lab4h/#on-your-own","text":"Load your class' timeuse data (remember to run timeuse_format so each row represents the mean time each student spent participating in the various activities). Create a scatterplot of homework and videogames variables. \u2013 Based on this graph, identify and remove any outliers by using the filter function. Use kclusters with k=2 for homework and videogames . \u2013 Describe how the groups differ from each other in terms of how long each group spends playing videogames and doing homework .","title":"On your own"},{"location":"unit4/lesson1/","text":"Lesson 1: Trash Objective: Students will learn about reducing the burden of trash landfills. Materials: Video: Fighting Pollution Through Data Video found at: https://www.youtube.com/watch?v=xOYAIXjHveA Landfill Readiness Questions handout ( LMR_4.1_Landfill Readiness Questions ) Trash Campaign Exploration handout ( LMR_4.2_Trash Campaign Exploration ) Trash Campaign Creation handout ( LMR_4.3_Trash Campaign Creation ) Essential Concepts: Essential Concepts: Exploring different datasets can give us insight about the same processes. Data from our Participatory Sensing campaigns rely on human sensors and limit the ability to generalize to the greater population. Lesson: Inform students that they will investigate a problem that faces many cities around the world today: trash. Using the 5 Ws strategy, ask students to write down the 5 Ws in their DS journals as they watch a video about trash. The 5 Ws summarize the What, Who, Why, When, and Where of the resource. Play the Fighting Plastic Pollution Through Data video, found at: https://www.youtube.com/watch?v=xOYAIXjHveA NOTE : While the video is just over 13 minutes long, students should be able to answer the 5 Ws from the content of the first 10 minutes. After they have finished watching the video, engage in a class discussion around the following questions and discuss their insights, questions, and/or reactions to the video: What types of data did they collect in the video? Answer: Photos, location, brand names, weight (kg), waste categories (plastic bags, straws, toothpaste, etc), and number of packaging. How are they useful in fighting plastic pollution? Answer: Brand accountability, community involvement, cleaning rivers/ dumping sites, and finding sources of pollution. Now we'll take inventory of our own understanding of landfills and how trash travels there. Distribute the Landfill Readiness Questions handout ( LMR_4.1_Landfill Readiness Questions ). Allow students private think-time before having them discuss in their teams. LMR_4.1 Let students know that they will be exploring data from a trash Participatory Sensing campaign, titled the \u201cTrash Campaign,\u201d that was conducted at a number of high schools in the Los Angeles Unified School District (LAUSD). Concerned students in LAUSD engaged in a model eliciting activity and created a trash participatory sensing campaign to investigate possible trash issues in their communities. Based on the data collected, they made recommendations to the Los Angeles County Sanitation District (LACSD) that would help reduce the use of the regional landfills. Distribute the Trash Campaign Exploration handout ( LMR_4.2_Trash Campaign Exploration ) to assist in students' interaction with the IDS public dashboard. LMR_4.2 Navigate students to the IDS public dashboard: https://portal.idsucla.org They should use the \u201cTrash\u201d campaign data and select \u201cDashboard\u201d from the \u201cAction\u201d button. The dashboard is a visual tool for exploring and analyzing data. An example screenshot of the Trash campaign in the dashboard is shown below. Students should \u201cplay\u201d with the data and think about characteristics of the campaign. Answers to the questions in the Trash Campaign Exploration handout ( LMR_4.2_Trash Campaign Exploration ) are provided here for reference: How many observations are in this data set? Answer: 2,631. Where was the majority of trash generated? Answer: School (1,254). How many observations were generated at school? At work? Answer: School has 1,254 and Work has 59. What material or item was most commonly thrown away? Answer: Recyclable (1,302) or Paper (477). Between what hours is the largest percentage of trash generated at home? Answer: Between 2100 and 2200, which is 9pm to 10pm (106). Which activity generates the largest percentage of landfill-destined trash? Answer: Eating/cooking with 69.02% (822/1,191). Is eating or drinking more likely to generate a recyclable piece of trash? Answer: Drinking, because it resulted in 480 recyclables versus Eating resulted in 399 recyclables, out of 1,302 total. When recycle bins were present, what percentage of time did a recyclable item end up in a trash bin? Answer: 26.6% (225/846). When recycle bins were present, did a higher proportion of recyclable items end up in the trash bin when people were at home or at school? Answer: School (134/225) had a higher proportion of recyclable items end up in the trash than Home (76/225). When someone littered, how many times was the person not arround any type of waste receptacle? Answer: 15 out of 58. Take time at the end of class to share out and discuss the components of the Trash Campaign. If needed, you may use the Trash Campaign Creation handout ( LMR_4.3_Trash Campaign Creation ) as an additional resource to help with the deconstruction of the Trash Campaign. LMR_4.3 Class Scribes: One team of students will give a brief talk to discuss what they think the 3 most important topics of the day were.","title":"Lesson 1: Trash"},{"location":"unit4/lesson1/#lesson-1-trash","text":"","title":"Lesson 1: Trash"},{"location":"unit4/lesson1/#objective","text":"Students will learn about reducing the burden of trash landfills.","title":"Objective:"},{"location":"unit4/lesson1/#materials","text":"Video: Fighting Pollution Through Data Video found at: https://www.youtube.com/watch?v=xOYAIXjHveA Landfill Readiness Questions handout ( LMR_4.1_Landfill Readiness Questions ) Trash Campaign Exploration handout ( LMR_4.2_Trash Campaign Exploration ) Trash Campaign Creation handout ( LMR_4.3_Trash Campaign Creation )","title":"Materials:"},{"location":"unit4/lesson1/#essential-concepts","text":"Essential Concepts: Exploring different datasets can give us insight about the same processes. Data from our Participatory Sensing campaigns rely on human sensors and limit the ability to generalize to the greater population.","title":"Essential Concepts:"},{"location":"unit4/lesson1/#lesson","text":"Inform students that they will investigate a problem that faces many cities around the world today: trash. Using the 5 Ws strategy, ask students to write down the 5 Ws in their DS journals as they watch a video about trash. The 5 Ws summarize the What, Who, Why, When, and Where of the resource. Play the Fighting Plastic Pollution Through Data video, found at: https://www.youtube.com/watch?v=xOYAIXjHveA NOTE : While the video is just over 13 minutes long, students should be able to answer the 5 Ws from the content of the first 10 minutes. After they have finished watching the video, engage in a class discussion around the following questions and discuss their insights, questions, and/or reactions to the video: What types of data did they collect in the video? Answer: Photos, location, brand names, weight (kg), waste categories (plastic bags, straws, toothpaste, etc), and number of packaging. How are they useful in fighting plastic pollution? Answer: Brand accountability, community involvement, cleaning rivers/ dumping sites, and finding sources of pollution. Now we'll take inventory of our own understanding of landfills and how trash travels there. Distribute the Landfill Readiness Questions handout ( LMR_4.1_Landfill Readiness Questions ). Allow students private think-time before having them discuss in their teams. LMR_4.1 Let students know that they will be exploring data from a trash Participatory Sensing campaign, titled the \u201cTrash Campaign,\u201d that was conducted at a number of high schools in the Los Angeles Unified School District (LAUSD). Concerned students in LAUSD engaged in a model eliciting activity and created a trash participatory sensing campaign to investigate possible trash issues in their communities. Based on the data collected, they made recommendations to the Los Angeles County Sanitation District (LACSD) that would help reduce the use of the regional landfills. Distribute the Trash Campaign Exploration handout ( LMR_4.2_Trash Campaign Exploration ) to assist in students' interaction with the IDS public dashboard. LMR_4.2 Navigate students to the IDS public dashboard: https://portal.idsucla.org They should use the \u201cTrash\u201d campaign data and select \u201cDashboard\u201d from the \u201cAction\u201d button. The dashboard is a visual tool for exploring and analyzing data. An example screenshot of the Trash campaign in the dashboard is shown below. Students should \u201cplay\u201d with the data and think about characteristics of the campaign. Answers to the questions in the Trash Campaign Exploration handout ( LMR_4.2_Trash Campaign Exploration ) are provided here for reference: How many observations are in this data set? Answer: 2,631. Where was the majority of trash generated? Answer: School (1,254). How many observations were generated at school? At work? Answer: School has 1,254 and Work has 59. What material or item was most commonly thrown away? Answer: Recyclable (1,302) or Paper (477). Between what hours is the largest percentage of trash generated at home? Answer: Between 2100 and 2200, which is 9pm to 10pm (106). Which activity generates the largest percentage of landfill-destined trash? Answer: Eating/cooking with 69.02% (822/1,191). Is eating or drinking more likely to generate a recyclable piece of trash? Answer: Drinking, because it resulted in 480 recyclables versus Eating resulted in 399 recyclables, out of 1,302 total. When recycle bins were present, what percentage of time did a recyclable item end up in a trash bin? Answer: 26.6% (225/846). When recycle bins were present, did a higher proportion of recyclable items end up in the trash bin when people were at home or at school? Answer: School (134/225) had a higher proportion of recyclable items end up in the trash than Home (76/225). When someone littered, how many times was the person not arround any type of waste receptacle? Answer: 15 out of 58. Take time at the end of class to share out and discuss the components of the Trash Campaign. If needed, you may use the Trash Campaign Creation handout ( LMR_4.3_Trash Campaign Creation ) as an additional resource to help with the deconstruction of the Trash Campaign. LMR_4.3","title":"Lesson:"},{"location":"unit4/lesson1/#class-scribes","text":"One team of students will give a brief talk to discuss what they think the 3 most important topics of the day were.","title":"Class Scribes:"},{"location":"unit4/lesson10/","text":"Lesson 10: What's the Best Line? Objective: Students will understand that the mean squared error (MSE) is a way to assess the fit of a linear model. The MSE measures the total squared distances between all the data values from the line of best fit and divides it by the number of observations in the dataset. Materials: Arm Span vs. Height Scatterplot ( LMR_4.9_Arm Span vs Height ) from lesson 8 Testing Line of Best Fit handout ( LMR_4.11_Testing Line of Best Fit ) Vocabulary: regression line observed value predicted value Essential Concepts: Essential Concepts: The regression line can be used to make good predictions about values of y for any given value of x . This works for exactly the same reason the mean works well for one variable: the predictions will make your score on the mean squared errors as small as possible. Lesson: Ask student teams to refer back to the Arm Span vs. Height handout ( LMR_4.9 ) but this time have them look at the zoomed out scatterplot. Using their understanding of a line of best fit from The Spaghetti Line lesson and Lab 4A, have them draw (or use strands of spaghetti) what they believe to be the line of best fit for the data. Note: They can use their equations from Lab 4A as a guide but note that it will be difficult to plot decimals on this scatterplot. Ask students: How does this line compare to the lines from the team posters in The Spaghetti Line lesson? Answers will vary but students may notice that the y-intercepts may be similar or that the overall slope appears similar (they are not writing an equation for the line in step 2, but they may notice where their line intercepts the y-axis and/or the steepness of the line, i.e., slope). Reveal the equation of the line of best fit for the Arm Span vs. Height data and ask students to check their equations from Lab 4A: Note: Any time a hat is on top of a variable, this means we are making \u201cpredicted values\u201d of that variable. Whose equation came closest to the equation of the regression line? Ask the student whose equation came closest to share how he/she came up with the equation. Inform students that the equation of the line is a rule that predicts the height based on a second variable, in this case, arm span. Data points are observed values and points on the line are predicted values . Team discussion question: Using the equation of the line of best fit provided, how can we predict the height of a student whose arm span is 67 inches? What was the actual height for someone with an arm span of 67 inches? Answer: There are three points on our Arm Span vs Height scatterplot at an arm span of 67 inches; 66-inch height, 67-inch height, and 70-inch height. How close was our predicted height? Answer: Our line of best fit predicted that someone with an arm span of 67 inches has a height of 66.5933 inches, which rounded to the nearest whole number is 67 inches. This is pretty close as it matches one of the possible heights for someone with an arm span of 67 inches in our data. Remind students know that lines of best fit are also known as regression lines and they are models that can be used to make predictions. Inform students that data scientists have a way of finding the best line. They choose the line so that the mean squared distances between the points and the line is as small as possible. Discuss with students: What methods have we used so far? Answer: We've used Mean Squared Error and Mean Absolute Error (Lesson 7) When is it appropriate to use each method? Answer: It was best to use Mean Squared Error when we were looking at mean and Mean Absolute Error when we were looking at median. Distribute the Testing Line of Best Fit handout ( LMR_4.11 ). Students will calculate MSE by using the distances between the actual heights (the points) and their predicted heights (the points on the line) of two different lines. They do this so that they can understand what those distances mean - that together they form our \"error\" that help us determine the best fitting line. LMR_4.11 Discuss with students: What did you have to do to your MSE value to make it useable for interpretation? Answer: We had to take the square root of our MSE value in order to convert it back to inches. Which linear model was the better fit? How do you know? Answers will vary but this is where students should compare the MSE values - a smaller MSE indicates a smaller error, and therefore a better fit. Note: Students may ask for an easier and/or faster way to calculate MSE. They will be using RStudio to calculate MSE in the next lab. Class Scribes: One team of students will give a brief talk to discuss what they think the 3 most important topics of the day were. Homework & Next Day LAB 4B:\u2026 LAB 4C: Complete Labs 4B and 4C prior to Lesson 11 .","title":"Lesson 10: What\u2019s the Best Line?"},{"location":"unit4/lesson10/#lesson-10-whats-the-best-line","text":"","title":"Lesson 10: What's the Best Line?"},{"location":"unit4/lesson10/#objective","text":"Students will understand that the mean squared error (MSE) is a way to assess the fit of a linear model. The MSE measures the total squared distances between all the data values from the line of best fit and divides it by the number of observations in the dataset.","title":"Objective:"},{"location":"unit4/lesson10/#materials","text":"Arm Span vs. Height Scatterplot ( LMR_4.9_Arm Span vs Height ) from lesson 8 Testing Line of Best Fit handout ( LMR_4.11_Testing Line of Best Fit )","title":"Materials:"},{"location":"unit4/lesson10/#vocabulary","text":"regression line observed value predicted value","title":"Vocabulary:"},{"location":"unit4/lesson10/#essential-concepts","text":"Essential Concepts: The regression line can be used to make good predictions about values of y for any given value of x . This works for exactly the same reason the mean works well for one variable: the predictions will make your score on the mean squared errors as small as possible.","title":"Essential Concepts:"},{"location":"unit4/lesson10/#lesson","text":"Ask student teams to refer back to the Arm Span vs. Height handout ( LMR_4.9 ) but this time have them look at the zoomed out scatterplot. Using their understanding of a line of best fit from The Spaghetti Line lesson and Lab 4A, have them draw (or use strands of spaghetti) what they believe to be the line of best fit for the data. Note: They can use their equations from Lab 4A as a guide but note that it will be difficult to plot decimals on this scatterplot. Ask students: How does this line compare to the lines from the team posters in The Spaghetti Line lesson? Answers will vary but students may notice that the y-intercepts may be similar or that the overall slope appears similar (they are not writing an equation for the line in step 2, but they may notice where their line intercepts the y-axis and/or the steepness of the line, i.e., slope). Reveal the equation of the line of best fit for the Arm Span vs. Height data and ask students to check their equations from Lab 4A: Note: Any time a hat is on top of a variable, this means we are making \u201cpredicted values\u201d of that variable. Whose equation came closest to the equation of the regression line? Ask the student whose equation came closest to share how he/she came up with the equation. Inform students that the equation of the line is a rule that predicts the height based on a second variable, in this case, arm span. Data points are observed values and points on the line are predicted values . Team discussion question: Using the equation of the line of best fit provided, how can we predict the height of a student whose arm span is 67 inches? What was the actual height for someone with an arm span of 67 inches? Answer: There are three points on our Arm Span vs Height scatterplot at an arm span of 67 inches; 66-inch height, 67-inch height, and 70-inch height. How close was our predicted height? Answer: Our line of best fit predicted that someone with an arm span of 67 inches has a height of 66.5933 inches, which rounded to the nearest whole number is 67 inches. This is pretty close as it matches one of the possible heights for someone with an arm span of 67 inches in our data. Remind students know that lines of best fit are also known as regression lines and they are models that can be used to make predictions. Inform students that data scientists have a way of finding the best line. They choose the line so that the mean squared distances between the points and the line is as small as possible. Discuss with students: What methods have we used so far? Answer: We've used Mean Squared Error and Mean Absolute Error (Lesson 7) When is it appropriate to use each method? Answer: It was best to use Mean Squared Error when we were looking at mean and Mean Absolute Error when we were looking at median. Distribute the Testing Line of Best Fit handout ( LMR_4.11 ). Students will calculate MSE by using the distances between the actual heights (the points) and their predicted heights (the points on the line) of two different lines. They do this so that they can understand what those distances mean - that together they form our \"error\" that help us determine the best fitting line. LMR_4.11 Discuss with students: What did you have to do to your MSE value to make it useable for interpretation? Answer: We had to take the square root of our MSE value in order to convert it back to inches. Which linear model was the better fit? How do you know? Answers will vary but this is where students should compare the MSE values - a smaller MSE indicates a smaller error, and therefore a better fit. Note: Students may ask for an easier and/or faster way to calculate MSE. They will be using RStudio to calculate MSE in the next lab.","title":"Lesson:"},{"location":"unit4/lesson10/#class-scribes","text":"One team of students will give a brief talk to discuss what they think the 3 most important topics of the day were.","title":"Class Scribes:"},{"location":"unit4/lesson10/#homework-next-day","text":"LAB 4B:\u2026 LAB 4C: Complete Labs 4B and 4C prior to Lesson 11 .","title":"Homework &amp; Next Day"},{"location":"unit4/lesson11/","text":"Lesson 11: What\u2019s the Trend? Objective: Students will understand that the regression line is a model for a linear association (trend). They will learn to identify the direction of trends and interpret the slope and the intercept of a linear model in the context of the data. Materials: What\u2019s the Trend? handout ( LMR_4.12_What\u2019s the Trend ) Predicting Values handout ( LMR_4.13_Predicting Values ) Vocabulary: trend positive association negative association no association linear model Essential Concepts: Essential Concepts: A positive or negative association between variables provides valuable insights into increasing or decreasing trends, particularly in making predictions. By understanding these associations, we can anticipate future outcomes or behaviors more accurately. Lesson: Distribute What\u2019s the Trend? handout ( LMR_4.12 ). Students will analyze the two scatterplots on the handout. The Profits per Explosion plot shows the relationship between the number of explosions in Michael Bay\u2019s movies and the profit earned by each movie. The Scores Over Time plot shows the relationship between M. Night Shyamalan movies made since The Sixth Sense was released in 1999 and their Internet Movie Database (IMBD) scores. LMR_4.12 In teams, students will discuss and record their responses to the following questions for each plot: What kind of plot is this? Answer: Scatterplot. What do the numbers on the x-axis represent? What do the numbers on the y-axis represent? Answer: The x-axis shows number of explosions and y-axis shows profit in millions of dollars. What is this plot telling us? Answers will vary. One example could be that if there are more explosions in a movie, then the movie will earn a greater profit. What kind of plot is this? Answer: Scatterplot. What do the numbers on the x-axis represent? What do the numbers on the y-axis represent? Answer: The x-axis shows the number of years since 1999 and the y-axis shows the movie\u2019s IMDB score. What is this plot telling us? Answers will vary. One example could be that as M. Night Shyamalan has produced more movies, their IMDB ratings have gone down. Allow students time to discuss and record their answers to the questions. Display both plots, if possible (students may also refer to the plots in their own handout). Discuss the following questions with the whole class: What is happening in each plot? What seems to be the trend? Answer: Guide students to understand that the Profits per Explosion plot shows an increasing trend, while the Scores Over Time plot shows a decreasing trend. An increasing trend is called a positive association and a decreasing trend is called a negative association . What does it mean to have an increasing trend and a positive association? Answer: In Profits per Explosion, it means that as the number of explosions increase, the movie profits also increase. What does it mean to have a decreasing trend and a negative association? Answer: In Scores Over Time, it means that as the years after 1999 pass, the movie IMBD ratings decrease. Quickwrite: What if we had a plot with no association ? Ask students to sketch what they think a scatterplot that shows no association looks like. Answer: A correct sketch will show a scatterplot with data points that show no positive or negative association; no trend or pattern. There would be no association or a very weak one. The data would be scattered. Select a couple of sketches to share with the whole class. Discuss why the sketches show no association. Ask students to discuss their thoughts about why a line was drawn through the points of the two plots and why there are equations for each plot. Conduct a share out of their observations. Guide students to the understanding that both plots follow a linear trend. This line then represents a model for the relationship between the two variables. The equations shown in the plots above represent the lines through the points. They provide a description of the data and the relationship between the variables. Ask student teams to refer back to the What\u2019s the Trend? handout ( LMR_4.12 ). They should discuss the following questions and record their responses on the Predicting Values handout ( LMR_4.13 ): LMR_4.13 What do you notice about where the points are and where the line is? Answer: Some points are near the line, others are further away, and one point is exactly on the line. Data points are observed values and points on the line are predicted values . Recall from Algebra that every line can be represented by an equation in the form y=mx+b. In this case, the equation of the regression line is y=3.2536x+154.3654. What do the x- and y-values represent in this equation? Answer: The x-values represent the number of explosions and the y-values represent the predicted profit. According to the equation, what is the slope of this line? What does the slope mean in relation to the number of explosions? Answer: The slope is 3.2536. It is the rate of change between the number of explosions and the profit. It means that for every explosion increase of 1 the profit increases by 3.2536 dollars. When the number of explosions (x-value) is zero, what is the profit (y-value)? How do you know? What does this mean? Answer: The profit is 154.3654 million dollars. Students may use the equation to show that they substituted zero for x, so the y-intercept is the profit. It means that if Michael Bay were to make a movie with NO explosions, this would be his projected profit. If you wanted to know the profit for the point that lies the closest to the line, what would the equation be? Write the equation and solve it. Answer: Profit=3.2536(105)+154.3654. Profit=495.9934or 495,993,400 million dollars. What was the actual profit for the point that lies closest to the line? Answer: The actual profit was 553,700,000 million dollars. Note: An estimate is okay in this case as the exact values are not given. What if Michael Bay made a movie that had 325 explosions? What would his predicted profit be? Show how you arrived at the solution. Answer: By substituting 325 in the value of x in the equation, predicted profit will be $1,211, 785, 400 or $ 1, 211.7854, or by finding the point on the line or both. If time permits, have students answer the following questions about the Scores Over Time scatterplot in LMR_4.12_What\u2019s the Trend . What do you notice about where the points are and where the line is? What do the y- and x-values represent in this equation? According to the equation, what is the slope of this line? What does the slope mean? When the x-value is zero, what is the y-value? How do you know? What does this mean? What would the predicted value of the score be if M. Night Shyamalan released a movie in 2015? How do you know? Class Scribes: One team of students will give a brief talk to discuss what they think the 3 most important topics of the day were. Homework Students will finish answering the questions above about the Scores Over Time scatterplot in LMR_4.12_What\u2019s the Trend referenced above.","title":"Lesson 11: What\u2019s the Trend?"},{"location":"unit4/lesson11/#lesson-11-whats-the-trend","text":"","title":"Lesson 11: What\u2019s the Trend?"},{"location":"unit4/lesson11/#objective","text":"Students will understand that the regression line is a model for a linear association (trend). They will learn to identify the direction of trends and interpret the slope and the intercept of a linear model in the context of the data.","title":"Objective:"},{"location":"unit4/lesson11/#materials","text":"What\u2019s the Trend? handout ( LMR_4.12_What\u2019s the Trend ) Predicting Values handout ( LMR_4.13_Predicting Values )","title":"Materials:"},{"location":"unit4/lesson11/#vocabulary","text":"trend positive association negative association no association linear model","title":"Vocabulary:"},{"location":"unit4/lesson11/#essential-concepts","text":"Essential Concepts: A positive or negative association between variables provides valuable insights into increasing or decreasing trends, particularly in making predictions. By understanding these associations, we can anticipate future outcomes or behaviors more accurately.","title":"Essential Concepts:"},{"location":"unit4/lesson11/#lesson","text":"Distribute What\u2019s the Trend? handout ( LMR_4.12 ). Students will analyze the two scatterplots on the handout. The Profits per Explosion plot shows the relationship between the number of explosions in Michael Bay\u2019s movies and the profit earned by each movie. The Scores Over Time plot shows the relationship between M. Night Shyamalan movies made since The Sixth Sense was released in 1999 and their Internet Movie Database (IMBD) scores. LMR_4.12 In teams, students will discuss and record their responses to the following questions for each plot: What kind of plot is this? Answer: Scatterplot. What do the numbers on the x-axis represent? What do the numbers on the y-axis represent? Answer: The x-axis shows number of explosions and y-axis shows profit in millions of dollars. What is this plot telling us? Answers will vary. One example could be that if there are more explosions in a movie, then the movie will earn a greater profit. What kind of plot is this? Answer: Scatterplot. What do the numbers on the x-axis represent? What do the numbers on the y-axis represent? Answer: The x-axis shows the number of years since 1999 and the y-axis shows the movie\u2019s IMDB score. What is this plot telling us? Answers will vary. One example could be that as M. Night Shyamalan has produced more movies, their IMDB ratings have gone down. Allow students time to discuss and record their answers to the questions. Display both plots, if possible (students may also refer to the plots in their own handout). Discuss the following questions with the whole class: What is happening in each plot? What seems to be the trend? Answer: Guide students to understand that the Profits per Explosion plot shows an increasing trend, while the Scores Over Time plot shows a decreasing trend. An increasing trend is called a positive association and a decreasing trend is called a negative association . What does it mean to have an increasing trend and a positive association? Answer: In Profits per Explosion, it means that as the number of explosions increase, the movie profits also increase. What does it mean to have a decreasing trend and a negative association? Answer: In Scores Over Time, it means that as the years after 1999 pass, the movie IMBD ratings decrease. Quickwrite: What if we had a plot with no association ? Ask students to sketch what they think a scatterplot that shows no association looks like. Answer: A correct sketch will show a scatterplot with data points that show no positive or negative association; no trend or pattern. There would be no association or a very weak one. The data would be scattered. Select a couple of sketches to share with the whole class. Discuss why the sketches show no association. Ask students to discuss their thoughts about why a line was drawn through the points of the two plots and why there are equations for each plot. Conduct a share out of their observations. Guide students to the understanding that both plots follow a linear trend. This line then represents a model for the relationship between the two variables. The equations shown in the plots above represent the lines through the points. They provide a description of the data and the relationship between the variables. Ask student teams to refer back to the What\u2019s the Trend? handout ( LMR_4.12 ). They should discuss the following questions and record their responses on the Predicting Values handout ( LMR_4.13 ): LMR_4.13 What do you notice about where the points are and where the line is? Answer: Some points are near the line, others are further away, and one point is exactly on the line. Data points are observed values and points on the line are predicted values . Recall from Algebra that every line can be represented by an equation in the form y=mx+b. In this case, the equation of the regression line is y=3.2536x+154.3654. What do the x- and y-values represent in this equation? Answer: The x-values represent the number of explosions and the y-values represent the predicted profit. According to the equation, what is the slope of this line? What does the slope mean in relation to the number of explosions? Answer: The slope is 3.2536. It is the rate of change between the number of explosions and the profit. It means that for every explosion increase of 1 the profit increases by 3.2536 dollars. When the number of explosions (x-value) is zero, what is the profit (y-value)? How do you know? What does this mean? Answer: The profit is 154.3654 million dollars. Students may use the equation to show that they substituted zero for x, so the y-intercept is the profit. It means that if Michael Bay were to make a movie with NO explosions, this would be his projected profit. If you wanted to know the profit for the point that lies the closest to the line, what would the equation be? Write the equation and solve it. Answer: Profit=3.2536(105)+154.3654. Profit=495.9934or 495,993,400 million dollars. What was the actual profit for the point that lies closest to the line? Answer: The actual profit was 553,700,000 million dollars. Note: An estimate is okay in this case as the exact values are not given. What if Michael Bay made a movie that had 325 explosions? What would his predicted profit be? Show how you arrived at the solution. Answer: By substituting 325 in the value of x in the equation, predicted profit will be $1,211, 785, 400 or $ 1, 211.7854, or by finding the point on the line or both. If time permits, have students answer the following questions about the Scores Over Time scatterplot in LMR_4.12_What\u2019s the Trend . What do you notice about where the points are and where the line is? What do the y- and x-values represent in this equation? According to the equation, what is the slope of this line? What does the slope mean? When the x-value is zero, what is the y-value? How do you know? What does this mean? What would the predicted value of the score be if M. Night Shyamalan released a movie in 2015? How do you know?","title":"Lesson:"},{"location":"unit4/lesson11/#class-scribes","text":"One team of students will give a brief talk to discuss what they think the 3 most important topics of the day were.","title":"Class Scribes:"},{"location":"unit4/lesson11/#homework","text":"Students will finish answering the questions above about the Scores Over Time scatterplot in LMR_4.12_What\u2019s the Trend referenced above.","title":"Homework"},{"location":"unit4/lesson12/","text":"Lesson 12: How Strong Is It? Objective: Students will learn that the correlation coefficient is a value that measures the strength in linear associations only. Materials: Strength of Association handout ( LMR_4.14_Strength of Association ) Correlation Coefficient handout ( LMR_4.15_Correlation Coefficient ) Note: Advance preparation required. This handout is the resource for the plot cutouts. DO NOT distribute as-is to students. Vocabulary: correlation coefficient strength of association Essential Concepts: Essential Concepts: A high absolute value for correlation means a strong linear trend. A value close to 0 means a weak linear trend. Lesson: Distribute the Strength of Association handout ( LMR_4.14_Strength of Association ). In teams, students will examine the scatterplots (b) through (e). Their task is to discuss the strength of the association for each plot. They will determine which plots they think show strong associations and which ones they believe show weak associations. They must explain how they made their decision. Reasons must reference the plots. As an example, demonstrate how to describe plot (a) in the Strength of Association handout. Possible description: Plot (a) shows a negative association, or decreasing trend. The association appears to be fairly strong because the points are relatively close together, forming a moderate linear pattern. LMR_4.14 Once all teams have completed the handout, assign one plot to each team for a share out. If two teams have the same plot, one team will share its explanation first and the second team can agree, disagree, or add to the first team\u2019s description Guide students to understand that a strong association has points closer to each other and a weak association has points more scattered. Inform students that, so far, they have been labeling associations as strong, very strong, or weak. A number called the correlation coefficient measures strength of association. The correlation coefficient only applies to linear relationships, which must be checked visually with a scatterplot. Later we will learn how to calculate this number using RStudio. Note to teacher: Advance preparation is needed for this lesson. Each team needs one envelope with cutouts of plots A-F in LMR_4.15 (Part 1). Make envelopes according to the number of teams in the class. This process will be repeated for LMR_4.15 (Part 2). Distribute the envelopes to the teams. Students will examine the strength of association in each plot. Their task is to assign the correlation coefficient that corresponds to each plot and to explain why they assigned that correlation coefficient to that particular plot. The only piece of information they will receive is that a correlation coefficient equal to 1 has the strongest linear association and a correlation coefficient equal to 0 has the weakest association. LMR_4.15 Assign each team one plot. If there are more teams than plots, these teams will be assigned a plot in the next round. Each team will share the correlation coefficient they assigned to their plot and the explanation that goes with it. Using the Voting Cards strategy (see Instructional Strategies), the rest of the teams will show whether they approve, disapprove, or are uncertain about the teams\u2019 assignment and/or explanation. Repeat for each plot. The correlation coefficients for each plot are: \u2022 Plot A: r = 1.00 \u2022 Plot B: r = 0.72 \u2022 Plot C: r = 0.19 \u2022 Plot D: r = 0.48 \u2022 Plot E: r = 0.98 \u2022 Plot F: r = 0.00 The last set of plots showed positive associations. Now students will assign the correlation coefficients for plots G-L for LMR_4.15 (Part 2). Distribute the envelopes to the teams. Students will examine the strength of association in each plot. Their task is to assign the correlation coefficient that corresponds to each plot and to explain why they assigned that correlation coefficient to that particular plot. The only piece of information they will receive is that a correlation coefficient equal to -1 has the strongest linear association and a correlation coefficient equal to 0 has the weakest association. Teams previously not assigned a plot are now assigned one. Each team will share the correlation coefficient they assigned to their plot and the explanation that goes with it. Using the Voting Cards strategy, the rest of the teams will show whether they approve, disapprove, or are uncertain about the teams\u2019 assignment and/or explanation. Lead a class discussion whenever there is disapproval or uncertainty. Repeat for each plot. The correlation coefficients for each plot are: \u2022 Plot G: r = -1.00 \u2022 Plot H: r = 0.72 \u2022 Plot I: r = -0.19 \u2022 Plot J: r = -0.48 \u2022 Plot K: r = 0.98 \u2022 Plot L: r = 0.00 Journal Entry: What is a correlation coefficient, what does it do, and what does it tell us about a scatterplot? Homework & Next Day Students will complete the journal entry for homework if not completed in class. LAB 4D: Interpreting Correlations Complete Lab 4D prior to Lesson 13 .","title":"Lesson 12: How Strong Is It?"},{"location":"unit4/lesson12/#lesson-12-how-strong-is-it","text":"","title":"Lesson 12: How Strong Is It?"},{"location":"unit4/lesson12/#objective","text":"Students will learn that the correlation coefficient is a value that measures the strength in linear associations only.","title":"Objective:"},{"location":"unit4/lesson12/#materials","text":"Strength of Association handout ( LMR_4.14_Strength of Association ) Correlation Coefficient handout ( LMR_4.15_Correlation Coefficient ) Note: Advance preparation required. This handout is the resource for the plot cutouts. DO NOT distribute as-is to students.","title":"Materials:"},{"location":"unit4/lesson12/#vocabulary","text":"correlation coefficient strength of association","title":"Vocabulary:"},{"location":"unit4/lesson12/#essential-concepts","text":"Essential Concepts: A high absolute value for correlation means a strong linear trend. A value close to 0 means a weak linear trend.","title":"Essential Concepts:"},{"location":"unit4/lesson12/#lesson","text":"Distribute the Strength of Association handout ( LMR_4.14_Strength of Association ). In teams, students will examine the scatterplots (b) through (e). Their task is to discuss the strength of the association for each plot. They will determine which plots they think show strong associations and which ones they believe show weak associations. They must explain how they made their decision. Reasons must reference the plots. As an example, demonstrate how to describe plot (a) in the Strength of Association handout. Possible description: Plot (a) shows a negative association, or decreasing trend. The association appears to be fairly strong because the points are relatively close together, forming a moderate linear pattern. LMR_4.14 Once all teams have completed the handout, assign one plot to each team for a share out. If two teams have the same plot, one team will share its explanation first and the second team can agree, disagree, or add to the first team\u2019s description Guide students to understand that a strong association has points closer to each other and a weak association has points more scattered. Inform students that, so far, they have been labeling associations as strong, very strong, or weak. A number called the correlation coefficient measures strength of association. The correlation coefficient only applies to linear relationships, which must be checked visually with a scatterplot. Later we will learn how to calculate this number using RStudio. Note to teacher: Advance preparation is needed for this lesson. Each team needs one envelope with cutouts of plots A-F in LMR_4.15 (Part 1). Make envelopes according to the number of teams in the class. This process will be repeated for LMR_4.15 (Part 2). Distribute the envelopes to the teams. Students will examine the strength of association in each plot. Their task is to assign the correlation coefficient that corresponds to each plot and to explain why they assigned that correlation coefficient to that particular plot. The only piece of information they will receive is that a correlation coefficient equal to 1 has the strongest linear association and a correlation coefficient equal to 0 has the weakest association. LMR_4.15 Assign each team one plot. If there are more teams than plots, these teams will be assigned a plot in the next round. Each team will share the correlation coefficient they assigned to their plot and the explanation that goes with it. Using the Voting Cards strategy (see Instructional Strategies), the rest of the teams will show whether they approve, disapprove, or are uncertain about the teams\u2019 assignment and/or explanation. Repeat for each plot. The correlation coefficients for each plot are: \u2022 Plot A: r = 1.00 \u2022 Plot B: r = 0.72 \u2022 Plot C: r = 0.19 \u2022 Plot D: r = 0.48 \u2022 Plot E: r = 0.98 \u2022 Plot F: r = 0.00 The last set of plots showed positive associations. Now students will assign the correlation coefficients for plots G-L for LMR_4.15 (Part 2). Distribute the envelopes to the teams. Students will examine the strength of association in each plot. Their task is to assign the correlation coefficient that corresponds to each plot and to explain why they assigned that correlation coefficient to that particular plot. The only piece of information they will receive is that a correlation coefficient equal to -1 has the strongest linear association and a correlation coefficient equal to 0 has the weakest association. Teams previously not assigned a plot are now assigned one. Each team will share the correlation coefficient they assigned to their plot and the explanation that goes with it. Using the Voting Cards strategy, the rest of the teams will show whether they approve, disapprove, or are uncertain about the teams\u2019 assignment and/or explanation. Lead a class discussion whenever there is disapproval or uncertainty. Repeat for each plot. The correlation coefficients for each plot are: \u2022 Plot G: r = -1.00 \u2022 Plot H: r = 0.72 \u2022 Plot I: r = -0.19 \u2022 Plot J: r = -0.48 \u2022 Plot K: r = 0.98 \u2022 Plot L: r = 0.00 Journal Entry: What is a correlation coefficient, what does it do, and what does it tell us about a scatterplot?","title":"Lesson:"},{"location":"unit4/lesson12/#homework-next-day","text":"Students will complete the journal entry for homework if not completed in class. LAB 4D: Interpreting Correlations Complete Lab 4D prior to Lesson 13 .","title":"Homework &amp; Next Day"},{"location":"unit4/lesson13/","text":"Lesson 13: Improving Your Model Objective: Students will learn to describe associations that are not linear. Materials: Describe the Association handout ( LMR_4.16_Describe the Association ) Vocabulary: non-linear polynomial trends Essential Concepts: Essential Concepts: If a linear model is fit to a non-linear trend, it will not do a good job of predicting. For this reason, we need to identify non-linear trends by looking at a scatterplot or the model needs to match the trend. Lesson: Remind students that they have been learning a great deal about linear associations. However, there are other types of associations, and today they will learn to describe them. Distribute the Describe the Association handout ( LMR_4.16 ). In teams, students will examine the trend of each plot. Their task is to write a description of the trend that they see in the data and what the trend means. LMR_4.16 Allow students time to discuss and record their descriptions for each plot in their DS journals. Walk around the room monitoring student teamwork. Look for descriptions that are interesting to share with the whole class. Select a team to present a description of one plot to the class. Teams will listen to each presentation, compare it to their description of the plot, and as a team they will agree or disagree. If there is disagreement, lead a discussion that guides students to reason toward the correct description. Summarize the discussion for each plot and ask students take notes or revise their descriptions in their DS journals. Repeat steps 4 and 5 for the rest of the plots. Plot Descriptions for Describe the Association ( LMR_4.16 ): Plot A: There is no trend (perhaps some may see a very, very weak linear trend), so there is no/hardly any association. There is a great deal of scatter in the data. It means that y does not depend on x. Plot B: There appears to be a linear trend. The association is negative and appears somewhat strong. It means that as x increases, y decreases. Plot C: There is a linear trend. The association is positive and it is very strong. It means that the y-value increases at approximately the same rate for every increase in x value. This is a line. Plot D: The trend is non-linear. There seems to be a weak association because there is scatter in the data. We cannot tell if the association is positive or negative. It has the shape of a parabola; therefore, it is quadratic. For smaller x-values, the y-value is decreasing and for larger x values, the y value is increasing. Plot E: The trend is non-linear. There seems to be a strong association because there is little scatter in the data. It is also in the shape of a parabola, so it is quadratic. Using the Cheat Notes strategy, ask teams to write notes about how to describe associations. Plots A, B, and C should be familiar to the students by now. However, plots D and E show a different type of trend. Although the trends are non-linear, they can still tell us important information about the y-values based on values of x. Ask: What happens if we were to fit a linear model to these non-linear trends? Would it still make good predictions? Answer: Fitting a linear model to a non-linear trend would not properly describe the trend of the data. Therefore no, it would not make good predictions. To examine why they would not make good predictors, draw an approximate linear best-fit line and get students to understand that in some regions, the model would almost always over-predict, and in others would almost always under-predict. We want a model that goes, more or less, through the 'middle' of the points. Ask: How can we get a model that goes, more or less, through the middle of all the data points? Answer: We need to change the model. Trends like the quadratic ones shown in plots D and E can be described as polynomial trends . Plots that follow quadratic, cubic, quartic, etc. shapes all exhibit polynomial trends. We need to adjust the model. You may show students several choices of equations (quadratic, trinomial, linear) along with their graphs and ask them which might be a good candidate. When investigating the data for trends, the model needs to fit the data. Class Scribes: One team of students will give a brief talk to discuss what they think the 3 most important topics of the day were. Homework & Next Day Students may finish their Cheat Notes for homework, if not completed in class. LAB 4E: Some Models Have Curves Complete Lab 4E prior to Lesson 14 .","title":"Lesson 13: Improving your Model"},{"location":"unit4/lesson13/#lesson-13-improving-your-model","text":"","title":"Lesson 13: Improving Your Model"},{"location":"unit4/lesson13/#objective","text":"Students will learn to describe associations that are not linear.","title":"Objective:"},{"location":"unit4/lesson13/#materials","text":"Describe the Association handout ( LMR_4.16_Describe the Association )","title":"Materials:"},{"location":"unit4/lesson13/#vocabulary","text":"non-linear polynomial trends","title":"Vocabulary:"},{"location":"unit4/lesson13/#essential-concepts","text":"Essential Concepts: If a linear model is fit to a non-linear trend, it will not do a good job of predicting. For this reason, we need to identify non-linear trends by looking at a scatterplot or the model needs to match the trend.","title":"Essential Concepts:"},{"location":"unit4/lesson13/#lesson","text":"Remind students that they have been learning a great deal about linear associations. However, there are other types of associations, and today they will learn to describe them. Distribute the Describe the Association handout ( LMR_4.16 ). In teams, students will examine the trend of each plot. Their task is to write a description of the trend that they see in the data and what the trend means. LMR_4.16 Allow students time to discuss and record their descriptions for each plot in their DS journals. Walk around the room monitoring student teamwork. Look for descriptions that are interesting to share with the whole class. Select a team to present a description of one plot to the class. Teams will listen to each presentation, compare it to their description of the plot, and as a team they will agree or disagree. If there is disagreement, lead a discussion that guides students to reason toward the correct description. Summarize the discussion for each plot and ask students take notes or revise their descriptions in their DS journals. Repeat steps 4 and 5 for the rest of the plots. Plot Descriptions for Describe the Association ( LMR_4.16 ): Plot A: There is no trend (perhaps some may see a very, very weak linear trend), so there is no/hardly any association. There is a great deal of scatter in the data. It means that y does not depend on x. Plot B: There appears to be a linear trend. The association is negative and appears somewhat strong. It means that as x increases, y decreases. Plot C: There is a linear trend. The association is positive and it is very strong. It means that the y-value increases at approximately the same rate for every increase in x value. This is a line. Plot D: The trend is non-linear. There seems to be a weak association because there is scatter in the data. We cannot tell if the association is positive or negative. It has the shape of a parabola; therefore, it is quadratic. For smaller x-values, the y-value is decreasing and for larger x values, the y value is increasing. Plot E: The trend is non-linear. There seems to be a strong association because there is little scatter in the data. It is also in the shape of a parabola, so it is quadratic. Using the Cheat Notes strategy, ask teams to write notes about how to describe associations. Plots A, B, and C should be familiar to the students by now. However, plots D and E show a different type of trend. Although the trends are non-linear, they can still tell us important information about the y-values based on values of x. Ask: What happens if we were to fit a linear model to these non-linear trends? Would it still make good predictions? Answer: Fitting a linear model to a non-linear trend would not properly describe the trend of the data. Therefore no, it would not make good predictions. To examine why they would not make good predictors, draw an approximate linear best-fit line and get students to understand that in some regions, the model would almost always over-predict, and in others would almost always under-predict. We want a model that goes, more or less, through the 'middle' of the points. Ask: How can we get a model that goes, more or less, through the middle of all the data points? Answer: We need to change the model. Trends like the quadratic ones shown in plots D and E can be described as polynomial trends . Plots that follow quadratic, cubic, quartic, etc. shapes all exhibit polynomial trends. We need to adjust the model. You may show students several choices of equations (quadratic, trinomial, linear) along with their graphs and ask them which might be a good candidate. When investigating the data for trends, the model needs to fit the data.","title":"Lesson:"},{"location":"unit4/lesson13/#class-scribes","text":"One team of students will give a brief talk to discuss what they think the 3 most important topics of the day were.","title":"Class Scribes:"},{"location":"unit4/lesson13/#homework-next-day","text":"Students may finish their Cheat Notes for homework, if not completed in class. LAB 4E: Some Models Have Curves Complete Lab 4E prior to Lesson 14 .","title":"Homework &amp; Next Day"},{"location":"unit4/lesson14/","text":"Lesson 14: More Variables to Make Better Predictions Objective: Students will see that different variables can be used to make predictions about the same outcome (response variable) and consider whether combining these variables could improve prediction accuracy. Materials: Advertising Plots Part 1 handout ( LMR_4.17_Advertising Plots 1 ) Advertising Plots Part 2 handout ( LMR_4.18_Advertising Plots 2 ) Article: How Long Can a Spinoff Like \u2018Better Call Saul\u2019 Last? http://fivethirtyeight.com/features/how-long-can-a-spinoff-like-better-call-saul-last/ Vocabulary: market Essential Concepts: Essential Concepts: We can use scatterplots to assess which variables might lead to strong predictive models. Sometimes using several predictors in one model can produce stronger models. Lesson: Remind students that models are used to make predictions. Ask a volunteer to think of a TV show that had a \u201cspinoff\u201d and to name both of the shows. Ask if he/she knows whether or not the original was more or less successful than the spinoff. Then, ask the class: Is there a way to predict spinoff success? Next, using the Talking to the Text instructional strategy, ask students to read the article titled: How Long Can a Spinoff Like Better Call Saul Last? Note: If this is the first time using this strategy with your students, make sure you model/explain it before they begin reading it. See Instructional Strategies in Teacher Resources for a description. After reading the article, ask students to discuss three Talking to the Text responses with a partner. You may set a time limit for each student to share with his/her partner. Then, in teams, students will answer the following questions pertaining to the article: What is the article trying to predict? Answer: The success of a spinoff show. How many variables are used? Answer: Four - the original show name, the number of episodes of the original show, the name of the spinoff show, and the number of episodes of the spinoff show. What other variables might affect a spinoff? Possible answers are budget or actors. The dotted line in the plot is not a regression line. How would you draw a regression line to make predictions? Answers will vary but we would want to try to \"fit\" a line to the plotted data. What other information would you like to know to predict a spinoff\u2019s success? Answers will vary but may be similar to (c) above. Allow students time to discuss and record their answers. Then conduct a share out of their responses to the discussion questions. Discuss the following questions with the class: What effect does advertising have on retail sales? Where do stores advertise (What mediums do they use)? Does each method of advertisement reach the same people? Does each method of advertisement have a similar effect? Or are some methods more effective than others? Distribute the 3 plots from the Advertising Plots Part 1 handout ( LMR_4.17 ) and inform the students about the data using the details below: (Plots are presented separately in the LMR) LMR_4.17 These 3 plots show the number of items sold by a retailer (in 200 different markets) and the amount of money the company spent on TV , Radio and Newspaper advertisements. The data has 200 observations, one for each different market. A market is simply a location where an item is sold. For example, Los Angeles and San Francisco are two different markets. Each observation has 4 variables: (1) The number of items sold (in 10\u2019s of thousands of units), (2) the money spent on TV ads (in thousands of dollars), (3) the money spent on radio ads (in thousands of dollars), and (4) the money spent on newspaper ads (in thousands of dollars). The data were collected using an observational study. To illustrate a-d above, ask students to refer to plot A (TV ads) and circle the market in which this retailer sold the least number of items (see circles in plots above). Ask: How many items did this market sell? Answer: About 20,000 items. The actual number of items sold was 1.6 (in 10,000\u2019s of units) which is 16,000 items. How much money did this retailer spend on TV ads in this market? Answer: This retailer spent zero dollars on TV ads. The actual amount the retailer spent on TV ads was 0.7 thousands of dollars, which is $700. Students should then refer to plot B (Radio ads), find the same market (the one in which the retailer sold about 20,000 items) and circle it. Ask: How much money did the retailer spend on Radio ads in the same market? Answer: About 40 thousand dollars. The actual amount spent on Radio ads was 39.6 thousands of dollars, which is $39,600. Finally, ask students to refer to plot C (Newspaper ads), find the same market (the one in which the retailer sold about 20,000 items), and circle it. Ask: How much money did the retailer spend on Newspaper ads in the same market? Answer: About 10 thousand dollars. About The actual amount spent on Newspaper ads is 8.7 thousands of dollars, which is $8,700 Based on the above plots, use a Pair-Share to discuss the following: Describe the relationship between advertisements and the number of items sold. Answers will vary but we would expect the number of items sold to increase with increased advertisements. Which type of advertisement is the most strongly correlated with the number of units sold? How can you tell? Answer: Plot A, TV advertisements, appears to be the most strongly correlated with the number of units sold. We can tell because the points are more closely packed/together than in plots B or C. Distribute the Advertising Plots Part 2 handout ( LMR_4.18 ), which contains plots A-C, but now include the line of best fit. (Plots are presented separately in the LMR) LMR_4.18 Ask students to recall from Lesson 7 that a method statisticians use to figure out which predicted values is closest to the actual data is the mean squared error (MSE). In teams, ask students to discuss the following: How would you determine which plot would make the most accurate predictions? Answers will vary, but you would expect to hear something like: \u201cthe prediction line that has the least amount of distance to all the points on the plot would make the most accurate prediction because the predicted values will be closer to the actual data\u201d. Next, have students select a statement they think is best (a or b), then write a justification for their selection based on what they learned in this lesson. This may be completed as homework. Combining multiple variables (e.g., TV and Newspaper ads, TV and Radio ads, TV, Radio, and Newspaper ads, etc.) into one model will lead to worse predictions because the variables that make poor predictions will contaminate those that make good predictions. Combining multiple variables (e.g., TV and Newspaper ads, TV and Radio ads, TV, Radio, and Newspaper ads, etc.) into one model will lead to better predictions because the model can use more information to make predictions. Inform students that RStudio has the capability of creating models that combine multiple variables to make predictions about another variable. For example, it can make a model to predict number of items sold using both money spent on TV and money spent on Newspaper ads. Students will learn more about it during the next lesson . Class Scribes: One team of students will give a brief talk to discuss what they think the 3 most important topics of the day were. Homework Students may continue writing their justifications for the selected statement in item 15 if they were unable to finish.","title":"Lesson 14: More Variables to Make Better Predictions"},{"location":"unit4/lesson14/#lesson-14-more-variables-to-make-better-predictions","text":"","title":"Lesson 14: More Variables to Make Better Predictions"},{"location":"unit4/lesson14/#objective","text":"Students will see that different variables can be used to make predictions about the same outcome (response variable) and consider whether combining these variables could improve prediction accuracy.","title":"Objective:"},{"location":"unit4/lesson14/#materials","text":"Advertising Plots Part 1 handout ( LMR_4.17_Advertising Plots 1 ) Advertising Plots Part 2 handout ( LMR_4.18_Advertising Plots 2 ) Article: How Long Can a Spinoff Like \u2018Better Call Saul\u2019 Last? http://fivethirtyeight.com/features/how-long-can-a-spinoff-like-better-call-saul-last/","title":"Materials:"},{"location":"unit4/lesson14/#vocabulary","text":"market","title":"Vocabulary:"},{"location":"unit4/lesson14/#essential-concepts","text":"Essential Concepts: We can use scatterplots to assess which variables might lead to strong predictive models. Sometimes using several predictors in one model can produce stronger models.","title":"Essential Concepts:"},{"location":"unit4/lesson14/#lesson","text":"Remind students that models are used to make predictions. Ask a volunteer to think of a TV show that had a \u201cspinoff\u201d and to name both of the shows. Ask if he/she knows whether or not the original was more or less successful than the spinoff. Then, ask the class: Is there a way to predict spinoff success? Next, using the Talking to the Text instructional strategy, ask students to read the article titled: How Long Can a Spinoff Like Better Call Saul Last? Note: If this is the first time using this strategy with your students, make sure you model/explain it before they begin reading it. See Instructional Strategies in Teacher Resources for a description. After reading the article, ask students to discuss three Talking to the Text responses with a partner. You may set a time limit for each student to share with his/her partner. Then, in teams, students will answer the following questions pertaining to the article: What is the article trying to predict? Answer: The success of a spinoff show. How many variables are used? Answer: Four - the original show name, the number of episodes of the original show, the name of the spinoff show, and the number of episodes of the spinoff show. What other variables might affect a spinoff? Possible answers are budget or actors. The dotted line in the plot is not a regression line. How would you draw a regression line to make predictions? Answers will vary but we would want to try to \"fit\" a line to the plotted data. What other information would you like to know to predict a spinoff\u2019s success? Answers will vary but may be similar to (c) above. Allow students time to discuss and record their answers. Then conduct a share out of their responses to the discussion questions. Discuss the following questions with the class: What effect does advertising have on retail sales? Where do stores advertise (What mediums do they use)? Does each method of advertisement reach the same people? Does each method of advertisement have a similar effect? Or are some methods more effective than others? Distribute the 3 plots from the Advertising Plots Part 1 handout ( LMR_4.17 ) and inform the students about the data using the details below: (Plots are presented separately in the LMR) LMR_4.17 These 3 plots show the number of items sold by a retailer (in 200 different markets) and the amount of money the company spent on TV , Radio and Newspaper advertisements. The data has 200 observations, one for each different market. A market is simply a location where an item is sold. For example, Los Angeles and San Francisco are two different markets. Each observation has 4 variables: (1) The number of items sold (in 10\u2019s of thousands of units), (2) the money spent on TV ads (in thousands of dollars), (3) the money spent on radio ads (in thousands of dollars), and (4) the money spent on newspaper ads (in thousands of dollars). The data were collected using an observational study. To illustrate a-d above, ask students to refer to plot A (TV ads) and circle the market in which this retailer sold the least number of items (see circles in plots above). Ask: How many items did this market sell? Answer: About 20,000 items. The actual number of items sold was 1.6 (in 10,000\u2019s of units) which is 16,000 items. How much money did this retailer spend on TV ads in this market? Answer: This retailer spent zero dollars on TV ads. The actual amount the retailer spent on TV ads was 0.7 thousands of dollars, which is $700. Students should then refer to plot B (Radio ads), find the same market (the one in which the retailer sold about 20,000 items) and circle it. Ask: How much money did the retailer spend on Radio ads in the same market? Answer: About 40 thousand dollars. The actual amount spent on Radio ads was 39.6 thousands of dollars, which is $39,600. Finally, ask students to refer to plot C (Newspaper ads), find the same market (the one in which the retailer sold about 20,000 items), and circle it. Ask: How much money did the retailer spend on Newspaper ads in the same market? Answer: About 10 thousand dollars. About The actual amount spent on Newspaper ads is 8.7 thousands of dollars, which is $8,700 Based on the above plots, use a Pair-Share to discuss the following: Describe the relationship between advertisements and the number of items sold. Answers will vary but we would expect the number of items sold to increase with increased advertisements. Which type of advertisement is the most strongly correlated with the number of units sold? How can you tell? Answer: Plot A, TV advertisements, appears to be the most strongly correlated with the number of units sold. We can tell because the points are more closely packed/together than in plots B or C. Distribute the Advertising Plots Part 2 handout ( LMR_4.18 ), which contains plots A-C, but now include the line of best fit. (Plots are presented separately in the LMR) LMR_4.18 Ask students to recall from Lesson 7 that a method statisticians use to figure out which predicted values is closest to the actual data is the mean squared error (MSE). In teams, ask students to discuss the following: How would you determine which plot would make the most accurate predictions? Answers will vary, but you would expect to hear something like: \u201cthe prediction line that has the least amount of distance to all the points on the plot would make the most accurate prediction because the predicted values will be closer to the actual data\u201d. Next, have students select a statement they think is best (a or b), then write a justification for their selection based on what they learned in this lesson. This may be completed as homework. Combining multiple variables (e.g., TV and Newspaper ads, TV and Radio ads, TV, Radio, and Newspaper ads, etc.) into one model will lead to worse predictions because the variables that make poor predictions will contaminate those that make good predictions. Combining multiple variables (e.g., TV and Newspaper ads, TV and Radio ads, TV, Radio, and Newspaper ads, etc.) into one model will lead to better predictions because the model can use more information to make predictions. Inform students that RStudio has the capability of creating models that combine multiple variables to make predictions about another variable. For example, it can make a model to predict number of items sold using both money spent on TV and money spent on Newspaper ads. Students will learn more about it during the next lesson .","title":"Lesson:"},{"location":"unit4/lesson14/#class-scribes","text":"One team of students will give a brief talk to discuss what they think the 3 most important topics of the day were.","title":"Class Scribes:"},{"location":"unit4/lesson14/#homework","text":"Students may continue writing their justifications for the selected statement in item 15 if they were unable to finish.","title":"Homework"},{"location":"unit4/lesson15/","text":"Lesson 15: Combination of Variables Objective: Students will learn that we can make better predictions by including more variables. Then they will wrestle with how the information should be combined. Materials: Advertising Plots Part 2 handout ( LMR_4.18 ) from Lesson 14 Essential Concepts: Essential Concepts: If multiple predictors are associated with the response variable, a better predictive model will be produced, as measured by the mean squared error. Lesson: Display the plots and statements from the previous day: LMR_4.18 Combining multiple variables (e.g., money spent on TV and Newspaper ads, TV and Radio ads, TV, Radio, and Newspaper ads, etc.) into one model will lead to worse predictions because the variables that make poor predictions will contaminate those that make good predictions. Combining multiple variables (e.g., TV and Newspaper ads, TV and Radio ads, TV, Radio, and Newspaper ads, etc.) into one model will lead to better predictions because the model can use more information to make predictions. Ask the students to share out their opinions using the Agree/Disagree strategy. Next, inform teams that they will have 2 minutes to come up with as many combinations of ads (variables) as they can think of (e.g., TV + Newspaper ads, TV+ Radio ads, TV + Radio + Newspaper ads, etc.) After 2 minutes, list all the different combinations by conducting a Whip Around and eliciting a combination from each team. By a show of hands, ask students to select which combination or single model will be the best predictor for the number of items sold by the retailer. Then inform students that we will determine which of the statements is true by comparing the mean aquared error (MSE) of single models (like the ones we showed in the previous lesson) vs. combined models. But first, use the line of best fit for the combined variables: Note: The function that produced the line of best fit using RStudio was lm(Sales ~ TV + Radio + Newspaper, data= retail) Use this equation to predict the amount of sales for the same market they circled in the previous lesson. Student calculations should yield the predicted value in (b), below. Note: Remind students that they need to substitute the values as they appear in the x-axis of the plots without converting to thousands of dollars. For example, the circled market spent about 10 thousand dollars on newspaper ads, so students should substitute 10 instead of the expanded value in the equation. Does the predicted value (10.81224) seem like a plausible number of sales? Why? Answer: It is not a plausible number of sales because the prediction is too high. The prediction says the retailer will sell about 108,122 units, when the actual sales were about 16,000 units. Although the model did not make a very good prediction for this market, it is not surprising because as LMR_4.18 displays, that market did not fit the overall pattern in any of the scatterplots. Reveal that RStudio calculated the mean squared error for different combinations plus the single models, and the results are displayed on the table below. This means that, for example, when using the TV model to predict number of items sold, our predictions will typically be off by about 3.373579 (in 10,000s) of units or 33,736 units. Then ask students the questions below. Note: Remember that the MSE will always be in square units. In order to convert back to the original units, simply take the square root of the MSE. Model Mean Squared Error (MSE) Square Root MSE TV 11.38103 3.373579 Radio 25.35521 5.035396 Newspaper 31.44164 5.607285 TV-Radio 4.456745 2.1111 TV-Newspaper 9.246567 3.040817 Radio-Newspaper 27.26889 5.221963 TV-Radio-Newspaper 4.70338 2.168728 Which model is the best predictor of number of items sold? Answer: The TV-Radio model is the best predictor of number of items sold because it had the least amount of error, on average. When using the TV-Radio model to predict number of items sold, our predictions will typically be off by 21,111 units. Which model was the least reliable in predicting the number of items sold? Answer: The Newspaper model is the least reliable predictor of number of items sold because it had the most amount of error, on average. When using the Newspaper model to predict number of items sold, our predictions will typically be off by 56,073 units. What else do you notice about the models? Answer: It appears that combining the variables into one model is much better than any of the single-variable models. Think back to our statements from the last lesson and the beginning of this lesson: Combining multiple variables (e.g., TV and Newspaper ads, TV and Radio ads, TV, Radio, and Newspaper ads, etc.) into one model will lead to worse predictions because the variables that make poor predictions will contaminate those that make good predictions. Combining multiple variables (e.g., TV and Newspaper ads, TV and Radio ads, TV, Radio, and Newspaper ads, etc.) into one model will lead to better predictions because the model can use more information to make predictions. Engage students in a discussion to check understanding of multiple variables in a model. Has their statement selection changed or stayed the same? Why? What evidence do they have to support their statement selection? Answers will vary because students can make an argument for either statement, both statements, or neither statements being true. There is evidence in step 7 that supports that more variables will lead to worse predictions (like TV-Radio-Newspaper) but there is also evidence that more variables lead to better predictions (like TV-Radio). Inform the students that, in the next lab, they will find out how to create the line of best fit for models that include many variables. Class Scribes: One team of students will give a brief talk to discuss what they think the 3 most important topics of the day were. Homework & Next Day Ask students to think of a reason or reasons about why it would not be a good idea to make a scatterplot for models that include more than 3 predictor variables? The answer is mainly because humans are limited to seeing things in 3 dimensions. For example, the model that combines all of the variables together is a 4 dimensional model. What does that look like? LAB 4F: This Model is Big Enough for All of Us Complete Lab 4F prior to Practicum .","title":"Lesson 15: Combination of Variables"},{"location":"unit4/lesson15/#lesson-15-combination-of-variables","text":"","title":"Lesson 15: Combination of Variables"},{"location":"unit4/lesson15/#objective","text":"Students will learn that we can make better predictions by including more variables. Then they will wrestle with how the information should be combined.","title":"Objective:"},{"location":"unit4/lesson15/#materials","text":"Advertising Plots Part 2 handout ( LMR_4.18 ) from Lesson 14","title":"Materials:"},{"location":"unit4/lesson15/#essential-concepts","text":"Essential Concepts: If multiple predictors are associated with the response variable, a better predictive model will be produced, as measured by the mean squared error.","title":"Essential Concepts:"},{"location":"unit4/lesson15/#lesson","text":"Display the plots and statements from the previous day: LMR_4.18 Combining multiple variables (e.g., money spent on TV and Newspaper ads, TV and Radio ads, TV, Radio, and Newspaper ads, etc.) into one model will lead to worse predictions because the variables that make poor predictions will contaminate those that make good predictions. Combining multiple variables (e.g., TV and Newspaper ads, TV and Radio ads, TV, Radio, and Newspaper ads, etc.) into one model will lead to better predictions because the model can use more information to make predictions. Ask the students to share out their opinions using the Agree/Disagree strategy. Next, inform teams that they will have 2 minutes to come up with as many combinations of ads (variables) as they can think of (e.g., TV + Newspaper ads, TV+ Radio ads, TV + Radio + Newspaper ads, etc.) After 2 minutes, list all the different combinations by conducting a Whip Around and eliciting a combination from each team. By a show of hands, ask students to select which combination or single model will be the best predictor for the number of items sold by the retailer. Then inform students that we will determine which of the statements is true by comparing the mean aquared error (MSE) of single models (like the ones we showed in the previous lesson) vs. combined models. But first, use the line of best fit for the combined variables: Note: The function that produced the line of best fit using RStudio was lm(Sales ~ TV + Radio + Newspaper, data= retail) Use this equation to predict the amount of sales for the same market they circled in the previous lesson. Student calculations should yield the predicted value in (b), below. Note: Remind students that they need to substitute the values as they appear in the x-axis of the plots without converting to thousands of dollars. For example, the circled market spent about 10 thousand dollars on newspaper ads, so students should substitute 10 instead of the expanded value in the equation. Does the predicted value (10.81224) seem like a plausible number of sales? Why? Answer: It is not a plausible number of sales because the prediction is too high. The prediction says the retailer will sell about 108,122 units, when the actual sales were about 16,000 units. Although the model did not make a very good prediction for this market, it is not surprising because as LMR_4.18 displays, that market did not fit the overall pattern in any of the scatterplots. Reveal that RStudio calculated the mean squared error for different combinations plus the single models, and the results are displayed on the table below. This means that, for example, when using the TV model to predict number of items sold, our predictions will typically be off by about 3.373579 (in 10,000s) of units or 33,736 units. Then ask students the questions below. Note: Remember that the MSE will always be in square units. In order to convert back to the original units, simply take the square root of the MSE. Model Mean Squared Error (MSE) Square Root MSE TV 11.38103 3.373579 Radio 25.35521 5.035396 Newspaper 31.44164 5.607285 TV-Radio 4.456745 2.1111 TV-Newspaper 9.246567 3.040817 Radio-Newspaper 27.26889 5.221963 TV-Radio-Newspaper 4.70338 2.168728 Which model is the best predictor of number of items sold? Answer: The TV-Radio model is the best predictor of number of items sold because it had the least amount of error, on average. When using the TV-Radio model to predict number of items sold, our predictions will typically be off by 21,111 units. Which model was the least reliable in predicting the number of items sold? Answer: The Newspaper model is the least reliable predictor of number of items sold because it had the most amount of error, on average. When using the Newspaper model to predict number of items sold, our predictions will typically be off by 56,073 units. What else do you notice about the models? Answer: It appears that combining the variables into one model is much better than any of the single-variable models. Think back to our statements from the last lesson and the beginning of this lesson: Combining multiple variables (e.g., TV and Newspaper ads, TV and Radio ads, TV, Radio, and Newspaper ads, etc.) into one model will lead to worse predictions because the variables that make poor predictions will contaminate those that make good predictions. Combining multiple variables (e.g., TV and Newspaper ads, TV and Radio ads, TV, Radio, and Newspaper ads, etc.) into one model will lead to better predictions because the model can use more information to make predictions. Engage students in a discussion to check understanding of multiple variables in a model. Has their statement selection changed or stayed the same? Why? What evidence do they have to support their statement selection? Answers will vary because students can make an argument for either statement, both statements, or neither statements being true. There is evidence in step 7 that supports that more variables will lead to worse predictions (like TV-Radio-Newspaper) but there is also evidence that more variables lead to better predictions (like TV-Radio). Inform the students that, in the next lab, they will find out how to create the line of best fit for models that include many variables.","title":"Lesson:"},{"location":"unit4/lesson15/#class-scribes","text":"One team of students will give a brief talk to discuss what they think the 3 most important topics of the day were.","title":"Class Scribes:"},{"location":"unit4/lesson15/#homework-next-day","text":"Ask students to think of a reason or reasons about why it would not be a good idea to make a scatterplot for models that include more than 3 predictor variables? The answer is mainly because humans are limited to seeing things in 3 dimensions. For example, the model that combines all of the variables together is a 4 dimensional model. What does that look like? LAB 4F: This Model is Big Enough for All of Us Complete Lab 4F prior to Practicum .","title":"Homework &amp; Next Day"},{"location":"unit4/lesson16/","text":"Lesson 16: Football or Futbol? Objective: Students will learn what decision trees look like and how they can be used to classify people or objects into groups. They will engage in an activity to see how making slight changes to the tree can lead to drastic rises or reductions in misclassifications. Materials: Decision Tree for Heart Attack Risk graphic ( LMR_4.19_CART Heart Attacks ) CART Activity Player Stats ( LMR_4.20_CART Player Stats ) CART Activity Round 1 Questions ( LMR_4.21_CART Round 1 ) CART Activity Round 2 Questions ( LMR_4.22_CART Round 2 ) Note: Advanced preparation required for LMR_4.20, 4.21, and 4.22 (see Step 8 below) Vocabulary: classify decision tree Classification and Regression Trees (CART) nodes Essential Concepts: Essential Concepts: Some trends are not linear, so the approaches we\u2019ve done so far won\u2019t be helpful. We need to model such trends differently. Decision trees are a non-linear tool for classifying observations into groups when the trend is non-linear. Lesson: Ask students the following question: If you were having chest pains, who would you trust more to diagnose you - a data scientist or a doctor? Give the students some time to think about the question and have a few of them share out their responses with the class. Note: It's likely that most students will choose to go to a doctor. As it turns out, back in the late 1970s, a cardiologist (and early data scientist) named Lee Goldman developed a decision tree based on millions of patient observations. It was made to diagnose whether people were or were not having a heart attack. The accuracy of the decision tree compared to the accuracy of actual doctor diagnoses are shown below. Correct diagnoses using the decision tree were above 95%. Correct diagnoses based on individual doctors' expertise was anywhere between 75-90%. Display the graphic from the Decision Tree for Heart Attack Risk handout ( LMR_4.19_CART Heart Attacks ) and explain that this is one example of what the decision tree that Goldman developed might have looked like. Note: This is NOT the actual tree that Goldman developed. LMR_4.19 Using a Pair-Share , ask students to discuss the following questions using the graphic above. Note: Answers will vary. These questions are meant to gauge student thinking before defining decision trees in the following steps of the lesson. What are decision trees? How do they work at classifying data into groups? Remind students that this unit has focused on linear models and making predictions. In the real world, data can be modeled in a variety of ways, many of which are non-linear, and because of this, we can\u2019t easily write down a mathematical equation to help us make predictions. However, we can use what we have learned so far to determine whether or not other models can provide a good fit to the data. Let students know that one method of modeling data in a non-linear way is with decision trees , like the one we saw with the heart attack classification. Explain that decision trees are \"grown\" by using algorithms, or rules, to test many, many different decision trees to find the one that makes the best predictions. A decision tree is basically a series of questions that are asked sequentially. Observations start by answering the first question (at the root of the tree), and then proceed along the different branches based on the answers they give to the questions that follow. At the end, based on all of the questions asked, observations are then classified as one of k classifications. Remind students that algorithms are a series of steps that are repeated a large number of times. For decision trees, this enables us to (1) explore many possible paths, beginning from the same initial point, or (2) find different starting points based on where we ended during the previous iteration. Inform students that, during today's lesson, they will be participating in an activity to try to classify professional athletes into one of two groups: (1) soccer players on the US Men's National Team, OR (2) football players in the National Football League (NFL). Ask students to recall that they created and worked with linear models earlier in the unit. We are continuing our work with models and will learn another method of modeling called CART , which stands for Classification and Regression Trees . This is an umbrella term to refer to the following types of decision trees. Classification Trees: the leaves predict the values of a categorical variable Regression Trees: the leaves predict a numerical value CART Activity: to get a sense of how decision trees work, the students will see one in action. We are going to try to classify 15 professional athletes into either soccer or football players based on some of their characteristics. Note: Advanced preparation required. The cards in LMR_4.20, 4.21, and 4.22 listed above (and previewed below) need to be cut out prior to class time. LMR_4.20 LMR_4.21 LMR_4.22 Ask for 15 volunteers and hand each of them a data card from the CART Activity Player Stats handout ( LMR_4.20 ). These students will be known as the \u201cplayers\u201d. Each card lists the following variables for 15 different professional athletes: a. team location b. name c. age d. height (in inches) e. weight (in pounds) f. league The \u201cplayers\u201d will only be allowed to say \u201cyes\u201d or \u201cno\u201d in this activity. No other talking is permitted. Now, ask for 7 additional volunteers to be the nodes on the classification tree. There are three types of nodes in a decision tree: Root node - the initial question/characteristic that splits the population/dataset Internal nodes - a question/characteristic that splits the data Leaf nodes - an \"end\" where no more splitting is possible Distribute one question/classification from the CART Activity Round 1 Questions ( LMR_4.21 ) to each node. Arrange the 7 nodes in the room as depicted by the graphic below: Now, each \u201cplayer\u201d, one at a time, will approach 1 - Root Node , who will ask the \u201cplayer\u201d the question listed on his/her card. Depending on the player\u2019s answer, 1 - Root Node will direct the \u201cplayer\u201d to the next node. The \u201cplayer\u201d continues through the nodes until a leaf declares the \u201cplayer\u201d to be either (1) a soccer player on the US Men\u2019s National Team, OR (2) a football player in the National Football League (NFL). Allow all the \u201cplayers\u201d to go through the nodes until each one is classified as either a soccer or football player. After each player has been classified, record the classifications in the table below. The tally marks below correspond with classified incorrectly (red) and classified correctly (black), if all the player stats cards are used. Note: This table will be referenced again later. A filled-out version is available in the next lesson. Ask students, \"How successful were we in classifying players correctly?\" Answers will vary but if all the activity player stats cards were used then 10/15, or 67%, were classfied correctly. Students might measure success by saying that we only misclassified players 5/15, or 33%, of the time - this is called the misclassification rate (MCR) and will be defined in the next lesson. After proceeding through \u201cRound 1,\u201d ask an additional 5 students to come up as more nodes, distribute the cards from the CART Activity Round 2 Questions strips ( LMR_4.22 ), and arrange the students like the diagram below: Have each \u201cplayer\u201d go through this new set of nodes until they are re-classified by these new rules. After each player has been classified, record the classifications in the table below. The tally marks below correspond with classified incorrectly (red) and classified correctly (black), if all the player stats cards are used. Note: This table will be referenced again later. A filled-out version is available in the next lesson. Ask students, \"How successful were we in classifying players correctly in this second round?\" Answers will vary but if all the activity player stats cards were used then 12/15, or 80%, were classfied correctly. Students might measure success by saying that we only misclassified players 3/15, or 20%, of the time - this is called the misclassification rate (MCR) and will be defined in the next lesson. Once the activity has been completed, ask students the following questions: How do decision trees classify objects/people as being a member of a group? Answer: By asking a series of questions, one at a time, and sending the participant down a particular path until he/she is classified. Did we do as well, worse, or better in Round 2 compared to Round 1 at correctly guessing which sport the \u201cplayers\u201d participate in? Explain. Answers will vary according to results of the activity. If all player stats cards were used then we did better in Round 2, with 80% success, than Round 1, with 67% success, in correctly guessing which sport the \"players\" participate in. How can we figure out what questions to ask and in what order to minimize the number of incorrect classifications (also known as misclassifications )? Answers will vary. This one might not be obvious but the point is for the students to wrestle with how they might think it can be done. Class Scribes: One team of students will give a brief talk to discuss what they think the 3 most important topics of the day were. Homework Students will find a decision tree online that aligns with their interests and: a. identify the nodes - root, internal, and leaf b. describe the population that it applies to c. describe what the decision tree is predicting, i.e., what are the classifications at the leaves?","title":"Lesson 16: Football or Futbol?"},{"location":"unit4/lesson16/#lesson-16-football-or-futbol","text":"","title":"Lesson 16: Football or Futbol?"},{"location":"unit4/lesson16/#objective","text":"Students will learn what decision trees look like and how they can be used to classify people or objects into groups. They will engage in an activity to see how making slight changes to the tree can lead to drastic rises or reductions in misclassifications.","title":"Objective:"},{"location":"unit4/lesson16/#materials","text":"Decision Tree for Heart Attack Risk graphic ( LMR_4.19_CART Heart Attacks ) CART Activity Player Stats ( LMR_4.20_CART Player Stats ) CART Activity Round 1 Questions ( LMR_4.21_CART Round 1 ) CART Activity Round 2 Questions ( LMR_4.22_CART Round 2 ) Note: Advanced preparation required for LMR_4.20, 4.21, and 4.22 (see Step 8 below)","title":"Materials:"},{"location":"unit4/lesson16/#vocabulary","text":"classify decision tree Classification and Regression Trees (CART) nodes","title":"Vocabulary:"},{"location":"unit4/lesson16/#essential-concepts","text":"Essential Concepts: Some trends are not linear, so the approaches we\u2019ve done so far won\u2019t be helpful. We need to model such trends differently. Decision trees are a non-linear tool for classifying observations into groups when the trend is non-linear.","title":"Essential Concepts:"},{"location":"unit4/lesson16/#lesson","text":"Ask students the following question: If you were having chest pains, who would you trust more to diagnose you - a data scientist or a doctor? Give the students some time to think about the question and have a few of them share out their responses with the class. Note: It's likely that most students will choose to go to a doctor. As it turns out, back in the late 1970s, a cardiologist (and early data scientist) named Lee Goldman developed a decision tree based on millions of patient observations. It was made to diagnose whether people were or were not having a heart attack. The accuracy of the decision tree compared to the accuracy of actual doctor diagnoses are shown below. Correct diagnoses using the decision tree were above 95%. Correct diagnoses based on individual doctors' expertise was anywhere between 75-90%. Display the graphic from the Decision Tree for Heart Attack Risk handout ( LMR_4.19_CART Heart Attacks ) and explain that this is one example of what the decision tree that Goldman developed might have looked like. Note: This is NOT the actual tree that Goldman developed. LMR_4.19 Using a Pair-Share , ask students to discuss the following questions using the graphic above. Note: Answers will vary. These questions are meant to gauge student thinking before defining decision trees in the following steps of the lesson. What are decision trees? How do they work at classifying data into groups? Remind students that this unit has focused on linear models and making predictions. In the real world, data can be modeled in a variety of ways, many of which are non-linear, and because of this, we can\u2019t easily write down a mathematical equation to help us make predictions. However, we can use what we have learned so far to determine whether or not other models can provide a good fit to the data. Let students know that one method of modeling data in a non-linear way is with decision trees , like the one we saw with the heart attack classification. Explain that decision trees are \"grown\" by using algorithms, or rules, to test many, many different decision trees to find the one that makes the best predictions. A decision tree is basically a series of questions that are asked sequentially. Observations start by answering the first question (at the root of the tree), and then proceed along the different branches based on the answers they give to the questions that follow. At the end, based on all of the questions asked, observations are then classified as one of k classifications. Remind students that algorithms are a series of steps that are repeated a large number of times. For decision trees, this enables us to (1) explore many possible paths, beginning from the same initial point, or (2) find different starting points based on where we ended during the previous iteration. Inform students that, during today's lesson, they will be participating in an activity to try to classify professional athletes into one of two groups: (1) soccer players on the US Men's National Team, OR (2) football players in the National Football League (NFL). Ask students to recall that they created and worked with linear models earlier in the unit. We are continuing our work with models and will learn another method of modeling called CART , which stands for Classification and Regression Trees . This is an umbrella term to refer to the following types of decision trees. Classification Trees: the leaves predict the values of a categorical variable Regression Trees: the leaves predict a numerical value CART Activity: to get a sense of how decision trees work, the students will see one in action. We are going to try to classify 15 professional athletes into either soccer or football players based on some of their characteristics. Note: Advanced preparation required. The cards in LMR_4.20, 4.21, and 4.22 listed above (and previewed below) need to be cut out prior to class time. LMR_4.20 LMR_4.21 LMR_4.22 Ask for 15 volunteers and hand each of them a data card from the CART Activity Player Stats handout ( LMR_4.20 ). These students will be known as the \u201cplayers\u201d. Each card lists the following variables for 15 different professional athletes: a. team location b. name c. age d. height (in inches) e. weight (in pounds) f. league The \u201cplayers\u201d will only be allowed to say \u201cyes\u201d or \u201cno\u201d in this activity. No other talking is permitted. Now, ask for 7 additional volunteers to be the nodes on the classification tree. There are three types of nodes in a decision tree: Root node - the initial question/characteristic that splits the population/dataset Internal nodes - a question/characteristic that splits the data Leaf nodes - an \"end\" where no more splitting is possible Distribute one question/classification from the CART Activity Round 1 Questions ( LMR_4.21 ) to each node. Arrange the 7 nodes in the room as depicted by the graphic below: Now, each \u201cplayer\u201d, one at a time, will approach 1 - Root Node , who will ask the \u201cplayer\u201d the question listed on his/her card. Depending on the player\u2019s answer, 1 - Root Node will direct the \u201cplayer\u201d to the next node. The \u201cplayer\u201d continues through the nodes until a leaf declares the \u201cplayer\u201d to be either (1) a soccer player on the US Men\u2019s National Team, OR (2) a football player in the National Football League (NFL). Allow all the \u201cplayers\u201d to go through the nodes until each one is classified as either a soccer or football player. After each player has been classified, record the classifications in the table below. The tally marks below correspond with classified incorrectly (red) and classified correctly (black), if all the player stats cards are used. Note: This table will be referenced again later. A filled-out version is available in the next lesson. Ask students, \"How successful were we in classifying players correctly?\" Answers will vary but if all the activity player stats cards were used then 10/15, or 67%, were classfied correctly. Students might measure success by saying that we only misclassified players 5/15, or 33%, of the time - this is called the misclassification rate (MCR) and will be defined in the next lesson. After proceeding through \u201cRound 1,\u201d ask an additional 5 students to come up as more nodes, distribute the cards from the CART Activity Round 2 Questions strips ( LMR_4.22 ), and arrange the students like the diagram below: Have each \u201cplayer\u201d go through this new set of nodes until they are re-classified by these new rules. After each player has been classified, record the classifications in the table below. The tally marks below correspond with classified incorrectly (red) and classified correctly (black), if all the player stats cards are used. Note: This table will be referenced again later. A filled-out version is available in the next lesson. Ask students, \"How successful were we in classifying players correctly in this second round?\" Answers will vary but if all the activity player stats cards were used then 12/15, or 80%, were classfied correctly. Students might measure success by saying that we only misclassified players 3/15, or 20%, of the time - this is called the misclassification rate (MCR) and will be defined in the next lesson. Once the activity has been completed, ask students the following questions: How do decision trees classify objects/people as being a member of a group? Answer: By asking a series of questions, one at a time, and sending the participant down a particular path until he/she is classified. Did we do as well, worse, or better in Round 2 compared to Round 1 at correctly guessing which sport the \u201cplayers\u201d participate in? Explain. Answers will vary according to results of the activity. If all player stats cards were used then we did better in Round 2, with 80% success, than Round 1, with 67% success, in correctly guessing which sport the \"players\" participate in. How can we figure out what questions to ask and in what order to minimize the number of incorrect classifications (also known as misclassifications )? Answers will vary. This one might not be obvious but the point is for the students to wrestle with how they might think it can be done.","title":"Lesson:"},{"location":"unit4/lesson16/#class-scribes","text":"One team of students will give a brief talk to discuss what they think the 3 most important topics of the day were.","title":"Class Scribes:"},{"location":"unit4/lesson16/#homework","text":"Students will find a decision tree online that aligns with their interests and: a. identify the nodes - root, internal, and leaf b. describe the population that it applies to c. describe what the decision tree is predicting, i.e., what are the classifications at the leaves?","title":"Homework"},{"location":"unit4/lesson16old/","text":"Lesson 16: Exploring Trash via the Dashboard Objective: Students will continue to investigate landfills and perform analyses via the IDS public dashboard. Materials: Computers IDS public dashboard: https://portal.idsucla.org Essential Concepts: Essential Concepts: Exploring the IDS Dashboard provides a visual approach to data analysis. Lesson: Today students will continue their data exploration of the Trash campaign via the IDS public dashboard. As a team, students should select statistical questions and provide appropriate plots and summaries from the dashboard to answer those questions. Leave 10-15 minutes at the end of class to share out some of the findings from each student team. Class Scribes: One team of students will give a brief talk to discuss what they think the 3 most important topics of the day were. Homework Students will brainstorm possible RStudio commands to complement their initial analyses from the dashboard. It is up to the teacher to ask for a minimum number of commands from each student.","title":"Lesson16old"},{"location":"unit4/lesson16old/#lesson-16-exploring-trash-via-the-dashboard","text":"","title":"Lesson 16: Exploring Trash via the Dashboard"},{"location":"unit4/lesson16old/#objective","text":"Students will continue to investigate landfills and perform analyses via the IDS public dashboard.","title":"Objective:"},{"location":"unit4/lesson16old/#materials","text":"Computers IDS public dashboard: https://portal.idsucla.org","title":"Materials:"},{"location":"unit4/lesson16old/#essential-concepts","text":"Essential Concepts: Exploring the IDS Dashboard provides a visual approach to data analysis.","title":"Essential Concepts:"},{"location":"unit4/lesson16old/#lesson","text":"Today students will continue their data exploration of the Trash campaign via the IDS public dashboard. As a team, students should select statistical questions and provide appropriate plots and summaries from the dashboard to answer those questions. Leave 10-15 minutes at the end of class to share out some of the findings from each student team.","title":"Lesson:"},{"location":"unit4/lesson16old/#class-scribes","text":"One team of students will give a brief talk to discuss what they think the 3 most important topics of the day were.","title":"Class Scribes:"},{"location":"unit4/lesson16old/#homework","text":"Students will brainstorm possible RStudio commands to complement their initial analyses from the dashboard. It is up to the teacher to ask for a minimum number of commands from each student.","title":"Homework"},{"location":"unit4/lesson17/","text":"Lesson 17: Grow Your Own Decision Tree Objective: Students will create their own decision trees based on training data (i.e., the data from the previous day's lessons), and then see how well their decision tree works on new test data. Materials: Make Your Own Decision Tree handout ( LMR_4.23_Your Own Decision Tree ) Vocabulary: misclassification rate training data testing data Essential Concepts: Essential Concepts: We can determine the usefulness of decision trees by comparing the number of misclassifications in each. Lesson: Begin the lesson by asking the followign question: How did we assess whether a linear model made good predictions for a set of data? Answers will vary but so far in this unit, we have used Mean Squared Error (MSE) and Mean Absolute Error (MAE). Tell students that much like linear models, classification trees also have a method for determining how well they make predictions for a set of data. Classification trees use a misclassification rate (MCR) , which is the proportion of observations who were predicted to be in one category but were actually in another. Refer back to your tallied decision trees and correct/incorrect classification tables from the previous lesson. What were the overall proportion of incorrect classifications for Round 1? What about for Round 2? Answers will vary. If all activity player stats cards were used then the MCR for Round 1 is 5/15, or 0.33, and for Round 2 is 3/15, or 0.20. You can reference the decision trees and tables from the previous lesson so that students can see the connection between them. These proportions of incorrect classifications are our misclassification rates. Today we'll be creating our own classification tree and assess its prediction accuracy. Display the following data (the same data from the player cards used in the previous lesson): Team Player Height (inches) Weight (pounds) Age League Carolina Cam Newton 77 245 26 NFL Chicago Sean Johnson 75 217 26 USMNT Dallas Matt Cassel 76 230 33 NFL Dallas Tony Romo 74 230 35 NFL Dallas Matt Hedges 76 190 25 USMNT Kansas City Alex Smith 76 216 31 NFL Kansas City Matt Besler 72 170 28 USMNT New England Tom Brady 76 225 38 NFL New England Jermaine Jones 72 179 34 USMNT Seattle Russell Wilson 71 206 27 NFL Seattle Clint Dempsey 73 170 32 USMNT Toronto Michael Bradley 74 185 30 USMNT Toronto Jozy Altidore 73 174 26 USMNT Washington, D.C. Robert Griffin III 74 223 25 NFL Washington, D.C. Steve Birnbaum 74 181 28 USMNT Distribute the Make Your Own Decision Tree handout ( LMR_4.23_Your Own Decision Tree ) and give students time to come up with their own decision trees based on the training data they are given. Students may work in pairs or teams. They should follow the directions on page 1 of the handout and come up with a series of possible yes/no questions that they could ask to classify each player into his correct league (the NFL or the USMNT). LMR_4.23 Once the students have finished creating their classification trees, ask the following questions: Will you be able to classify other players from a new dataset correctly using this particular classification tree? Do you think this classification tree is too specific to the training data? Inform the students that they should now use the test data on page 2 of the handout to try to classify 5 mystery players into one of the two leagues. They should record the classification that their tree outputs in the data table on page 2 of the Make Your Own Decision Tree handout ( LMR_4.23_Your Own Decision Tree ). Let the students compare their decision trees and league assignments with one another. Hopefully, there will be a bit of variety in terms of the trees and the classifications. Next, show students the correct league classifications for the 5 mystery players. The mystery player names are also included in this table. Team Player Height (inches) Weight (pounds) Age League Baltimore, MD Justin Tucker 73 181 34 NFL New York Eli Manning 76 218 34 NFL New Orleans Drew Brees 72 209 36 NFL Washington, DC Perry Kitchen 72 160 23 USMNT New England Lee Nguyen 68 150 29 USMNT By a show of hands, ask: a. How many students misclassified all of the players in the testing data? b. How many misclassified 4 of the 5 players? c. How many misclassified 3 of the 5 players? d. How many misclassified 2 of the 5 players? e. Did anyone correctly classify ALL 5 mystery players? If so, ask those students to share their decision trees with the rest of the class. Inform students that, when faced with much more data, creating classification trees becomes much harder to make by hand. It is so difficult, in fact, that data scientists rely on software to grow their trees for them. Students will learn how to create decision trees in RStudio during the next lab. Note: Included below is a front-load of the calculation/interpretation of MCR in RStudio. In the next lab, you will use RStudio to create tree models that will make good predictions without needing a lot of branches. RStudio can also calculate the misclassification rate. However, you might find the visual a little confusing to interpret, so we will preview and break down one of the firsts outputs you\u2019ll see in the lab. Project the image above and explain to the students that the lab uses the titanic data, so we are looking at a total of 1000 observations (in this case, people). In Unit 2, we investigated whether those who survived paid a higher fare than those who died. This lab begins with predicting whether a person survived or not based on their gender \u2013 testing the idea that women were given preference on lifeboats. Much like the ratios we saw for our Round 1 and Round 2 classification trees, RStudio gives us ratios for each of the leaf nodes where a classification was made. The denominator tells us how many observations ended up in that node and the numerator tells us the number of misclassifications. Also notice that the questions are not shown in the classification tree, but the characteristics are visible instead. Ask students: What does the output 125/641 represent? Answer: The 641 tells us that six-hundred-forty-one people were classified as not surviving based solely on the fact that they were male. The 125 represents people who were misclassified (they actually survived), which means that 516 people were classified correctly (they indeed did not survive). How would you calculate the misclassification rate (MCR)? Answer: You would add all of the numerators which represent the misclassifications and divide by the total number of observations which you could obtain by adding all the denominators. (125+96)/(641+359)=221/1000 or 0.221. Class Scribes: One team of students will give a brief talk to discuss what they think the 3 most important topics of the day were. Homework & Next Day Students will record their responses to the following discussion questions: a. How is a decision tree/CART similar to or different than a linear model? Answers will vary but a similarity is that they both make predictions, and a difference is that decision trees can make predictions for both numerical and categorical data while linear models only make predictions for numerical data. b. Why is a decision tree considered a model? Possible answer: We consider a decision tree a model because it still represents relationships between variables. c. Describe the role that training data and test data play in creating a classification tree. Possible answer: We use training data to build a classification tree and then use test data to see how well our tree makes predictions - we shouldn't use the entirety of our data because then the model will be too specific/overfitted and will not make good predictions when presented with new data. LAB 4G: Growing Trees Complete Lab 4G prior to Lesson 18 .","title":"Lesson 17: Grow Your Own Decision Tree"},{"location":"unit4/lesson17/#lesson-17-grow-your-own-decision-tree","text":"","title":"Lesson 17: Grow Your Own Decision Tree"},{"location":"unit4/lesson17/#objective","text":"Students will create their own decision trees based on training data (i.e., the data from the previous day's lessons), and then see how well their decision tree works on new test data.","title":"Objective:"},{"location":"unit4/lesson17/#materials","text":"Make Your Own Decision Tree handout ( LMR_4.23_Your Own Decision Tree )","title":"Materials:"},{"location":"unit4/lesson17/#vocabulary","text":"misclassification rate training data testing data","title":"Vocabulary:"},{"location":"unit4/lesson17/#essential-concepts","text":"Essential Concepts: We can determine the usefulness of decision trees by comparing the number of misclassifications in each.","title":"Essential Concepts:"},{"location":"unit4/lesson17/#lesson","text":"Begin the lesson by asking the followign question: How did we assess whether a linear model made good predictions for a set of data? Answers will vary but so far in this unit, we have used Mean Squared Error (MSE) and Mean Absolute Error (MAE). Tell students that much like linear models, classification trees also have a method for determining how well they make predictions for a set of data. Classification trees use a misclassification rate (MCR) , which is the proportion of observations who were predicted to be in one category but were actually in another. Refer back to your tallied decision trees and correct/incorrect classification tables from the previous lesson. What were the overall proportion of incorrect classifications for Round 1? What about for Round 2? Answers will vary. If all activity player stats cards were used then the MCR for Round 1 is 5/15, or 0.33, and for Round 2 is 3/15, or 0.20. You can reference the decision trees and tables from the previous lesson so that students can see the connection between them. These proportions of incorrect classifications are our misclassification rates. Today we'll be creating our own classification tree and assess its prediction accuracy. Display the following data (the same data from the player cards used in the previous lesson): Team Player Height (inches) Weight (pounds) Age League Carolina Cam Newton 77 245 26 NFL Chicago Sean Johnson 75 217 26 USMNT Dallas Matt Cassel 76 230 33 NFL Dallas Tony Romo 74 230 35 NFL Dallas Matt Hedges 76 190 25 USMNT Kansas City Alex Smith 76 216 31 NFL Kansas City Matt Besler 72 170 28 USMNT New England Tom Brady 76 225 38 NFL New England Jermaine Jones 72 179 34 USMNT Seattle Russell Wilson 71 206 27 NFL Seattle Clint Dempsey 73 170 32 USMNT Toronto Michael Bradley 74 185 30 USMNT Toronto Jozy Altidore 73 174 26 USMNT Washington, D.C. Robert Griffin III 74 223 25 NFL Washington, D.C. Steve Birnbaum 74 181 28 USMNT Distribute the Make Your Own Decision Tree handout ( LMR_4.23_Your Own Decision Tree ) and give students time to come up with their own decision trees based on the training data they are given. Students may work in pairs or teams. They should follow the directions on page 1 of the handout and come up with a series of possible yes/no questions that they could ask to classify each player into his correct league (the NFL or the USMNT). LMR_4.23 Once the students have finished creating their classification trees, ask the following questions: Will you be able to classify other players from a new dataset correctly using this particular classification tree? Do you think this classification tree is too specific to the training data? Inform the students that they should now use the test data on page 2 of the handout to try to classify 5 mystery players into one of the two leagues. They should record the classification that their tree outputs in the data table on page 2 of the Make Your Own Decision Tree handout ( LMR_4.23_Your Own Decision Tree ). Let the students compare their decision trees and league assignments with one another. Hopefully, there will be a bit of variety in terms of the trees and the classifications. Next, show students the correct league classifications for the 5 mystery players. The mystery player names are also included in this table. Team Player Height (inches) Weight (pounds) Age League Baltimore, MD Justin Tucker 73 181 34 NFL New York Eli Manning 76 218 34 NFL New Orleans Drew Brees 72 209 36 NFL Washington, DC Perry Kitchen 72 160 23 USMNT New England Lee Nguyen 68 150 29 USMNT By a show of hands, ask: a. How many students misclassified all of the players in the testing data? b. How many misclassified 4 of the 5 players? c. How many misclassified 3 of the 5 players? d. How many misclassified 2 of the 5 players? e. Did anyone correctly classify ALL 5 mystery players? If so, ask those students to share their decision trees with the rest of the class. Inform students that, when faced with much more data, creating classification trees becomes much harder to make by hand. It is so difficult, in fact, that data scientists rely on software to grow their trees for them. Students will learn how to create decision trees in RStudio during the next lab. Note: Included below is a front-load of the calculation/interpretation of MCR in RStudio. In the next lab, you will use RStudio to create tree models that will make good predictions without needing a lot of branches. RStudio can also calculate the misclassification rate. However, you might find the visual a little confusing to interpret, so we will preview and break down one of the firsts outputs you\u2019ll see in the lab. Project the image above and explain to the students that the lab uses the titanic data, so we are looking at a total of 1000 observations (in this case, people). In Unit 2, we investigated whether those who survived paid a higher fare than those who died. This lab begins with predicting whether a person survived or not based on their gender \u2013 testing the idea that women were given preference on lifeboats. Much like the ratios we saw for our Round 1 and Round 2 classification trees, RStudio gives us ratios for each of the leaf nodes where a classification was made. The denominator tells us how many observations ended up in that node and the numerator tells us the number of misclassifications. Also notice that the questions are not shown in the classification tree, but the characteristics are visible instead. Ask students: What does the output 125/641 represent? Answer: The 641 tells us that six-hundred-forty-one people were classified as not surviving based solely on the fact that they were male. The 125 represents people who were misclassified (they actually survived), which means that 516 people were classified correctly (they indeed did not survive). How would you calculate the misclassification rate (MCR)? Answer: You would add all of the numerators which represent the misclassifications and divide by the total number of observations which you could obtain by adding all the denominators. (125+96)/(641+359)=221/1000 or 0.221.","title":"Lesson:"},{"location":"unit4/lesson17/#class-scribes","text":"One team of students will give a brief talk to discuss what they think the 3 most important topics of the day were.","title":"Class Scribes:"},{"location":"unit4/lesson17/#homework-next-day","text":"Students will record their responses to the following discussion questions: a. How is a decision tree/CART similar to or different than a linear model? Answers will vary but a similarity is that they both make predictions, and a difference is that decision trees can make predictions for both numerical and categorical data while linear models only make predictions for numerical data. b. Why is a decision tree considered a model? Possible answer: We consider a decision tree a model because it still represents relationships between variables. c. Describe the role that training data and test data play in creating a classification tree. Possible answer: We use training data to build a classification tree and then use test data to see how well our tree makes predictions - we shouldn't use the entirety of our data because then the model will be too specific/overfitted and will not make good predictions when presented with new data. LAB 4G: Growing Trees Complete Lab 4G prior to Lesson 18 .","title":"Homework &amp; Next Day"},{"location":"unit4/lesson17old/","text":"Lesson 17: Exploring Trash via RStudio Objective: Students will continue to investigate landfills and perform analyses via RStudio. Materials: Computers RStudio Essential Concepts: Essential Concepts: RStudio can be used to verify initial results/findings from data analysis done via the IDS Dashboard. Lesson: Today students will continue their data analysis of the Trash campaign via RStudio. They should share their answers from the previous lesson\u2019s homework assignment in their teams to help them get started with their code. After having shared their initial code, they should spend some time discussing other ideas The Recorder/Reporter should keep a list of the code that the team has agreed to use. By the end of class, students should begin writing their recommendations for reducing the burden on landfills. Inform students that each team will prepare their presentations during the next class period. Class Scribes: One team of students will give a brief talk to discuss what they think the 3 most important topics of the day were. Next 2 Days Students will finalize their recommendations for reducing the burden on landfills and have a draft of their letter to send to LACSD and prepare for their team presentations.","title":"Lesson17old"},{"location":"unit4/lesson17old/#lesson-17-exploring-trash-via-rstudio","text":"","title":"Lesson 17: Exploring Trash via RStudio"},{"location":"unit4/lesson17old/#objective","text":"Students will continue to investigate landfills and perform analyses via RStudio.","title":"Objective:"},{"location":"unit4/lesson17old/#materials","text":"Computers RStudio","title":"Materials:"},{"location":"unit4/lesson17old/#essential-concepts","text":"Essential Concepts: RStudio can be used to verify initial results/findings from data analysis done via the IDS Dashboard.","title":"Essential Concepts:"},{"location":"unit4/lesson17old/#lesson","text":"Today students will continue their data analysis of the Trash campaign via RStudio. They should share their answers from the previous lesson\u2019s homework assignment in their teams to help them get started with their code. After having shared their initial code, they should spend some time discussing other ideas The Recorder/Reporter should keep a list of the code that the team has agreed to use. By the end of class, students should begin writing their recommendations for reducing the burden on landfills. Inform students that each team will prepare their presentations during the next class period.","title":"Lesson:"},{"location":"unit4/lesson17old/#class-scribes","text":"One team of students will give a brief talk to discuss what they think the 3 most important topics of the day were.","title":"Class Scribes:"},{"location":"unit4/lesson17old/#next-2-days","text":"Students will finalize their recommendations for reducing the burden on landfills and have a draft of their letter to send to LACSD and prepare for their team presentations.","title":"Next 2 Days"},{"location":"unit4/lesson18/","text":"Lesson 18: Where Do I Belong? Objective: Students will learn what clustering is and how to classify groups of people into clusters based on unknown similarities. Materials: Find the Clusters handout ( LMR_4.24_Find the Clusters ) Vocabulary: clustering cluster k-means Essential Concepts: Essential Concepts: We can identify groups, or \u201cclusters,\u201d in data based on a few characteristics. For example, it is easy to classify a group of people into football players and swimmers, but what if you only knew each person\u2019s arm span? How well could you classify them into football players and swimmers now? Lesson: Inform the students that they will continue to explore different types of models, and today they will be focusing on clustering . Clustering is the process of grouping a set of objects (or people) together in such a way that people in the same group (called a cluster ) are more similar to each other than to those in other groups. Have the students recall that, in the previous lessons, they used decision trees/CART to classify people into different groups based on whether or not a person had a specific characteristic (e.g., whether or not a professional athlete\u2019s team is based in the US). But, sometimes we don\u2019t know what these specific characteristics are. We are simply given numerical variables and asked to find similarities. This is where clustering comes in \u2013 similar people will congregate towards each other, and we want to see if we can identify their groupings. We will look at a very basic example first. Suppose the following 6 observations are given: Obs X 1 X 2 1 160 74 2 165 72 3 165 74 4 175 68 5 180 70 6 185 72 Plot the X 1 and X 2 points on a scatterplot either on the board or on poster paper (X 1 can be on the horizontal axis and X 2 can be on the vertical axis). The graph should look like the one below: Ask students if they think there are any clusters, or groups, that stand out to them. It is likely that they will say there are 2 clusters in the graph: the top left corner 3 points, and the bottom right 3 points. Now pose the following scenario that further describes the data: A doctor provides yearly physicals to the football and swimming teams at a local high school. The doctor has collected data over the past few years on each player\u2019s weight (in pounds) and height (in inches). She informs us that weight was coded as the variable X 1 , and height was coded as the variable X 2 . You can re-label the scatterplot with this new information. Unfortunately, the doctor never recorded what sport each person played. Using the information about height and weight, ask the students to decide: Which group of points most likely represents players from the swimming team? Answer: The points in the upper left corner are probably swimmers because swimmers are usually tall (and have large arm spans) and thin. Which group of points most likely represents players from the football team? Answer: The points in the bottom right corner are probably football players because they tend to be heavier and more muscular. Now suppose a new player comes into the doctor\u2019s office for a physical. His weight and height are recorded as 166 pounds and 73 inches, respectively, but the doctor forgets to ask what sport he plays. Plot this point on the graph and ask students to determine which sport they think this student plays. Answer: This student is most likely a swimmer because he is tall and thin, and his point is near the swimming cluster. That was an easy one! But what if a player comes in and has the following measurements: weight = 173 pounds, height = 73 inches? Distribute the Find the Clusters handout ( LMR_4.24 ) and tell the students that the new point has been added to the \u201cRound 0\u201d graph. LMR_4.24 Ask students: On which team do you think this person plays? Answer: It is much more difficult to tell now because it looks like it is right in between the two clusters. In order to determine group placement, we can use a process called k-means clustering . With this method, we select k clusters that we want to identify. Since we know we only have 2 types of athletes, football players and swimmers, we will be finding k = 2 clusters. To introduce the students to this idea, circle the 3 points in the upper left corner (the ones that are likely the swimmers) and have students find the \u201cmean point\u201d. This means that they should find the mean x-value and the mean y-value of the 3 points. They can then plot this new point and use it as the mean of this particular group, or cluster. The goal of this algorithm is to keep recalculating means as the clusters change. To begin, we will pick 2 points, A and B, to represent the center of each cluster. We will be referring to this as the initialization step. If you would like to use the point found in Step 14 and label it as \"A\", that is completely fine. You can simply pick just one other random point near the other cluster and label it as \"B\". Initialization: For now, we will start with the following two points as our initial cluster centers of each group: A: (170, 70) and B: (175, 74). In the \u201cRound 0\u201d plot on the Find the Clusters handout, each student should plot and label these two points. Assignment Step: Inform the students that they will be determining the distance between the 7 observations and point A and point B. Then, they will decide if the point is closer to cluster center A or cluster center B and label the point with that letter. Lines have been drawn from the top left point to the cluster centers in the plot below as a guide. You can draw this on the board as a reference for the students as well. Since the line to point A is smaller, we would classify that point as being in cluster A (as shown below). The students should draw similar lines for every point on the graph, or they can simply eyeball it, to make a decision as to which cluster each belongs in. Even if they guess incorrectly, the algorithm should be able to find the correct groups after some time. The correct classifications for Round 0 are as follows, using our points from step 16: Update Step: Once the class has agreed on the Round 0's cluster classifications, they should compute new values for points A and B by using the clustered point means. For point A, they simply need to find the mean x-value for the 4 points and the mean y-value for the 4 points. Repeat this process to find the new point B \u2013 these will be our new cluster centers as we move onto Round 1. Calculating means to derive cluster centers, like points A and B, for grouping data points is part of the k-means algorithm. The new points for A and B have been calculated below. The students should be calculating these on their own and recording their new cluster centers on the handout. x-value for A = (160 + 165 + 165 + 175)/4 = 166.25 y-value for A = (74 + 72 + 74 + 68)/4 = 72 x-value for B = (173 + 180 + 185)/3 = 179.3 y-value for B = (73 + 70 + 72)/3 = 71.67 new A = (166.25, 72) new B = (179.3, 71.67) Have the students continue working through the handout until the cluster membership remains the same between 2 consecutive rounds. This means that, from one iteration to the next, the points in each cluster do not change. Where you choose your initial points matters in determining which points end up in which clusters. Demonstrate this to the class using the K-means Clustering App, located on the Applications page on Portal under Explore ( https://portal.idsucla.org/#curriculum/applications/ ). In the app, \"centroids\" is the academic term for cluster centers. For this example, we will use 3 centroids and choose a \"Clustered Initialization\" with 100 points and 3 Clusters. See image below for how to adjust the settings. Move two of the centroids so that they are close to/within one cluster. Move the remaining centroid in between the other two clusters. An example is shown below. Click \"Next Step\" until no points change from the previous update. See image below of end of clustering for the example from (b). While we still have three clusters, we can clearly see that our initial points have influenced the end result of those clusters. You can click \u201cRestart and play again\u201d to move the centroids to the center of the clusters and click \u201cNext step\u201d until no points change from the previous update to illustrate this point (see image below of correctly clustered groups). Note: Clicking \"Restart and play again\" allows you to move your cluster centers to new positions while still using the same 100 points. You can repeat the same process as above to create different additional groupings. Class Scribes: One team of students will give a brief talk to discuss what they think the 3 most important topics of the day were. Homework & Next Day Write a paragraph that describes k-means clustering in your own words. LAB 4H: Finding Clusters Complete Lab 4H prior to Lesson 21 .","title":"Lesson 18: Where Do I Belong?"},{"location":"unit4/lesson18/#lesson-18-where-do-i-belong","text":"","title":"Lesson 18: Where Do I Belong?"},{"location":"unit4/lesson18/#objective","text":"Students will learn what clustering is and how to classify groups of people into clusters based on unknown similarities.","title":"Objective:"},{"location":"unit4/lesson18/#materials","text":"Find the Clusters handout ( LMR_4.24_Find the Clusters )","title":"Materials:"},{"location":"unit4/lesson18/#vocabulary","text":"clustering cluster k-means","title":"Vocabulary:"},{"location":"unit4/lesson18/#essential-concepts","text":"Essential Concepts: We can identify groups, or \u201cclusters,\u201d in data based on a few characteristics. For example, it is easy to classify a group of people into football players and swimmers, but what if you only knew each person\u2019s arm span? How well could you classify them into football players and swimmers now?","title":"Essential Concepts:"},{"location":"unit4/lesson18/#lesson","text":"Inform the students that they will continue to explore different types of models, and today they will be focusing on clustering . Clustering is the process of grouping a set of objects (or people) together in such a way that people in the same group (called a cluster ) are more similar to each other than to those in other groups. Have the students recall that, in the previous lessons, they used decision trees/CART to classify people into different groups based on whether or not a person had a specific characteristic (e.g., whether or not a professional athlete\u2019s team is based in the US). But, sometimes we don\u2019t know what these specific characteristics are. We are simply given numerical variables and asked to find similarities. This is where clustering comes in \u2013 similar people will congregate towards each other, and we want to see if we can identify their groupings. We will look at a very basic example first. Suppose the following 6 observations are given: Obs X 1 X 2 1 160 74 2 165 72 3 165 74 4 175 68 5 180 70 6 185 72 Plot the X 1 and X 2 points on a scatterplot either on the board or on poster paper (X 1 can be on the horizontal axis and X 2 can be on the vertical axis). The graph should look like the one below: Ask students if they think there are any clusters, or groups, that stand out to them. It is likely that they will say there are 2 clusters in the graph: the top left corner 3 points, and the bottom right 3 points. Now pose the following scenario that further describes the data: A doctor provides yearly physicals to the football and swimming teams at a local high school. The doctor has collected data over the past few years on each player\u2019s weight (in pounds) and height (in inches). She informs us that weight was coded as the variable X 1 , and height was coded as the variable X 2 . You can re-label the scatterplot with this new information. Unfortunately, the doctor never recorded what sport each person played. Using the information about height and weight, ask the students to decide: Which group of points most likely represents players from the swimming team? Answer: The points in the upper left corner are probably swimmers because swimmers are usually tall (and have large arm spans) and thin. Which group of points most likely represents players from the football team? Answer: The points in the bottom right corner are probably football players because they tend to be heavier and more muscular. Now suppose a new player comes into the doctor\u2019s office for a physical. His weight and height are recorded as 166 pounds and 73 inches, respectively, but the doctor forgets to ask what sport he plays. Plot this point on the graph and ask students to determine which sport they think this student plays. Answer: This student is most likely a swimmer because he is tall and thin, and his point is near the swimming cluster. That was an easy one! But what if a player comes in and has the following measurements: weight = 173 pounds, height = 73 inches? Distribute the Find the Clusters handout ( LMR_4.24 ) and tell the students that the new point has been added to the \u201cRound 0\u201d graph. LMR_4.24 Ask students: On which team do you think this person plays? Answer: It is much more difficult to tell now because it looks like it is right in between the two clusters. In order to determine group placement, we can use a process called k-means clustering . With this method, we select k clusters that we want to identify. Since we know we only have 2 types of athletes, football players and swimmers, we will be finding k = 2 clusters. To introduce the students to this idea, circle the 3 points in the upper left corner (the ones that are likely the swimmers) and have students find the \u201cmean point\u201d. This means that they should find the mean x-value and the mean y-value of the 3 points. They can then plot this new point and use it as the mean of this particular group, or cluster. The goal of this algorithm is to keep recalculating means as the clusters change. To begin, we will pick 2 points, A and B, to represent the center of each cluster. We will be referring to this as the initialization step. If you would like to use the point found in Step 14 and label it as \"A\", that is completely fine. You can simply pick just one other random point near the other cluster and label it as \"B\". Initialization: For now, we will start with the following two points as our initial cluster centers of each group: A: (170, 70) and B: (175, 74). In the \u201cRound 0\u201d plot on the Find the Clusters handout, each student should plot and label these two points. Assignment Step: Inform the students that they will be determining the distance between the 7 observations and point A and point B. Then, they will decide if the point is closer to cluster center A or cluster center B and label the point with that letter. Lines have been drawn from the top left point to the cluster centers in the plot below as a guide. You can draw this on the board as a reference for the students as well. Since the line to point A is smaller, we would classify that point as being in cluster A (as shown below). The students should draw similar lines for every point on the graph, or they can simply eyeball it, to make a decision as to which cluster each belongs in. Even if they guess incorrectly, the algorithm should be able to find the correct groups after some time. The correct classifications for Round 0 are as follows, using our points from step 16: Update Step: Once the class has agreed on the Round 0's cluster classifications, they should compute new values for points A and B by using the clustered point means. For point A, they simply need to find the mean x-value for the 4 points and the mean y-value for the 4 points. Repeat this process to find the new point B \u2013 these will be our new cluster centers as we move onto Round 1. Calculating means to derive cluster centers, like points A and B, for grouping data points is part of the k-means algorithm. The new points for A and B have been calculated below. The students should be calculating these on their own and recording their new cluster centers on the handout. x-value for A = (160 + 165 + 165 + 175)/4 = 166.25 y-value for A = (74 + 72 + 74 + 68)/4 = 72 x-value for B = (173 + 180 + 185)/3 = 179.3 y-value for B = (73 + 70 + 72)/3 = 71.67 new A = (166.25, 72) new B = (179.3, 71.67) Have the students continue working through the handout until the cluster membership remains the same between 2 consecutive rounds. This means that, from one iteration to the next, the points in each cluster do not change. Where you choose your initial points matters in determining which points end up in which clusters. Demonstrate this to the class using the K-means Clustering App, located on the Applications page on Portal under Explore ( https://portal.idsucla.org/#curriculum/applications/ ). In the app, \"centroids\" is the academic term for cluster centers. For this example, we will use 3 centroids and choose a \"Clustered Initialization\" with 100 points and 3 Clusters. See image below for how to adjust the settings. Move two of the centroids so that they are close to/within one cluster. Move the remaining centroid in between the other two clusters. An example is shown below. Click \"Next Step\" until no points change from the previous update. See image below of end of clustering for the example from (b). While we still have three clusters, we can clearly see that our initial points have influenced the end result of those clusters. You can click \u201cRestart and play again\u201d to move the centroids to the center of the clusters and click \u201cNext step\u201d until no points change from the previous update to illustrate this point (see image below of correctly clustered groups). Note: Clicking \"Restart and play again\" allows you to move your cluster centers to new positions while still using the same 100 points. You can repeat the same process as above to create different additional groupings.","title":"Lesson:"},{"location":"unit4/lesson18/#class-scribes","text":"One team of students will give a brief talk to discuss what they think the 3 most important topics of the day were.","title":"Class Scribes:"},{"location":"unit4/lesson18/#homework-next-day","text":"Write a paragraph that describes k-means clustering in your own words. LAB 4H: Finding Clusters Complete Lab 4H prior to Lesson 21 .","title":"Homework &amp; Next Day"},{"location":"unit4/lesson19/","text":"Lesson 19: Our Class Network Objective: Students will participate in an activity to map out their own network based on acquaintances between two people. Materials: Friend Network Graphic ( LMR_4.25_Friend Network Graphic ) Index cards Network Code file ( LMR_4.26_Network Code R Script ) Vocabulary: network Essential Concepts: Essential Concepts: Networks are made when observations are interconnected. In a social setting, we can examine how different people are connected by finding relationships between other people in a network. Lesson: Display the Friend Network Graphic ( LMR_4.25 ), which shows a WolframAlpha visualization of someone\u2019s Facebook friends. Inform the students that this type of model is called a network , which is simply a group of people or things that are interconnected in some way. LMR_4.25 Ask the following questions about the graphic: What does each dot represent? Answer: Each dot represents one person. What does each line represent? Answer: Each line represents a friendship between two people. How are all the people in this graphic connected to each other? Answer: They are all friends with the person whose Facebook this is. Why are some areas denser than others? Answer: A lot of people in the darker spots know each other, so there are more connections/friendships. Why are some people not in groups at all (the dots at the edges of the graphic)? Answer: The main person does not have any friends in common with this person. What might some of the groupings (the denser spots) represent? Answers will vary. Some examples include high school friends, college friends, graduate school friends, family members, or people who participate in similar hobbies. Ask the students what other types of social networks, other than Facebook, they belong to? Responses will most likely include TikTok, Twitter, Instagram, Snapchat, LinkedIn, Google+, etc. Next, inform the students that networks can be as big or as small as we want. We can even determine our own class\u2019s social network and create visualizations from it! Network Activity: Distribute index cards to students. Each student will need enough cards to make a connection with every other person in the class. For example, if there are 20 students in a class, then each student needs 19 cards. On EVERY index card, the student should write his/her first AND last name in the lower left-hand corner (see image below). Next, each student will walk around the classroom and put another student\u2019s first AND last name in the top right-hand corner of an index card (see image below). In the center of the index card, the students should write the name of the closest 3 rd person that they BOTH know (see image below). The person can be someone in the class, someone outside of the class, or someone who doesn\u2019t even attend the same school. Once all of the students have completed their cards, they will turn them in to the teacher so the teacher can create a visualization of the network. This will probably take an entire class period to complete, which is fine because the graphics can be created and shown the next day. At this point, the teacher will need to manually input the data from the index cards into a spreadsheet. It is recommended that the spreadsheet be saved as a .csv file. Two sample index cards are included, along with how you would input the data. Note: The first index card corresponds to rows 1 and 2 in the spreadsheet (the purple box). The second index card corresponds to rows 3 and 4 in the spreadsheet (the pink box). So, each card will take up two rows in the spreadsheet. Note: It is probably best to input the data after class and present the visualization during the next day. Once all data has been input into a spreadsheet, use the code provided in the Network Code file ( LMR_4.26 ) to produce graphs for the class\u2019s social network. Note: The R Script file can be opened and viewed in the \u201csource\u201d pane of RStudio. There are 2 places where the code needs to be edited by the teacher: Be sure to change the file name when reading in the .csv file in Line 7 of the code. Read the comments in Lines 91-96 to help find the 5 most popular people in the class\u2019s network. This may require some edits to Lines 97 and 108. Class Scribes: One team of students will give a brief talk to discuss what they think the 3 most important topics of the day were. Next Day Students will end their Team Participatory Sensing campaign data collection after today\u2019s lesson. Starting the next day, they will analyze their data as part of the End of Unit 4 project .","title":"Lesson 19: Our Class Network"},{"location":"unit4/lesson19/#lesson-19-our-class-network","text":"","title":"Lesson 19: Our Class Network"},{"location":"unit4/lesson19/#objective","text":"Students will participate in an activity to map out their own network based on acquaintances between two people.","title":"Objective:"},{"location":"unit4/lesson19/#materials","text":"Friend Network Graphic ( LMR_4.25_Friend Network Graphic ) Index cards Network Code file ( LMR_4.26_Network Code R Script )","title":"Materials:"},{"location":"unit4/lesson19/#vocabulary","text":"network","title":"Vocabulary:"},{"location":"unit4/lesson19/#essential-concepts","text":"Essential Concepts: Networks are made when observations are interconnected. In a social setting, we can examine how different people are connected by finding relationships between other people in a network.","title":"Essential Concepts:"},{"location":"unit4/lesson19/#lesson","text":"Display the Friend Network Graphic ( LMR_4.25 ), which shows a WolframAlpha visualization of someone\u2019s Facebook friends. Inform the students that this type of model is called a network , which is simply a group of people or things that are interconnected in some way. LMR_4.25 Ask the following questions about the graphic: What does each dot represent? Answer: Each dot represents one person. What does each line represent? Answer: Each line represents a friendship between two people. How are all the people in this graphic connected to each other? Answer: They are all friends with the person whose Facebook this is. Why are some areas denser than others? Answer: A lot of people in the darker spots know each other, so there are more connections/friendships. Why are some people not in groups at all (the dots at the edges of the graphic)? Answer: The main person does not have any friends in common with this person. What might some of the groupings (the denser spots) represent? Answers will vary. Some examples include high school friends, college friends, graduate school friends, family members, or people who participate in similar hobbies. Ask the students what other types of social networks, other than Facebook, they belong to? Responses will most likely include TikTok, Twitter, Instagram, Snapchat, LinkedIn, Google+, etc. Next, inform the students that networks can be as big or as small as we want. We can even determine our own class\u2019s social network and create visualizations from it! Network Activity: Distribute index cards to students. Each student will need enough cards to make a connection with every other person in the class. For example, if there are 20 students in a class, then each student needs 19 cards. On EVERY index card, the student should write his/her first AND last name in the lower left-hand corner (see image below). Next, each student will walk around the classroom and put another student\u2019s first AND last name in the top right-hand corner of an index card (see image below). In the center of the index card, the students should write the name of the closest 3 rd person that they BOTH know (see image below). The person can be someone in the class, someone outside of the class, or someone who doesn\u2019t even attend the same school. Once all of the students have completed their cards, they will turn them in to the teacher so the teacher can create a visualization of the network. This will probably take an entire class period to complete, which is fine because the graphics can be created and shown the next day. At this point, the teacher will need to manually input the data from the index cards into a spreadsheet. It is recommended that the spreadsheet be saved as a .csv file. Two sample index cards are included, along with how you would input the data. Note: The first index card corresponds to rows 1 and 2 in the spreadsheet (the purple box). The second index card corresponds to rows 3 and 4 in the spreadsheet (the pink box). So, each card will take up two rows in the spreadsheet. Note: It is probably best to input the data after class and present the visualization during the next day. Once all data has been input into a spreadsheet, use the code provided in the Network Code file ( LMR_4.26 ) to produce graphs for the class\u2019s social network. Note: The R Script file can be opened and viewed in the \u201csource\u201d pane of RStudio. There are 2 places where the code needs to be edited by the teacher: Be sure to change the file name when reading in the .csv file in Line 7 of the code. Read the comments in Lines 91-96 to help find the 5 most popular people in the class\u2019s network. This may require some edits to Lines 97 and 108.","title":"Lesson:"},{"location":"unit4/lesson19/#class-scribes","text":"One team of students will give a brief talk to discuss what they think the 3 most important topics of the day were.","title":"Class Scribes:"},{"location":"unit4/lesson19/#next-day","text":"Students will end their Team Participatory Sensing campaign data collection after today\u2019s lesson. Starting the next day, they will analyze their data as part of the End of Unit 4 project .","title":"Next Day"},{"location":"unit4/lesson2/","text":"Lesson 2: Drought Objective: Students will learn about the cause of California droughts and its effect on other states. Materials: Article: California, 'America's garden', is drying out https://yaleclimateconnections.org/2021/06/california-americas-garden-is-drying-out/ U.S. drought data interactive map https://www.drought.gov/historical-information?dataset=0&selectedDateUSDM=20120710 Team Campaign Creation handout ( LMR_4.4_Team Campaign Creation ) Essential Concepts: Essential Concepts: Data can be used to make predictions. Official datasets rely on censuses or random samples and can be used to make generalizations. Lesson: Begin the lesson with the quote: \u201cThe consequences of drought in California are felt well outside the state\u2019s borders. California is effectively America\u2019s garden \u2013 it produces two-thirds of all fruits and nuts grown in the U.S.\u201d Using the K-L-W strategy in their DS journals, give students a couple of minutes to write what they Know about droughts. Then, students will write what they Learned about the California drought as they read the article titled California, \u2018America\u2019s garden\u2019, is drying up. The article is found at: https://yaleclimateconnections.org/2021/06/california-americas-garden-is-drying-out/ Finally, they will write 2-3 questions about what they Want to know/learn about droughts. Do a quick Whip Around to share some of the students\u2019 responses to the K-L-W. Next, load an interactive map of U.S. drought data by visiting: https://www.drought.gov/historical-information?dataset=0&selectedDateUSDM=20120710 Lead a discussion about what is on the page. Ask: What do the colors and percentages on the legend mean? Answer: The color is the drought type (Abnormally Dry, Moderate Drought, etc.) and the percentage is the percentage of area of the U.S. that is that type of drought. What do the colors, percentages, and years on the graph mean? Answer: The colors correspond to the drought type (as mentioned in the previous question), the percentages correspond to the percentage of area of the U.S. that is that type of drought (as mentioned in the previous question), and the years signify the dates for which the data is available. What information are the map and graph displaying? Answer: The map tells us the areas of drought and the graph tells us percentages of drought over time. Which date is selected by default and what percent of the U.S. is in Exceptional Drought that day?? Answer: July 10, 2012 and 0.62% is in Exceptional Drought. NOTE : If you used a different link other than was listed here, you may have a different default date. In order to see the types of droughts and their percentages, hover your cursor over the graph to line up with the date. Then click on a new date to update the map. Ask: What date is displayed now? Answers will vary. What information is the map displaying? Answers will depend on the chosen date above. Which states are affected by drought? Answers will depend on the chosen date above. NOTE : You can click on a state to display its name. Then click on the Combine States option, choose a state, and click Combine. Ask: What information is the map displaying now? Answers will vary but students should see the chosen state as the new map. What else do you see? Answers will depend on the chosen state above. What are some wonderings you have about the data? Answers will vary. NOTE : There are multiple options here. You can choose to display multiple states by adding another state (or states) and choosing Combine. You can also designate a specific time period in the Time Series Options. Now that they know the overall layout of the interactive map and its options, ask student teams to complete the Team Campaign Creation handout ( LMR_4.4_Team Campaign Creation ) NOTE : The interactive map has a download data feature that allows customization/ filtering of US drought data. Keep this in mind as an option for students who may be interested in this topic. Class Scribes: One team of students will give a brief talk to discuss what they think the 3 most important topics of the day were.","title":"Lesson 2: Drought"},{"location":"unit4/lesson2/#lesson-2-drought","text":"","title":"Lesson 2: Drought"},{"location":"unit4/lesson2/#objective","text":"Students will learn about the cause of California droughts and its effect on other states.","title":"Objective:"},{"location":"unit4/lesson2/#materials","text":"Article: California, 'America's garden', is drying out https://yaleclimateconnections.org/2021/06/california-americas-garden-is-drying-out/ U.S. drought data interactive map https://www.drought.gov/historical-information?dataset=0&selectedDateUSDM=20120710 Team Campaign Creation handout ( LMR_4.4_Team Campaign Creation )","title":"Materials:"},{"location":"unit4/lesson2/#essential-concepts","text":"Essential Concepts: Data can be used to make predictions. Official datasets rely on censuses or random samples and can be used to make generalizations.","title":"Essential Concepts:"},{"location":"unit4/lesson2/#lesson","text":"Begin the lesson with the quote: \u201cThe consequences of drought in California are felt well outside the state\u2019s borders. California is effectively America\u2019s garden \u2013 it produces two-thirds of all fruits and nuts grown in the U.S.\u201d Using the K-L-W strategy in their DS journals, give students a couple of minutes to write what they Know about droughts. Then, students will write what they Learned about the California drought as they read the article titled California, \u2018America\u2019s garden\u2019, is drying up. The article is found at: https://yaleclimateconnections.org/2021/06/california-americas-garden-is-drying-out/ Finally, they will write 2-3 questions about what they Want to know/learn about droughts. Do a quick Whip Around to share some of the students\u2019 responses to the K-L-W. Next, load an interactive map of U.S. drought data by visiting: https://www.drought.gov/historical-information?dataset=0&selectedDateUSDM=20120710 Lead a discussion about what is on the page. Ask: What do the colors and percentages on the legend mean? Answer: The color is the drought type (Abnormally Dry, Moderate Drought, etc.) and the percentage is the percentage of area of the U.S. that is that type of drought. What do the colors, percentages, and years on the graph mean? Answer: The colors correspond to the drought type (as mentioned in the previous question), the percentages correspond to the percentage of area of the U.S. that is that type of drought (as mentioned in the previous question), and the years signify the dates for which the data is available. What information are the map and graph displaying? Answer: The map tells us the areas of drought and the graph tells us percentages of drought over time. Which date is selected by default and what percent of the U.S. is in Exceptional Drought that day?? Answer: July 10, 2012 and 0.62% is in Exceptional Drought. NOTE : If you used a different link other than was listed here, you may have a different default date. In order to see the types of droughts and their percentages, hover your cursor over the graph to line up with the date. Then click on a new date to update the map. Ask: What date is displayed now? Answers will vary. What information is the map displaying? Answers will depend on the chosen date above. Which states are affected by drought? Answers will depend on the chosen date above. NOTE : You can click on a state to display its name. Then click on the Combine States option, choose a state, and click Combine. Ask: What information is the map displaying now? Answers will vary but students should see the chosen state as the new map. What else do you see? Answers will depend on the chosen state above. What are some wonderings you have about the data? Answers will vary. NOTE : There are multiple options here. You can choose to display multiple states by adding another state (or states) and choosing Combine. You can also designate a specific time period in the Time Series Options. Now that they know the overall layout of the interactive map and its options, ask student teams to complete the Team Campaign Creation handout ( LMR_4.4_Team Campaign Creation ) NOTE : The interactive map has a download data feature that allows customization/ filtering of US drought data. Keep this in mind as an option for students who may be interested in this topic.","title":"Lesson:"},{"location":"unit4/lesson2/#class-scribes","text":"One team of students will give a brief talk to discuss what they think the 3 most important topics of the day were.","title":"Class Scribes:"},{"location":"unit4/lesson3/","text":"Lesson 3: Community Connection Objective: Students will research and review articles from accredited sources to guide the design of their community focused Participatory Sensing campaign. Materials: Resource: Web Literacy for Student Fact-Checkers https://pressbooks.pub/webliteracy/ Video: Online Verification Skills - Investigate the Source found at: https://www.youtube.com/watch_popup?v=hB6qjIxKltA https://www.youtube.com/watch_popup?v=hB6qjIxKltA Video: Online Verification Skills - Verifying Images and Videos found at: https://www.youtube.com/watch_popup?v=7eKG9RuqUE4 https://www.youtube.com/watch_popup?v=7eKG9RuqUE4 Video: Online Verification Skills - Find the Original Source found at: https://www.youtube.com/watch_popup?v=tRZ-N3OvvUs https://www.youtube.com/watch_popup?v=tRZ-N3OvvUs Team Campaign Creation handout ( LMR_4.4_Team Campaign Creation ) NOTE : This handout was used in Lesson 2 but a new copy should be used for this lesson. Article: Great Pacific Garbage Patch: The World's Biggest Landfill in the Pacific Ocean https://science.howstuffworks.com/environmental/earth/oceanography/great-pacific-garbage-patch.htm Data file: US Landfills https://www.epa.gov/lmop/landfill-technical-data Article: 3 reasons why California's drought isn't really over, despite the rain https://www.npr.org/2023/03/23/1165378214/3-reasons-why-californias-drought-isnt-really-over-despite-all-the-rain/ Data file: US Drought https://www.drought.gov/historical-information?dataset=0&selectedDateUSDM=20120710 Article: The number of homeless people in America grew in 2023 as high cost of living took a toll https://www.usatoday.com/story/news/nation/2023/12/15/homelessness-in-america-grew-2023/71926354007/ Data file: Annual Homeless Assessment Report (AHAR) : https://www.huduser.gov/portal/datasets/ahar.html Article: COVID-19 pandemeic triggers 25% increase in prevalence of anxiety and depression worldwide https://www.who.int/news/item/02-03-2022-covid-19-pandemic-triggers-25-increase-in-prevalence-of-anxiety-and-depression-worldwide Data file: National Health Interview Survey (NHIS) : https://www.cdc.gov/nchs/nhis/data-questionnaires-documentation.htm Essential Concepts: Essential Concepts: Data collected through Participatory Sensing Campaigns will be used to create models that answer real-world problems related to our community. Lesson: In this lesson, students will decide on a topic and research question for their Team Participatory Sensing Campaign. They will review articles and choose at least one to set the context for the real-world problem that they will be addressing with their campaign. It is important to check the reliability of your sources so that you are not spreading misinformation or basing your research on untrustworthy or inaccurate information. An option for checking the credibility of sources is the SIFT method (The \"SIFT\" method is adapted from https://pressbooks.pub/webliteracy/ by Michael A. Caulfield): Stop - First, ask yourself if you know and/or trust the source of the page. Don't read it or share it until you know what it is. Second, if you have started working through the moves and find yourself in a \u201cclick cycle\u201d, STOP and remind yourself what your goal is - don\u2019t lead yourself astray down a rabbit hole of facts/links. Investigate the Source - Take time to figure out where it is from before reading it. If it\u2019s from an unfamiliar source, do a Google or Wikipedia search. Here\u2019s a video to help with investigating a source: https://www.youtube.com/watch_popup?v=hB6qjIxKltA Find Trusted Coverage - This is where we determine if a claim is reliable. Do a Google News search for relevant stories and use known fact-checking sites like snopes.com, factcheck.org, or politifact.com. If there are images in your source, you can do a reverse image search via Google to check its validity or if it\u2019s used elsewhere. Here\u2019s a helpful video on how to do a reverse image search: https://www.youtube.com/watch_popup?v=7eKG9RuqUE4 Trace claims, quotes, and media back to the original context - A lot of what we see on the web is a re-reporting or commentary of a story. When you see phrases like \u201cAs reported by\u2026\u201d or \u201cAccording to\u2026\u201d, it can be an indication of this. Here\u2019s a video to help navigate finding an original source: https://www.youtube.com/watch_popup?v=tRZ-N3OvvUs Now that you know how to check the credibility of sources, we will review characteristics of a participatory sensing campaign. Refer back to their class created campaign from Unit 3 to review. Using a Pair-Share strategy, ask students to discuss when a Participatory Sensing campaign should be used rather than a survey. Answers will vary. Research questions that include variation across time or across locations are good candidates for Participatory Sensing campaigns; therefore, a trigger is necessary in order to record observations at multiple time points and locations. If a question needs to be answered only once, then a survey is a better method. Remind students that in the last unit, they created one campaign for the entire class. In this unit, each student team will be creating and implementing a campaign on a topic that addresses a community concern or interest. Distribute the Team Campaign Creation handout ( LMR_4.4 ). Use the remainder of the class for students to find and review articles that will be the basis for their team participatory sensing campaigns. They will decide on a topic today and continue to create their campaigns in the next class period. Facilitate the student teams' brainstorm session by circulating around the room to check for understanding. If teams need help with finding an article and choosing a topic, you may recommend one of the following: LMR_4.4 Article: Great Pacific Garbage Patch: The World's Biggest Landfill in the Pacific Ocean https://science.howstuffworks.com/environmental/earth/oceanography/great-pacific-garbage-patch.htm Data file: US Landfills https://www.epa.gov/lmop/landfill-technical-data NOTE : This article and supporting data align with the topic of trash brought up in lesson 1. Article: 3 reasons why California's drought isn't really over, despite the rain https://www.npr.org/2023/03/23/1165378214/3-reasons-why-californias-drought-isnt-really-over-despite-all-the-rain/ Data file: US Drought https://www.drought.gov/historical-information?dataset=0&selectedDateUSDM=20120710 NOTE : This article and supporting data align with the topic of drought brought up in lesson 2. Article: The number of homeless people in America grew in 2023 as high cost of living took a toll https://www.usatoday.com/story/news/nation/2023/12/15/homelessness-in-america-grew-2023/71926354007/ Data file: Annual Homeless Assessment Report (AHAR) : https://www.huduser.gov/portal/datasets/ahar.html NOTE : This article and supporting data align with the topic of homelessness. Article: COVID-19 pandemic triggers 25% increase in prevalence of anxiety and depression worldwide https://www.who.int/news/item/02-03-2022-covid-19-pandemic-triggers-25-increase-in-prevalence-of-anxiety-and-depression-worldwide Data file: National Health Interview Survey (NHIS) : https://www.cdc.gov/nchs/nhis/data-questionnaires-documentation.htm NOTE : This article and supporting data align with the topic of mental health. Take time near the end of class to have student teams share out their chosen topics. Class Scribes: One team of students will give a brief talk to discuss what they think the 3 most important topics of the day were. Homework Students will come up with at least 2 possible research questions for their participatory sensing campaign. They will come to a consensus about their research question with their team in the next class period.","title":"Lesson 3: Community Connection"},{"location":"unit4/lesson3/#lesson-3-community-connection","text":"","title":"Lesson 3: Community Connection"},{"location":"unit4/lesson3/#objective","text":"Students will research and review articles from accredited sources to guide the design of their community focused Participatory Sensing campaign.","title":"Objective:"},{"location":"unit4/lesson3/#materials","text":"Resource: Web Literacy for Student Fact-Checkers https://pressbooks.pub/webliteracy/ Video: Online Verification Skills - Investigate the Source found at: https://www.youtube.com/watch_popup?v=hB6qjIxKltA https://www.youtube.com/watch_popup?v=hB6qjIxKltA Video: Online Verification Skills - Verifying Images and Videos found at: https://www.youtube.com/watch_popup?v=7eKG9RuqUE4 https://www.youtube.com/watch_popup?v=7eKG9RuqUE4 Video: Online Verification Skills - Find the Original Source found at: https://www.youtube.com/watch_popup?v=tRZ-N3OvvUs https://www.youtube.com/watch_popup?v=tRZ-N3OvvUs Team Campaign Creation handout ( LMR_4.4_Team Campaign Creation ) NOTE : This handout was used in Lesson 2 but a new copy should be used for this lesson. Article: Great Pacific Garbage Patch: The World's Biggest Landfill in the Pacific Ocean https://science.howstuffworks.com/environmental/earth/oceanography/great-pacific-garbage-patch.htm Data file: US Landfills https://www.epa.gov/lmop/landfill-technical-data Article: 3 reasons why California's drought isn't really over, despite the rain https://www.npr.org/2023/03/23/1165378214/3-reasons-why-californias-drought-isnt-really-over-despite-all-the-rain/ Data file: US Drought https://www.drought.gov/historical-information?dataset=0&selectedDateUSDM=20120710 Article: The number of homeless people in America grew in 2023 as high cost of living took a toll https://www.usatoday.com/story/news/nation/2023/12/15/homelessness-in-america-grew-2023/71926354007/ Data file: Annual Homeless Assessment Report (AHAR) : https://www.huduser.gov/portal/datasets/ahar.html Article: COVID-19 pandemeic triggers 25% increase in prevalence of anxiety and depression worldwide https://www.who.int/news/item/02-03-2022-covid-19-pandemic-triggers-25-increase-in-prevalence-of-anxiety-and-depression-worldwide Data file: National Health Interview Survey (NHIS) : https://www.cdc.gov/nchs/nhis/data-questionnaires-documentation.htm","title":"Materials:"},{"location":"unit4/lesson3/#essential-concepts","text":"Essential Concepts: Data collected through Participatory Sensing Campaigns will be used to create models that answer real-world problems related to our community.","title":"Essential Concepts:"},{"location":"unit4/lesson3/#lesson","text":"In this lesson, students will decide on a topic and research question for their Team Participatory Sensing Campaign. They will review articles and choose at least one to set the context for the real-world problem that they will be addressing with their campaign. It is important to check the reliability of your sources so that you are not spreading misinformation or basing your research on untrustworthy or inaccurate information. An option for checking the credibility of sources is the SIFT method (The \"SIFT\" method is adapted from https://pressbooks.pub/webliteracy/ by Michael A. Caulfield): Stop - First, ask yourself if you know and/or trust the source of the page. Don't read it or share it until you know what it is. Second, if you have started working through the moves and find yourself in a \u201cclick cycle\u201d, STOP and remind yourself what your goal is - don\u2019t lead yourself astray down a rabbit hole of facts/links. Investigate the Source - Take time to figure out where it is from before reading it. If it\u2019s from an unfamiliar source, do a Google or Wikipedia search. Here\u2019s a video to help with investigating a source: https://www.youtube.com/watch_popup?v=hB6qjIxKltA Find Trusted Coverage - This is where we determine if a claim is reliable. Do a Google News search for relevant stories and use known fact-checking sites like snopes.com, factcheck.org, or politifact.com. If there are images in your source, you can do a reverse image search via Google to check its validity or if it\u2019s used elsewhere. Here\u2019s a helpful video on how to do a reverse image search: https://www.youtube.com/watch_popup?v=7eKG9RuqUE4 Trace claims, quotes, and media back to the original context - A lot of what we see on the web is a re-reporting or commentary of a story. When you see phrases like \u201cAs reported by\u2026\u201d or \u201cAccording to\u2026\u201d, it can be an indication of this. Here\u2019s a video to help navigate finding an original source: https://www.youtube.com/watch_popup?v=tRZ-N3OvvUs Now that you know how to check the credibility of sources, we will review characteristics of a participatory sensing campaign. Refer back to their class created campaign from Unit 3 to review. Using a Pair-Share strategy, ask students to discuss when a Participatory Sensing campaign should be used rather than a survey. Answers will vary. Research questions that include variation across time or across locations are good candidates for Participatory Sensing campaigns; therefore, a trigger is necessary in order to record observations at multiple time points and locations. If a question needs to be answered only once, then a survey is a better method. Remind students that in the last unit, they created one campaign for the entire class. In this unit, each student team will be creating and implementing a campaign on a topic that addresses a community concern or interest. Distribute the Team Campaign Creation handout ( LMR_4.4 ). Use the remainder of the class for students to find and review articles that will be the basis for their team participatory sensing campaigns. They will decide on a topic today and continue to create their campaigns in the next class period. Facilitate the student teams' brainstorm session by circulating around the room to check for understanding. If teams need help with finding an article and choosing a topic, you may recommend one of the following: LMR_4.4 Article: Great Pacific Garbage Patch: The World's Biggest Landfill in the Pacific Ocean https://science.howstuffworks.com/environmental/earth/oceanography/great-pacific-garbage-patch.htm Data file: US Landfills https://www.epa.gov/lmop/landfill-technical-data NOTE : This article and supporting data align with the topic of trash brought up in lesson 1. Article: 3 reasons why California's drought isn't really over, despite the rain https://www.npr.org/2023/03/23/1165378214/3-reasons-why-californias-drought-isnt-really-over-despite-all-the-rain/ Data file: US Drought https://www.drought.gov/historical-information?dataset=0&selectedDateUSDM=20120710 NOTE : This article and supporting data align with the topic of drought brought up in lesson 2. Article: The number of homeless people in America grew in 2023 as high cost of living took a toll https://www.usatoday.com/story/news/nation/2023/12/15/homelessness-in-america-grew-2023/71926354007/ Data file: Annual Homeless Assessment Report (AHAR) : https://www.huduser.gov/portal/datasets/ahar.html NOTE : This article and supporting data align with the topic of homelessness. Article: COVID-19 pandemic triggers 25% increase in prevalence of anxiety and depression worldwide https://www.who.int/news/item/02-03-2022-covid-19-pandemic-triggers-25-increase-in-prevalence-of-anxiety-and-depression-worldwide Data file: National Health Interview Survey (NHIS) : https://www.cdc.gov/nchs/nhis/data-questionnaires-documentation.htm NOTE : This article and supporting data align with the topic of mental health. Take time near the end of class to have student teams share out their chosen topics.","title":"Lesson:"},{"location":"unit4/lesson3/#class-scribes","text":"One team of students will give a brief talk to discuss what they think the 3 most important topics of the day were.","title":"Class Scribes:"},{"location":"unit4/lesson3/#homework","text":"Students will come up with at least 2 possible research questions for their participatory sensing campaign. They will come to a consensus about their research question with their team in the next class period.","title":"Homework"},{"location":"unit4/lesson4/","text":"Lesson 4: Evaluate and Implement the Campaign Objective: Students will complete the design of their community focused Participatory Sensing campaign and implement a mock campaign to evaluate the feasibility of the campaign. Materials: Team Campaign Creation handout ( LMR_4.4_Team Campaign Creation ) from previous lesson Essential Concepts: Essential Concepts: Statistical questions guide a participatory sensing campaign so that we can learn about a community or ourselves. These campaigns should be evaluated before implementing to make sure they are reasonable and ethically sound. Lesson: Student teams will continue designing their Participatory Sensing campaign. Allow them some time to review their possible research questions with their team and to decide on a team research question before moving on to round 3. Round 3: Allow student teams a reasonable amount of time to engage in a brainstorm, in which they will discuss what kind of data needs to be collected in order to answer this research question and when is the best time to trigger the data collection/ completion of the survey. Before they begin, ask students to keep the following question in mind: Which of these data will give us information that addresses our research question? Once teams have decided on their types of data and trigger, they will create survey questions/ prompts to collect this type of data with this trigger. Round 4: Now that the teams have decided on a trigger and the type of data needed, they will discuss and create survey questions/ prompts to ask when the trigger is set. The questions should consider all of the possible data they might collect at this trigger event. Once teams have created their survey questions/ prompts, they will evaluate each survey question. For each question they should consider: What type of survey question/ prompt will this be (e.g. number, text, photo, time, single choice)? How does this question/ prompt help address the research question? Does the question/ prompt need to be reworded? (Is it clear what is being asked for? Do they know how to answer it?) One way to do this is to pair teams and take turns asking each other prompts. The team that is being asked may explain what information they think the question is asking for. If survey questions need to be rewritten, students will decide as a team on the changes. Once finalized, they will record the survey question/ prompt that goes along with each data variable on their Team Campaign Creation handout ( LMR_4.4 ), being cognizant of question bias. Round 5: In teams, students will now generate three statistical investigative questions that they might answer with the data they will collect and to guide their campaign. They need to make sure that their statistical investigative questions are interesting and relevant to their chosen topic. Remind students that they will also have data about the date, time, and place of data collection. Confirm that the questions are statistical and that they can be answered with the data the students propose to collect by circulating around the room to check on each team. Each team will decide on no more than 3 statistical investigative questions to guide their campaign. Now that they have all the pieces of the campaign, teams will evaluate whether their campaign is reasonable and ethically sound. Each team will hold a discussion on the following questions: Are answers to your survey questions likely to vary when the trigger occurs? (If not, you'll get bored entering the same data again and again) Can the team carry out the campaign? Do triggers occur so rarely that you'll have very little data? Do they occur so often that you'll get frustrated entering too much data? Ethics: Would sharing these data with strangers or friends be embarrassing or undermine someone's privacy? Can you change your trigger or survey questions to improve your evaluation? Will you be able to gather enough relevant data from your survey questions to be able to answer your statistical investigative questions? During their discussion about whether their campaign is reasonable and ethically sound, if teams discover that they need to make changes, they can make adjustments at this time. Students have collaboratively created their Team Participatory Sensing campaign. Class Scribes: One team of students will give a brief talk to discuss what they think the 3 most important topics of the day were. Homework Students will collect data by mock implementing their Team Participatory Sensing campaign.","title":"Lesson 4: Evaluate and Implement the Campaign"},{"location":"unit4/lesson4/#lesson-4-evaluate-and-implement-the-campaign","text":"","title":"Lesson 4: Evaluate and Implement the Campaign"},{"location":"unit4/lesson4/#objective","text":"Students will complete the design of their community focused Participatory Sensing campaign and implement a mock campaign to evaluate the feasibility of the campaign.","title":"Objective:"},{"location":"unit4/lesson4/#materials","text":"Team Campaign Creation handout ( LMR_4.4_Team Campaign Creation ) from previous lesson","title":"Materials:"},{"location":"unit4/lesson4/#essential-concepts","text":"Essential Concepts: Statistical questions guide a participatory sensing campaign so that we can learn about a community or ourselves. These campaigns should be evaluated before implementing to make sure they are reasonable and ethically sound.","title":"Essential Concepts:"},{"location":"unit4/lesson4/#lesson","text":"Student teams will continue designing their Participatory Sensing campaign. Allow them some time to review their possible research questions with their team and to decide on a team research question before moving on to round 3. Round 3: Allow student teams a reasonable amount of time to engage in a brainstorm, in which they will discuss what kind of data needs to be collected in order to answer this research question and when is the best time to trigger the data collection/ completion of the survey. Before they begin, ask students to keep the following question in mind: Which of these data will give us information that addresses our research question? Once teams have decided on their types of data and trigger, they will create survey questions/ prompts to collect this type of data with this trigger. Round 4: Now that the teams have decided on a trigger and the type of data needed, they will discuss and create survey questions/ prompts to ask when the trigger is set. The questions should consider all of the possible data they might collect at this trigger event. Once teams have created their survey questions/ prompts, they will evaluate each survey question. For each question they should consider: What type of survey question/ prompt will this be (e.g. number, text, photo, time, single choice)? How does this question/ prompt help address the research question? Does the question/ prompt need to be reworded? (Is it clear what is being asked for? Do they know how to answer it?) One way to do this is to pair teams and take turns asking each other prompts. The team that is being asked may explain what information they think the question is asking for. If survey questions need to be rewritten, students will decide as a team on the changes. Once finalized, they will record the survey question/ prompt that goes along with each data variable on their Team Campaign Creation handout ( LMR_4.4 ), being cognizant of question bias. Round 5: In teams, students will now generate three statistical investigative questions that they might answer with the data they will collect and to guide their campaign. They need to make sure that their statistical investigative questions are interesting and relevant to their chosen topic. Remind students that they will also have data about the date, time, and place of data collection. Confirm that the questions are statistical and that they can be answered with the data the students propose to collect by circulating around the room to check on each team. Each team will decide on no more than 3 statistical investigative questions to guide their campaign. Now that they have all the pieces of the campaign, teams will evaluate whether their campaign is reasonable and ethically sound. Each team will hold a discussion on the following questions: Are answers to your survey questions likely to vary when the trigger occurs? (If not, you'll get bored entering the same data again and again) Can the team carry out the campaign? Do triggers occur so rarely that you'll have very little data? Do they occur so often that you'll get frustrated entering too much data? Ethics: Would sharing these data with strangers or friends be embarrassing or undermine someone's privacy? Can you change your trigger or survey questions to improve your evaluation? Will you be able to gather enough relevant data from your survey questions to be able to answer your statistical investigative questions? During their discussion about whether their campaign is reasonable and ethically sound, if teams discover that they need to make changes, they can make adjustments at this time. Students have collaboratively created their Team Participatory Sensing campaign.","title":"Lesson:"},{"location":"unit4/lesson4/#class-scribes","text":"One team of students will give a brief talk to discuss what they think the 3 most important topics of the day were.","title":"Class Scribes:"},{"location":"unit4/lesson4/#homework","text":"Students will collect data by mock implementing their Team Participatory Sensing campaign.","title":"Homework"},{"location":"unit4/lesson5/","text":"Lesson 5: Refine and Create the Campaign Objective: Students will revise their community focused Participatory Sensing campaign according to the finding from the mock implementation of their campaign to refine it. Student teams will then create their campaigns using the campaign authoring tool. Materials: Campaign Authoring handout ( LMR_4.5_Campaign Authoring ) Essential Concepts: Essential Concepts: Statistical investigative questions guide a Participatory Sensing campaign so that we can learn about a community or ourselves. These campaigns should be tried before implementing to make sure they are collecting the data they are meant to collect and refined accordingly. Lesson: Student teams will come together to discuss their findings regarding the mock implementation of their campaign. Allow them time to share their findings with their team members. Next, ask student teams to discuss the revisions they need to make according to their findings. Now they will use the Campaign Authoring tool to create a campaign like the ones they see on their smart devices or the computer. Distribute the Campaign Authoring handout ( LMR_4.5_Campaign Authoring ). Each team will select a member to type the information required to create their campaign. Then, they will follow the instructions on the handout. NOTE : To name their campaign, a naming convention is suggested. Otherwise, you will have multiple campaigns with the same name. For example, teams may include their team name or number in order to easily identify their campaigns. Ask teams to refresh their campaigns on their smartphones or the web browser to verify that their campaign appears as one of the choices. Now that they are finished with their campaigns, student teams will use the remaining time to plan the expectations for their End of Unit 4 Model Eliciting Activity (MEA) Project. Class Scribes: One team of students will give a brief talk to discuss what they think the 3 most important topics of the day were. Homework Students may begin collecting data by implementing their Team Participatory Sensing campaign. They will have until the end of Unit 4 to collect data. Since only the members of their team will be involved in gathering data for their campaign, this allows them time to ensure they have a sufficient amount.","title":"Lesson 5: Refine and Create the Campaign"},{"location":"unit4/lesson5/#lesson-5-refine-and-create-the-campaign","text":"","title":"Lesson 5: Refine and Create the Campaign"},{"location":"unit4/lesson5/#objective","text":"Students will revise their community focused Participatory Sensing campaign according to the finding from the mock implementation of their campaign to refine it. Student teams will then create their campaigns using the campaign authoring tool.","title":"Objective:"},{"location":"unit4/lesson5/#materials","text":"Campaign Authoring handout ( LMR_4.5_Campaign Authoring )","title":"Materials:"},{"location":"unit4/lesson5/#essential-concepts","text":"Essential Concepts: Statistical investigative questions guide a Participatory Sensing campaign so that we can learn about a community or ourselves. These campaigns should be tried before implementing to make sure they are collecting the data they are meant to collect and refined accordingly.","title":"Essential Concepts:"},{"location":"unit4/lesson5/#lesson","text":"Student teams will come together to discuss their findings regarding the mock implementation of their campaign. Allow them time to share their findings with their team members. Next, ask student teams to discuss the revisions they need to make according to their findings. Now they will use the Campaign Authoring tool to create a campaign like the ones they see on their smart devices or the computer. Distribute the Campaign Authoring handout ( LMR_4.5_Campaign Authoring ). Each team will select a member to type the information required to create their campaign. Then, they will follow the instructions on the handout. NOTE : To name their campaign, a naming convention is suggested. Otherwise, you will have multiple campaigns with the same name. For example, teams may include their team name or number in order to easily identify their campaigns. Ask teams to refresh their campaigns on their smartphones or the web browser to verify that their campaign appears as one of the choices. Now that they are finished with their campaigns, student teams will use the remaining time to plan the expectations for their End of Unit 4 Model Eliciting Activity (MEA) Project.","title":"Lesson:"},{"location":"unit4/lesson5/#class-scribes","text":"One team of students will give a brief talk to discuss what they think the 3 most important topics of the day were.","title":"Class Scribes:"},{"location":"unit4/lesson5/#homework","text":"Students may begin collecting data by implementing their Team Participatory Sensing campaign. They will have until the end of Unit 4 to collect data. Since only the members of their team will be involved in gathering data for their campaign, this allows them time to ensure they have a sufficient amount.","title":"Homework"},{"location":"unit4/lesson6/","text":"Lesson 6: Statistical Predictions Using One Variable Objective: Students will devise a rule to determine how to choose a winner when predicting the typical height of all students in a large high school and measure the success of their prediction. They will consider different measures of success. Materials: Heights of Students at a Large High School handout ( LMR_4.6_HS Student Heights ) Vocabulary: rule Essential Concepts: Essential Concepts: Anyone can make a prediction. But statisticians measure the success of their predictions. This lesson encourages the classroom to consider different measures of success. Lesson: Inform the class that for this lesson, our class will help judge a contest held at a particular high school. This school held a contest in which they selected students at random from a classroom and reported their height. The information in Steps 3 \u2013 7 is included in the Heights of Students at a Large High School handout ( LMR_4.6 ). LMR_4.6 Our class will help judge a contest held at a particular high school to see who can make the best predictions. Height data for 40 randomly selected students were provided to three teams. Using this data, each team was asked to predict the heights of a random sample of 10 students. Here is the catch: teams were allowed to give only ONE number that had to be used to predict all 10 heights. As the judges of this contest, you will determine the winner. Your job is to determine the winning team. You must come up with two things: You must support your choice of a winner by using a rule for calculating a total score for each team. The rule must be applied to each team\u2019s prediction, and you must be able to explain how your rule helped select the winner. For example, do you choose the team with the largest score? The smallest? Here are the predictions of the three teams: Team A: 67.9 inches Team B: 68.1 inches Team C: 70.9 inches Display Dataset A, found on page 1 of the Heights of Students at a High School handout ( LMR_4.6 ). Notes to teacher: Students may have to be reminded that negative values with large absolute value are larger than positive values with small absolute values (e.g., |-10| is larger than |3| because 10 is larger than 3). Let students struggle for a little bit. A prompt to get them started: Look at the difference between a team's prediction and the actual outcomes (e.g., for the first height, Team A predicted 67.9, actual outcome was 70.1, so 67.9-70.1=-2.2=|-2.2|=2.2). They might also need to be nudged towards the sum of these differences \u2013 they need to produce a single score, not 10 separate scores. Here are some rules you can \u201cfeed\u201d to the class to move them along. Ask them: (a) Describe this rule in words. (b) Is it better to get a high score or a low score or some other score? (c) Which teams win for each? (Note, some of these rules produce ties). Rule 1: sum(heights-predicted.value == 0) Translated into words: the number of exactly correct predictions Rule 2: sum(heights-predicted.value) Translated into words: the sum of the differences between predicted value and the actual heights Rule 3: sum(abs(heights-estimate)) Translated into words: the sum of the absolute values of the deviations Rule 4: sum((heights-estimate)^2) Translated into words: the sum of the squared deviations Note: It is unlikely that students will think of the last two. That\u2019s okay, because we will introduce them in a future lesson, but you might want to present one (or both) to see what they think about these rules. Allow student teams time to discuss and complete the task for Dataset A. Do not share their responses to Dataset A. Instead, display the following questions: What if we had a different set of 10 randomly selected students? Would the same team win? Allow teams to discuss the questions, then share a couple of responses to the questions in the previous step. Display Dataset B, found on page 2 of the Heights of Students at a High School handout ( LMR_4.6 ), then have them find the winner using this new sample. Is it the same as they chose before? Note: We do NOT know the value of the true population mean/ typical value. This is what we are really trying to predict. Teams will take turns to share their work as follows: Which team did you select as the winner using Dataset A? Explain the method, or rule , your team used to declare the winner. Which team did you select as the winner using Dataset B? Is the winner the same? Did you use the same rule to select a winner or did it change? If it changed, explain. During the share out, students will take notes about the other teams\u2019 rules in their DS journals. Teams may continue to share at the start of the next lesson , if they run out of time. Class Scribes: One team of students will give a brief talk to discuss what they think the 3 most important topics of the day were.","title":"Lesson 6: Statistical Predictions Using One Variable"},{"location":"unit4/lesson6/#lesson-6-statistical-predictions-using-one-variable","text":"","title":"Lesson 6: Statistical Predictions Using One Variable"},{"location":"unit4/lesson6/#objective","text":"Students will devise a rule to determine how to choose a winner when predicting the typical height of all students in a large high school and measure the success of their prediction. They will consider different measures of success.","title":"Objective:"},{"location":"unit4/lesson6/#materials","text":"Heights of Students at a Large High School handout ( LMR_4.6_HS Student Heights )","title":"Materials:"},{"location":"unit4/lesson6/#vocabulary","text":"rule","title":"Vocabulary:"},{"location":"unit4/lesson6/#essential-concepts","text":"Essential Concepts: Anyone can make a prediction. But statisticians measure the success of their predictions. This lesson encourages the classroom to consider different measures of success.","title":"Essential Concepts:"},{"location":"unit4/lesson6/#lesson","text":"Inform the class that for this lesson, our class will help judge a contest held at a particular high school. This school held a contest in which they selected students at random from a classroom and reported their height. The information in Steps 3 \u2013 7 is included in the Heights of Students at a Large High School handout ( LMR_4.6 ). LMR_4.6 Our class will help judge a contest held at a particular high school to see who can make the best predictions. Height data for 40 randomly selected students were provided to three teams. Using this data, each team was asked to predict the heights of a random sample of 10 students. Here is the catch: teams were allowed to give only ONE number that had to be used to predict all 10 heights. As the judges of this contest, you will determine the winner. Your job is to determine the winning team. You must come up with two things: You must support your choice of a winner by using a rule for calculating a total score for each team. The rule must be applied to each team\u2019s prediction, and you must be able to explain how your rule helped select the winner. For example, do you choose the team with the largest score? The smallest? Here are the predictions of the three teams: Team A: 67.9 inches Team B: 68.1 inches Team C: 70.9 inches Display Dataset A, found on page 1 of the Heights of Students at a High School handout ( LMR_4.6 ). Notes to teacher: Students may have to be reminded that negative values with large absolute value are larger than positive values with small absolute values (e.g., |-10| is larger than |3| because 10 is larger than 3). Let students struggle for a little bit. A prompt to get them started: Look at the difference between a team's prediction and the actual outcomes (e.g., for the first height, Team A predicted 67.9, actual outcome was 70.1, so 67.9-70.1=-2.2=|-2.2|=2.2). They might also need to be nudged towards the sum of these differences \u2013 they need to produce a single score, not 10 separate scores. Here are some rules you can \u201cfeed\u201d to the class to move them along. Ask them: (a) Describe this rule in words. (b) Is it better to get a high score or a low score or some other score? (c) Which teams win for each? (Note, some of these rules produce ties). Rule 1: sum(heights-predicted.value == 0) Translated into words: the number of exactly correct predictions Rule 2: sum(heights-predicted.value) Translated into words: the sum of the differences between predicted value and the actual heights Rule 3: sum(abs(heights-estimate)) Translated into words: the sum of the absolute values of the deviations Rule 4: sum((heights-estimate)^2) Translated into words: the sum of the squared deviations Note: It is unlikely that students will think of the last two. That\u2019s okay, because we will introduce them in a future lesson, but you might want to present one (or both) to see what they think about these rules. Allow student teams time to discuss and complete the task for Dataset A. Do not share their responses to Dataset A. Instead, display the following questions: What if we had a different set of 10 randomly selected students? Would the same team win? Allow teams to discuss the questions, then share a couple of responses to the questions in the previous step. Display Dataset B, found on page 2 of the Heights of Students at a High School handout ( LMR_4.6 ), then have them find the winner using this new sample. Is it the same as they chose before? Note: We do NOT know the value of the true population mean/ typical value. This is what we are really trying to predict. Teams will take turns to share their work as follows: Which team did you select as the winner using Dataset A? Explain the method, or rule , your team used to declare the winner. Which team did you select as the winner using Dataset B? Is the winner the same? Did you use the same rule to select a winner or did it change? If it changed, explain. During the share out, students will take notes about the other teams\u2019 rules in their DS journals. Teams may continue to share at the start of the next lesson , if they run out of time.","title":"Lesson:"},{"location":"unit4/lesson6/#class-scribes","text":"One team of students will give a brief talk to discuss what they think the 3 most important topics of the day were.","title":"Class Scribes:"},{"location":"unit4/lesson7/","text":"Lesson 7: Statistical Predictions Applying the Rule Objective: Students will apply the rule statisticians use to determine the best method for predicting heights for students at a high school. Materials: Each team\u2019s rule for determining a winner (from previous lesson) A Tale of Two Rules handout ( LMR_4.7_A Tale of Two Rules ) Prediction Games handout ( LMR_4.8_Prediction Games ) Vocabulary: training data testing data mean squared error mean absolute error residual Essential Concepts: Essential Concepts: If we use the mean squared errors rule, then the mean of our current data is the best prediction of future values. If we use the mean absolute errors rule, then the median of the current data is the best prediction of future values. Lesson: Ask students to recall that in the previous lesson, each student team created a rule to determine a winner. Which team\u2019s rules worked well for determining a winner? Remind them that in their DS Journals, they took notes about each team\u2019s rule as they presented. This time, they will be switching roles \u2013 instead of creating a rule to judge the given predictions, they will be given a rule and it\u2019s their job to find the best prediction to win the contest. Have students refer back to the Heights of Students at a Large High School handout (LMR_4.6) from the previous lesson. Recall that the student teams were provided with height data on 40 selected students to come up with their predictions for future observations. This is a common practice with statisticians and data scientists. The first dataset of 40 students is called the training data where we train a model to make predictions. Then we use the testing data (dataset A and dataset B) to test those predictions. Using the training data, the teams used different statistics for their predictions: Team A used the mean. Team B used the median. Team C used the third quartile. In the previous lesson, you created your own rules to determine the winner. Today, you will learn rules that statisticians and data scientists use. The first is called the mean squared error rule. Note to teacher : acknowledge any groups who came up with MSE or MAE on their own in the previous lesson. An \" error \" is the difference between our prediction and the actual outcome and is sometimes called a \"residual\". The mean squared error is also called: Mean squared deviation Mean squared residual Residual sum of squares The formula looks like this, where stands for the predicted value: Using this formula, the teams' scores are determined by finding the average of the squared differences between their predictions and the actual values. The winner is the team with the lowest mean squared error. Let\u2019s use R to do the heavy lifting for us. Demonstrate how to find the mean squared error by typing the following commands in the console (throughout the process, show what is happening in the Environment and the dataframe): #First, let's create a vector of the heights in our first dataset: height <- c(70.1, 61, 70.1, 68.1, 63, 66.1, 61, 70.1, 72.8, 70.9) #Next, convert this vector into a dataframe: datasetA <- data.frame(height) #Now we find the residuals using one of the statistics. #For this example, we'll use the first quartile from the training data (65 inches): datasetA <- mutate(datasetA, residual=height-65) #Next, we will square each residual: datasetA <- mutate(datasetA, sq_res=residual^2) #Finally, we use the mean function to: #sum up the squared residuals and divide by 10 to find our mean squared deviation: mean(~sq_res, data=datasetA) This process gives us the mean squared error of 22.05. Note to teacher: The value of the mean squared error will always be in square units. In order to convert back to the original units, simply take the square root of the mean squared error. Interpretation: When using the Q1 height to make predictions about all heights, our predictions will typically be off by inches. Here is the vector of heights for Dataset B: heightB <- c(70.1, 72, 68.9, 61.8, 70.9, 59.8, 72, 65, 66.1, 68.9) Distribute the A Tale of Two Rules handout ( LMR_4.7 ). LMR_4.7 Let's see how well our teams' predictions did on the heights of the testing data. Students will work in teams to answer: Using the mean squared errors, which statistic is the winner? Discuss which statistic made the best predictions in all three games. Answers: Note to teacher: Explain that the mean/Team A was the winner of this contest. Data scientists (and mathematicians) can prove that the mean will always work best (except in a few weird cases from time to time). So if you want to predict the future, the mean is the best single guess you can make. Ask: What if another data science class has a best rule that is different from ours? Another agreed upon method that data scientists and statisticians often use is the mean absolute error . It\u2019s unlikely that students will figure this out on their own. The reasons why we do it in statistics can be proven mathematically but it's beyond the scope of this course. The mean absolute error is expressed as (where stands for the predicted value): Explain that each team will now use the statisticians\u2019 method for declaring a winner. Display the mean absolute error formula and discuss what each symbol means. Here is the script for MAE: #Find the absolute value of the residuals using one of the statistics #For this example, we'll use the first quartile from the training data (65 inches): datasetA <- mutate(datasetA, residual=abs(heightA-65)) #Finally, we use the mean function to: #sum up the residuals and divide by 10 to find our mean absolute error: mean(~residual, data=datasetA) This process gives us the mean absolute error of 4.32. Using our previous examples, recalculate your predictions using the MAE. Using the mean absolute error, which statistic/team is the winner? Answers: Note to teacher: Explain that in this instance, the median/Team B is the \u201cwinner.\u201d This means that the way you play the game depends on the rules of the game. If we used the mean squared error (MSE), play with the mean. If we use the mean absolute error (MAE), play with the median. Optional practice: Students can practice finding the mean squared error and mean absolute error using the mean and median with the Prediction Games handout ( LMR_4.8 ). The LMR includes the five number summary if they were curious how the MSE and MAE for other statistics compare. LMR_4.8 Class Scribes: One team of students will give a brief talk to discuss what they think the 3 most important topics of the day were.","title":"Lesson 7: Statistical Predictions by Applying the Rule"},{"location":"unit4/lesson7/#lesson-7-statistical-predictions-applying-the-rule","text":"","title":"Lesson 7: Statistical Predictions Applying the Rule"},{"location":"unit4/lesson7/#objective","text":"Students will apply the rule statisticians use to determine the best method for predicting heights for students at a high school.","title":"Objective:"},{"location":"unit4/lesson7/#materials","text":"Each team\u2019s rule for determining a winner (from previous lesson) A Tale of Two Rules handout ( LMR_4.7_A Tale of Two Rules ) Prediction Games handout ( LMR_4.8_Prediction Games )","title":"Materials:"},{"location":"unit4/lesson7/#vocabulary","text":"training data testing data mean squared error mean absolute error residual","title":"Vocabulary:"},{"location":"unit4/lesson7/#essential-concepts","text":"Essential Concepts: If we use the mean squared errors rule, then the mean of our current data is the best prediction of future values. If we use the mean absolute errors rule, then the median of the current data is the best prediction of future values.","title":"Essential Concepts:"},{"location":"unit4/lesson7/#lesson","text":"Ask students to recall that in the previous lesson, each student team created a rule to determine a winner. Which team\u2019s rules worked well for determining a winner? Remind them that in their DS Journals, they took notes about each team\u2019s rule as they presented. This time, they will be switching roles \u2013 instead of creating a rule to judge the given predictions, they will be given a rule and it\u2019s their job to find the best prediction to win the contest. Have students refer back to the Heights of Students at a Large High School handout (LMR_4.6) from the previous lesson. Recall that the student teams were provided with height data on 40 selected students to come up with their predictions for future observations. This is a common practice with statisticians and data scientists. The first dataset of 40 students is called the training data where we train a model to make predictions. Then we use the testing data (dataset A and dataset B) to test those predictions. Using the training data, the teams used different statistics for their predictions: Team A used the mean. Team B used the median. Team C used the third quartile. In the previous lesson, you created your own rules to determine the winner. Today, you will learn rules that statisticians and data scientists use. The first is called the mean squared error rule. Note to teacher : acknowledge any groups who came up with MSE or MAE on their own in the previous lesson. An \" error \" is the difference between our prediction and the actual outcome and is sometimes called a \"residual\". The mean squared error is also called: Mean squared deviation Mean squared residual Residual sum of squares The formula looks like this, where stands for the predicted value: Using this formula, the teams' scores are determined by finding the average of the squared differences between their predictions and the actual values. The winner is the team with the lowest mean squared error. Let\u2019s use R to do the heavy lifting for us. Demonstrate how to find the mean squared error by typing the following commands in the console (throughout the process, show what is happening in the Environment and the dataframe): #First, let's create a vector of the heights in our first dataset: height <- c(70.1, 61, 70.1, 68.1, 63, 66.1, 61, 70.1, 72.8, 70.9) #Next, convert this vector into a dataframe: datasetA <- data.frame(height) #Now we find the residuals using one of the statistics. #For this example, we'll use the first quartile from the training data (65 inches): datasetA <- mutate(datasetA, residual=height-65) #Next, we will square each residual: datasetA <- mutate(datasetA, sq_res=residual^2) #Finally, we use the mean function to: #sum up the squared residuals and divide by 10 to find our mean squared deviation: mean(~sq_res, data=datasetA) This process gives us the mean squared error of 22.05. Note to teacher: The value of the mean squared error will always be in square units. In order to convert back to the original units, simply take the square root of the mean squared error. Interpretation: When using the Q1 height to make predictions about all heights, our predictions will typically be off by inches. Here is the vector of heights for Dataset B: heightB <- c(70.1, 72, 68.9, 61.8, 70.9, 59.8, 72, 65, 66.1, 68.9) Distribute the A Tale of Two Rules handout ( LMR_4.7 ). LMR_4.7 Let's see how well our teams' predictions did on the heights of the testing data. Students will work in teams to answer: Using the mean squared errors, which statistic is the winner? Discuss which statistic made the best predictions in all three games. Answers: Note to teacher: Explain that the mean/Team A was the winner of this contest. Data scientists (and mathematicians) can prove that the mean will always work best (except in a few weird cases from time to time). So if you want to predict the future, the mean is the best single guess you can make. Ask: What if another data science class has a best rule that is different from ours? Another agreed upon method that data scientists and statisticians often use is the mean absolute error . It\u2019s unlikely that students will figure this out on their own. The reasons why we do it in statistics can be proven mathematically but it's beyond the scope of this course. The mean absolute error is expressed as (where stands for the predicted value): Explain that each team will now use the statisticians\u2019 method for declaring a winner. Display the mean absolute error formula and discuss what each symbol means. Here is the script for MAE: #Find the absolute value of the residuals using one of the statistics #For this example, we'll use the first quartile from the training data (65 inches): datasetA <- mutate(datasetA, residual=abs(heightA-65)) #Finally, we use the mean function to: #sum up the residuals and divide by 10 to find our mean absolute error: mean(~residual, data=datasetA) This process gives us the mean absolute error of 4.32. Using our previous examples, recalculate your predictions using the MAE. Using the mean absolute error, which statistic/team is the winner? Answers: Note to teacher: Explain that in this instance, the median/Team B is the \u201cwinner.\u201d This means that the way you play the game depends on the rules of the game. If we used the mean squared error (MSE), play with the mean. If we use the mean absolute error (MAE), play with the median. Optional practice: Students can practice finding the mean squared error and mean absolute error using the mean and median with the Prediction Games handout ( LMR_4.8 ). The LMR includes the five number summary if they were curious how the MSE and MAE for other statistics compare. LMR_4.8","title":"Lesson:"},{"location":"unit4/lesson7/#class-scribes","text":"One team of students will give a brief talk to discuss what they think the 3 most important topics of the day were.","title":"Class Scribes:"},{"location":"unit4/lesson8/","text":"Lesson 8: Statistical Predictions Using Two Variables Objective: Students will learn how to predict height using arm span data - and vice versa - visually on a scatterplot. Materials: Arm span vs. Height Scatterplot ( LMR_4.9_Arm Span vs Height ) Note: This handout will be referenced in subsequent lessons. Assorted color markers (dry erase or overhead) \u2014 See step 3 of lesson. Overhead or LCD projector Essential Concepts: Essential Concepts: When predicting values of a variable y , and if y is linearly associated with x , then we can get improved predictions by using our knowledge about x . For every value of x , find the mean of the y values for that value of x . If the resulting mean follows a trend, we can model this trend to generalize to unseen values of x . Lesson: Remind students that in the previous lessons they were working with height data to predict the typical height of all the students at a large high school, implementing a method used by statisticians to help them make good predictions. In addition to the height data, it turns out that each student\u2019s arm span data was also collected and recorded. Display the Arm Span vs. Height Scatterplot ( LMR_4.9 ) on a white board or overhead projector (you will write on the board or the transparency later in the lesson \u2014 see step 9). LMR_4.9 Distribute the Arm Span vs. Height handout ( LMR_4.9 ). Students will refer to this handout again later in a subsequent lesson. In teams, ask students to analyze the plot and discuss the following questions: \u2022 What kind of plot is this? Answer: Scatterplot. \u2022 How many variables are displayed in this plot? Answer: Two variables. \u2022 Which variable is shown on the x-axis? On the y-axis? Answer: Arm span is shown on the x-axis and height is shown on the y-axis. \u2022 What is this plot showing? Answer: It is showing the relationship between a person\u2019s height and the person\u2019s corresponding arm span measurement. \u2022 How can I find out the height of the person whose arm span measures 68 inches? Answer: Find 68 on the x-axis. Then find the data point located at 68. Place finger on the data point and track its location on the y-axis. The height is also 68 inches. Using Talk Moves, conduct a class discussion of the questions in step 5. Remind students that we\u2019ve learned that the mean is the best way of predicting heights. The mean heights of these people is 64 inches. Ask the students: Do you think we can do better? Is 64\" a good prediction for someone whose arm span is 72\u201d? What about 60\u201d? How can you come up with a rule for determining the best predicted height if you know the person\u2019s arm span? Note to teacher: Lead students to realize that they can do this by \u201csubsetting\u201d the data for the fixed x value. For example, if arm span is 60\", they should consider only the heights of people whose arm span is 60\" and find the mean. In teams, ask students to approximate the mean height for people whose arm span is 60\", 64\", 68\", and 72\". Note: Because the plot does not clearly show duplicate ordered pairs, an approximation is sufficient at this point. You may have students use RStudio to calculate the mean height for the specific armspans. Refer to the OPTIONAL section at the end of the lesson. Then plot these points on the graph. We will use this later \u2013 the points should be roughly along a straight line. Note: These arm spans have a range of height values associated with them. Students may take a mean of the heights, but answers may vary. Ask students if they see any patterns or rules they can use from this to help with predictions. Because there were multiple height values associated with each arm span length, you will likely get multiple answers from students. The goal now is to come up with a rule that suggests a plausible height value for anyone with a particular arm span. A sentence starter to guide students: If a person has a bigger arm span, then we should predict [a bigger height] . If time permits, you might push them to be more precise. Let\u2019s take someone who has a 60-inch arm span. You predicted a height of . How much should we increase our prediction for people with a 62-inch arm span? Can you do this without subsetting the data and re-calculating? Conceptually, students are wrestling with the notion of the slope of the regression line but there\u2019s no need to point this out just yet. Important: The equation of the line of best fit will be revealed in Lesson 9 . OPTIONAL FOR ITEM 9 If you want to obtain the exact mean height for each arm span value in step 9, copy the code below and run it in an RScript. xyplot(height~armspan, data = arm_span, scales = list(x = list(at = seq(58, 72, 1)), y = list(at = seq(52, 72, 1))), xlab = \"Arm span (inches)\", ylab = \"Height (inches)\") armspan_60 <- filter(arm_span, armspan==60) mean(~height, data = armspan_60) #62.66667 armspan_64 <- filter(arm_span, armspan==64) mean(~height, data = armspan_64) #64 armspan_68 <- filter(arm_span, armspan==68) mean(~height, data = armspan_68) #68 armspan_72 <- filter(arm_span, armspan==72) mean(~height, data = armspan_72) #71.5 #Base R Code #syntax to create a scatterplot using base R plot(arm_span$armspan, arm_span$height) #Points function in base R is more user friendly points(60, 62.6667, col = \"red\", cex = 2) points(64, 64, col = \"red\", cex = 2) points(68, 68, col = \"red\", cex = 2) points(72, 71.5, col = \"red\", cex = 2) Class Scribes: One team of students will give a brief talk to discuss what they think the 3 most important topics of the day were.","title":"Lesson 8: Statistical Predictions Using Two Variables"},{"location":"unit4/lesson8/#lesson-8-statistical-predictions-using-two-variables","text":"","title":"Lesson 8: Statistical Predictions Using Two Variables"},{"location":"unit4/lesson8/#objective","text":"Students will learn how to predict height using arm span data - and vice versa - visually on a scatterplot.","title":"Objective:"},{"location":"unit4/lesson8/#materials","text":"Arm span vs. Height Scatterplot ( LMR_4.9_Arm Span vs Height ) Note: This handout will be referenced in subsequent lessons. Assorted color markers (dry erase or overhead) \u2014 See step 3 of lesson. Overhead or LCD projector","title":"Materials:"},{"location":"unit4/lesson8/#essential-concepts","text":"Essential Concepts: When predicting values of a variable y , and if y is linearly associated with x , then we can get improved predictions by using our knowledge about x . For every value of x , find the mean of the y values for that value of x . If the resulting mean follows a trend, we can model this trend to generalize to unseen values of x .","title":"Essential Concepts:"},{"location":"unit4/lesson8/#lesson","text":"Remind students that in the previous lessons they were working with height data to predict the typical height of all the students at a large high school, implementing a method used by statisticians to help them make good predictions. In addition to the height data, it turns out that each student\u2019s arm span data was also collected and recorded. Display the Arm Span vs. Height Scatterplot ( LMR_4.9 ) on a white board or overhead projector (you will write on the board or the transparency later in the lesson \u2014 see step 9). LMR_4.9 Distribute the Arm Span vs. Height handout ( LMR_4.9 ). Students will refer to this handout again later in a subsequent lesson. In teams, ask students to analyze the plot and discuss the following questions: \u2022 What kind of plot is this? Answer: Scatterplot. \u2022 How many variables are displayed in this plot? Answer: Two variables. \u2022 Which variable is shown on the x-axis? On the y-axis? Answer: Arm span is shown on the x-axis and height is shown on the y-axis. \u2022 What is this plot showing? Answer: It is showing the relationship between a person\u2019s height and the person\u2019s corresponding arm span measurement. \u2022 How can I find out the height of the person whose arm span measures 68 inches? Answer: Find 68 on the x-axis. Then find the data point located at 68. Place finger on the data point and track its location on the y-axis. The height is also 68 inches. Using Talk Moves, conduct a class discussion of the questions in step 5. Remind students that we\u2019ve learned that the mean is the best way of predicting heights. The mean heights of these people is 64 inches. Ask the students: Do you think we can do better? Is 64\" a good prediction for someone whose arm span is 72\u201d? What about 60\u201d? How can you come up with a rule for determining the best predicted height if you know the person\u2019s arm span? Note to teacher: Lead students to realize that they can do this by \u201csubsetting\u201d the data for the fixed x value. For example, if arm span is 60\", they should consider only the heights of people whose arm span is 60\" and find the mean. In teams, ask students to approximate the mean height for people whose arm span is 60\", 64\", 68\", and 72\". Note: Because the plot does not clearly show duplicate ordered pairs, an approximation is sufficient at this point. You may have students use RStudio to calculate the mean height for the specific armspans. Refer to the OPTIONAL section at the end of the lesson. Then plot these points on the graph. We will use this later \u2013 the points should be roughly along a straight line. Note: These arm spans have a range of height values associated with them. Students may take a mean of the heights, but answers may vary. Ask students if they see any patterns or rules they can use from this to help with predictions. Because there were multiple height values associated with each arm span length, you will likely get multiple answers from students. The goal now is to come up with a rule that suggests a plausible height value for anyone with a particular arm span. A sentence starter to guide students: If a person has a bigger arm span, then we should predict [a bigger height] . If time permits, you might push them to be more precise. Let\u2019s take someone who has a 60-inch arm span. You predicted a height of . How much should we increase our prediction for people with a 62-inch arm span? Can you do this without subsetting the data and re-calculating? Conceptually, students are wrestling with the notion of the slope of the regression line but there\u2019s no need to point this out just yet. Important: The equation of the line of best fit will be revealed in Lesson 9 . OPTIONAL FOR ITEM 9 If you want to obtain the exact mean height for each arm span value in step 9, copy the code below and run it in an RScript. xyplot(height~armspan, data = arm_span, scales = list(x = list(at = seq(58, 72, 1)), y = list(at = seq(52, 72, 1))), xlab = \"Arm span (inches)\", ylab = \"Height (inches)\") armspan_60 <- filter(arm_span, armspan==60) mean(~height, data = armspan_60) #62.66667 armspan_64 <- filter(arm_span, armspan==64) mean(~height, data = armspan_64) #64 armspan_68 <- filter(arm_span, armspan==68) mean(~height, data = armspan_68) #68 armspan_72 <- filter(arm_span, armspan==72) mean(~height, data = armspan_72) #71.5 #Base R Code #syntax to create a scatterplot using base R plot(arm_span$armspan, arm_span$height) #Points function in base R is more user friendly points(60, 62.6667, col = \"red\", cex = 2) points(64, 64, col = \"red\", cex = 2) points(68, 68, col = \"red\", cex = 2) points(72, 71.5, col = \"red\", cex = 2)","title":"Lesson:"},{"location":"unit4/lesson8/#class-scribes","text":"One team of students will give a brief talk to discuss what they think the 3 most important topics of the day were.","title":"Class Scribes:"},{"location":"unit4/lesson9/","text":"Lesson 9: The Spaghetti Line Objective: Students will estimate the line of best fit for a height and arm span dataset using a strand of spaghetti as a modeling tool. Materials: The Spaghetti Line handout ( LMR_4.10_The Spaghetti Line ) Note: Advance preparation is required. Cut out plots prior to beginning the lesson. 1 lb. of Uncooked Spaghetti Grid Paper Tape or Glue Poster paper Vocabulary: line of best fit Essential Concepts: Essential Concepts: We can often use a straight line to summarize a trend. \u201cEyeballing\u201d a straight line to a scatterplot is one way to do this. Lesson: Note: Lab 4A may be done in the same class period as this lesson. Inform students that in this lesson, they will estimate the equation of the line of best fit for a height and arm span dataset. Distribute The Spaghetti Line handout( LMR_4.10_The Spaghetti Line ) to each student and a couple of spaghetti strands per team. Students will estimate the line of best fit as outlined in the handout. Team solutions should be recorded on poster paper. They will glue their assigned plot on the poster and record their responses to the questions on the poster paper. Note to teacher: If necessary, review how to find the slope of a line using two points and how to write an equation using the slope and y-intercept. LMR_4.10_The Spaghetti Line Ask teams to post their work around the room. Conduct a Gallery Walk so that teams can see each other\u2019s work. Lead a discussion about the teams\u2019 lines. Ask: Which team has the best line? Why? Note to teacher: Push the students a bit by adding an obviously bad line to the graph and asking why their line is better than this one. Push them to come to an understanding that the \u201cbest\u201d line comes close to the most points and this line is called the line of best fit .. Class Scribes: One team of students will give a brief talk to discuss what they think the 3 most important topics of the day were. Homework & Next Day LAB 4A: If the Line Fits... Complete Lab 4A prior to Lesson 10 .","title":"Lesson 9: Spaghetti Line"},{"location":"unit4/lesson9/#lesson-9-the-spaghetti-line","text":"","title":"Lesson 9: The Spaghetti Line"},{"location":"unit4/lesson9/#objective","text":"Students will estimate the line of best fit for a height and arm span dataset using a strand of spaghetti as a modeling tool.","title":"Objective:"},{"location":"unit4/lesson9/#materials","text":"The Spaghetti Line handout ( LMR_4.10_The Spaghetti Line ) Note: Advance preparation is required. Cut out plots prior to beginning the lesson. 1 lb. of Uncooked Spaghetti Grid Paper Tape or Glue Poster paper","title":"Materials:"},{"location":"unit4/lesson9/#vocabulary","text":"line of best fit","title":"Vocabulary:"},{"location":"unit4/lesson9/#essential-concepts","text":"Essential Concepts: We can often use a straight line to summarize a trend. \u201cEyeballing\u201d a straight line to a scatterplot is one way to do this.","title":"Essential Concepts:"},{"location":"unit4/lesson9/#lesson","text":"Note: Lab 4A may be done in the same class period as this lesson. Inform students that in this lesson, they will estimate the equation of the line of best fit for a height and arm span dataset. Distribute The Spaghetti Line handout( LMR_4.10_The Spaghetti Line ) to each student and a couple of spaghetti strands per team. Students will estimate the line of best fit as outlined in the handout. Team solutions should be recorded on poster paper. They will glue their assigned plot on the poster and record their responses to the questions on the poster paper. Note to teacher: If necessary, review how to find the slope of a line using two points and how to write an equation using the slope and y-intercept. LMR_4.10_The Spaghetti Line Ask teams to post their work around the room. Conduct a Gallery Walk so that teams can see each other\u2019s work. Lead a discussion about the teams\u2019 lines. Ask: Which team has the best line? Why? Note to teacher: Push the students a bit by adding an obviously bad line to the graph and asking why their line is better than this one. Push them to come to an understanding that the \u201cbest\u201d line comes close to the most points and this line is called the line of best fit ..","title":"Lesson:"},{"location":"unit4/lesson9/#class-scribes","text":"One team of students will give a brief talk to discuss what they think the 3 most important topics of the day were.","title":"Class Scribes:"},{"location":"unit4/lesson9/#homework-next-day","text":"LAB 4A: If the Line Fits... Complete Lab 4A prior to Lesson 10 .","title":"Homework &amp; Next Day"},{"location":"unit4/overview/","text":"Introduction to Data Science Daily Overview: Unit 4 Unit 4 Daily Overview: Unit 4 .tg {border-collapse:collapse;border-spacing:0;} .tg td{font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;} .tg th{font-family:Arial, sans-serif;font-size:14px;font-weight:normal;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;} .tg .tg-88nc{font-weight:bold;border-color:inherit;text-align:center} .tg .tg-c3ow{border-color:inherit;text-align:center;vertical-align:top} .tg .tg-uys7{border-color:inherit;text-align:center} .tg .tg-pwj7{background-color:#efefef;border-color:inherit;text-align:left} .tg .tg-5e9r{background-color:#efefef;border-color:inherit;text-align:center} .tg .tg-xldj{border-color:inherit;text-align:left} .tg .tg-0pky{border-color:inherit;text-align:left;vertical-align:top} Theme Day Lessons and Labs Campaign Topics Page Campaigns and Community (5 days) 1 Lesson 1: Trash Modeling to answer real world problems, official data sets 315 2 Lesson 2: Drought Exploratory data analysis, campaign creation 319 3 Lesson 3: Community Connection Team Campaign\u2014data Community topic research, campaign creation 321 4 Lesson 4: Evaluate and Implement the Campaign Team Campaign\u2014data Statistical questions, evaluate & mock implement campaign 323 5 Lesson 5: Refine and Create the Campaign Team Campaign\u2014data Revise and edit campaign, data collection 325 Predictions and Models (13 days) 6 Lesson 6: Statistical Predictions Using One Variable Team Campaign\u2014data One-variable predictions using a rule 328 7 Lesson 7: Statistical Predictions by Applying the Rule Team Campaign\u2014data Predictions applying mean square deviation, mean absolute error 333 8 Lesson 8: Statistical Predictions Using Two Variables Team Campaign\u2014data Two-variable statistical predictions, scatterplots 335 9 Lesson 9: The Spaghetti Line Team Campaign\u2014data Estimate line of best fit, single linear regression 337 10 LAB 4A: If the Line Fits\u2026 Team Campaign\u2014data Estimate line of best fit 339 11 Lesson 10: What\u2019s the Best Line? Team Campaign\u2014data Predictions based on linear models 343 12 LAB 4B: What\u2019s the Score? Team Campaign\u2014data Comparing predictions to real data 345 13 LAB 4C: Cross-Validation Team Campaign\u2014data Use training and test data for predictions 346 14 Lesson 11: What\u2019s the Trend? Team Campaign\u2014data Trend, associations, linear model 348 15 Lesson 12: How Strong Is It? Team Campaign\u2014data Correlation coefficient, strength of trend 351 16 LAB 4D: Interpreting Correlations Team Campaign\u2014data Use correlation coefficient to determine best model 353 17 Lesson 13: Improving Your Model Team Campaign\u2014data Non-linear regression 355 18 LAB 4E: Some Models Have Curves Team Campaign\u2014data Non-linear regression 357 Piecing it Together (4 days) 19 Lesson 14: More Variables to Make Better Predictions Team Campaign\u2014data Multiple linear regression 359 20 Lesson 15: Combination of Variables Team Campaign\u2014data Multiple linear regression 361 21 LAB 4F: This Model Is Big Enough for All of Us Team Campaign\u2014data Multiple linear regression 364 22 Practicum: Predictions Team Campaign\u2014data Linear regression 365 Decisions, Decisions! (3 days) 23 Lesson 16: Footbal or Futbol? Team Campaign\u2014data Multiple predictors, classifying into groups, decision trees 369 24 Lesson 17: Grow Your Own Decision Tree Team Campaign\u2014data Decision trees based on training and test data 371 25 LAB 4G: Growing Trees Team Campaign\u2014data Decision trees to classify observations 374 Ties that Bind (3 days) 26 Lesson 18: Where Do I Belong? Team Campaign\u2014data Clustering, k-means 379 27 LAB 4H: Finding Clusters Team Campaign\u2014data Clustering, k-means 382 28 Lesson 19: Our Class Network Team Campaign\u2014data Clustering, networks 385 End of Unit Project (7 days) 29- 36 End of Unit 4 Modeling Activity Project and Presentation Team Campaign Synthesis of above 388 ^=Data collection window begins. +=Data collection window ends.","title":"Daily Overview"},{"location":"unit4/overview/#introduction-to-data-science-daily-overview-unit-4","text":"","title":"Introduction to Data Science Daily Overview: Unit 4"},{"location":"unit4/overview/#daily-overview-unit-4","text":".tg {border-collapse:collapse;border-spacing:0;} .tg td{font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;} .tg th{font-family:Arial, sans-serif;font-size:14px;font-weight:normal;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;} .tg .tg-88nc{font-weight:bold;border-color:inherit;text-align:center} .tg .tg-c3ow{border-color:inherit;text-align:center;vertical-align:top} .tg .tg-uys7{border-color:inherit;text-align:center} .tg .tg-pwj7{background-color:#efefef;border-color:inherit;text-align:left} .tg .tg-5e9r{background-color:#efefef;border-color:inherit;text-align:center} .tg .tg-xldj{border-color:inherit;text-align:left} .tg .tg-0pky{border-color:inherit;text-align:left;vertical-align:top} Theme Day Lessons and Labs Campaign Topics Page Campaigns and Community (5 days) 1 Lesson 1: Trash Modeling to answer real world problems, official data sets 315 2 Lesson 2: Drought Exploratory data analysis, campaign creation 319 3 Lesson 3: Community Connection Team Campaign\u2014data Community topic research, campaign creation 321 4 Lesson 4: Evaluate and Implement the Campaign Team Campaign\u2014data Statistical questions, evaluate & mock implement campaign 323 5 Lesson 5: Refine and Create the Campaign Team Campaign\u2014data Revise and edit campaign, data collection 325 Predictions and Models (13 days) 6 Lesson 6: Statistical Predictions Using One Variable Team Campaign\u2014data One-variable predictions using a rule 328 7 Lesson 7: Statistical Predictions by Applying the Rule Team Campaign\u2014data Predictions applying mean square deviation, mean absolute error 333 8 Lesson 8: Statistical Predictions Using Two Variables Team Campaign\u2014data Two-variable statistical predictions, scatterplots 335 9 Lesson 9: The Spaghetti Line Team Campaign\u2014data Estimate line of best fit, single linear regression 337 10 LAB 4A: If the Line Fits\u2026 Team Campaign\u2014data Estimate line of best fit 339 11 Lesson 10: What\u2019s the Best Line? Team Campaign\u2014data Predictions based on linear models 343 12 LAB 4B: What\u2019s the Score? Team Campaign\u2014data Comparing predictions to real data 345 13 LAB 4C: Cross-Validation Team Campaign\u2014data Use training and test data for predictions 346 14 Lesson 11: What\u2019s the Trend? Team Campaign\u2014data Trend, associations, linear model 348 15 Lesson 12: How Strong Is It? Team Campaign\u2014data Correlation coefficient, strength of trend 351 16 LAB 4D: Interpreting Correlations Team Campaign\u2014data Use correlation coefficient to determine best model 353 17 Lesson 13: Improving Your Model Team Campaign\u2014data Non-linear regression 355 18 LAB 4E: Some Models Have Curves Team Campaign\u2014data Non-linear regression 357 Piecing it Together (4 days) 19 Lesson 14: More Variables to Make Better Predictions Team Campaign\u2014data Multiple linear regression 359 20 Lesson 15: Combination of Variables Team Campaign\u2014data Multiple linear regression 361 21 LAB 4F: This Model Is Big Enough for All of Us Team Campaign\u2014data Multiple linear regression 364 22 Practicum: Predictions Team Campaign\u2014data Linear regression 365 Decisions, Decisions! (3 days) 23 Lesson 16: Footbal or Futbol? Team Campaign\u2014data Multiple predictors, classifying into groups, decision trees 369 24 Lesson 17: Grow Your Own Decision Tree Team Campaign\u2014data Decision trees based on training and test data 371 25 LAB 4G: Growing Trees Team Campaign\u2014data Decision trees to classify observations 374 Ties that Bind (3 days) 26 Lesson 18: Where Do I Belong? Team Campaign\u2014data Clustering, k-means 379 27 LAB 4H: Finding Clusters Team Campaign\u2014data Clustering, k-means 382 28 Lesson 19: Our Class Network Team Campaign\u2014data Clustering, networks 385 End of Unit Project (7 days) 29- 36 End of Unit 4 Modeling Activity Project and Presentation Team Campaign Synthesis of above 388 ^=Data collection window begins. +=Data collection window ends.","title":"Daily Overview: Unit 4"},{"location":"unit4/practicum1/","text":"Practicum: Predictions Objective: Students will create a linear model to predict the nutritional component that is most closely associated with the amount of sugar contained in a cereal. Materials: Predictions Practicum ( LMR_U4_Practicum_Predictions ) Practicum Predictions Data about the nutritional components of popular cereal brands has been collected and made available for your team\u2019s use. We are interested in determining which other nutritional component is most closely associated with the amount of sugar contained in a cereal. Your team will use the data to make predictions using linear models and compare the accuracy of your model to the rest of your classmates. Finally, the class will determine which team had the best prediction. Follow the directions below to explore and analyze the data: You will have two data sets: one training named cereal and one test named cereal_test . Load both data sets. Write down the code you used. Explore the training data. Which variable do you think is the best predictor of sugar? Choose at least 3 variables, make a plot for each one, and fit a linear regression line through each of them. Select the model that you think best makes the best prediction. For the linear model your team selected: a. Describe what the plot shows. b. Explain why you selected that particular model. c. Compute the mean squared error of your model using your test data. d. Now make a set of predictions with your test data. Calculate the mean squared error for the test data. Is it better or worse than for the training data, or about the same? Present your team\u2019s linear model to the class. Explain why you chose your model and the typical amount of error in its predictions. Give an example of a prediction for one value of x. State that value, give the predicted sugar, and describe, based on the testing data, how far off your prediction might actually be.","title":"Practicum: Predictions"},{"location":"unit4/practicum1/#practicum-predictions","text":"","title":"Practicum: Predictions"},{"location":"unit4/practicum1/#objective","text":"Students will create a linear model to predict the nutritional component that is most closely associated with the amount of sugar contained in a cereal.","title":"Objective:"},{"location":"unit4/practicum1/#materials","text":"Predictions Practicum ( LMR_U4_Practicum_Predictions ) Practicum Predictions Data about the nutritional components of popular cereal brands has been collected and made available for your team\u2019s use. We are interested in determining which other nutritional component is most closely associated with the amount of sugar contained in a cereal. Your team will use the data to make predictions using linear models and compare the accuracy of your model to the rest of your classmates. Finally, the class will determine which team had the best prediction. Follow the directions below to explore and analyze the data: You will have two data sets: one training named cereal and one test named cereal_test . Load both data sets. Write down the code you used. Explore the training data. Which variable do you think is the best predictor of sugar? Choose at least 3 variables, make a plot for each one, and fit a linear regression line through each of them. Select the model that you think best makes the best prediction. For the linear model your team selected: a. Describe what the plot shows. b. Explain why you selected that particular model. c. Compute the mean squared error of your model using your test data. d. Now make a set of predictions with your test data. Calculate the mean squared error for the test data. Is it better or worse than for the training data, or about the same? Present your team\u2019s linear model to the class. Explain why you chose your model and the typical amount of error in its predictions. Give an example of a prediction for one value of x. State that value, give the predicted sugar, and describe, based on the testing data, how far off your prediction might actually be.","title":"Materials:"},{"location":"unit4/section1/","text":"Unit4, Section1: Campaigns and Community Instructional Days: 5 Enduring Understandings Modeling Activities are posed as open-ended problems that are designed to challenge students to build models in order to solve complex, real-world problems. They may be used to engage students in statistical reasoning and provide a means to better understand students' thinking. Engagement Students will watch a video called Fighting Pollution Through Data . This video will set the context of the real-world problem facing many cities around the world \u2014 trash. The video provides background information as well as a baseline topic to launch ideas for the modeling process. The video can be found at: https://www.youtube.com/watch?v=xOYAIXjHveA Learning Objectives Statistical/Mathematical: According to the California Common Core State Standards-Mathematics (CCSS-M) Framework: \u201cModeling links classroom mathematics and statistics to everyday life, work, and decision-making. Modeling is the process of choosing and using appropriate mathematics and statistics to analyze empirical situations, to understand them better, and to improve decisions. Quantities and their relationships in physical, economic, public policy, social, and everyday situations can be modeled using mathematical and statistical methods. When making mathematical models, technology is valuable for varying assumptions, exploring consequences, and comparing predictions with data.\" Focus Standards for Mathematical Practice: SMP-4: Model with mathematics. Data Science: Students will apply the conceptual understandings learned up to this point in the curriculum. Applied Computational Thinking using RStudio: \u2022 Create a Participatory Sensing campaign using a campaign Authoring Tool. Real-World Connections: Engineers, data scientists, and statisticians, to name a few, use modeling in their everyday work. Whether it is for creating a scale model of a bridge or a mathematical model of force impact measures, modeling is an integral part of what they do in the real world. Language Objectives Students will use complex sentences to construct summary statements about their understanding of data, how it is collected, how it used, and how to work with it. Students will engage in partner and whole group discussions to express their understanding of data science concepts. Students will read informative texts to evaluate claims based on data. Data File or Data Collection Method Data File: Trash: data(trash) Legend for Activity Icons","title":"Campaigns and Community"},{"location":"unit4/section1/#unit4-section1-campaigns-and-community","text":"Instructional Days: 5","title":"Unit4, Section1: Campaigns and Community"},{"location":"unit4/section1/#enduring-understandings","text":"Modeling Activities are posed as open-ended problems that are designed to challenge students to build models in order to solve complex, real-world problems. They may be used to engage students in statistical reasoning and provide a means to better understand students' thinking.","title":"Enduring Understandings"},{"location":"unit4/section1/#engagement","text":"Students will watch a video called Fighting Pollution Through Data . This video will set the context of the real-world problem facing many cities around the world \u2014 trash. The video provides background information as well as a baseline topic to launch ideas for the modeling process. The video can be found at: https://www.youtube.com/watch?v=xOYAIXjHveA","title":"Engagement"},{"location":"unit4/section1/#learning-objectives","text":"Statistical/Mathematical: According to the California Common Core State Standards-Mathematics (CCSS-M) Framework: \u201cModeling links classroom mathematics and statistics to everyday life, work, and decision-making. Modeling is the process of choosing and using appropriate mathematics and statistics to analyze empirical situations, to understand them better, and to improve decisions. Quantities and their relationships in physical, economic, public policy, social, and everyday situations can be modeled using mathematical and statistical methods. When making mathematical models, technology is valuable for varying assumptions, exploring consequences, and comparing predictions with data.\" Focus Standards for Mathematical Practice: SMP-4: Model with mathematics. Data Science: Students will apply the conceptual understandings learned up to this point in the curriculum. Applied Computational Thinking using RStudio: \u2022 Create a Participatory Sensing campaign using a campaign Authoring Tool. Real-World Connections: Engineers, data scientists, and statisticians, to name a few, use modeling in their everyday work. Whether it is for creating a scale model of a bridge or a mathematical model of force impact measures, modeling is an integral part of what they do in the real world.","title":"Learning Objectives"},{"location":"unit4/section1/#language-objectives","text":"Students will use complex sentences to construct summary statements about their understanding of data, how it is collected, how it used, and how to work with it. Students will engage in partner and whole group discussions to express their understanding of data science concepts. Students will read informative texts to evaluate claims based on data.","title":"Language Objectives"},{"location":"unit4/section1/#data-file-or-data-collection-method","text":"Data File: Trash: data(trash)","title":"Data File or Data Collection Method"},{"location":"unit4/section1/#legend-for-activity-icons","text":"","title":"Legend for Activity Icons"},{"location":"unit4/section2/","text":"Unit4, Section2: Predictions and Models Instructional Days: 16 Enduring Understandings The regression line is a prediction machine. We give it an x-value, it gives us a predicted y-value. The regression line summarizes the trend in the data, but there may still remain variability in the dependent variable that is not explained by the independent variable. Although the regression line provides optimal predictions when the association is linear, other models are needed for when it is not linear. Engagement Students will will explore and make predictions with a dataset consisting of arm span and height values from a group of Los Angeles high school students. The Arm Span vs. Height data allows for a real-world connection while learning about linear models and predictions. They will engage in multiple discussions as they build their understanding of linear models, refine how they make their predictions, and test the accuracy of those predictions. Learning Objectives Statistical/Mathematical: S-ID 6: Represent data on two quantitative variables on a scatter plot, and describe how the variables are related. a. Fit a function to the data; use functions fitted to data to solve problems in the context of the data. Use given functions or choose a function suggested by the context. Emphasize linear models. b. Informally assess the fit of a function by plotting and analyzing residuals. c. Fit a linear function for a scatter plot that suggests a linear association. S-ID 7: Interpret the slope (rate of change) and the intercept (constant term) of a linear model in the context of the data. S-ID 8: Compute (using technology) and interpret the correlation coefficient of a linear fit. S-IC 6: Evaluate reports based on data.* *This standard is woven throughout the course. It is a recurring standard for every unit. Focus Standards for Mathematical Practice for All of Unit 4: SMP-2: Reason abstractly and quantitatively. SMP-4: Model with mathematics. SMP-7: Look for and make use of structure. Data Science: Judge whether or not the linear model is appropriate. Learn to interpret a correlation coefficient in a linear model and interpret slope and intercept. Evaluate the strength of a linear association. Evaluate the potential error in a linear model. Applied Computational Thinking using RStudio: \u2022 Use linear regression models to predict response values based on sets of predictors. \u2022 Fit a regression line to data and predict outcomes. \u2022 Compute the correlation coefficient of a linear model. Real-World Connections: Many studies are published in which predictions are made, and media reports often cite data that make predictions. They involve one or more explanatory variable and a response variable, such as income vs. education, weight vs. exercise, and cost of insurance vs. age. Understanding linear regression helps evaluate these studies and reports. Language Objectives Students will use complex sentences to construct summary statements about their understanding of Mean Squared Error and Mean Absolute Error. Students will engage in partner and whole group discussions to express their understanding of linear regression and how to measure its accuracy. Students will use mathematical vocabulary to explain orally and in writing the attributes of various scatterplots. Students will make connections, in writing, between predictions using different types of models (i.e., linear, quadratic, cubic). Data File or Data Collection Method Data File: Arm Spans vs. Heights: data(arm_span) Movies: data(movie) Data Collection: Students will collect data for their Team Participatory Sensing campaign. Legend for Activity Icons","title":"Predictions and Models"},{"location":"unit4/section2/#unit4-section2-predictions-and-models","text":"Instructional Days: 16","title":"Unit4, Section2: Predictions and Models"},{"location":"unit4/section2/#enduring-understandings","text":"The regression line is a prediction machine. We give it an x-value, it gives us a predicted y-value. The regression line summarizes the trend in the data, but there may still remain variability in the dependent variable that is not explained by the independent variable. Although the regression line provides optimal predictions when the association is linear, other models are needed for when it is not linear.","title":"Enduring Understandings"},{"location":"unit4/section2/#engagement","text":"Students will will explore and make predictions with a dataset consisting of arm span and height values from a group of Los Angeles high school students. The Arm Span vs. Height data allows for a real-world connection while learning about linear models and predictions. They will engage in multiple discussions as they build their understanding of linear models, refine how they make their predictions, and test the accuracy of those predictions.","title":"Engagement"},{"location":"unit4/section2/#learning-objectives","text":"Statistical/Mathematical: S-ID 6: Represent data on two quantitative variables on a scatter plot, and describe how the variables are related. a. Fit a function to the data; use functions fitted to data to solve problems in the context of the data. Use given functions or choose a function suggested by the context. Emphasize linear models. b. Informally assess the fit of a function by plotting and analyzing residuals. c. Fit a linear function for a scatter plot that suggests a linear association. S-ID 7: Interpret the slope (rate of change) and the intercept (constant term) of a linear model in the context of the data. S-ID 8: Compute (using technology) and interpret the correlation coefficient of a linear fit. S-IC 6: Evaluate reports based on data.* *This standard is woven throughout the course. It is a recurring standard for every unit. Focus Standards for Mathematical Practice for All of Unit 4: SMP-2: Reason abstractly and quantitatively. SMP-4: Model with mathematics. SMP-7: Look for and make use of structure. Data Science: Judge whether or not the linear model is appropriate. Learn to interpret a correlation coefficient in a linear model and interpret slope and intercept. Evaluate the strength of a linear association. Evaluate the potential error in a linear model. Applied Computational Thinking using RStudio: \u2022 Use linear regression models to predict response values based on sets of predictors. \u2022 Fit a regression line to data and predict outcomes. \u2022 Compute the correlation coefficient of a linear model. Real-World Connections: Many studies are published in which predictions are made, and media reports often cite data that make predictions. They involve one or more explanatory variable and a response variable, such as income vs. education, weight vs. exercise, and cost of insurance vs. age. Understanding linear regression helps evaluate these studies and reports.","title":"Learning Objectives"},{"location":"unit4/section2/#language-objectives","text":"Students will use complex sentences to construct summary statements about their understanding of Mean Squared Error and Mean Absolute Error. Students will engage in partner and whole group discussions to express their understanding of linear regression and how to measure its accuracy. Students will use mathematical vocabulary to explain orally and in writing the attributes of various scatterplots. Students will make connections, in writing, between predictions using different types of models (i.e., linear, quadratic, cubic).","title":"Language Objectives"},{"location":"unit4/section2/#data-file-or-data-collection-method","text":"Data File: Arm Spans vs. Heights: data(arm_span) Movies: data(movie) Data Collection: Students will collect data for their Team Participatory Sensing campaign.","title":"Data File or Data Collection Method"},{"location":"unit4/section2/#legend-for-activity-icons","text":"","title":"Legend for Activity Icons"},{"location":"unit4/section3/","text":"Unit4, Section3: Piecing it Together Instructional Days: 5 Enduring Understandings Real-life phenomena are often complex. Data scientists use multiple regression models to create simple equations to help explain and predict these phenomena. Data scientists can also use polynomial transformations to add flexibility to rigid linear models. Engagement Students will read the article titled How Long Can a Spinoff Like Better Call Saul Last? that will set the context for students to begin thinking about more than one explanatory variable to make better predictions. The article can be found at: http://fivethirtyeight.com/features/how-long-can-a-spinoff-like-better-call-saul-last/ Learning Objectives Statistical/Mathematical: S-ID 6: Represent data on two quantitative variables on a scatter plot, and describe how the variables are related. a. Fit a function to the data; use functions fitted to data to solve problems in the context of the data. Use given functions or choose a function suggested by the context. Emphasize linear models. Data Science: Understand that multiple regression can be a better tool for predicting that simple linear regression and know when it is appropriate to use multiple regression versus simple linear regression. Understand when linear models are not appropriate based on the shape of the scatterplot. Applied Computational Thinking using RStudio: \u2022 Use multiple linear regression models with other predictor variables \u2022 Fit regression lines to data and predict outcomes. \u2022 Fit polynomials functions to data. Real-World Connections: Economists and marketing firms use multiple regression to predict changes in the market and adjust strategies to fit the demands of changes in the marketplace. Language Objectives Students will read informative texts to evaluate claims based on data. Students will engage in partner and whole group discussions about how adding variables to a model will help or hinder its predictions. Students will construct their own linear model using multiple variables to compare and contrast which model makes the best predictions. Data File or Data Collection Method Data File: Movies: data(movie) Cereal brands: data(cereal) Data Collection: Students will collect data for their Team Participatory Sensing campaign. Legend for Activity Icons","title":"Piecing it Together"},{"location":"unit4/section3/#unit4-section3-piecing-it-together","text":"Instructional Days: 5","title":"Unit4, Section3: Piecing it Together"},{"location":"unit4/section3/#enduring-understandings","text":"Real-life phenomena are often complex. Data scientists use multiple regression models to create simple equations to help explain and predict these phenomena. Data scientists can also use polynomial transformations to add flexibility to rigid linear models.","title":"Enduring Understandings"},{"location":"unit4/section3/#engagement","text":"Students will read the article titled How Long Can a Spinoff Like Better Call Saul Last? that will set the context for students to begin thinking about more than one explanatory variable to make better predictions. The article can be found at: http://fivethirtyeight.com/features/how-long-can-a-spinoff-like-better-call-saul-last/","title":"Engagement"},{"location":"unit4/section3/#learning-objectives","text":"Statistical/Mathematical: S-ID 6: Represent data on two quantitative variables on a scatter plot, and describe how the variables are related. a. Fit a function to the data; use functions fitted to data to solve problems in the context of the data. Use given functions or choose a function suggested by the context. Emphasize linear models. Data Science: Understand that multiple regression can be a better tool for predicting that simple linear regression and know when it is appropriate to use multiple regression versus simple linear regression. Understand when linear models are not appropriate based on the shape of the scatterplot. Applied Computational Thinking using RStudio: \u2022 Use multiple linear regression models with other predictor variables \u2022 Fit regression lines to data and predict outcomes. \u2022 Fit polynomials functions to data. Real-World Connections: Economists and marketing firms use multiple regression to predict changes in the market and adjust strategies to fit the demands of changes in the marketplace.","title":"Learning Objectives"},{"location":"unit4/section3/#language-objectives","text":"Students will read informative texts to evaluate claims based on data. Students will engage in partner and whole group discussions about how adding variables to a model will help or hinder its predictions. Students will construct their own linear model using multiple variables to compare and contrast which model makes the best predictions.","title":"Language Objectives"},{"location":"unit4/section3/#data-file-or-data-collection-method","text":"Data File: Movies: data(movie) Cereal brands: data(cereal) Data Collection: Students will collect data for their Team Participatory Sensing campaign.","title":"Data File or Data Collection Method"},{"location":"unit4/section3/#legend-for-activity-icons","text":"","title":"Legend for Activity Icons"},{"location":"unit4/section4/","text":"Unit4, Section4: Decisions, Decisions Instructional Days: 3 Enduring Understandings Decision trees are used to classify observations into similar groupings based on known characteristics. Questions are asked, then the observations are sorted based on the responses to the questions. After a specified number of iterations, a final group membership is decided. One particular modeling tool we use for decision trees is known as CART (Classification and Regression Trees). Engagement Students will be presented with the question about whether they would rather trust a doctor or a data scientist to diagnose them if they were having a chest pains. This will set the context for decision trees and how they are used to make predictions. Learning Objectives Statistical/Mathematical: S-IC 2: Decide if a specified model is consistent with results from a given data-generating process, e.g., using simulation. Data Science: Understand that classification and regression trees can be used to predict membership in groups. Applied Computational Thinking using RStudio: \u2022 Create classification and regression trees. Real-World Connections: Cardiologists may use a decision tree to diagnose whether people are or are not having a heart attack. Since the late 1870\u2019s, this method has been found to correctly diagnose a heart attack in over 95% of cases compared to correct diagnoses based on individual doctors\u2019 expertise, which ranged between 75 and 90%. Language Objectives Students will engage in partner and whole group discussions to express their understanding of classification trees. Students will explain orally and in writing how to determine the accuracy of their non-linear model. Students will make connections between decision trees and linear models in writing. Data File or Data Collection Method Dataset: USMNT/NFL dataset Data File: Titanic: data(titanic) Data Collection: Students will collect data for their Team Participatory Sensing campaign. Legend for Activity Icons","title":"Decisions, Decisions!"},{"location":"unit4/section4/#unit4-section4-decisions-decisions","text":"Instructional Days: 3","title":"Unit4, Section4: Decisions, Decisions"},{"location":"unit4/section4/#enduring-understandings","text":"Decision trees are used to classify observations into similar groupings based on known characteristics. Questions are asked, then the observations are sorted based on the responses to the questions. After a specified number of iterations, a final group membership is decided. One particular modeling tool we use for decision trees is known as CART (Classification and Regression Trees).","title":"Enduring Understandings"},{"location":"unit4/section4/#engagement","text":"Students will be presented with the question about whether they would rather trust a doctor or a data scientist to diagnose them if they were having a chest pains. This will set the context for decision trees and how they are used to make predictions.","title":"Engagement"},{"location":"unit4/section4/#learning-objectives","text":"Statistical/Mathematical: S-IC 2: Decide if a specified model is consistent with results from a given data-generating process, e.g., using simulation. Data Science: Understand that classification and regression trees can be used to predict membership in groups. Applied Computational Thinking using RStudio: \u2022 Create classification and regression trees. Real-World Connections: Cardiologists may use a decision tree to diagnose whether people are or are not having a heart attack. Since the late 1870\u2019s, this method has been found to correctly diagnose a heart attack in over 95% of cases compared to correct diagnoses based on individual doctors\u2019 expertise, which ranged between 75 and 90%.","title":"Learning Objectives"},{"location":"unit4/section4/#language-objectives","text":"Students will engage in partner and whole group discussions to express their understanding of classification trees. Students will explain orally and in writing how to determine the accuracy of their non-linear model. Students will make connections between decision trees and linear models in writing.","title":"Language Objectives"},{"location":"unit4/section4/#data-file-or-data-collection-method","text":"Dataset: USMNT/NFL dataset Data File: Titanic: data(titanic) Data Collection: Students will collect data for their Team Participatory Sensing campaign.","title":"Data File or Data Collection Method"},{"location":"unit4/section4/#legend-for-activity-icons","text":"","title":"Legend for Activity Icons"},{"location":"unit4/section5/","text":"Unit4, Section5: Ties that Bind Instructional Days: 3 Enduring Understandings Clustering is another way to classify data into groups. We classify observations based on numerical characteristics and their similarities. We use k-means to determine the mean value for each group of k clusters by randomly assigning an initial value for the mean and then moving the mean based on its proximity to the points. Networks classify people into groupings based on who knows whom. Nodes are formed when a relationship between two people is present. Engagement Students will determine which points in a plot should be grouped as football players and which points should be grouped as swimmers based on clustering of characteristics. Learning Objectives Statistical/Mathematical: S-IC 2: Decide if a specified model is consistent with results from a given data-generating process, e.g., using simulation. Data Science: Understand what RStudio is doing when using the k-means function to find clusters in a group of data and when creating networks in order to learn how to classify data into groups. Applied Computational Thinking using RStudio: \u2022 Use the k-means function to find clusters in a group of data. \u2022 Plot the data with the cluster assignments based on the k-means function. Real-World Connections: Network analysis is used by many private and public entities such as the National Security Agency when they want to find terrorist networks to have maximum impact on communications. The k-means algorithm is a technique for grouping entities according to the similarity of their attributes. For example, dividing countries into similar groups using k-means to make fair comparisons is applicable. Language Objectives Students will write, in their own words, an explanation of k-means clustering. Students will describe the differences between time spent on videogames and time spent on homework, from their own class data. Students will create visualizations and numerical summaries to explain and justify, orally and in writing, a recommendation to better their community. Data File or Data Collection Method Data File: USMNT and NFL: data(titanic) Students' TimeUse campaign data Data Collection: Students will collect data for their Team Participatory Sensing campaign. Legend for Activity Icons","title":"Ties That Bind"},{"location":"unit4/section5/#unit4-section5-ties-that-bind","text":"Instructional Days: 3","title":"Unit4, Section5: Ties that Bind"},{"location":"unit4/section5/#enduring-understandings","text":"Clustering is another way to classify data into groups. We classify observations based on numerical characteristics and their similarities. We use k-means to determine the mean value for each group of k clusters by randomly assigning an initial value for the mean and then moving the mean based on its proximity to the points. Networks classify people into groupings based on who knows whom. Nodes are formed when a relationship between two people is present.","title":"Enduring Understandings"},{"location":"unit4/section5/#engagement","text":"Students will determine which points in a plot should be grouped as football players and which points should be grouped as swimmers based on clustering of characteristics.","title":"Engagement"},{"location":"unit4/section5/#learning-objectives","text":"Statistical/Mathematical: S-IC 2: Decide if a specified model is consistent with results from a given data-generating process, e.g., using simulation. Data Science: Understand what RStudio is doing when using the k-means function to find clusters in a group of data and when creating networks in order to learn how to classify data into groups. Applied Computational Thinking using RStudio: \u2022 Use the k-means function to find clusters in a group of data. \u2022 Plot the data with the cluster assignments based on the k-means function. Real-World Connections: Network analysis is used by many private and public entities such as the National Security Agency when they want to find terrorist networks to have maximum impact on communications. The k-means algorithm is a technique for grouping entities according to the similarity of their attributes. For example, dividing countries into similar groups using k-means to make fair comparisons is applicable.","title":"Learning Objectives"},{"location":"unit4/section5/#language-objectives","text":"Students will write, in their own words, an explanation of k-means clustering. Students will describe the differences between time spent on videogames and time spent on homework, from their own class data. Students will create visualizations and numerical summaries to explain and justify, orally and in writing, a recommendation to better their community.","title":"Language Objectives"},{"location":"unit4/section5/#data-file-or-data-collection-method","text":"Data File: USMNT and NFL: data(titanic) Students' TimeUse campaign data Data Collection: Students will collect data for their Team Participatory Sensing campaign.","title":"Data File or Data Collection Method"},{"location":"unit4/section5/#legend-for-activity-icons","text":"","title":"Legend for Activity Icons"},{"location":"vocabulary/unit1/","text":"algorithm a process or set of rules for solving a mathematical problem bimodal a distribution which has two peaks bin widths the width of the rectangle with shows data is graphed in groups on the x-axis bin(s) a bar whose height corresponds to how many data points are in that bin campaign gather and collect data categorical variables values that have words center useful for numerical variables, the center of the distribution often corresponds to our notion of \u2018typical value\u2019 claim a statement of something collect the process of gathering and measuring information columns a structured data item in a table conditional relative frequency the ratio of a joint relative frequency and related marginal relative frequency console a pane within RStudio; the place where RStudio is waiting for you to tell it what to do, and where it will show the results of a command; you type your codes directly into the console data Data are information, or observations, that have been gathered and recorded data analysis tables, graphs, and summaries of the data that are produced to help us find patterns and relationships data collection the process of observing and recording data, or of examining previously collected data to make sure it meets the needs of an investigation data cycle a guide we can use when learning to think about data data interpretation the statistical questions are answered by referring to the tables, graphs, and summaries made in the Data Analysis phase data point a single fact or piece of information dataset(s) a collection of data data table arrangement of data data trails the data collected about us as individuals that could be used to see the patterns in our personal lives distribution a function or a listing which shows all the possible values dotplot a graphical display of data using dots environment a pane within RStudio; where values and objects can be viewed ethics a code of behavior, specifically what is right and wrong evaluate to think carefully frequency the number of times an outcome occurs GPS stands for Global Positioning System; it is a radio navigation system that allows land, sea, and airborne users to determine their exact location grouping when the data are split into categories histogram an approximate representation of the distribution of numerical data images a representation of the external form of a person, thing, or picture input the value you place into the algorithm joint (relative) frequency a fraction that tells you how many members of a group have a particular characteristic left-hand rule when multiple data points can appear in more than one bin, observations would go in the bin on the left-hand side left-skewed the mean is typically less than the median; the tail of the distribution is longer on the left-hand side than on the right-hand side marginal (relative) frequency the margins on the table that show the cells with the initial total counts maximum the largest value minimum the smallest value numerical variables values that have numbers observations Data that have been gathered and recorded organize the method of classifying and organizing data sets to make them more useful output the value(s) that are produced by an algorithm pane a rectangular area within RStudio participatory sensing an approach to data collection and interpretation in which individuals, acting alone or in groups, use their personal mobile devices and web services to explore interesting aspects of their worlds ranging from health to culture photo ethics the principles that guide how we take and share photographs plot a pane within RStudio; where plots/graphs/visualizations will be generated preview a pane within RStudio; (spreadsheet) - where they will be able to see the variables and observations (index); rows and columns of data privacy the right of individuals to have control over how their personal information is collected and used range the largest value minus the smallest value record a collection of data rectangular or spreadsheet format information that is stored in a rectangular or spreadsheet format representations the form in which data are stored, processed, and transmitted right-hand rule when multiple data points can appear in more than one bin, observations would go in the bin on the right-hand side right-skewed the mean is typicallygreater than the median; the tail of the distribution is longer on the right-hand side than on the left-hand side rows a structured data item in a table scatterplot a plot that uses dots to represent values for two different numeric variables shape the placement of points in a distribution side-by-side bar plot a plot where the bars are split into colored bar segments, used to compare things between different groups or to track changes over time spread how dense the distribution is at certain values statistical investigative questions questions that address variability and can be answered with data surveys a research method used for collecting data to gain information and insights into various topics of interest symmetric a type of distribution where the left side of the distribution mirrors the right side two-way frequency table a table that displays the data that pertains to two categories from one group typical \u201cmean\u201d or \u201caverage\u201d; expected values unimodal a distribution which has a single peak variability how spread out a set of data is; variability gives you a way to describe how much data sets vary and allows you to compare your data to other sets of data variables characteristics of an object or person visualization a picture of the data x-axis horizontal axis of a coordinate plane y-axis vertical axis of a coordinate plane","title":"Unit 1 Vocabulary"},{"location":"vocabulary/unit1/#algorithm","text":"a process or set of rules for solving a mathematical problem","title":"algorithm"},{"location":"vocabulary/unit1/#bimodal","text":"a distribution which has two peaks","title":"bimodal"},{"location":"vocabulary/unit1/#bin-widths","text":"the width of the rectangle with shows data is graphed in groups on the x-axis","title":"bin widths"},{"location":"vocabulary/unit1/#bins","text":"a bar whose height corresponds to how many data points are in that bin","title":"bin(s)"},{"location":"vocabulary/unit1/#campaign","text":"gather and collect data","title":"campaign"},{"location":"vocabulary/unit1/#categorical-variables","text":"values that have words","title":"categorical variables"},{"location":"vocabulary/unit1/#center","text":"useful for numerical variables, the center of the distribution often corresponds to our notion of \u2018typical value\u2019","title":"center"},{"location":"vocabulary/unit1/#claim","text":"a statement of something","title":"claim"},{"location":"vocabulary/unit1/#collect","text":"the process of gathering and measuring information","title":"collect"},{"location":"vocabulary/unit1/#columns","text":"a structured data item in a table","title":"columns"},{"location":"vocabulary/unit1/#conditional-relative-frequency","text":"the ratio of a joint relative frequency and related marginal relative frequency","title":"conditional relative frequency"},{"location":"vocabulary/unit1/#console","text":"a pane within RStudio; the place where RStudio is waiting for you to tell it what to do, and where it will show the results of a command; you type your codes directly into the console","title":"console"},{"location":"vocabulary/unit1/#data","text":"Data are information, or observations, that have been gathered and recorded","title":"data"},{"location":"vocabulary/unit1/#data-analysis","text":"tables, graphs, and summaries of the data that are produced to help us find patterns and relationships","title":"data analysis"},{"location":"vocabulary/unit1/#data-collection","text":"the process of observing and recording data, or of examining previously collected data to make sure it meets the needs of an investigation","title":"data collection"},{"location":"vocabulary/unit1/#data-cycle","text":"a guide we can use when learning to think about data","title":"data cycle"},{"location":"vocabulary/unit1/#data-interpretation","text":"the statistical questions are answered by referring to the tables, graphs, and summaries made in the Data Analysis phase","title":"data interpretation"},{"location":"vocabulary/unit1/#data-point","text":"a single fact or piece of information","title":"data point"},{"location":"vocabulary/unit1/#datasets","text":"a collection of data","title":"dataset(s)"},{"location":"vocabulary/unit1/#data-table","text":"arrangement of data","title":"data table"},{"location":"vocabulary/unit1/#data-trails","text":"the data collected about us as individuals that could be used to see the patterns in our personal lives","title":"data trails"},{"location":"vocabulary/unit1/#distribution","text":"a function or a listing which shows all the possible values","title":"distribution"},{"location":"vocabulary/unit1/#dotplot","text":"a graphical display of data using dots","title":"dotplot"},{"location":"vocabulary/unit1/#environment","text":"a pane within RStudio; where values and objects can be viewed","title":"environment"},{"location":"vocabulary/unit1/#ethics","text":"a code of behavior, specifically what is right and wrong","title":"ethics"},{"location":"vocabulary/unit1/#evaluate","text":"to think carefully","title":"evaluate"},{"location":"vocabulary/unit1/#frequency","text":"the number of times an outcome occurs","title":"frequency"},{"location":"vocabulary/unit1/#gps","text":"stands for Global Positioning System; it is a radio navigation system that allows land, sea, and airborne users to determine their exact location","title":"GPS"},{"location":"vocabulary/unit1/#grouping","text":"when the data are split into categories","title":"grouping"},{"location":"vocabulary/unit1/#histogram","text":"an approximate representation of the distribution of numerical data","title":"histogram"},{"location":"vocabulary/unit1/#images","text":"a representation of the external form of a person, thing, or picture","title":"images"},{"location":"vocabulary/unit1/#input","text":"the value you place into the algorithm","title":"input"},{"location":"vocabulary/unit1/#joint-relative-frequency","text":"a fraction that tells you how many members of a group have a particular characteristic","title":"joint (relative) frequency"},{"location":"vocabulary/unit1/#left-hand-rule","text":"when multiple data points can appear in more than one bin, observations would go in the bin on the left-hand side","title":"left-hand rule"},{"location":"vocabulary/unit1/#left-skewed","text":"the mean is typically less than the median; the tail of the distribution is longer on the left-hand side than on the right-hand side","title":"left-skewed"},{"location":"vocabulary/unit1/#marginal-relative-frequency","text":"the margins on the table that show the cells with the initial total counts","title":"marginal (relative) frequency"},{"location":"vocabulary/unit1/#maximum","text":"the largest value","title":"maximum"},{"location":"vocabulary/unit1/#minimum","text":"the smallest value","title":"minimum"},{"location":"vocabulary/unit1/#numerical-variables","text":"values that have numbers","title":"numerical variables"},{"location":"vocabulary/unit1/#observations","text":"Data that have been gathered and recorded","title":"observations"},{"location":"vocabulary/unit1/#organize","text":"the method of classifying and organizing data sets to make them more useful","title":"organize"},{"location":"vocabulary/unit1/#output","text":"the value(s) that are produced by an algorithm","title":"output"},{"location":"vocabulary/unit1/#pane","text":"a rectangular area within RStudio","title":"pane"},{"location":"vocabulary/unit1/#participatory-sensing","text":"an approach to data collection and interpretation in which individuals, acting alone or in groups, use their personal mobile devices and web services to explore interesting aspects of their worlds ranging from health to culture","title":"participatory sensing"},{"location":"vocabulary/unit1/#photo-ethics","text":"the principles that guide how we take and share photographs","title":"photo ethics"},{"location":"vocabulary/unit1/#plot","text":"a pane within RStudio; where plots/graphs/visualizations will be generated","title":"plot"},{"location":"vocabulary/unit1/#preview","text":"a pane within RStudio; (spreadsheet) - where they will be able to see the variables and observations (index); rows and columns of data","title":"preview"},{"location":"vocabulary/unit1/#privacy","text":"the right of individuals to have control over how their personal information is collected and used","title":"privacy"},{"location":"vocabulary/unit1/#range","text":"the largest value minus the smallest value","title":"range"},{"location":"vocabulary/unit1/#record","text":"a collection of data","title":"record"},{"location":"vocabulary/unit1/#rectangular-or-spreadsheet-format","text":"information that is stored in a rectangular or spreadsheet format","title":"rectangular or spreadsheet format"},{"location":"vocabulary/unit1/#representations","text":"the form in which data are stored, processed, and transmitted","title":"representations"},{"location":"vocabulary/unit1/#right-hand-rule","text":"when multiple data points can appear in more than one bin, observations would go in the bin on the right-hand side","title":"right-hand rule"},{"location":"vocabulary/unit1/#right-skewed","text":"the mean is typicallygreater than the median; the tail of the distribution is longer on the right-hand side than on the left-hand side","title":"right-skewed"},{"location":"vocabulary/unit1/#rows","text":"a structured data item in a table","title":"rows"},{"location":"vocabulary/unit1/#scatterplot","text":"a plot that uses dots to represent values for two different numeric variables","title":"scatterplot"},{"location":"vocabulary/unit1/#shape","text":"the placement of points in a distribution","title":"shape"},{"location":"vocabulary/unit1/#side-by-side-bar-plot","text":"a plot where the bars are split into colored bar segments, used to compare things between different groups or to track changes over time","title":"side-by-side bar plot"},{"location":"vocabulary/unit1/#spread","text":"how dense the distribution is at certain values","title":"spread"},{"location":"vocabulary/unit1/#statistical-investigative-questions","text":"questions that address variability and can be answered with data","title":"statistical investigative questions"},{"location":"vocabulary/unit1/#surveys","text":"a research method used for collecting data to gain information and insights into various topics of interest","title":"surveys"},{"location":"vocabulary/unit1/#symmetric","text":"a type of distribution where the left side of the distribution mirrors the right side","title":"symmetric"},{"location":"vocabulary/unit1/#two-way-frequency-table","text":"a table that displays the data that pertains to two categories from one group","title":"two-way frequency table"},{"location":"vocabulary/unit1/#typical","text":"\u201cmean\u201d or \u201caverage\u201d; expected values","title":"typical"},{"location":"vocabulary/unit1/#unimodal","text":"a distribution which has a single peak","title":"unimodal"},{"location":"vocabulary/unit1/#variability","text":"how spread out a set of data is; variability gives you a way to describe how much data sets vary and allows you to compare your data to other sets of data","title":"variability"},{"location":"vocabulary/unit1/#variables","text":"characteristics of an object or person","title":"variables"},{"location":"vocabulary/unit1/#visualization","text":"a picture of the data","title":"visualization"},{"location":"vocabulary/unit1/#x-axis","text":"horizontal axis of a coordinate plane","title":"x-axis"},{"location":"vocabulary/unit1/#y-axis","text":"vertical axis of a coordinate plane","title":"y-axis"},{"location":"vocabulary/unit2/","text":"average an alternative word for mean balancing point the point on a number line where the data distribution is balanced bell-shaped the bell shape that is created on a graph of a normal distribution bias the act of favoring one outcome over another boxplot A special type of diagram showing Quartiles 1, 2 and 3 (where the data can be split into quarters) in a box, with lines extending to the lowest and highest values chance the possibility that something will happen compound probabilities AND/OR probabilities; the likeliness of two independent events occurring deviation the act of departing from an established course or accepted standard Empirical Rule in a normal data set, virtually every piece of data will fall within three standard deviations of the mean event a set of possible outcomes resulting from a particular experiment first quartile (Q1) a number for which 25% of the data is less than that number five-number summary a set of numbers that provides information about a dataset; consist of minimum, first quartile, median, third quartile, and maximum independence if one event doesn't affect the outcome of another event interquartile range (IQR) the range from Quartile 1 to Quartile 3 maximum the largest value mean a calculated \"central\" value of a set of numbers, where you add the numbers and divide by how many there are mean of absolute deviations (MAD) the average distance between each data value and the mean measures of central tendency (or center) a central or typical value for a probability distribution measures of variability (or spread) how far away the data points tend to fall from the center median the middle value in a group of ordered observations merge to come together or combine minimum the smallest value model a way of representing real world situations so that predictions can be made normal curve curve or the graph is the common type of distribution for a variable normal distribution an arrangement of a data set in which most values cluster in the middle of the range percentage parts per 100; for each hundred; a fraction whose denominator (bottom) is 100 probability how likely it is that some event will occur proportion when two ratios (or fractions) are equal quantiles a term that can be used in place of percentiles because they represent a quantity of data that is lower than that value quartiles the values that divide a list of numbers into quarters randomness happening by chance; not able to be predicted range the largest value minus the smallest value rebuttal an opposing argument or debate representation the form in which data is stored, processed, and transmitted sample proportion the fraction of samples which were successes shuffle rearrange so as to occupy different random positions or to be in a different random order simulation a way of creating random events that are close to real-life situations without actually doing them standard deviation (SD) a measure of how spread out numbers are; the square root of the variance standardized score another name for z-score subset a set of which all the variables are contained in another set. third quartile (Q3) a number for which 75% of the data is less than that number typical \u201cmean\u201d or \u201caverage\u201d; expected values with replacement when a population element can be selected more than one time without replacement when a population element can be selected only one time z-score tells us how many standard deviations away from the mean an observation is","title":"Unit 2 Vocabulary"},{"location":"vocabulary/unit2/#average","text":"an alternative word for mean","title":"average"},{"location":"vocabulary/unit2/#balancing-point","text":"the point on a number line where the data distribution is balanced","title":"balancing point"},{"location":"vocabulary/unit2/#bell-shaped","text":"the bell shape that is created on a graph of a normal distribution","title":"bell-shaped"},{"location":"vocabulary/unit2/#bias","text":"the act of favoring one outcome over another","title":"bias"},{"location":"vocabulary/unit2/#boxplot","text":"A special type of diagram showing Quartiles 1, 2 and 3 (where the data can be split into quarters) in a box, with lines extending to the lowest and highest values","title":"boxplot"},{"location":"vocabulary/unit2/#chance","text":"the possibility that something will happen","title":"chance"},{"location":"vocabulary/unit2/#compound-probabilities","text":"AND/OR probabilities; the likeliness of two independent events occurring","title":"compound probabilities"},{"location":"vocabulary/unit2/#deviation","text":"the act of departing from an established course or accepted standard","title":"deviation"},{"location":"vocabulary/unit2/#empirical-rule","text":"in a normal data set, virtually every piece of data will fall within three standard deviations of the mean","title":"Empirical Rule"},{"location":"vocabulary/unit2/#event","text":"a set of possible outcomes resulting from a particular experiment","title":"event"},{"location":"vocabulary/unit2/#first-quartile-q1","text":"a number for which 25% of the data is less than that number","title":"first quartile (Q1)"},{"location":"vocabulary/unit2/#five-number-summary","text":"a set of numbers that provides information about a dataset; consist of minimum, first quartile, median, third quartile, and maximum","title":"five-number summary"},{"location":"vocabulary/unit2/#independence","text":"if one event doesn't affect the outcome of another event","title":"independence"},{"location":"vocabulary/unit2/#interquartile-range-iqr","text":"the range from Quartile 1 to Quartile 3","title":"interquartile range (IQR)"},{"location":"vocabulary/unit2/#maximum","text":"the largest value","title":"maximum"},{"location":"vocabulary/unit2/#mean","text":"a calculated \"central\" value of a set of numbers, where you add the numbers and divide by how many there are","title":"mean"},{"location":"vocabulary/unit2/#mean-of-absolute-deviations-mad","text":"the average distance between each data value and the mean","title":"mean of absolute deviations (MAD)"},{"location":"vocabulary/unit2/#measures-of-central-tendency-or-center","text":"a central or typical value for a probability distribution","title":"measures of central tendency (or center)"},{"location":"vocabulary/unit2/#measures-of-variability-or-spread","text":"how far away the data points tend to fall from the center","title":"measures of variability (or spread)"},{"location":"vocabulary/unit2/#median","text":"the middle value in a group of ordered observations","title":"median"},{"location":"vocabulary/unit2/#merge","text":"to come together or combine","title":"merge"},{"location":"vocabulary/unit2/#minimum","text":"the smallest value","title":"minimum"},{"location":"vocabulary/unit2/#model","text":"a way of representing real world situations so that predictions can be made","title":"model"},{"location":"vocabulary/unit2/#normal-curve","text":"curve or the graph is the common type of distribution for a variable","title":"normal curve"},{"location":"vocabulary/unit2/#normal-distribution","text":"an arrangement of a data set in which most values cluster in the middle of the range","title":"normal distribution"},{"location":"vocabulary/unit2/#percentage","text":"parts per 100; for each hundred; a fraction whose denominator (bottom) is 100","title":"percentage"},{"location":"vocabulary/unit2/#probability","text":"how likely it is that some event will occur","title":"probability"},{"location":"vocabulary/unit2/#proportion","text":"when two ratios (or fractions) are equal","title":"proportion"},{"location":"vocabulary/unit2/#quantiles","text":"a term that can be used in place of percentiles because they represent a quantity of data that is lower than that value","title":"quantiles"},{"location":"vocabulary/unit2/#quartiles","text":"the values that divide a list of numbers into quarters","title":"quartiles"},{"location":"vocabulary/unit2/#randomness","text":"happening by chance; not able to be predicted","title":"randomness"},{"location":"vocabulary/unit2/#range","text":"the largest value minus the smallest value","title":"range"},{"location":"vocabulary/unit2/#rebuttal","text":"an opposing argument or debate","title":"rebuttal"},{"location":"vocabulary/unit2/#representation","text":"the form in which data is stored, processed, and transmitted","title":"representation"},{"location":"vocabulary/unit2/#sample-proportion","text":"the fraction of samples which were successes","title":"sample proportion"},{"location":"vocabulary/unit2/#shuffle","text":"rearrange so as to occupy different random positions or to be in a different random order","title":"shuffle"},{"location":"vocabulary/unit2/#simulation","text":"a way of creating random events that are close to real-life situations without actually doing them","title":"simulation"},{"location":"vocabulary/unit2/#standard-deviation-sd","text":"a measure of how spread out numbers are; the square root of the variance","title":"standard deviation (SD)"},{"location":"vocabulary/unit2/#standardized-score","text":"another name for z-score","title":"standardized score"},{"location":"vocabulary/unit2/#subset","text":"a set of which all the variables are contained in another set.","title":"subset"},{"location":"vocabulary/unit2/#third-quartile-q3","text":"a number for which 75% of the data is less than that number","title":"third quartile (Q3)"},{"location":"vocabulary/unit2/#typical","text":"\u201cmean\u201d or \u201caverage\u201d; expected values","title":"typical"},{"location":"vocabulary/unit2/#with-replacement","text":"when a population element can be selected more than one time","title":"with replacement"},{"location":"vocabulary/unit2/#without-replacement","text":"when a population element can be selected only one time","title":"without replacement"},{"location":"vocabulary/unit2/#z-score","text":"tells us how many standard deviations away from the mean an observation is","title":"z-score"},{"location":"vocabulary/unit3/","text":"algorithm a process or set of rules that are followed anecdote stories that someone tells about his/her own experience or the experience of someone he/she knows associated joined together, often in a working relationship bootstrapping where we take random samples of really large samples cause a reason for an action or condition closed-ended questions give a fixed set of choices confidence interval an estimated range of values which is likely to include an unknown population parameter, the estimated range being calculated from a given set of sample data confounding factors an \u201cextra\u201d variable that you didn\u2019t account for control group the group that does not receive a treatment cost limitations the limitation of funds or money data information, or observations, that have been gathered and recorded data farm a physical space where high capacity servers are placed to store large amounts of data ethics a system of moral principles experiment one method of data collection; something that can be repeated that has a set of possible results feasibility how easy or difficult it is to do something HTML (Hyper Text Markup Language) a standardized system for tagging text files to achieve font, color, graphic, and hyperlink effects on web pages inferences the process of drawing conclusions about an underlying population based on a sample or subset of the data interval a data type which is measured along a scale, in which each point is placed at equal distance from one another margin of error tells you how many percentage points your results will differ from the real population value observational study a data collection method in which subjects are observed and outcomes are recorded open-ended questions offer a free-response/text approach outcome the variable that the treatment is meant to influence; this is sometimes known as the response, or dependent, variable over-represented represented excessively especially; having representatives in a proportion higher than the average parameter any number that summarizes a population Participatory Sensing an approach to data collection and interpretation in which individuals, acting alone or in groups, use their personal mobile devices and web services to systematically explore interesting aspects of their worlds ranging from health to culture population consists of all of the people we want to learn something about random assignment subjects are randomly assigned to either the treatment or control group random sample a sample that is chosen randomly random sampling a sample that is chosen randomly representative sample a subset of a population that seeks to accurately reflect the characteristics of the larger group research question the question to be answered by the experiment sample people (or objects) that are selected from the population sampling bias occurs when the resulting samples tend to produce results that are influenced in one particular direction self-reported when participants answer questions themselves sensor a converter that measures a physical quantity and converts it into a signal, which can be read by an observer or by an instrument statistic a term used for numbers that summarize a sample subjects people or objects that are participating in the experiment survey an investigation about the characteristics of a given population by means of collecting data from a sample of that population and estimating their characteristics through the systematic use of statistical methodology survey sample people who are asked to participate in a survey tags the variable names are stored at the beginning of the code, in between <th> and </th> theory an idea used to explain a situation treatment the variable that is deliberately manipulated to investigate its influence on the outcome; this is sometimes known as the explanatory, or independent, variable treatment group the group of subjects that receive the treatment trigger something that responds to an event so that an action can occur under-represented a subset of a population that holds a smaller percentage within a significant subgroup than the subset holds in the general population XML (Extensible Markup Language) a popular format for storing data on the internet; it creates readable web pages, and also because it allows programmers to easily update values in the data table if those values change","title":"Unit 3 Vocabulary"},{"location":"vocabulary/unit3/#algorithm","text":"a process or set of rules that are followed","title":"algorithm"},{"location":"vocabulary/unit3/#anecdote","text":"stories that someone tells about his/her own experience or the experience of someone he/she knows","title":"anecdote"},{"location":"vocabulary/unit3/#associated","text":"joined together, often in a working relationship","title":"associated"},{"location":"vocabulary/unit3/#bootstrapping","text":"where we take random samples of really large samples","title":"bootstrapping"},{"location":"vocabulary/unit3/#cause","text":"a reason for an action or condition","title":"cause"},{"location":"vocabulary/unit3/#closed-ended-questions","text":"give a fixed set of choices","title":"closed-ended questions"},{"location":"vocabulary/unit3/#confidence-interval","text":"an estimated range of values which is likely to include an unknown population parameter, the estimated range being calculated from a given set of sample data","title":"confidence interval"},{"location":"vocabulary/unit3/#confounding-factors","text":"an \u201cextra\u201d variable that you didn\u2019t account for","title":"confounding factors"},{"location":"vocabulary/unit3/#control-group","text":"the group that does not receive a treatment","title":"control group"},{"location":"vocabulary/unit3/#cost-limitations","text":"the limitation of funds or money","title":"cost limitations"},{"location":"vocabulary/unit3/#data","text":"information, or observations, that have been gathered and recorded","title":"data"},{"location":"vocabulary/unit3/#data-farm","text":"a physical space where high capacity servers are placed to store large amounts of data","title":"data farm"},{"location":"vocabulary/unit3/#ethics","text":"a system of moral principles","title":"ethics"},{"location":"vocabulary/unit3/#experiment","text":"one method of data collection; something that can be repeated that has a set of possible results","title":"experiment"},{"location":"vocabulary/unit3/#feasibility","text":"how easy or difficult it is to do something","title":"feasibility"},{"location":"vocabulary/unit3/#html-hyper-text-markup-language","text":"a standardized system for tagging text files to achieve font, color, graphic, and hyperlink effects on web pages","title":"HTML (Hyper Text Markup Language)"},{"location":"vocabulary/unit3/#inferences","text":"the process of drawing conclusions about an underlying population based on a sample or subset of the data","title":"inferences"},{"location":"vocabulary/unit3/#interval","text":"a data type which is measured along a scale, in which each point is placed at equal distance from one another","title":"interval"},{"location":"vocabulary/unit3/#margin-of-error","text":"tells you how many percentage points your results will differ from the real population value","title":"margin of error"},{"location":"vocabulary/unit3/#observational-study","text":"a data collection method in which subjects are observed and outcomes are recorded","title":"observational study"},{"location":"vocabulary/unit3/#open-ended-questions","text":"offer a free-response/text approach","title":"open-ended questions"},{"location":"vocabulary/unit3/#outcome","text":"the variable that the treatment is meant to influence; this is sometimes known as the response, or dependent, variable","title":"outcome"},{"location":"vocabulary/unit3/#over-represented","text":"represented excessively especially; having representatives in a proportion higher than the average","title":"over-represented"},{"location":"vocabulary/unit3/#parameter","text":"any number that summarizes a population","title":"parameter"},{"location":"vocabulary/unit3/#participatory-sensing","text":"an approach to data collection and interpretation in which individuals, acting alone or in groups, use their personal mobile devices and web services to systematically explore interesting aspects of their worlds ranging from health to culture","title":"Participatory Sensing"},{"location":"vocabulary/unit3/#population","text":"consists of all of the people we want to learn something about","title":"population"},{"location":"vocabulary/unit3/#random-assignment","text":"subjects are randomly assigned to either the treatment or control group","title":"random assignment"},{"location":"vocabulary/unit3/#random-sample","text":"a sample that is chosen randomly","title":"random sample"},{"location":"vocabulary/unit3/#random-sampling","text":"a sample that is chosen randomly","title":"random sampling"},{"location":"vocabulary/unit3/#representative-sample","text":"a subset of a population that seeks to accurately reflect the characteristics of the larger group","title":"representative sample"},{"location":"vocabulary/unit3/#research-question","text":"the question to be answered by the experiment","title":"research question"},{"location":"vocabulary/unit3/#sample","text":"people (or objects) that are selected from the population","title":"sample"},{"location":"vocabulary/unit3/#sampling-bias","text":"occurs when the resulting samples tend to produce results that are influenced in one particular direction","title":"sampling bias"},{"location":"vocabulary/unit3/#self-reported","text":"when participants answer questions themselves","title":"self-reported"},{"location":"vocabulary/unit3/#sensor","text":"a converter that measures a physical quantity and converts it into a signal, which can be read by an observer or by an instrument","title":"sensor"},{"location":"vocabulary/unit3/#statistic","text":"a term used for numbers that summarize a sample","title":"statistic"},{"location":"vocabulary/unit3/#subjects","text":"people or objects that are participating in the experiment","title":"subjects"},{"location":"vocabulary/unit3/#survey","text":"an investigation about the characteristics of a given population by means of collecting data from a sample of that population and estimating their characteristics through the systematic use of statistical methodology","title":"survey"},{"location":"vocabulary/unit3/#survey-sample","text":"people who are asked to participate in a survey","title":"survey sample"},{"location":"vocabulary/unit3/#tags","text":"the variable names are stored at the beginning of the code, in between <th> and </th>","title":"tags"},{"location":"vocabulary/unit3/#theory","text":"an idea used to explain a situation","title":"theory"},{"location":"vocabulary/unit3/#treatment","text":"the variable that is deliberately manipulated to investigate its influence on the outcome; this is sometimes known as the explanatory, or independent, variable","title":"treatment"},{"location":"vocabulary/unit3/#treatment-group","text":"the group of subjects that receive the treatment","title":"treatment group"},{"location":"vocabulary/unit3/#trigger","text":"something that responds to an event so that an action can occur","title":"trigger"},{"location":"vocabulary/unit3/#under-represented","text":"a subset of a population that holds a smaller percentage within a significant subgroup than the subset holds in the general population","title":"under-represented"},{"location":"vocabulary/unit3/#xml-extensible-markup-language","text":"a popular format for storing data on the internet; it creates readable web pages, and also because it allows programmers to easily update values in the data table if those values change","title":"XML (Extensible Markup Language)"},{"location":"vocabulary/unit4/","text":"census an official count or survey of a population, typically recording various details of individuals Classification and Regression Trees (CART) a predictive algorithm used in machine leanring; it explains how a target variable's values can be predicted based on other values classify is the problem of identifying which of a set of categories (sub-populations) an observation (or observations), belongs to cluster a group of similar things or people positioned or occurring closely together clustering is the process of grouping a set of objects (or people) in such a way that objects (or people) in the same group are more similar to each other than those in other groups correlation coefficient a statistical measure that calculates the strength of the relationship between the relative movements of two variables decision tree a decision support tool that uses a tree-like model of decisions and their possible consequences, including chance outcomes k-means aims to partition data into k clusters in a way that data points in the same cluster are similar and data points in the different clusters are farther apart linear used to describe a straight-line relationship between two variables line of best fit a line through a scatterplot of data points that best expresses the relationship between those points market refers to the live streaming of trade-related data; it encompasses a range of information such as price, bid/ask quotes and market volume mean absolute error the amount of error in your measurements; it is the difference between the measured value adn the \"true\" value mean squared error tells you how close a regression line is to a set of points; is determined by finding the average of the squared differences between your guess and the actual values misclassification rate the proportion of observations who were predicted to be in one category but were actually in another model provides a simplified version or representation of real-life situations or data. It is used to make sense of data or make predictions based on it. negative assocation when the values of one variable tend to decrease as the values of the other variable increase network a system designed to transfer data from one network access point to one other or more network access points via data switching, transmission lines, and system controls no association means that there is no line and all the dots are scattered nodes a point of intersection/connection within a data communication network non-linear a form of regression analysis in which observational data are modeled by a function which is a nonlinear combination of the model parameters and depends on one or more independent variables; the data are fitted by a method of successive approximations observed value the value that is actually observed (what actually happened) polynomial trends describes a pattern in data that is curved or breaks from a straight linear trend; it often occurs in a large set of data that contains many fluctuations positive association when the values of one variable tend to increase as the values of the other variable increase predicted value shows the projected equation of the line of best fit regression line a regression line is a line that best describes the behavior of a set of data residual the difference between our prediction and the actual outcome; also called an \"error\" rule a set way to calculate or solve a problem shape describes the distribution (or pattern) of the data within a dataset strength of association how much two variables covary and the extent to which the INDEPENDENT VARIABLE affects the DEPENDENT VARIABLE testing data !!! note \" a random subset consisting of about 15-25% of the original dataset on which a model is tested training data !!! note \" a random subset consisting of about 75-85% of the original dataset on which a model is trained trend often referred to as a line of best fit, is a line that is used to represent the behavior of a set of data to determine if there is a certain pattern","title":"Unit 4 Vocabulary"},{"location":"vocabulary/unit4/#census","text":"an official count or survey of a population, typically recording various details of individuals","title":"census"},{"location":"vocabulary/unit4/#classification-and-regression-trees-cart","text":"a predictive algorithm used in machine leanring; it explains how a target variable's values can be predicted based on other values","title":"Classification and Regression Trees (CART)"},{"location":"vocabulary/unit4/#classify","text":"is the problem of identifying which of a set of categories (sub-populations) an observation (or observations), belongs to","title":"classify"},{"location":"vocabulary/unit4/#cluster","text":"a group of similar things or people positioned or occurring closely together","title":"cluster"},{"location":"vocabulary/unit4/#clustering","text":"is the process of grouping a set of objects (or people) in such a way that objects (or people) in the same group are more similar to each other than those in other groups","title":"clustering"},{"location":"vocabulary/unit4/#correlation-coefficient","text":"a statistical measure that calculates the strength of the relationship between the relative movements of two variables","title":"correlation coefficient"},{"location":"vocabulary/unit4/#decision-tree","text":"a decision support tool that uses a tree-like model of decisions and their possible consequences, including chance outcomes","title":"decision tree"},{"location":"vocabulary/unit4/#k-means","text":"aims to partition data into k clusters in a way that data points in the same cluster are similar and data points in the different clusters are farther apart","title":"k-means"},{"location":"vocabulary/unit4/#linear","text":"used to describe a straight-line relationship between two variables","title":"linear"},{"location":"vocabulary/unit4/#line-of-best-fit","text":"a line through a scatterplot of data points that best expresses the relationship between those points","title":"line of best fit"},{"location":"vocabulary/unit4/#market","text":"refers to the live streaming of trade-related data; it encompasses a range of information such as price, bid/ask quotes and market volume","title":"market"},{"location":"vocabulary/unit4/#mean-absolute-error","text":"the amount of error in your measurements; it is the difference between the measured value adn the \"true\" value","title":"mean absolute error"},{"location":"vocabulary/unit4/#mean-squared-error","text":"tells you how close a regression line is to a set of points; is determined by finding the average of the squared differences between your guess and the actual values","title":"mean squared error"},{"location":"vocabulary/unit4/#misclassification-rate","text":"the proportion of observations who were predicted to be in one category but were actually in another","title":"misclassification rate"},{"location":"vocabulary/unit4/#model","text":"provides a simplified version or representation of real-life situations or data. It is used to make sense of data or make predictions based on it.","title":"model"},{"location":"vocabulary/unit4/#negative-assocation","text":"when the values of one variable tend to decrease as the values of the other variable increase","title":"negative assocation"},{"location":"vocabulary/unit4/#network","text":"a system designed to transfer data from one network access point to one other or more network access points via data switching, transmission lines, and system controls","title":"network"},{"location":"vocabulary/unit4/#no-association","text":"means that there is no line and all the dots are scattered","title":"no association"},{"location":"vocabulary/unit4/#nodes","text":"a point of intersection/connection within a data communication network","title":"nodes"},{"location":"vocabulary/unit4/#non-linear","text":"a form of regression analysis in which observational data are modeled by a function which is a nonlinear combination of the model parameters and depends on one or more independent variables; the data are fitted by a method of successive approximations","title":"non-linear"},{"location":"vocabulary/unit4/#observed-value","text":"the value that is actually observed (what actually happened)","title":"observed value"},{"location":"vocabulary/unit4/#polynomial-trends","text":"describes a pattern in data that is curved or breaks from a straight linear trend; it often occurs in a large set of data that contains many fluctuations","title":"polynomial trends"},{"location":"vocabulary/unit4/#positive-association","text":"when the values of one variable tend to increase as the values of the other variable increase","title":"positive association"},{"location":"vocabulary/unit4/#predicted-value","text":"shows the projected equation of the line of best fit","title":"predicted value"},{"location":"vocabulary/unit4/#regression-line","text":"a regression line is a line that best describes the behavior of a set of data","title":"regression line"},{"location":"vocabulary/unit4/#residual","text":"the difference between our prediction and the actual outcome; also called an \"error\"","title":"residual"},{"location":"vocabulary/unit4/#rule","text":"a set way to calculate or solve a problem","title":"rule"},{"location":"vocabulary/unit4/#shape","text":"describes the distribution (or pattern) of the data within a dataset","title":"shape"},{"location":"vocabulary/unit4/#strength-of-association","text":"how much two variables covary and the extent to which the INDEPENDENT VARIABLE affects the DEPENDENT VARIABLE","title":"strength of association"},{"location":"vocabulary/unit4/#testing-data","text":"!!! note \" a random subset consisting of about 15-25% of the original dataset on which a model is tested","title":"testing data"},{"location":"vocabulary/unit4/#training-data","text":"!!! note \" a random subset consisting of about 75-85% of the original dataset on which a model is trained","title":"training data"},{"location":"vocabulary/unit4/#trend","text":"often referred to as a line of best fit, is a line that is used to represent the behavior of a set of data to determine if there is a certain pattern","title":"trend"}]}